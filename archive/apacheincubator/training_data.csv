IS,Identity mapping symbol in mxnet,Just want to dump some inner result in the computation graph For example net mx sym Variable name 'X' net mx sym Convolution data net name 'conv' kernel 7 7 num filter 32 net mx sym Identity net,,,2016-08-23 09:37:37,2016-08-23 09:48:15
IS,Is there a simple way to make two similar networks share same weights,Hi i have some doubts about sharing weights Here is my naive way to define siamese network Any example code or hits will be appreciated thanks,,piiswrong,2016-08-23 04:56:26,2016-08-23 13:50:16
IS,batch norm cannot support unrolled rnn lstm,For unrolled rnn lstm we unroll the input sequence When i tried to use batch norm for input data after embed layer i fount it impossible for all characters in a sequnce with just one batch norm first batch norm calc mean and var in forward function every invoking just store in output and used in backward function but in unrolled lstm for one sequence with length m it may invoke forward function m times and we only get the last invoke mean and var which store in output so we loss some mean and var second moving mean and moving var are two ndarrays which we can not assign when the batch norm operator defined the two ndarrays are defined when the batch norm op created which is not support copy for sequence with m length in rnn lstm there may m times aux states for batch norm Is there any good way to use batch norm in unrolled rnn lstm,,piiswrong,2016-08-23 03:39:38,2016-08-23 15:00:21
IS,Is the iterator asychronous,I can not figure out if the standard mx io ndarrayiter will load another batch before it is required by e g the GPU Also I see there is an mx io csviter is that IO asynchronous and will get the data from disk whilst the previous batch is running,,mli,2016-08-23 07:56:03,2016-08-23 21:16:39
IS,Manipulating a network that is only giving by a symbol json,If one loads a pre defined network like Inception v3 from the MXNet model zoo you do not normally have the code that generated the graph Now I would like to take parts of the network and use it for my own purposes Do we have currently tools to do that besides manipulating the json file directly If not that should maybe be on the road map The only reference to this that I could find is this issuecomment 231419750 Could you maybe elaborate or did you actually mean recreating the model definition or manipulating the symbol graph after the fact,,"vchuravy,tqchen,vchuravy",2016-08-24 02:55:18,2016-08-24 04:34:48
IS,Gradient reversal layer without custom operator,I'm looking to implement unsupervised transfer learning architectures based on gradient reversal The idea is outlined nicely in this graphic from the paper img width 565 alt screen shot 2016 08 23 at 9 54 08 am src Any ideas on how to implement this approach without defining a custom gradient reversal operator layer I would prefer to use the Scala Package which does not have support for custom layers yet,,piiswrong,2016-08-23 14:07:36,2016-08-24 17:08:42
IS,mxnet python very slow compared to Torch,I have been trying to replicate the crepe model in mxnet And I think my model set up is correct this should run if you wanted to replicate However even after trying out various methods cpu 1 gpu 4 gpus mx mod Module mx simple bind mx FeedForward etc the best speed I can get is 10 hours per epoch of around 2 5 million words This means to do 50 000 epochs it would take me 57 years I think it took Zhang a week When I did the experiments there was no CuDNN no Titan X and the experiment for dbpedia takes about a week for 10 eras each era being 5000 epoches And others have mentioned For the record when I train the network on a Titan X Maxwell using CuDNN library and Torch in DIGITS on DBPedia dataset it takes 15 minutes per epoch 498400 training samples 56000 validation samples with samples truncated to max 1024 characters I made a post here I am not sure if I am doing something wrong perhaps not using full power of mxnet or if mxnet is just much slower than Torch and the module is complicated,,"mli,mli",2016-08-23 21:21:49,2016-08-24 23:59:13
IS,why simple batch return dataname and data shape,I analysed io util py in speech demo and warp ctc demo showing the the iterators return simplebatch and in simplebatch class only init provide data and provide label method I think provide data method return data when called Is it right but why it returns data name and data shape the code like follwing,,piiswrong,2016-08-25 01:33:47,2016-08-25 02:21:48
IS,warp ctc can not work on gtx1080,error test gpu,,"piiswrong,piiswrong",2016-08-25 03:51:03,2016-08-25 08:09:30
IS,How to free GPU momory of DataIter,I am trying to prefetch data when training the DataIter python code is somehow like this When training there are two processes on GPU and when I terminate the training process with Ctrl C only one process is terminated which occupied more memory The left process always occupied about 102M memory,,piiswrong,2016-08-25 09:43:14,2016-08-26 02:39:59
IS,Does mxnet has the ready made spp operation,I want to construct a network which need the spatial pyramid pooling spp spatial pyramid pooling in deep convolutional network for visual recognition operation Does mxnet has the ready made spp operation,,,2016-08-26 04:10:46,2016-08-26 06:33:48
IS,TBlob get with shape error when backforward in complicated netwok,I want to add regularization to my network but failed The following is my graph of computation bi rnn def bi lstmUnroll reg numLstmLayer Int seqLen Int inputSize Int numHidden Int numEmbed Int numLabel Int dropout Float 0f Symbol val embedWeight Symbol Variable embed weight val clsWeight Symbol Variable cls weight val clsBias Symbol Variable cls bias Any reply is appreciated Thank you,,,2016-08-25 14:58:45,2016-08-26 07:31:01
IS,Multi task learning network structure design,Hi everyone I'm trying to do text classification using CNN Here is some of my label dog cat bird football basketball As these classes are somehow too fine grained to have good precision plus the relatively small amount of training data I group them into animal sports Then I design a simple multi task learning structure as below but it does not improve final performance on my fine grained label So do you guys have any idea or experience in design such network Any ideas are welcomed thank you,,"zihaolucky,zihaolucky",2016-08-25 13:23:12,2016-08-27 09:00:08
IS,Fix python3 compatibility for examples,Currently most python examples are based on python 2 only Can we try to fix compatibility with python3 for them and for future examples,,wangg12,2016-08-24 15:55:24,2016-08-29 16:22:34
IS,There is no source in source jar for scala package 0 1 1,javelinjs Hi I use the mxnet full 2 10 linux x86 64 cpu artifact but after I download the relative source jar on maven repo I did not see any source code in jar Can you check the jar file Or do I make some mistake Because if there is no source file it is hard for me to debug Thanks,,yzhliu,2016-08-30 10:29:43,2016-08-30 12:50:47
IS,How to load mean img using C API,I test the cpp image classification and the there is no load mean image stage but setting a default value float mean 117 0,,"thirdwing,thirdwing,thirdwing",2016-08-26 10:57:08,2016-08-30 14:25:52
IS,Some caffe op like augmentation can not do backward will cause error,some error F0830 18 43 16 734480 558 downsample layer cu 135 DownsamplingLayer cannot do backward How to deal with this problem,,"dsqx71,piiswrong",2016-08-30 10:56:05,2016-08-31 07:27:25
IS,An fatal error when using Scala pkg,Hi Background I trained the model in Python and uploaded its param and json file which is to be used in Scala I also change something in json to run the code below Seems the reshape operation,,"zihaolucky,zihaolucky,zihaolucky",2016-08-31 07:06:16,2016-08-31 08:09:55
IS,Weird design for broadcast to Op,sxjscience Why does broadcast to force the user to specify trailing 1 is to make the input dimension the same rank as the broadcast shape rank and thus forcing users to use an extra reshape layer For example the Numpy example does not even work with broadcast to it throws when inferring the output shape,,"sbodenstein,sbodenstein",2016-08-31 11:25:49,2016-08-31 12:02:16
IS,Where is the documentation now,Hi all Is there a link that I can look up all the functions It used to have one but now I can not find it anywhere seems like that the links is not very stable and always disappear,,vchuravy,2016-08-30 10:23:16,2016-09-01 00:46:18
IS,Errror Happen when use Batch Normal Layer,when i use batch norm layer in network it fail when the first epoch end the error message list below How can I fixed it code like below data mx symbol Convolution data data kernel self kernel size num filter self feature count name self name pad self pad data mx symbol BatchNorm data data name self name batch normal report errors checkpoint to output data normal deepyannet data normal deepyannet 0001 params 10 44 59 home meter Desktop mxnet dmlc core include dmlc logging h 235 10 44 59 src operator cudnn batch norm inl h 138 Check failed cudnnBatchNormalizationForwardInference s dnn handle CUDNN BATCHNORM SPATIAL a b io desc x dptr io desc y dptr mean desc gamma dptr beta dptr moving mean dptr moving inv var dptr param eps CUDNN STATUS SUCCESS 10 44 59 home meter Desktop mxnet dmlc core include dmlc logging h 235 10 44 59 src engine threaded engine h 306 10 44 59 src operator cudnn batch norm inl h 138 Check failed cudnnBatchNormalizationForwardInference s dnn handle CUDNN BATCHNORM SPATIAL a b io desc x dptr io desc y dptr mean desc gamma dptr beta dptr moving mean dptr moving inv var dptr param eps CUDNN STATUS SUCCESS An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 10 44 59 src engine threaded engine h 306 10 44 59 src operator cudnn batch norm inl h 138 Check failed cudnnBatchNormalizationForwardInference s dnn handle CUDNN BATCHNORM SPATIAL a b io desc x dptr io desc y dptr mean desc gamma dptr beta dptr moving mean dptr moving inv var dptr param eps CUDNN STATUS SUCCESS An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,piiswrong,2016-08-31 02:49:48,2016-09-01 01:13:02
IS,R Results are not reproducible,Hello when using mx mlp in R or even running basic model R the fitted neural networks obtained are not reproducible The seed is set however accuracies change each time the model is run without any change in parameters Is there a fix for that,,"thirdwing,thirdwing,thirdwing,thirdwing",2016-08-31 20:04:40,2016-09-01 06:27:26
IS,Any convinient way to judge whether it is first layer convolution in backward function of operators,In the backward function of operators if the layer is first convolution layer of CNN model there is no need to calculate the gradient of data as the input data is the input image data So is there any convinient way to judge whether it is first convolution layer or not Or can you add some information to indicate that there is no need to do backward data just like caffe do,,piiswrong,2016-08-29 02:31:54,2016-09-01 08:24:28
IS,How to explain the use of symbol,what is the difference of sym gen buckets 0 and sym gen,,sxjscience,2016-09-02 06:56:04,2016-09-02 09:36:40
IS,vote for the new logo,,,mli,2016-09-02 18:27:55,2016-09-02 18:32:02
IS,ImportError cannot import name base how to solve it,when I run python example image classification train mnist py An error occurred how to solve it,,tornadomeet,2016-09-02 07:36:05,2016-09-03 07:58:26
IS,How do you correctly shape arrays for NDArrayIter in Python,I'm trying to format my own data for use with NDArrayIter and I'm currently having a few issues figuring out how to correctly do this I have currently loaded in my data into numpy arrays via opencv2 and processed them as I need to before creating the NDArrayIter I have placed them in a standard python list for the moment with each exemplar having the shape 85 200 3 I think this is where the real problem is but I will continue to describe the rest of what I have done I have created a numpy array with all the labels from 0 to 8 It is shape is 7878 the number of exemplars I realise this is a bit inconsistent at the moment but I'm still in early testing on how to use the library Now when I create the NDArrayIter I now get the following error Traceback most recent call last File mxnet test py line 114 in module test iter mxnet io NDArrayIter exemplars test labels test 86 File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet io py line 371 in init batch size need to be smaller than data size AssertionError batch size need to be smaller than data size In my testing of why this was happening I realised that mxnet must be thinking that the height of an image is the number of data items as the height is 85 and errors on 86 So my points questions are as follows I presume my primary issue here is how I'm providing the exemplars to NDArrayIter In the code comments of io py it says it takes a list of them so I presumed that it meant a standard python list of the exemplars however it appears that I was wrong How might I shape this correctly Are the labels in this case specified correctly Could I just provide them in a standard python list or would it have to be in some form of numpy mxnet array Thanks,,piiswrong,2016-09-02 15:30:49,2016-09-03 15:40:03
IS,OOM when allocating too fast,I have a tight loop that allocates and deallocates arrays immediately and sleeps between iterations Code shown below Each array is around 600 MiB Program runs smoothly I can sense that every second allocation uses the space freed by previous deallocation since arrays gets to ref count 0 and could be freed immediately But if I remove that time sleep line and let it run it throws OOM exception I guess it is because allocations went before deallocations of previous arrays So it is allocating instead of reusing every iteration I wonder if there is a way to strictly specify the order of allocation and deallocation so it would behave exactly in function call order,,"hotpxl,tqchen,piiswrong,tqchen,piiswrong,piiswrong,piiswrong,hotpxl,tqchen",2016-08-30 14:28:23,2016-09-03 15:40:32
IS,Does the motherboard need the SLI technology to support multiple gpus in mxnet,My motherboard is Intel B85 which does not support SLI technology and has two PCIE interfaces and I have two gtx1080 gpu cards to construct my mxnet system But I found the utilizations of two gtx1080 are very slow the speed of two gtx1080 is slower than single gtx1080 Although I increased the batch sizes the speed of two gpus is still lower than one gpu How can I solve this problem,,piiswrong,2016-09-03 09:29:36,2016-09-03 15:40:48
IS,R Reduce memory consumption when training with big matrix,I have a matrix that occupies 68 4 Gb When I feed that into the training function the size doubles and I get an out of memory Any idea how to reduce the size Or can I use any other training method to reduce memory consumption,,"miguelgfierro,miguelgfierro,miguelgfierro",2016-08-29 07:45:28,2016-09-05 19:30:53
IS,How to read flexible data from csv format file,I extracted features from wav files and saved as csv format each frame has 120 dimensional feature each line saved a whole wav is features it is size is frame num 120 that is maybe first line in the csv is 120 1000 dimensions the second line maybe has 120 1000 dimensions and the total size of the features is 52G I used csv iterator to read the file Please give me some suggestion on how should I read the csv file thx,,miguelgfierro,2016-08-29 09:27:07,2016-09-06 01:22:09
IS,data preparation for multi machine training,I have a training date set in RecordIO format Now I want to train a model using this dataset and multi machine I watch the multi machine training tutorial and it seems that what I need to do is just scp train rec to the worker machines right I just find how the excutor split training data for different gpu in single machine in executor manager py but how does it split data for different machines Thank you,,,2016-09-05 08:41:11,2016-09-06 03:34:38
IS,Not able to resume the test accuracy after loading the pretrain model,I am using the lstm for some classification tasks but met a strange problem For example after the last training step I tested the model on the test set the accuracy is 96 and I save the model Then I want to resume training I load the pretrain model and copy the pretrain parameters to the network And before training I test the pretrain model on the same test set but only got the 87 accuracy It is werid anyone helps,,"Ldpe2G,tornadomeet,Ldpe2G,tornadomeet,Ldpe2G,tornadomeet,Ldpe2G",2016-09-02 09:30:19,2016-09-06 05:30:07
IS,Why the internal ndarray py file is empty And how to solve this error,,,,2016-09-06 09:41:21,2016-09-06 10:23:34
IS,Reshape error when using multi cpu gpu while single cpu gpu is work,Hi I got a error message about Reshape Op when changing ctx mx cpu to ctx mx cpu i for i in range 12,,"zihaolucky,piiswrong,zihaolucky,zihaolucky,zihaolucky,zihaolucky,zihaolucky",2016-09-05 09:17:17,2016-09-06 12:44:50
IS,Performance of multi machine is worse than one machine,I used example image classification train cifar10 py on two computers which were directedly connected by wire The speed of net is about 100MB s Could anyone tell me how much net bandwidth do I need Should I use infiniband Sincere Thanks,,"tornadomeet,winstywang",2016-09-06 08:15:55,2016-09-07 13:04:28
IS,ImportError No module named captcha image,I compiled warp ctc and mxnet successful I add warp ctc in config mk and compiled mxnet but when I run lstm ocr py the problem disappear from captcha image import ImageCaptcha Traceback most recent call last File stdin line 1 in module ImportError No module named captcha image Did you have idea about this,,piiswrong,2016-09-02 09:03:07,2016-09-07 13:06:10
IS,Pkg test on v 0 6 succeeds but fails on 0 4 6,I have checked out the latest master in both Julia 0 4 6 and latest 0 6 Pkg test runs correctly in 0 6 but fails in 0 4 6 at,,"vchuravy,vchuravy,vchuravy",2016-09-07 13:36:58,2016-09-07 15:55:38
IS,fatal error unity lib image util hpp,I have installed mxnet and baidu warp cdc successfully then follow the document comment add SFRAME PATH HOME SFrame MXNET PLUGINS plugin frame plugin mk to config mk when I rebuild mxnet by make clean make j4 it occurs an error plugin frame iter sframe cc 16 36 fatal error unity lib image util hpp No such file or directory include unity lib image util hpp how can I correct this problem,,piiswrong,2016-09-08 11:06:03,2016-09-09 02:15:02
IS,What does group symbol treat with shared gradients ndarray,For example I have c mx sym group a b where a and b are two output symbols And they share some internal symbols What happened when I do a single backprop with exe binding c Are two grads from a and b are added for a shared ndarray when grad req 'write' Or I should explicitly bind the exe with grad req 'add',,"JeffDong,piiswrong",2016-09-08 06:03:33,2016-09-09 02:17:25
IS,pre built packages windows10 gpu not working properly,windows binary build 20160531 import mxnet as mx a mx nd ones 2 3 2 b mx nd ones 2 3 mx gpu 3 c a copyto mx gpu b print c asnumpy 0 0 0 0 0 0,,,2016-09-06 15:32:49,2016-09-09 03:49:58
IS,usage about the symbol RNN,I'm trying to use the built in symbol mx sym RNN I encountered a shape issue any help is appreciated Here is a part of my script It complains the shape is not consistent does that mean I have to reshape the label all the time MXNetError InferShape Error in softmax is label argument Corresponding keyword of symbol softmax label Shape inconsistent Provided 3 inferred shape 3 1,,,2016-09-06 08:01:09,2016-09-09 06:06:35
IS,can not run example image classifition train cifar10 py,When I do the command ' python train cifar10 py' it starts to download cifar zip and then shows error raise MXNetError py str LIB MXGetLastError mxnet base MXNetError InferShape Error in concat2 12 06 24 src operator concat inl h 152 Check failed dshape j tmp j Incorrect shape 1 128 80 13 13 first input shape 128 160 14 14 How can I solve this problem,,"pluskid,antinucleon,pluskid,pluskid,pluskid",2016-08-29 04:09:41,2016-09-09 15:00:58
IS,Python Computer gets stuck when import mxnet,My language is Python The system is Ubuntu 16 04 I installed mxnet following the installation guide I think I successfully installed mxnet to my laptop since I could import it and run my code yesterday But today when I try to import mxnet the laptop would get stuck without reporting any problem it seems that there is an infinite loop in it Rebooting does not work Like this image,,,2016-09-09 13:43:42,2016-09-09 19:20:23
IS,Convolution Converting RGB Image from OpenCV Shape,I'm trying to shape my data correctly to pass to a convolution layer but I keep getting the following error 14 50 27 src operator convolution inl h 370 Check failed ksize y dshape 2 2 param pad 0 ksize x dshape 3 2 param pad 1 kernel size exceed input The size of the training set is 7878 256 256 3 The images have been loaded in via cv2 and had some operations prior to being added to the training set array The convolution layer is as follows conv1 mx symbol Convolution data input data kernel 11 11 stride 4 4 num filter 96 Does the convolution layer have a different ordering of dimensions to the cv2 ordering,,,2016-09-06 14:01:40,2016-09-12 11:29:34
IS,Cross build the scala package to scala 2 11,,,"yzhliu,yzhliu,yzhliu",2016-09-09 15:25:31,2016-09-14 05:42:26
IS,compile amalgamation for android,I try to compile amalgamation for android after setup NDK and toolchain But I come to an error as follows rm linux androideabi g std c 11 Wno unknown pragmas Wall I usr local Cellar openblas 0 2 18 2 include L usr local Cellar openblas 0 2 18 2 lib L usr local Cellar opencv 2 4 13 lib mhard float D NDK MATH NO SOFTFP 1 O3 fPIC o jni libmxnet predict o c jni predictor cc make arm linux androideabi g No such file or directory make jni libmxnet predict o Error 1,,,2016-09-13 01:59:36,2016-09-14 06:06:32
IS,self defined operator if immigrate to Android platform,I have successfully compiled the amalgamation for android But when I train the model on PC I use the self defined operator I am wondering whether the compiled libmxnet predict so can support this self defined operator or not after immigrating to Android platform,,tqchen,2016-09-14 06:06:15,2016-09-14 17:39:37
IS,make error when compile with CUDA,Hello when i make with CUDA 0 it everything ok then make CUDA 1 some error about make usr lib gcc x86 64 linux gnu 5 include mwaitxintrin h 42 error identifier builtin ia32 mwaitx is undefined usr lib gcc x86 64 linux gnu 5 include mwaitxintrin h 36 error identifier builtin ia32 monitorx is undefined usr lib gcc x86 64 linux gnu 5 include mwaitxintrin h 42 error identifier builtin ia32 mwaitx is undefined it seems gcc5 do not match cuda but i already make caffe with success what can i do Thx my PC Ubuntu CUDA7 5 cudnn 5 1 3 gcc5 4 gtx960,,tqchen,2016-09-14 07:35:35,2016-09-14 17:43:39
IS,NNVM Branch Windows CMake,We are having a progressive refactor on NNVM branch Currently we have upgraded the make system to build this branch However CMake is not yet updated it would be great if some one can take over and make the cmake system work in particular for windows,,"tqchen,yajiedesign,yajiedesign,tqchen,tqchen,yajiedesign,tqchen,yajiedesign,tqchen",2016-09-07 23:45:48,2016-09-14 18:05:46
IS,setup py error,keithyin keithyin PC Downloads mxnet1 python sudo python setup py install Traceback most recent call last File setup py line 14 in module LIB PATH libinfo 'find lib path' File mxnet libinfo py line 36 in find lib path 'List of candidates n' str ' n' join dll path RuntimeError Cannot find the files here is my error information what should i do correct it,,tqchen,2016-09-14 11:09:03,2016-09-15 00:45:14
IS,Is there any method to save weights after N epoch not a issue,The size of the weights file for my custom CNN is about 1GB So after about 300 epoch the hard disk will be full and the training process will be killed I 'm wondering if there is a method that could save the weights file after N epoch not every epoch Thanks,,,2016-09-16 01:35:48,2016-09-18 03:03:20
IS,Why out shape is 100 x 10 not 10 x 100,,,sxjscience,2016-09-18 06:04:29,2016-09-18 07:42:50
IS,Bucket iterator how to set bucket key and default bucket key,I write an iterator using bucket and I want to transform seqlen and label len I set bucket key and default bucket key as follows how could I do if I want to sed the key to combination of seq len and label len,,,2016-09-18 08:05:28,2016-09-18 08:19:54
IS,No synset txt and preprocess description in pretrained resnet networks,I notice that there are two pretrained resnet networks in And I would like to use them to try some experitment However there is no synset txt and preprocess description or function in the directories as the one in pretrained inception models Would you please provide the necessary information,,tornadomeet,2016-09-05 15:14:39,2016-09-18 08:57:22
IS,some about BatchNorm,Recently I do some job in converting torch model to mxnet model and i stucked in mxnet is BatchNorm layer In torch model my nn SpatialBatchNormalization doing batchnormalization in a manner like use global stats False in caffe But in mxnet I found I can not do the same because predict do is train False twice which leads BatchNorm layer always doing moving average and moving var other than doing current minibatch mean and var here is my test code code start import find mxnet import mxnet as mx import numpy as np def get Symbol bn momentum 0 1 input data mx symbol Variable name data bn1 mx symbol BatchNorm name 'bn1' data input data fix gamma False momentum bn momentum eps 1e 5 use global stats False return bn1 def creat MxnetModel mxnetModelPrefix net part get Symbol input dim 1 5 4 4 part shapes output shapes aux shapes net part infer shape data tuple input dim code end is there another method which can replace predict function and set is train True to do forward,,piiswrong,2016-09-18 13:16:36,2016-09-19 01:18:29
IS,Fail to make j4 on Centos all dependencies are installed already,config mk,,tornadomeet,2016-09-19 04:43:25,2016-09-19 04:59:08
IS,usr bin ld cannot find lopencv contrib,centos 7 1 openCV3 1 0,,"phunterlau,phunterlau",2016-09-21 11:38:04,2016-09-22 05:12:17
IS,the Accuracy of lstm ocr py is always around 0 01,i tried to run lstm ocr py but the Accuracy is always around 0 01 Does anyone know how to increase the Accuracy Thanks a lot,,,2016-09-22 03:44:00,2016-09-23 03:26:24
IS,Install Failure in Macbook Pro nvcc fatal redefinition of argument istd',Hi I was trying to install MXNet in my Macbook Pro OS X 10 11 6 GT 750M with GPU support but the same error occurs in several trials I guess there might be something wrong within CMakeList txt but I'm not familiar with it so I do not know the exact reason Could you please take a look into this BTW this problem occurs no matter how I set USE CUDA USE CUDNN and USE NVRTC,,"piiswrong,piiswrong,yajiedesign",2016-09-24 05:15:46,2016-09-24 16:48:04
IS,how to extract the features of the entire image dataset using model predict,I want to extract the features of all the images in the entire image dataset And I use the following codes import mxnet as mx import numpy as np import logging import argparse import os parser argparse ArgumentParser description 'train an image classifer on imagenet' parser add argument ' data dir' type str default ' media ResearchProject deeplearning code mxnet experiments data caltech101 ' help 'the input data directory' parser add argument ' batch size' type int default 1 help 'the batch size' parser add argument ' gpus' type str default '0' help 'the gpus will be used e g 0 1 2 3 ' parser add argument ' kv store' type str default wouldevice' help 'the kvstore type' parser add argument ' num examples' type int default 3657 help 'the number of training examples' parser add argument ' num classes' type int default 102 help 'the number of classes' parser add argument ' train dataset' type str default train rec help 'train dataset name' parser add argument ' val dataset' type str default test rec help validation dataset name parser add argument ' data shape' type int default 256 help iset image is shape' args parser parse args def get iterator args kv data shape 3 args data shape args data shape kv mx kvstore create args kv store test get iterator args kv devs mx cpu if args gpus is None else mx gpu int i for i in args gpus split ' ' prefix 'models vgg16' numround 1 model mx model FeedForward load prefix prefix epoch numround prob model predict X test But some errors happen usr bin python2 7 media ResearchProject deeplearning code mxnet experiments model predict py 15 59 59 src io iter image recordio cc 209 ImageRecordIOParser media ResearchProject deeplearning code mxnet experiments data caltech101 test rec use 3 threads for decoding 15 59 59 src io iter normalize h 103 Load mean image from media ResearchProject deeplearning code mxnet experiments data caltech101 mean bin 16 00 00 media ResearchProject deeplearning software mxnet dmlc core include dmlc logging h 235 16 00 00 src ndarray ndarray cc 291 Check failed from shape to shape operands shape mismatch Traceback most recent call last File media ResearchProject deeplearning code mxnet experiments model predict py line 56 in module prob model predict X test File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 597 in predict self init predictor data shapes File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 529 in init predictor pred exec copy params from self arg params self aux params File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor py line 235 in copy params from array astype dst dtype copyto dst File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 499 in copyto return internal copyto self out other File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 1174 in unary ndarray function c array ctypes c char p c str str i for i in kwargs values File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 16 00 00 src ndarray ndarray cc 291 Check failed from shape to shape operands shape mismatch Process finished with exit code 1 All the test images of the entire image dataset are converted to test rec How can I solve this problem or how I can find the correct way to extract the features of all the images in an entire image dataset,,,2016-09-23 08:03:46,2016-09-25 04:08:22
IS,Bug LRN layer is broken for CPU,The LRN layer is broken for CPU but not GPU The following crashes,,"sbodenstein,sbodenstein,piiswrong,sbodenstein",2016-09-23 15:14:53,2016-09-26 00:39:57
IS,Link error when compliling with intel mkl,Got a link error when compile with mkl on ubuntu 14 04 usr bin ld cannot find liomp5 My config mk file is as follows I tried source opt intel bin compilervars sh and adding opt intel lib intel64 lin where the file libiomp5 so located to LD LIBRARY PATH but the error still exists,,"JeffDong,miguelgfierro,JeffDong",2016-09-25 14:53:25,2016-09-26 03:06:46
IS,Cannot run the example of training cifar10 with alexnet,Hi I am trying examples provided to get to know MXNet but I failed to launch the example of cifar10 under mxnet example image classification with command python train cifar10 py network alexnet gpus 0 Error mxnet base MXNetError InferShape Error in pooling1 11 44 21 src operator pooling inl h 200 Check failed param kernel 0 dshape 2 2 param pad 0 param kernel 1 dshape 3 2 param pad 1 kernel size exceed input What does that mean and how to fix it Thanks,,piiswrong,2016-09-22 04:03:02,2016-09-26 06:14:08
IS,Correct Speedometer Callback Usage,I tried to follow the example here on how to use the callback Speedometer But when I used it it did not produce anything throughout the entire training I'm using my own images approx 7000 training exemplars of 3 256 256 I used an epoch callback and that saved things as I expected every 10 epochs import mxnet import symbol alexnet train iter mxnet io NDArrayIter exemplars train labels train 100 test iter mxnet io NDArrayIter exemplars test labels test 100 alexnet symbol alexnet get symbol len class list epochs 80 model mxnet model FeedForward ctx mxnet gpu 0 symbol alexnet num epoch epochs learning rate 0 1 momentum 0 9 model fit X train iter eval data test iter epoch end callback mxnet callback do checkpoint chks train 10 batch end callback mxnet callback Speedometer 100 10 Could anyone tell me what I got wrong I presume it has to do with the batch size but I'm a little uncertain on it is exact meaning,,piiswrong,2016-09-12 14:18:51,2016-09-26 14:37:59
IS,Use symbol infer shape return None None None LSTM example,I am working on benchmark for DNN tools For fair comparison I need to check the network shapes among these tools And MXNet perform excellent speed except in RNN my benchmark result so I want to double check about this I searched for the solutions and it says I should use the symbol infer shape However when i use it in LSTM examples it returns None None None Here is my source code clone from mxnet is example Thanks,,,2016-09-24 09:28:27,2016-09-27 12:11:20
IS,Compiling error tensor blob h 36 error namespace std has no member is pod,Whiling compiling MxNet I got the following errors I am using Ubuntu 14 04 with gcc 4 8 4 CUDA 7 5 and cuDNN 5 0 5 It is like the compiler is complaining C 11 errors I am checking out the current latest version of MxNet at d67964ca1358b624d3407a7d6fbb50573b19d1b0 Anyone else has the same problem,,piiswrong,2016-09-10 05:29:44,2016-09-29 01:01:26
IS,No set gpu data when installing caffe plugin,I tried to install mxnet with caffe However I got the following errors plugin caffe caffe blob cc 40 14 error class caffe Blob float has no member named set gpu data blob set gpu data data ptr plugin caffe caffe blob cc 42 14 error class caffe Blob float has no member named set gpu diff blob set gpu diff data ptr I check the caffe code and I actually do not find these two functions in Blob class Are you doing something to fix it,,"piiswrong,HrWangChengdu,piiswrong",2016-09-28 21:02:12,2016-09-29 06:03:05
IS,Questions about,,,,2016-09-29 07:38:39,2016-09-29 07:43:01
IS,Questions about arg max operator,MxNet provides arg max operator in file broadcast reduce op inl h but backward function is not implemented How to use this operator and is there exits a option to turn off the backward pass in an operator like caffe,,,2016-09-29 07:43:39,2016-09-29 08:56:56
IS,data dmlc ml is down causing the example script example image classification train mnist py to fail,,,"anirudh2290,mli,anirudh2290,mli,mli,anirudh2290",2016-09-29 00:51:56,2016-09-29 22:34:12
IS,404 not found in downloading mnist zip in the example,The url for mnist zip file is changed The quick test example in the installation page python package installation can not find the file This issue breaks the quick test,,"philipskokoh,mli,philipskokoh",2016-09-29 11:11:56,2016-09-29 23:38:51
IS,Soft target in classification,Does mxnet support soft target in classification tasks i e for each sample instead of a label specifying a single class I have a target distribution over all classes The network is then trained to minimize some sort of loss function e g cross entropy loss between model prediction and the target,,piiswrong,2016-09-20 02:13:54,2016-09-30 01:56:51
IS,Any way to get classifier score before normalization,Hi everyone I trained a k class softmax classifier with SoftmaxOutupt as output layer and handle 'unknown' class by simply using thresholding But SoftmaxOutput already normalized its log probability for users in which thresholding is not easy to achieve I'm wondering whether I can get origin score before normalization,,"zihaolucky,pluskid,zihaolucky",2016-09-28 02:54:46,2016-09-30 13:49:44
IS,Loss function of LogisticRegressionOutput,It seems that LogisticRegressionOutput has squared loss function and logit link function Could you confirm my understanding is correct L19 issuecomment 161472054 issuecomment 203815088,,pluskid,2016-09-23 04:13:42,2016-10-01 13:32:30
IS,Anyway to avoid the requirement of using CXXABI 1 3 8,Hi I'm trying to install mxnet on our cluster sever And when I try to import mxnet in python I got error OSError usr lib64 libstdc so 6 version CXXABI 1 3 8' not found required by home nfs rluo rluo mxnet python mxnet lib libmxnet so I have checked our libstdc does not have CXXABI 1 3 8 However since it is a server it is relatively hard for me to update the gcc Is there anyway to bypass this problem,,"piiswrong,piiswrong",2016-10-01 22:09:09,2016-10-02 05:23:19
IS,Doc build broken for Python model API,The official docs build at is missing the API reference When I do the build locally following the instructions in docs README md using docker the docs build correctly I'm not sure how else to debug this Either readthedocs RTD is having problems or there is a discrepency between the docker based build and RTD,,"leopd,piiswrong,mli",2016-08-31 00:24:50,2016-10-03 18:15:28
IS,OSError usr lib libgdal so 1 undefined symbol sqlite3 column table name,Hi all I followed the installation process on the official web site but when I tried to import mxnet in Python I did this on my laptop without GPU support in Anaconda IPython Console under Ubuntu 15 04 Any idea to fix this,,"WellyZhang,sxjscience,WellyZhang,WellyZhang",2016-10-03 09:21:21,2016-10-04 08:36:10
IS,installation Link error,Ubuntu 14 04 Cuda 8 0 Cudnn 5 when I compile with USE CUDNN 1 USE NVRTC 1 USE CUDA 1 I got this error msg build src operator convolution gpu o In function CuDNNConvolutionOp' home ucar data huazhong mxnet src operator cudnn convolution inl h 42 undefined reference to mxnet op TuneCudnnConvolution mxnet op ConvolutionParam std vector mxnet TShape std allocator mxnet TShape std vector mxnet TShape std allocator mxnet TShape mxnet Context cudnnDataType t cudnnConvolutionFwdAlgo t cudnnConvolutionBwdDataAlgo t cudnnConvolutionBwdFilterAlgo t ' home ucar data huazhong mxnet src operator cudnn convolution inl h 42 undefined reference to mxnet op TuneCudnnConvolution mxnet op ConvolutionParam std vector mxnet TShape std allocator mxnet TShape std vector mxnet TShape std allocator mxnet TShape mxnet Context cudnnDataType t cudnnConvolutionFwdAlgo t cudnnConvolutionBwdDataAlgo t cudnnConvolutionBwdFilterAlgo t ' home ucar data huazhong mxnet src operator cudnn convolution inl h 42 undefined reference to mxnet op TuneCudnnConvolution mxnet op ConvolutionParam std vector mxnet TShape std allocator mxnet TShape std vector mxnet TShape std allocator mxnet TShape mxnet Context cudnnDataType t cudnnConvolutionFwdAlgo t cudnnConvolutionBwdDataAlgo t cudnnConvolutionBwdFilterAlgo t ' collect2 error ld returned 1 exit status,,,2016-10-04 18:04:34,2016-10-04 18:39:20
IS,About the docs,I found the docs is much more simplified than old version why some useful info deleted for example there are detail informations for use of each layer dataiter and so on but now it is gone why I think the old one is much more friendly to freshman however I can find the use of detail in source code,,"piiswrong,pluskid,mli,mli",2016-10-03 07:34:49,2016-10-05 05:28:13
IS,document Python Symbol APIs disappeared again,Hi The doc of Symbol APIs on disappeared again It might be overwritten by some autogeneration tool Could you help fixing it Thank you,,"minazou,antinucleon,mli,sxjscience,minazou",2016-10-05 01:53:08,2016-10-05 11:54:12
IS,training on multiple machines error,try to train on 2 machines using command tools launch py n 2 H hosts sync dst dir tmp mxnet python tmp mxnet train mnist py network lenet kv store dist sync And I was asked to input ssh password multiple times and eventually it shows error msg 2016 10 04 20 00 07 614 INFO rsync media ucar 3a6142db a49f 45b3 bdf4 27766e3ec89e huazhong mxnet example image classification 10 110 0 211 tmp mxnet ucar 10 110 0 211 is password 2016 10 04 20 00 13 823 INFO rsync media ucar 3a6142db a49f 45b3 bdf4 27766e3ec89e huazhong mxnet example image classification 10 110 0 212 tmp mxnet ucar 10 110 0 212 is password ucar 10 110 0 211 is password ucar 10 110 0 212 is password ucar 10 110 0 212 is password ucar 10 110 0 211 is password Permission denied please try again ucar 10 110 0 211 is password Permission denied please try again ucar 10 110 0 212 is password Permission denied please try again ucar 10 110 0 212 is password Permission denied please try again ucar 10 110 0 211 is password Permission denied please try again ucar 10 110 0 212 is password Permission denied publickey password Exception in thread Thread 2 Traceback most recent call last File usr lib python2 7 threading py line 810 in bootstrap inner self run File usr lib python2 7 threading py line 763 in run self target self args self kwargs File media ucar 3a6142db a49f 45b3 bdf4 27766e3ec89e huazhong mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File usr lib python2 7 subprocess py line 540 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no 10 110 0 211 p 22 'export LD LIBRARY PATH usr local lib python2 7 dist packages tensorflow core user ops usr local cuda lib64 export DMLC ROLE server export DMLC PS ROOT PORT 9091 export DMLC PS ROOT URI 10 110 0 211 export DMLC NUM SERVER 2 export DMLC NUM WORKER 2 cd tmp mxnet python tmp mxnet train mnist py network lenet kv store dist sync'' returned non zero exit status 255 Permission denied publickey password Exception in thread Thread 3 Traceback most recent call last File usr lib python2 7 threading py line 810 in bootstrap inner self run File usr lib python2 7 threading py line 763 in run self target self args self kwargs File media ucar 3a6142db a49f 45b3 bdf4 27766e3ec89e huazhong mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File usr lib python2 7 subprocess py line 540 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no 10 110 0 212 p 22 'export LD LIBRARY PATH usr local lib python2 7 dist packages tensorflow core user ops usr local cuda lib64 export DMLC ROLE server export DMLC PS ROOT PORT 9091 export DMLC PS ROOT URI 10 110 0 211 export DMLC NUM SERVER 2 export DMLC NUM WORKER 2 cd tmp mxnet python tmp mxnet train mnist py network lenet kv store dist sync'' returned non zero exit status 255 I can see in the two machines the tmp mxnet folder is copied correctly And in ps there are multiple python tmp mxnet train mnist py processes actually 13 on each machine But seems they are all zombie processes Anyone know what is wrong,,piiswrong,2016-10-05 03:09:48,2016-10-05 18:59:01
IS,Understanding the behavior of CuDNNBatchNorm,Recently I met a problem that during training the validation accuracy is always low although the training accuracy is high There are many similar issues like 3032 3424 3378 I notice that all these issues are related to batch normalization Hence I do the following simple experiment This toy example trained a two class softmax to classify some random generated examples I run the above example in two situation 1 ctx mx cpu 2 ctx mx gpu 0 with cuDNN v5 In the first situation everything is ok both the training and the validation accuracy are closed to 1 In the second situation the training accuracy is closed to 1 but the validation accuracy is 0 5 which means the network does not output correctly when is train False Considering that Situation 1 used BatchNorm Op as the backend and Situation 2 used CuDNNBatchNorm Op as the backend is there any special thing that I should pay attention to for CuDNNBatchNorm,,"ArtanisCV,ArtanisCV",2016-10-06 10:53:38,2016-10-06 11:17:53
IS,The predict function in 'base module py,I train a model using the new interface 'module' Here if the batch size is smaller than 100 then it will output the following error MXNetError 20 14 41 include mxnet ndarray h 244 Check failed shape 0 end Slice end index out of range But it will be all right if the batch size is not smaller than 100 Why,,,2016-10-06 12:20:10,2016-10-06 12:23:36
IS,Moving all language bindings to new NNVM interface,pluskid NNVM refactor is almost done and CAPI is now stable Please help move other language bindings to the new CAPI The difference is mostly in imperative api Please read the code in python mxnet ctypes ndarray py and python mxnet ctypes symbol py and adapt it to R Scala and Julia Specifically init ndarray module and make ndarray function in are changed in ndarray py and init symbol module and make atomic symbol function are changed in symbol py 1 You should now use MXListAllOpNames to get the NAMES of all ops and then use NNGetOpHandle to get the corresponding handle instead of directly listing handle as before 2 Use MXImperativeInvoke instead of MXFuncInvoke to call imperative ops To support variable number of output num output and output vars should now be passed by reference Also returned NDArrayHandle can be different from the ones passed in So always create new NDArray objects from returned handle 3 Note that we now support alias for symbol and ndarray funcs The name returned by mxdescribe is alway is original name instead of the alias So you need to register ops under the name returned by mxlist to support alias in frontend,,"piiswrong,pluskid,tqchen,pluskid,yzhliu,sbodenstein",2016-09-16 22:29:54,2016-10-07 04:13:54
IS,why mxnet do not have split layer like caffe,Caffe use InsertSplits to add split layer implicit why mxnet do not have it for googlenet I need to know to accelerate the split proedure,,"zhenlinluo,piiswrong,zhenlinluo,piiswrong",2016-10-05 13:36:15,2016-10-07 04:14:04
IS,performance of using multiple nodes,I have used multiple GPUs within one node and got good scalability But When I use multiple nodes it does not scale well Does anyone know why Let me explain in detail I was using inception bn network to train the imagenet data The speed is around 187 samples sec with 1 GPU batch size 64 373 samples sec with 2 GPUs batch size 128 741 samples sec with 4 GPUs batch size 256 Now I am using two nodes where each node uses 4 GPUs I expect that in each node the speed is 741 samples sec But the reality is on node 1 the speed is 741 samples sec but on node 2 the speed is only around 338 samples sec I was using the train imagenet py in examples images classcification My launch command is tools launch py H hosts n 2 launcher ssh python train imagenet py network inception bn gpus 0 1 2 3 batch size 256 num epochs 1 data shape 224 data dir data ilsvrc12 kv store dist async Both nodes have 1 ethernet address and 1 infiniband address The hosts file contains the infiniband addresses of two nodes 10 149 0 1 10 149 0 2 I ran the above command from 10 149 0 1 Although I specified the infiniband addresses in hosts file the log file still shows the program is using the ethernet address So I want to know is the poor performance on node 2 because of using the ethernet for communication or any other reasons Should I change the batch size when using multiple nodes,,"piiswrong,piiswrong,formath",2016-09-30 20:27:39,2016-10-07 20:09:40
IS,How to get the output of the intermediate layer,I want to use the output of the intermediate layer in the model How to get it,,"pluskid,zihaolucky",2016-10-03 16:11:52,2016-10-08 01:58:28
IS,Let caffe convert support Alexnet,convert symbol py 57 58,,winstywang,2016-09-30 21:39:16,2016-10-08 02:00:42
IS,If the input data is features not original images how to train the network,mx io ImageRecordIter can produce the image batch like the dimension 16 3 224 224 from the data rec In my new model the input data is the features the dimension of the features is 16 512 30 30 produced from another network I want to train a new network from those features How to introduce the features as the input data of a network,,piiswrong,2016-10-01 14:03:04,2016-10-08 02:03:14
IS,Is there any easy way to monitor internal output of each layers,One can easily access weights biases and gradient of each layer in a DNN through the 'arg dict' and 'grad dict' attribute of a executor object but I can not find anything like 'internal output dict' that I can use to access the internal output of each layer I wander if there exists any easy way to monitor internal output of each layers just like how I monitor weights and biases of each layers Thanks,,"tornadomeet,tornadomeet,piiswrong",2016-10-08 04:16:01,2016-10-08 08:51:49
IS,CUDA driver version is insufficient for CUDA runtime version,I try to train a model on gpu it crashes and throws an exception saying CUDA driver version is insufficient for CUDA runtime version with centos 7 0 cuda7 5 Driver Version 352 99 attemps to fix the problem update driver version to 367 44 update cuda version to 8 0 or downgraded to 6 0 or 7 0 without the driver No use Finally I uninstall driver 367 44 install cuda 8 0 totally and it is fine now,,,2016-10-09 14:29:16,2016-10-09 15:54:26
IS,ndarray h 244 Check failed shape 0 end Slice end index out of range,Hi Can you please help me with this I have a data iterator that returns the following self data and self label are numpy arrays mx io DataBatch data mx nd array self data label mx nd array self label pad 0 index self batch index self batch size 120 self data shape 120 8 32 32 self label shape 120 1 I am using the FeedForward model implementation I am getting the slicing error can you please help me with this Thank you MXNetError Traceback most recent call last ipython input 3 f2ebcff16884 in module 15 kvstore kv 16 logger logger 17 epoch end callback epoch end callback usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model pyc in fit self X y eval data eval metric epoch end callback batch end callback kvstore logger work load list monitor eval batch end callback 787 logger logger work load list work load list monitor monitor 788 eval batch end callback eval batch end callback 789 sym gen self sym gen 790 791 usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model pyc in train multi device symbol ctx arg names param names aux names arg params aux params begin epoch end epoch epoch size optimizer kvstore update on kvstore train data eval data eval metric epoch end callback batch end callback logger work load list monitor eval batch end callback sym gen 220 do reset True 221 for data batch in train data 222 executor manager load data batch data batch 223 224 if monitor is not None usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager pyc in load data batch self data batch 392 self curr execgrp self execgrp 393 394 self curr execgrp load data batch data batch 395 396 def forward self is train False usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager pyc in load data batch self data batch 243 def load data batch self data batch 244 load data and labels into arrays 245 load data data batch self data arrays 246 load label data batch self label arrays 247 usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager pyc in load data batch targets 86 def load data batch targets 87 Load data into sliced arrays 88 load general batch data targets 89 90 def load label batch targets usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager pyc in load general data targets 82 else 83 for slice idx d dst in d targets 84 d src slice idx copyto d dst 85 86 def load data batch targets usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray pyc in getitem self in slice 234 raise ValueError 'NDArray only support continuous slicing on axis 0' 235 if in slice start is not None or in slice stop is not None 236 return self slice in slice start in slice stop 237 else 238 return self usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray pyc in slice self start stop 274 stop mx uint stop if stop else mx uint self shape 0 275 check call LIB MXNDArraySlice 276 self handle start stop ctypes byref handle 277 return NDArray handle handle writable self writable 278 usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 if sys version info 0 3 MXNetError 22 59 51 include mxnet ndarray h 244 Check failed shape 0 end Slice end index out of range,,,2016-10-07 17:37:54,2016-10-10 07:02:46
IS,Fail to load model in a cpu only machine,Sorry for disturbing I trained a model in a gpu server where the mxnet is compiled with cuda and cudnn4 and I want to deploy it in a cpu only machine I do not know what exactly should I do Should I change something in the json file When I loaded the model directly with this comman ldNet ldargs ldaux mx model load checkpoint prefix nepoch I met the problem following What shoul I do Error Promotion MXNetError Failed loading Op conv1 1 of type Convolution Cannot find argument 'cudnn off' Possible Arguments kernel Shape tuple required convolution kernel size y x or d y x stride Shape tuple optional default 1 1 convolution stride y x or d y x dilate Shape tuple optional default 1 1 convolution dilate y x pad Shape tuple optional default 0 0 pad for convolution y x or d y x num filter int non negative required convolution filter channel number num group int non negative optional default 1 Number of groups partition This option is not supported by CuDNN you can use SliceChannel to num group apply convolution and concat instead to achieve the same need workspace long non negative optional default 1024 Tmp workspace for convolution MB no bias boolean optional default False Whether to disable bias parameter cudnn tune 'fastest' 'limited workspace' 'off' optional default 'off' Whether to find convolution algo by running performance test Leads to higher startup time but may give better speed auto tune is turned off by default Set environment varialbe MXNET CUDNN AUTOTUNE DEFAULT 1 to turn on by default End Promotion,,winstywang,2016-10-06 14:58:03,2016-10-10 07:06:28
IS,How to train a single rpn model using mxnet not faster rcnn,How to train a single rpn model not faster rcnn using mxnet,,winstywang,2016-10-04 09:41:05,2016-10-10 07:53:07
IS,why arrays can not be used in NumpyOp for multi gpu training,Could someone explain that Thanks,,piiswrong,2016-09-27 06:43:14,2016-10-10 07:56:13
IS,what is the meaning of the error,screenshot from 2016 09 29 17 32 39,,"zihaolucky,winstywang",2016-09-29 09:39:45,2016-10-10 07:58:20
IS,docker image kaixhin cuda mxnet cannot run mxnet python demo,Mxnet python demo python example image classification train mnist py network lenet gpus 0 works properly on kaixhin cuda mxnet 7 0 but it fails on kaixhin cuda mxnet Looks like mxnet has some problems with CUDA 7 5 Error messages and earlier discussion about this problem can be seen here,,"philipskokoh,philipskokoh,winstywang,philipskokoh",2016-10-03 15:19:54,2016-10-11 01:34:38
IS,add symbol name to error message,when has some InferShape or InferType error I got the wrong message like 15 40 35 g deeplearn mxnet src operator convolution inl h 431 Check failed in type size 1 There is not enough information to tell me which symbol is wrong if I have more than one symbol of the same type I think it should be add the name of the symbol in the error message like 15 40 35 g deeplearn mxnet src operator convolution inl h 431 Check failed in type size 1 with convolution0,,"yajiedesign,piiswrong,winstywang,yajiedesign,winstywang",2016-09-28 07:46:25,2016-10-11 04:37:53
IS,Training error with 'nan' loss by using Tesla M40 on Windows,MxNet on windows works fine by using Tesla K40 but I always get nan or huge number of loss by using M40 on Windows Server 2012 CUDA 7 5 CUDNN 5 CUDNN 4 Here are some logs Everything is ok when I use K40 and Caffe works fine with the M40 cards Maybe I missed something for using M40 please help,,"tornadomeet,tornadomeet,tornadomeet,tornadomeet",2016-10-10 09:52:23,2016-10-11 07:18:17
IS,use intranet IP to run MXNet on multiple machines with data parallel,My machine has two IPs eth0 uses extranet IP 211 69 198 x and eth1 uses intranet IP 192 168 0 x and I want to run MXNet on multiple machines with data parallel using internet IP the hosts file is use intranet IP so how can I use the intranet IP thx,,"mli,mli,mli",2016-09-29 02:57:29,2016-10-11 11:40:52
IS,so many warnings when compiling mxnet0 7 0 in TX1 with cuda 8 0,,,"tqchen,tornadomeet",2016-10-12 03:38:59,2016-10-12 05:21:30
IS,Error running prediction example on Inception BN,I am trying to run the Use Pretrained Inception BatchNorm Network in python The execution throws and error on line prob model predict batch 0 with the error message,,"winstywang,winstywang,winstywang,thirdwing,winstywang",2016-10-03 13:52:54,2016-10-12 07:44:15
IS,Sample speed with and without MKL in GPU VM,I'm training Imagenet in python with 4 GPUs In one machine I have installed MKL and in the other one I have not The one with MKL has a speed of 200 samples sec while the one without MKL has 50 samples sec I do not understand this behavior since I'm computing with GPU Can anybody explain to me how MKL affects the computation in CPU and GPU,,"miguelgfierro,mli,miguelgfierro",2016-09-29 08:05:10,2016-10-12 12:20:37
IS,Implementing ImageNet in R,I'm trying to implement Imagenet in R using alexnet architecture I successfully made it work in python with the exception of a couple of architectures that are not working However alexnet is working in python What I get right now is that the accuracy is not going up 0 00100694444444444 0 00112268518518518 0 00107638888888889 0 00110416666666666 0 00107638888888889 It goes like this forever I'm using 4 GPUs and a batch size of 576 I tried different batch sizes This is the code I have Any ideas,,"miguelgfierro,mli,miguelgfierro,mli,miguelgfierro,miguelgfierro,miguelgfierro,miguelgfierro,thirdwing,miguelgfierro",2016-09-24 17:01:12,2016-10-12 12:21:57
IS,Can I train the faster RCNN with multi GPU with multi machine,,,,2016-10-13 09:04:48,2016-10-13 10:53:26
IS,The problems appear at run time mxnet nnvm include nnvm op h 448 Check failed p second plevel Attribute,Now when I use new mxnet nnvm branch I always run into troubles These troubles appear at run time though compiling can work normally For example mxnet nnvm include nnvm op h 448 Check failed p second plevel Attribute FInferShape of operator argmax is already registered with same plevel 10 mxnet nnvm include nnvm op h 448 Check failed p second plevel Attribute FCompute cpu of operator softmax cross entropy is already registered with same plevel 10,,piiswrong,2016-10-13 07:34:52,2016-10-13 18:28:24
IS,What happened to old tutorials,E g I am looking for how to create rec files from your own dataset All the old links are missing create dataset using recordio Any help,,"piiswrong,sandeep-krishnamurthy,sandeep-krishnamurthy",2016-10-14 05:59:09,2016-10-14 06:41:09
IS,training accuracy dont go up by using MXnetR to build CNN network for CIFAR 10 dataset,here is my mxnetR script rm list ls library mxnet setwd home mac R Projects MXnet CIFAR 10 train mx io ImageRecordIter path imglist home mac R Projects MXnet CIFAR 10 img2 txt path imgrec home mac R Projects MXnet CIFAR 10 img2 rec data shape c 32 32 3 batch size 100 shuffle TRUE val mx io ImageRecordIter path imglist home mac R Projects MXnet CIFAR 10 val2 txt path imgrec home mac R Projects MXnet CIFAR 10 val2 rec data shape c 32 32 3 batch size 100 shuffle TRUE Define deep learning network act type can be relu sigmoid softrelu and tanh input data mx symbol Variable name data stage 1 conv1 mx symbol Convolution data input data kernel c 5 5 pad c 2 2 num filter 32 name conv1 relu1 mx symbol Activation data conv1 act type relu name relu1 pool1 mx symbol Pooling data relu1 kernel c 3 3 stride c 2 2 pool type max name pool1 stage 2 conv2 mx symbol Convolution data pool1 kernel c 5 5 pad c 2 2 num filter 32 name conv2 relu2 mx symbol Activation data conv2 act type relu name relu2 pool2 mx symbol Pooling data relu2 kernel c 3 3 stride c 2 2 pool type max name pool2 stage 3 conv3 mx symbol Convolution data pool2 kernel c 5 5 pad c 2 2 num filter 64 name conv3 relu3 mx symbol Activation data conv3 act type relu name relu3 pool3 mx symbol Pooling data relu3 kernel c 3 3 stride c 2 2 pool type max name pool3 stage 4 flatten mx symbol Flatten data pool3 stage 5 fc1 mx symbol FullyConnected data flatten num hidden 64 name fc1 relu4 mx symbol Activation data fc1 act type relu name relu4 stage 6 fc2 mx symbol FullyConnected data relu4 num hidden 10 name fc2 softmax mx symbol SoftmaxOutput data fc2 name isoftmax' mx set seed 2016 device gpu mx gpu 0 lr rate c 0 0001 0 001 0 01 0 1 for n in 1 length lr rate cat Process model train with learning rate lr rate n for IO data model is on going please be patient n model mx model FeedForward create softmax X train eval data val ctx device gpu num round 100 learning rate lr rate n momentum 0 9 eval metric mx metric accuracy initializer mx init Xavier factor type 'in' magnitude 2 34 wd 0 00001 batch end callback mx callback log train metric 100 the training accuracy is around 0 1 attached please find my img txt file for prepare the img rec data can anyone help on this img txt val txt img2 txt val2 txt CIFAR 20161014V2 txt,,"thirdwing,thirdwing",2016-10-14 13:21:11,2016-10-15 14:16:44
IS,Training of Imagenet with inception bn in python gets good training accuracy but low validation after epoch 1,I'm training Imagenet with inception bn in python Any suggestion,,"miguelgfierro,piiswrong,miguelgfierro,miguelgfierro,miguelgfierro,winstywang,winstywang,winstywang,miguelgfierro,miguelgfierro,miguelgfierro",2016-10-01 05:53:48,2016-10-15 20:27:12
IS,Some networks not working with Imagenet in python,I have been experimenting with Imagenet example in python Inception bn inception bn full alexnet vgg networks works well However if I try to do the same with googlenet I get the following error,,"miguelgfierro,piiswrong,miguelgfierro,mli,miguelgfierro,miguelgfierro,miguelgfierro",2016-09-23 10:24:56,2016-10-15 20:28:52
IS,Error AttributeError 'module' object has no attribute 'SpatialTransformer',I got the error on title on the second line with python 2 7 Is it a breaking change,,tornadomeet,2016-10-06 18:28:34,2016-10-16 19:43:50
IS,About the implement of LSTM,When I read the example of lstm py I am confused that the example build as many LSTMParams as the length of input Should not all the LSTM layer share a single set of parameters Could I only build one LSTMParam and recursively feed if into lstm layer That is the part where I am confused Thanks for any help,,piiswrong,2016-10-17 03:47:25,2016-10-17 06:13:16
IS,How to get length of sentence in the mxnet,I'm dealing with an NLP task And I pad all the sentence to the same length pad with 0 vector However I need the real length of sentence in the network Such as TextCNN How could I get this,,,2016-10-17 05:49:39,2016-10-17 18:12:56
IS,a warning when compiling the newest mxnet,,,pluskid,2016-10-12 07:03:04,2016-10-17 22:50:50
IS,What is the difference between SliceChannel and Slice axis,What is the difference between SliceChannel and Slice axis,,piiswrong,2016-10-17 18:13:42,2016-10-18 00:35:07
IS,Dose mxnet has scalar symbol element wise operation,I want to train a scalar paramter from the data like this data mx symbol Variable wouldata' data shape 100 10 15 15 par mx symbol Variable 'parm' par is a scalr par broad mx symbol broadcast to par shape 100 10 15 15 out data par broad If the batch size 100 the above code is Ok however if the batch size is not 100 Those code can not work how can I do the element wise multiplication like out data par data and par are symbol data is feature map and par is a scalar symbol,,"pluskid,pluskid,pluskid,tqchen",2016-10-16 07:15:41,2016-10-18 16:44:17
IS,NDArray error after install mxnet,Hi I installed mxnet with the latest commit cf00ca064d5f6e46b062c2027ae2905c6147a04c in our server and the code works fine before can not run now Error message Start training Traceback most recent call last File train rnn py line 155 in module optimizer params 'learning rate' learning rate 'momentum' momentum 'wd' 0 00001 File home comp pengfeixu local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module base module py line 358 in fit self update metric eval metric data batch label File home comp pengfeixu local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module module py line 453 in update metric self exec group update metric eval metric labels File home comp pengfeixu local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module executor group py line 391 in update metric labels slice append label copy slice to axis islice start AttributeError 'NDArray' object has no attribute 'copy slice to',,"piiswrong,pluskid",2016-10-18 03:32:44,2016-10-18 23:37:30
IS,Failed running mxnet faster rcnn end to end training example,After convert caffe vgg16 model and link voc data i run python train end2end py and get the following error message INFO root TRAIN FASTER RCNN WITH APPROXIMATE JOINT END2END providing maximum shape wouldata' 1 3 1000 1000 'label' 1 34596 'bbox target' 1 36 62 62 'bbox inside weight' 1 36 62 62 'bbox outside weight' 1 36 62 62 'gt boxes' 1 500 voc 2007 trainval gt roidb loaded from home cc mxnet example rcnn data cache voc 2007 trainval gt roidb pkl append flipped images to roidb prepare roidb Traceback most recent call last File train end2end py line 170 in module args work load list args resume not args no flip args factor step File train end2end py line 117 in end2end train arg params args aux params auxs begin epoch begin epoch num epoch num epoch File home cc mxnet python mxnet module base module py line 334 in fit for training True force rebind force rebind layout mapper layout mapper TypeError bind got an unexpected keyword argument 'layout mapper' does i missed anything,,"piiswrong,piiswrong",2016-10-17 04:55:56,2016-10-19 04:14:20
IS,How to assign a variable with shape,Is there any convenient way to assign a variable with shape when constructing a graph,,,2016-10-19 01:54:50,2016-10-19 06:32:01
IS,update windows binary,The last windows binary is too old can someone update windows binary,,,2016-10-20 01:20:46,2016-10-20 05:34:45
IS,Weighted loss when using multi loss,Hi I am trying to train a network with multi task however can I set the different losses with different wights Thanks,,piiswrong,2016-10-19 18:00:58,2016-10-20 05:37:10
IS,Training speed changing rapidly,I'm curious about this phenomenon could anyone provide some insights,,"zihaolucky,zihaolucky",2016-10-18 03:35:37,2016-10-20 05:43:54
IS,undefined symbol,I complie mxnet with cuda sucessfully and installed python mxnet after that I import mxnet it raise an undefined symbol problem OSError home sherwood mxnet python mxnet lib libmxnet so undefined symbol ZN5mxnet2op8CreateOpIN7mshadow3gpuEEEPNS 8OperatorENS0 15ROIPoolingParamEi,,,2016-10-20 08:39:42,2016-10-20 13:33:24
IS,Fastest way to evaluate a big amount of data,I have a pretrained cnn model and I want to evaluate 50000 images I have two methods for prediction but they take about 1 day to evaluate I'm wondering if there is a way to do it faster Sumary of the code So it takes around 4 min to evaluate 144 images so for 50000 images it will take around a day Is there a faster way,,"miguelgfierro,piiswrong,miguelgfierro",2016-10-19 18:40:30,2016-10-20 15:12:05
IS,the probability of prediction on the same test data based on three different framework,only test on the data with label 1 1 VGG feature and SVM image 2 lasagne framwork with googlenet image 3 mxnet framwork with resnet image Why the distribution is so different Especially mxnet is prediction results is polarized badly,,"thirdwing,piiswrong,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet",2016-10-18 08:54:39,2016-10-21 03:55:35
IS,Why resnet params have GPU specific but not generic,I am using resnet pretrained model to test resnet on CPU However it will report error like below Traceback most recent call last File train imagenet scoring py line 87 in module loaded mx model FeedForward load args model params 1 File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet model py line 835 in load symbol arg params aux params load checkpoint prefix epoch File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet model py line 361 in load checkpoint save dict nd load ' s 04d params' prefix epoch File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 983 in load ctypes byref names File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 00 35 20 src ndarray ndarray cc 347 GPU is not enabled,,"zhenlinluo,piiswrong,mli,tornadomeet",2016-10-18 16:05:55,2016-10-21 17:55:09
IS,More elegant way to set specific initial value,For many times I have encountered a trivial problem how to set specific initial value to weights bias I used to ignore this since random initialization is somewhat enough in many situations The last time when I trying to implement a simple layer Scale which in brief scales the input by a factor the initial value of this factor is very important also it should be learned overtime Thus it should be something very similar to weight bias stored as input argument Again there is no elegant way to initialize this value as in my opinion Hack 1 not working for multi gpu so this is actually useless Dirty but works However I do not think this hack is very reliable Seems like attribute is very promising but initializer can only access name,,"zhreshold,piiswrong,zhreshold,piiswrong,zhreshold",2016-10-21 22:26:21,2016-10-21 23:12:36
IS,How to gradually change a variable is value,See a multi task loss toatal loss loss1 lambda loss2 How can I gradually change the lambda is value I know for tensorflow How could mxnet do this,,piiswrong,2016-10-21 03:15:05,2016-10-22 01:28:53
IS,In R prediction is running only on CPU very sow,I have quite a deep net trained on 4 gpus which all works fine But when I try and run predict it is very very slow only 1 cpu is doing anything out of 16 and the GPUs are doing nothing Looks like the model is loaded in gpu memory and actually I think it was never unloaded after training Not sure if that should happen automatically or not How do I force the predict to run on GPU,,thirdwing,2016-10-22 07:27:11,2016-10-22 14:29:14
IS,element wise variable and matrix product,I find there is no operation that supporting ndarray and variable production is there any other solution vae mx sym Variable vae data mx sym SliceChannel data vae num outputs 2 mean data 0 stddev data 1 label mx sym Variable label epsilon mx random normal 0 1 batch size 16 stddev2 mx sym sqrt mx sym exp stddev input data mean stddev2 epsilon input data mean mx sym broadcast mul stddev2 epsilon,,"pluskid,piiswrong",2016-10-21 19:42:26,2016-10-23 08:52:23
IS,Error cifar10 recipe notebook,When I tried to run cifar10 recipe in iPython I got below error Would you please help to solve Thanks Ubuntu 16 04LTS MXNetError Traceback most recent call last ipython input 13 c482f3d68edb in module 2 eval data test dataiter 3 eval metric accuracy 4 batch end callback mx callback Speedometer batch size 5 6 if we want to save model after every epoch we can add check point call back home dc mxnet python mxnet model pyc in fit self X y eval data eval metric epoch end callback batch end callback kvstore logger work load list monitor eval batch end callback 742 743 arg names param names aux names 744 self init params dict data provide data data provide label 745 746 setup metric home dc mxnet python mxnet model pyc in init params self input shapes overwrite 482 def init params self input shapes overwrite False 483 Initialize weight parameters and auxiliary states 484 arg shapes aux shapes self symbol infer shape input shapes 485 assert arg shapes is not None 486 home dc mxnet python mxnet symbol pyc in infer shape self args kwargs 456 The order is in the same order as list auxiliary 457 458 return self infer shape impl False args kwargs 459 460 def infer shape partial self args kwargs home dc mxnet python mxnet symbol pyc in infer shape impl self partial args kwargs 516 ctypes byref aux shape ndim 517 ctypes byref aux shape data 518 ctypes byref complete 519 if complete value 0 520 arg shapes home dc mxnet python mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 if sys version info 0 3 MXNetError InferShape Error in concat2 02 25 49 src operator concat inl h 152 Check failed dshape j tmp j Incorrect shape 1 128 80 13 13 first input shape 128 160 14 14,,tornadomeet,2016-10-23 06:38:13,2016-10-24 15:07:30
IS,AttributeError 'list' object has no attribute ishape',10 22 22 43 18 Start training with gpu 1 Traceback most recent call last File home xiaomin wxm Code KaggleSeizure2016 ex mxnet py line 229 in module fitter fit File home xiaomin wxm Code KaggleSeizure2016 ex mxnet py line 220 in fit batch end callback call back File home xiaomin MXNet mxnet python mxnet model py line 829 in fit sym gen self sym gen File home xiaomin MXNet mxnet python mxnet model py line 234 in train multi device executor manager load data batch data batch File home xiaomin MXNet mxnet python mxnet executor manager py line 447 in load data batch self curr execgrp load data batch data batch File home xiaomin MXNet mxnet python mxnet executor manager py line 281 in load data batch load data data batch self data arrays File home xiaomin MXNet mxnet python mxnet executor manager py line 93 in load data load general batch data targets File home xiaomin MXNet mxnet python mxnet executor manager py line 84 in load general if d src slice idx shape d dst shape AttributeError 'list' object has no attribute ishape' seems somethings wrong with the mxnet is python source code,,"piiswrong,piiswrong,piiswrong",2016-10-22 14:45:44,2016-10-25 02:18:44
IS,About the data shuffling when training a feed forward model,I tried to use Mxnet to train a feed forward model the data input are a list of file each file has several samples all samples in one file share a common label what I did is to write samples in all files to a single big csv file and use mxnet csv data loader to load it What I did now is shuffle the whole CSV file before giving it to mxnet csv data loader I wonder whether there is such load cache and shuffle functions built in a data loader,,,2016-10-25 07:29:16,2016-10-25 07:39:22
IS,How to directly access Tensor element in a c gpu Operator,I try to access Tensor element in Forward function in my new c gpu Operator but got violation access error Following is code snippet I am new to cuda programming In this case I prefer to access Tensor element directly shoud I write a new mshadow op to do this The code runs well in cpu mode,,piiswrong,2016-10-23 00:41:26,2016-10-25 13:17:42
IS,Deepmark not working for current change,13 16 23 home zhenlin Mxnet upstream mxnet mxnet dmlc core include dmlc logging h 235 13 16 23 src operator convolution inl h 433 Check failed dtype 1 First input must have specified type Traceback most recent call last File benchmark py line 91 in module kvstore args kv store batch size args batch size File benchmark py line 31 in get module mod bind data shapes provide data label shapes provide label for training True inputs need grad False File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module module py line 265 in bind grad req grad req File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module executor group py line 173 in init self bind exec data shapes label shapes shared group File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module executor group py line 213 in bind exec self execs append self bind ith exec i data shapes label shapes shared group File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module executor group py line 442 in bind ith exec arg types aux types self symbol infer type input types File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet symbol py line 417 in infer type ctypes byref complete File root anaconda2 lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError InferType Error in convolution0 13 16 23 src operator convolution inl h 433 Check failed dtype 1 First input must have specified type,,"zhenlinluo,piiswrong",2016-10-24 19:21:16,2016-10-25 20:11:42
IS,Is there any doc about bucketing API,Is there any doc about bucketing API,,"piiswrong,pluskid",2016-10-23 23:52:53,2016-10-26 03:04:06
IS,Fast poor net pretrained model,Hi is there the Fast poor net from the mxnet js demo available for download anywhere I just found this broken link,,"antinucleon,antinucleon,Piyush3dB",2016-10-20 05:54:49,2016-10-26 06:34:21
IS,convert model from caffe failed,When I convert the vgg16 net an error happened as follows python convert model py VGG ILSVRC 16 layers deploy prototxt VGG ILSVRC 16 layers caffemodel vgg16 Traceback most recent call last File convert model py line 110 in module main File convert model py line 37 in main prob input dim proto2symbol args caffe prototxt File D git checkout mxnet tools caffe converter convert symbol py line 198 in proto2symbol exec sym File string line 7 in module File E Programs Anaconda3 envs py27 lib site packages mxnet 0 7 0 py2 7 egg mxnet symbol py line 1062 in creator ctypes byref sym handle File E Programs Anaconda3 envs py27 lib site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Cannot find argument 'pooling convention' Possible Arguments global pool boolean optional default False Ignore kernel size do global pooling based on current input feature map This is useful for input with different shape kernel Shape tuple required pooling kernel size y x pool type 'avg' 'max' isum' required Pooling type to be applied stride Shape tuple optional default 1 1 stride for pooling y x pad Shape tuple optional default 0 0 pad for pooling y x In ReadMe md it is said that The user needs to add padding to stride 2 pooling to make this work right now Anyone know how to add that padding,,,2016-10-26 08:22:38,2016-10-26 09:11:11
IS,Multi device training,I trying to do multi device training in several GPUs I'm following the example you have here If I try to synchronize the directory synchronize directory and copy the folders but it does nothing however in the hosts the folder tmp mxnet is created with all the content of the folder from where I launch the script How can I debug this I have the last version of mxnet,,"miguelgfierro,mli,miguelgfierro,miguelgfierro",2016-09-29 07:55:25,2016-10-26 18:20:22
IS,infer shape of mx sym Concat,Thanks for your attention I am studying the mxnet example cnn text classification text cnn py example I want to know some shape information about the Concat Layer So I add this code after L44 url input shapes input shapes wouldata' 100 1 100 1 100 1 arg shape out shape aux shape concat infer shape input shapes I think the shape of every max pooling outputs is 100 1 but im not sure about that but the arg shape out shape and aux shape it returns are all None Is the input shapes wrong How could i get the shape info of the concat Thanks a lot,,piiswrong,2016-10-26 04:09:28,2016-10-27 02:25:19
IS,xlvector ocr 0 mxnet bug,ocr 98 81 0 mxnet bug,,"winstywang,winstywang,xlvector,xlvector,yajiedesign,thirdwing,yajiedesign,yajiedesign,thirdwing",2016-10-15 00:05:55,2016-10-27 23:38:10
IS,import mxnet error with caffe plugin in Pycharm,I encountered the same problem with 3108 and I fixed it as it is ok in command line but when I import mxnet in Pycharm it reported the same error OSError libcaffe so 1 0 0 rc3 cannot open shared object file No such file or directory Can anyone help me,,"HrWangChengdu,HrWangChengdu",2016-10-28 12:07:29,2016-10-29 00:05:06
IS,generate train lst from VOCdevkit in example fcn xs,i download the VOC2012 from when i extract the tar file i do not find the lst file because i am a greenhand in segmentation i do not know how to generate these lst file i would really appreciate it if there is anyone who can tell me how to make it thanks,,"tornadomeet,tornadomeet,tornadomeet",2016-10-29 08:40:13,2016-10-29 09:41:44
IS,how to make softmax label shape same as target,I am running the cnn classification example and getting error array shape do not match the shape of NDArray Any idea where i should be looking I appreciate all your help output checks y train shape Out 31 9662 2 y train 1 Out 32 array 1 0 label shape 50 error ValueError Traceback most recent call last ipython input 30 2b003c065d39 in module 10 11 data batchX 12 label batchY 13 14 forward C Users amitb Anaconda3 lib site packages mxnet 0 7 0 py3 5 egg mxnet ndarray py in setitem self in slice value 203 NDArray set value float value out self 204 elif isinstance value np ndarray np generic 205 self sync copyfrom value 206 else 207 raise TypeError 'type s not supported' str type value C Users amitb Anaconda3 lib site packages mxnet 0 7 0 py3 5 egg mxnet ndarray py in sync copyfrom self source array 232 source array np ascontiguousarray source array dtype self dtype 233 if source array shape self shape 234 raise ValueError 'array shape do not match the shape of NDArray' 235 check call LIB MXNDArraySyncCopyFromCPU 236 self handle ValueError array shape do not match the shape of NDArray,,,2016-10-25 15:56:22,2016-10-31 16:56:10
IS,when will mxnet 1 0 release,when will mxnet 1 0 release,,"piiswrong,tqchen",2016-10-27 06:57:26,2016-11-01 05:38:59
IS,ubuntu 16 04 compile errror,usr lib nvidia cuda toolkit bin nvcc std c 11 Xcompiler D FORCE INLINES g O3 ccbin g Xcompiler D MWAITXINTRIN H INCLUDED O3 I home weizi MXNet mxnet mshadow I home weizi MXNet mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas msse3 I usr lib nvidia cuda toolkit include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 pkg config cflags opencv fopenmp D FORCE INLINES DMXNET USE NVRTC 0 M MT build src operator sequence reverse gpu o src operator sequence reverse cu build src operator sequence reverse gpu d usr lib nvidia cuda toolkit bin nvcc c o build src operator sequence reverse gpu o std c 11 Xcompiler D FORCE INLINES g O3 ccbin g Xcompiler D MWAITXINTRIN H INCLUDED O3 I home weizi MXNet mxnet mshadow I home weizi MXNet mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas msse3 I usr lib nvidia cuda toolkit include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 pkg config cflags opencv fopenmp D FORCE INLINES DMXNET USE NVRTC 0 src operator sequence reverse cu usr include c 5 bits stl iterator base types h 154 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator sequence reverse inl h 91 here implicit generation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of class mxnet op SequenceReverseOp xpu DType with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp mxnet op SequenceReverseParam with xpu mxnet gpu DType float src operator sequence reverse cu 14 here usr include c 5 bits stl iterator base types h 155 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator sequence reverse inl h 91 here implicit generation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of class mxnet op SequenceReverseOp xpu DType with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp mxnet op SequenceReverseParam with xpu mxnet gpu DType float src operator sequence reverse cu 14 here usr include c 5 bits stl iterator base types h 156 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator sequence reverse inl h 91 here implicit generation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of class mxnet op SequenceReverseOp xpu DType with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp mxnet op SequenceReverseParam with xpu mxnet gpu DType float src operator sequence reverse cu 14 here usr include c 5 bits stl iterator base types h 157 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator sequence reverse inl h 91 here implicit generation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of class mxnet op SequenceReverseOp xpu DType with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp mxnet op SequenceReverseParam with xpu mxnet gpu DType float src operator sequence reverse cu 14 here usr include c 5 bits stl iterator base types h 158 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator sequence reverse inl h 91 here implicit generation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of class mxnet op SequenceReverseOp xpu DType with xpu mxnet gpu DType float src operator sequence reverse inl h 45 here instantiation of mxnet op SequenceReverseOp xpu DType SequenceReverseOp mxnet op SequenceReverseParam with xpu mxnet gpu DType float src operator sequence reverse cu 14 here 5 errors detected in the compilation of tmp tmpxft 00000139 00000000 7 sequence reverse cpp1 ii Makefile 179 recipe for target 'build src operator sequence reverse gpu o' failed make build src operator sequence reverse gpu o Error 2,,"piiswrong,piiswrong,wangg12",2016-10-30 16:30:55,2016-11-01 05:53:05
IS,How to point mshadow to the latest commit,piiswrong I noticed that the mshadow still point to older commit How to point to the latest commit,,"zhenlinluo,sxjscience,zhenlinluo",2016-11-01 00:19:01,2016-11-01 07:47:47
IS,Error launching distributed mxnet on EC2 invalid command,When I ran the command as directed in the website mxnet tools launch py n 2 H hosts sync dir tmp mxnet python train mnist py kv store dist sync I get an error CalledProcessError Command ' tmp mxnet python train mnist py sync dir kv store dist sync' returned non zero exit status 126 Which looks like the command is concated incorrectly It should be python train mnist py sync dir tmp mxnet kv store dist sync Kindly let me know if there was a clear mistake,,"piiswrong,piiswrong",2016-11-01 07:25:59,2016-11-01 17:15:51
IS,Logical errors in CNN for Text Classification example,Hi I am new to Github Please provide me a way to reach the author of the CNN for text classification example as i have found logical issued with the code 1 The build vocab function splits the sentences into characters rather than words I have the suggested code change I would like to know how i can offer the code I appreciate the author in providing the example Its been really helpful in learning about cnn Regards Amit,,"piiswrong,qcl6355",2016-10-31 15:30:17,2016-11-01 20:13:11
IS,why mxnet does not have api reference like tensorflow,,,,2016-11-02 01:58:45,2016-11-02 05:34:02
IS,Inconsistent iterator interface in regards to label names,The interface of the different iterators in is inconsistent in regards to the handling of label names While for ImageRecordIter and MNISTIter the label name is specified via the label name argument NDArrayIter takes a more general approach and accepts a dictionary made up of label name labels It might be desirable to use the more general interface in all cases,,"leezu,piiswrong",2016-11-02 07:32:54,2016-11-02 07:47:26
IS,Distributed training on single machine error,I use a 4 card machine to test distributed training If I start 3 workers 1 server 1 scheduler every worker use different card Everything works OK If I start 4 workers instead of 3 there will be randomly a worker have following error 13 34 50 data00 tiger mxnet build dmlc core include dmlc logging h 235 13 34 50 data00 tiger mxnet build mshadow mshadow str eam gpu inl h 45 Check failed e cudaSuccess CUDA an illegal memory access was encountered 13 34 50 data00 tiger mxnet build dmlc core include dmlc logging h 235 13 34 50 src engine threaded engine h 306 13 34 50 data00 tiger mxnet build mshadow mshadow stream gpu inl h 45 Check failed e cudaSuccess CUDA an illegal memory access was en countered An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment vari able MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrac e will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 13 34 50 src engine threaded engine h 306 13 34 50 data00 tiger mxnet build mshadow mshadow stream gpu inl h 45 Check failed e cudaSuccess CUDA an illegal memory access was encountered An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment vari able MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrac e will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,"xlvector,xlvector,xlvector",2016-11-02 06:11:52,2016-11-02 08:21:51
IS,what does mx sym Embedding do for vocab index data,Hi I am developing for text classification using cnn based on the python example provided I needed to understand what does mx sym Embedding do I have looked at but could not follow Here is my steps I take a sentence Mxnet is cool and first i pad it to fixed length 56 and then convert each word to vocabulary index Hence the input to cnn will look like 1 3450 2303 0 0 0 This is fed into mx sym Embedding as per the exmple embed layer mx sym Embedding data input x input dim vocab size output dim num embed name 'vocab embed' here vocab size is 18765 and num embed is 300 My question is what is num embed and what will be the output of mx sym Embedding How will the result look like,,"zihaolucky,CNevd,pluskid",2016-10-19 20:16:07,2016-11-02 16:26:15
IS,NameError name 'CNNModel' is not defined,Hi I am getting an error in the cnn text classification example I believe the error is how the return statement is defined as i was able to run the setup cnn model function step by step I am new to python so i am not able to catch the issue error NameError Traceback most recent call last ipython input 79 41ff8a3dd1cd in module 1 cnn model setup cnn model mx cpu batch size sentence size num embed vocab size dropout 0 5 with embedding False ipython input 78 d7501af934ba in setup cnn model ctx batch size sentence size num embed vocab size dropout initializer with embedding 36 label cnn exec arg dict isoftmax label' 37 38 return CNNModel cnn exec cnn exec symbol cnn data data label label param blocks param blocks NameError name 'CNNModel' is not defined code def setup cnn model ctx batch size sentence size num embed vocab size dropout 0 5 initializer mx initializer Uniform 0 1 with embedding True cnn model setup cnn model mx cpu batch size sentence size num embed vocab size dropout 0 5 with embedding False,,qcl6355,2016-10-24 18:24:48,2016-11-02 16:28:50
IS,Fix the parameters of first few layers,There are good examples on how to finetune from a pretrained model for MXNet However in these examples actually the gradient of the first few layers were still computed during the training just the learning rate is different But I actually only want to train the last layer and fix the parameters of the first few convolution layers which would save a lot of time Is there anyway to do it in MXNet,,piiswrong,2016-11-03 05:31:17,2016-11-03 05:34:36
IS,symbo Group do not support return,I am trying to run example rcnn demo py on windows when run the following code group mx symbol Group rois cls prob bbox pred return group an error occured node e source get Symbol GetName only works for non grouped symbol It seems symbol group do not support return How to fix this,,piiswrong,2016-11-03 02:23:45,2016-11-03 06:04:11
IS,Dynamic batch size,Dynamic batch size is naturally supported in Tensorflow however not in MXNet Is it possible to implement such feature in MXNet and how Also is there any way to share weights and bias params between two executors except for copying param arrays,,piiswrong,2016-09-20 03:47:00,2016-11-03 07:32:28
IS,Cannot train on multiple nodes because port blocked by firewall,I followed the tutorial distributed training with multiple machines to try to train example image classification train mnist py I failed with launcher mpi and ssh I can successfully run the script it never trains nor does it report any error It just stuck there until I kill it manually I set environment variables on all of the nodes export DMLC INTERFACE em3 export PS VERBOSE 2 hosts 158 1xx x 80 158 1xx x 81 SSH While the script is running training processes are distributed to remover servers Nothing happens afterwards tools launch py launcher ssh n 2 s 1 H hosts python train mnist py network lenet gpus 0 1 2 3 kv store dist sync 17 16 55 src van cc 69 Bind to role scheduler id 1 ip 158 1xx x 80 port 9091 is recovery 0 17 17 00 src van cc 155 1 Meta request 0 timestamp 0 control cmd ADD NODE node role server ip 158 1xx x 80 port 50368 is recovery 0 17 17 00 src van cc 155 1 Meta request 0 timestamp 0 control cmd ADD NODE node role worker ip 158 1xx x 80 port 51877 is recovery 0 Killed by signal 2 Killed by signal 2 Killed by signal 2 2016 11 01 17 22 15 238 INFO Stop luancher Then I try to increase the number of worker tools launch py launcher ssh n 4 s 1 H hosts python train mnist py network lenet gpus 0 1 2 3 kv store dist sync 17 25 19 src van cc 69 Bind to role scheduler id 1 ip 158 1xx x 80 port 9092 is recovery 0 17 25 24 src van cc 155 1 Meta request 0 timestamp 0 control cmd ADD NODE node role worker ip 158 1xx x 80 port 36202 is recovery 0 17 25 29 src van cc 155 1 Meta request 0 timestamp 0 control cmd ADD NODE node role server ip 158 1xx x 80 port 52936 is recovery 0 17 25 29 src van cc 155 1 Meta request 0 timestamp 0 control cmd ADD NODE node role worker ip 158 1xx x 80 port 45130 is recovery 0 Killed by signal 2 Killed by signal 2 Killed by signal 2 Killed by signal 2 Killed by signal 2 2016 11 01 17 28 41 265 INFO Stop luancher I find that all roles are assigned to 158 1xx x 80 only is it normal MPI Open MPI 1 10 1 Training processes can not even launch at remote server tools launch py launcher mpi n 4 s 1 H hosts python train mnist py network lenet gpus 0 1 2 3 kv store dist sync 2016 11 01 17 35 05 797 INFO Start 4 workers by mpirun 17 35 06 src van cc 69 Bind to role scheduler id 1 ip 158 182 9 80 port 9091 is recovery 0 2016 11 01 17 49 08 476 INFO Stop luancher I have tested other MPI applications they work fine MPI bandwidth test can reach up to 105 93MB sec Am I missing anything that makes training on multiple nodes fail Please help The document gives little info about this situation Thanks in advance,,"piiswrong,mli,mli",2016-11-01 10:19:37,2016-11-03 08:35:25
IS,Fix R Travis,Seems the R Travis errored during fetching things from CRAN we need to look into what happened can you look into it,,"tqchen,thirdwing",2016-11-03 01:07:45,2016-11-03 17:27:13
IS,backword source id 1 is missed in resnet152 json file,When running pretrained resnet152 json file the error report that backword source id is not defined When looking at resnet50 json file it is defined as 1 for all layers But resnet152 does not have that param anyone tried resnet152 before,,"zhenlinluo,mli,piiswrong,zhenlinluo,mli,tornadomeet",2016-11-02 17:04:28,2016-11-03 17:30:40
PR,add tutorial for class activation maps,A demo script of the method proposed in Zhou Bolei et al Learning Deep Features for Discriminative Localization arXiv preprint arXiv 1512 04150 2015 This method can automatically localize the discriminative regions in an image using global average pooling in CNNs,,"nicklhy,nicklhy,piiswrong,nicklhy",2016-11-04 03:12:10,2016-11-04 05:40:08
IS,failed to repeat the mx callback early stop example from mxnet documentation website for MXnetR,example from the following website library mxnet data BostonHousing package mlbench train ind seq 1 506 3 train x data matrix BostonHousing train ind 14 train y BostonHousing train ind 14 test x data matrix BostonHousing train ind 14 test y BostonHousing train ind 14 data mx symbol Variable data fc1 mx symbol FullyConnected data num hidden 1 lro mx symbol LinearRegressionOutput fc1 mx set seed 0 model mx model FeedForward create lro X train x y train y eval data list data test x label test y ctx mx cpu num round 10 array batch size 20 learning rate 2e 6 momentum 0 9 eval metric mx metric rmse mx callback early stop function eval metric function iteration nbatch env if is null env metric if is null eval metric result env metric get env eval metric if result value eval metric return FALSE return TRUE model mx model FeedForward create lro X train x y train y eval data list data test x label test y ctx mx cpu num round 10 array batch size 20 learning rate 2e 6 momentum 0 9 eval metric mx metric rmse epoch end callback mx callback early stop 10 error message Start training with 1 devices 1 Train rmse 18 5897974180201 1 Validation rmse 13 5555190846661 Error in epoch end callback iteration 0 environment verbose verbose unused argument verbose verbose In addition Warning message In mx model select layout train X y Auto detect layout of input matrix use rowmajor Called from epoch end callback iteration 0 environment verbose verbose Browse 1 Q,,,2016-11-01 06:26:42,2016-11-04 07:40:35
IS,CNN Text Classification Example Error,qcl6355 I am successful in running CNN Text Classification example upto 8 iterations then it error is out I think it is due to this line m symbol save 'checkpoint s symbol json' prefix in the train cnn function I am using python 3 and mxnet cpu version I have already changed all print statements to be python 3 compliant I appreciate all the help error Iter 0 Train Time 118 9245674610138 Training Accuracy 64 60103626943005 Dev Accuracy thus far 73 0 Iter 1 Train Time 123 77794885635376 Training Accuracy 82 02072538860104 Dev Accuracy thus far 74 1 Iter 2 Train Time 126 14777970314026 Training Accuracy 90 92227979274611 Dev Accuracy thus far 74 0 Iter 3 Train Time 126 88927102088928 Training Accuracy 94 48704663212435 Dev Accuracy thus far 75 7 Iter 4 Train Time 122 98363375663757 Training Accuracy 95 96891191709845 Dev Accuracy thus far 75 3 Iter 5 Train Time 123 61021280288696 Training Accuracy 97 35751295336787 Dev Accuracy thus far 75 9 Iter 6 Train Time 123 61346220970154 Training Accuracy 97 28497409326425 Dev Accuracy thus far 75 0 Iter 7 Train Time 123 4625952243805 Training Accuracy 97 69948186528498 Dev Accuracy thus far 76 4 Iter 8 Train Time 123 22590136528015 Training Accuracy 98 54922279792746 Dev Accuracy thus far 76 6 MXNetError Traceback most recent call last ipython input 26 fef1d47d8af8 in module 1 train cnn cnn model x train y train x dev y dev batch size ipython input 24 7617b641ca21 in train cnn model X train batch y train batch X dev batch y dev batch batch size optimizer max grad norm learning rate epoch 162 if iteration 1 10 0 163 prefix 'cnn' 164 m symbol save 'checkpoint s symbol json' prefix 165 save dict 'arg s' k v for k v in m cnn exec arg dict items 166 save dict update 'aux s' k v for k v in m cnn exec aux dict items C Users amitb Anaconda3 lib site packages mxnet 0 7 0 py3 5 egg mxnet symbol py in save self fname 559 if not isinstance fname string types 560 raise TypeError 'fname need to be string' 561 check call LIB MXSymbolSaveToFile self handle c str fname 562 563 def tojson self C Users amitb Anaconda3 lib site packages mxnet 0 7 0 py3 5 egg mxnet base py in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 def c str string MXNetError 13 04 03 D chhong mxnet dmlc core src io local filesys cc 149 Check failed allow null LocalFileSystem fail to open checkpoint cnn symbol json code def train cnn model X train batch y train batch X dev batch y dev batch batch size optimizer 'rmsprop' max grad norm 5 0 learning rate 0 0005 epoch 200 m model create optimizer opt mx optimizer create optimizer opt lr learning rate updater mx optimizer get updater opt for iteration in range epoch tic time time num correct 0 num total 0 for begin in range 0 X train batch shape 0 batch size batchX X train batch begin begin batch size batchY y train batch begin begin batch size if batchX shape 0 batch size continue m data batchX m label batchY forward m cnn exec forward is train True backward m cnn exec backward eval on training data num correct sum batchY np argmax m cnn exec outputs 0 asnumpy axis 1 num total len batchY update weights norm 0 for idx weight grad name in m param blocks grad batch size l2 norm mx nd norm grad asscalar norm l2 norm l2 norm norm math sqrt norm for idx weight grad name in m param blocks if norm max grad norm grad max grad norm norm updater idx grad weight reset gradient to zero grad 0 0 decay learning rate if iteration 50 0 and iteration 0 opt lr 0 5 print logs areset learning rate to g' opt lr print areset learning rate to ' format opt lr file logs end of training loop toc time time train time toc tic train acc num correct 100 float num total saving checkpoint if iteration 1 10 0 prefix 'cnn' m symbol save 'checkpoint s symbol json' prefix save dict 'arg s' k v for k v in m cnn exec arg dict items save dict update 'aux s' k v for k v in m cnn exec aux dict items param name 'checkpoint s 04d params' prefix iteration mx nd save param name save dict print logs 'Saved checkpoint to s' param name print 'Saved checkpoint to ' format param name file logs evaluate on dev set num correct 0 num total 0 for begin in range 0 X dev batch shape 0 batch size batchX X dev batch begin begin batch size batchY y dev batch begin begin batch size if batchX shape 0 batch size continue m data batchX m cnn exec forward is train False num correct sum batchY np argmax m cnn exec outputs 0 asnumpy axis 1 num total len batchY dev acc num correct 100 float num total print logs 'Iter d Train Time 3fs Training Accuracy 3f Dev Accuracy thus far 3f' iteration train time train acc dev acc print 'Iter Train Time Training Accuracy Dev Accuracy thus far ' format iteration train time train acc dev acc file logs,,,2016-11-02 19:45:40,2016-11-04 16:24:03
IS,Python 3 encoding error in operator CustomOpPropInfo creator,I have encountered the following error using python 3,,"peterzcc,piiswrong,peterzcc",2016-11-01 07:59:36,2016-11-04 22:19:01
IS,Ca not use self defined optimizer when training with multiple nodes,When I try the example with multiple nodes the error shows as follows File ctypes callbacks c line 314 in 'calling callback function' File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet kvstore server py line 38 in server controller optimizer pickle loads cmd body File usr lib64 python2 7 pickle py line 1382 in loads return Unpickler file load File usr lib64 python2 7 pickle py line 858 in load dispatch key self File usr lib64 python2 7 pickle py line 1090 in load global klass self find class module name File usr lib64 python2 7 pickle py line 1126 in find class klass getattr mod name AttributeError 'module' object has no attribute 'Nesterov' How to pass the self defined optimizer to kv store server Thanks,,"piiswrong,mli,piiswrong,piiswrong,mli",2016-11-03 10:55:12,2016-11-05 03:52:28
PR,Add introduction of class active maps in example README md,,,nicklhy,2016-11-04 06:42:39,2016-11-05 19:42:16
IS,install error in ubuntu 16 04,dear team when i try to install mxnet on ubuntu 16 04 with R language i encounter some problems as follows mxnet R package Rscript e library devtools library methods options repos c CRAN '' install deps dependencies TRUE Error in download file url destfile method mode wb cannot download all files Warning messages 1 In download file url destfile method mode wb downloaded length 131072 reported length 3505013 2 In download file url destfile method mode wb URL '' status was 'Failure when receiving data from the peer' Warning in download packages x name destdir dest dir repos x repos download of package DiagrammeR failed Error in download packages x name destdir dest dir repos x repos Calls install deps FUN remote download remote download cran remote best,,thirdwing,2016-11-05 09:16:09,2016-11-05 21:38:59
IS,Creat a NDarray in GPU and return zeros Prebuilt in win10,import mxnet as mx a mx nd ones 2 3 mx gpu print a 2 asnumpy 0 0 0 0 0 0 import mxnet as mx a mx nd ones 2 3 mx cpu print a 2 asnumpy 2 2 2 2 2 2 But the ndarray in cpu is correct i'm using the prebuilt version in win10 vs2015 cuda8 0 cudnn v4,,piiswrong,2016-11-02 08:51:34,2016-11-06 03:41:05
PR,scala add example Gan,follows and,,Ldpe2G,2016-11-05 10:19:48,2016-11-06 05:30:24
IS,How to build MXNet in debug mode,It looks like if I set an environment variable called DEBUG that it changes some aspects of the build Is there more to it than this,,"leopd,thirdwing",2016-11-05 20:27:44,2016-11-06 22:12:19
PR,Fix link and head issue,Fix broken links and quick fix for heading issue,,"kevinthesun,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,kevinthesun,piiswrong,piiswrong,kevinthesun",2016-11-04 17:41:33,2016-11-06 22:51:07
PR,Feature pad3d,Added support for padding 3d images to mx Pad,,sbodenstein,2016-11-06 12:44:46,2016-11-06 22:51:18
PR,Fix linkand head issue,Update links except,,kevinthesun,2016-11-06 22:58:58,2016-11-07 03:31:52
IS,How to uninstall mxnet thoroughly and reinstall it again on Ubuntu,I had installed mxnet in Quick Installation way But it could not use gpu I thought that there must be sth wrong in installation And I installed it again without reading the doc which introduced how to edit config mk And i found two same lines PYHONPATH In the end of my bashrc And then my Theano did not work too After i deleted those two lines Theano worked again But when I make the config mk it got erro I want to reinstall mxnet thoroughly but I do not know what had been edited by mxnet in my system What can I do,,,2016-11-07 00:58:00,2016-11-07 06:04:09
IS,Ca not use GPU,I'm using Ubuntu16 04 gtx1060 cuda8 0 cudnnv5 1 My Theano and TF worked well with GPU I have complied config mk and got following last several lines,,piiswrong,2016-11-07 06:09:50,2016-11-07 06:15:40
IS,mx io ImageRecordIter For Regression,On classification images mxnet provide very good IO for training classification model including very good data augmentation However for linear regression model label be continuous variables for example the steer angle prediction from camera images there is no such IO Is there any tutorial on how to make it Best zmonoid,,"piiswrong,piiswrong",2016-11-05 07:22:30,2016-11-07 12:01:38
PR,Fix incorrect assert in InstanceNorm layer,,,"sbodenstein,piiswrong,sbodenstein",2016-11-07 12:25:50,2016-11-07 18:31:52
IS,Training Accuracy and Validation Accuracy problem,Hello When I ran some mlp experiments using mxnet I found it strange even I set the validation data the same as the training data the outputted accuracy which I think should be exactly the same are very different I also tried this in the example module mnist mlp py by setting it as Should the Train accuracy be the same as the validation accuracy each epoch Thanks,,miguelgfierro,2016-11-07 14:20:24,2016-11-08 02:09:14
PR,Fix setup toc,Quick fix for setup page table of contents,,kevinthesun,2016-11-08 01:32:38,2016-11-08 02:38:10
IS,Error when build with MXNet in macOS Sierra,my config mk is Could anyone help to solve this,,"burness,burness,piiswrong,burness",2016-11-06 15:49:11,2016-11-08 06:09:29
IS,how to get result of intermediate network,,,,2016-11-08 11:23:06,2016-11-08 11:32:28
PR,Fix bugs when running the code in Windows,Use Python libraries for downloading unzipping and removing instead of system commands which do not support Windows,,WellyZhang,2016-11-08 13:03:18,2016-11-08 13:05:52
IS,Would predict and NDArray toArray cause BLOCK,Hi I use MXNet Scala to deploy my model for a while and it works great the engineering team stress tested my service and found the service could stop response after a while They printed the info with jstack,,"zihaolucky,zihaolucky,yzhliu",2016-11-01 09:57:16,2016-11-08 15:25:02
PR,move julia test to jenkins,,,"piiswrong,thirdwing",2016-11-08 20:36:42,2016-11-08 22:58:49
PR,Updates setup instructions for mac to include pkg config installation,This PR updates setup instructions for mac Current instructions did not work for me out of box I had to install missing pkg config,,madjam,2016-11-08 20:07:30,2016-11-09 01:16:37
PR,CloudFormation template to launch a cluster of EC2 GPU instances base,more detailed documentation will shortly follow,,nswamy,2016-11-09 01:23:07,2016-11-09 01:25:49
PR,Fix bugs when running the code in Windows,fix downloading and unzipping scripts in Windows,,WellyZhang,2016-11-08 13:18:39,2016-11-09 05:32:37
PR,Fix bugs when running the code in Windows,fix bugs in Windows,,WellyZhang,2016-11-09 05:38:46,2016-11-09 07:24:28
IS,the python library loading failed,it turned out to be opneblas issue so close this,,,2016-11-09 08:30:18,2016-11-09 09:25:11
PR,Fixing broken links Fix format issue in set up guide Making how to contribute page more accessible,Fixing broken links Fix format issue in set up guide Making how to contribute page more accessible contribute page more accessible by adding links in multiple relevant pages,,sandeep-krishnamurthy,2016-11-09 16:54:55,2016-11-09 18:25:34
PR,Modifying FindMKL to respect i386 and allow ILP64,These changes should not impact the default behavior of the FindMKL module on 64 bit Linux platforms The major change is that the gnu fortran interface is removed from the list of MKL dependencies when using gcc Add MKL USE ILP64 to allow the ILP64 data model in MKL off by default Add the MKL USE CLUSTER option disabled on 32 bit platforms as the functionality is unavailable Limit the gf interface to instances where gfortran is used,,alextnewman,2016-11-08 13:09:41,2016-11-09 19:51:28
IS,documentation for mx symbol Slicechannel confusing r pac,The R documentation for several function Slicechannel and Concat and possibly others is confusing in that axis int optional default '1' Dimension along which to slice does not mean axis 1 in the R sense col major So is actually dim 1 not dim 1,,,2016-11-09 20:03:43,2016-11-09 21:01:17
PR,Add USE MXNET LIB NAMING for DLL irregularities,Adding a default ON CMake option to activate dynamic library naming irregularities preferred by the MXNet project This allows projects that rely on DLLs using standard file naming conventions to trivially suppress this behavior This should have no impact on existing builds,,alextnewman,2016-11-08 13:12:01,2016-11-09 23:09:38
PR,Apply AWSDoc Css,Updated css for whole website Also some small fixes for source file,,kevinthesun,2016-11-09 22:23:56,2016-11-10 02:28:28
PR,Fix make config mk for aarch64,,,"leoncamel,leoncamel",2016-11-08 13:40:56,2016-11-10 03:44:26
PR,Typo gpu to cuda,,,,2016-11-09 04:53:52,2016-11-10 07:43:10
IS,something wrong with my cnn model,Thanks for your attention I run the cnn text classification example to classify my data to 0 and 1 Here is the result The performance is bad and the acc of dev is almost the same during different epoch pic list path 2Fdata So I think whether there is something wrong with my way of loading data I debug it but find nothing I use the pretrained word2vec to represent the words and set the fixed length for sentences 64 also tried 128 but result almost the same Here are my code and dataset Could you please help me find out what is wrong with it Many thanks data and code list path 2Fdata,,,2016-11-09 07:22:58,2016-11-10 08:29:45
PR,API class and function formatting,Formatting API class and function,,kevinthesun,2016-11-10 21:06:17,2016-11-11 01:41:29
PR,Update CloudFormationTemplate and add Readme,Following are the changes I made Please squash the multiple commits into one with below commit message Thanks Naveen Update Template to launch master and workers in the same Availability zone to avoid network latency Add detailed Readme on how to launch CloudFormation Template,,nswamy,2016-11-11 00:51:59,2016-11-11 05:33:39
IS,mxnet under docker,I meet two problems If I use nvidia image with cuda8 cudnn5 I will get error about driver version less than runtime version If I use nvidia image with cuda7 5 cudnn4 the init step will be very slow and It will fail with following errors 05 22 46 mxnet dmlc core include dmlc logging h 235 05 22 46 mxnet mshadow mshadow random h 344 Check failed status CURAND STATUS SUCCESS CURAND Gen Uniform float failed size 131072 05 22 46 mxnet dmlc core include dmlc logging h 235 05 22 46 src engine threaded engine h 306 05 22 46 mxnet mshadow mshadow random h 344 Check failed status CURAND STATUS SUCCESS CURAND Gen Uniform float failed size 131072 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 05 22 46 src engine threaded engine h 306 05 22 46 mxnet mshadow mshadow random h 344 Check failed status CURAND STATUS SUCCESS CURAND Gen Uniform float failed size 131072 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,"xlvector,piiswrong,mli,xlvector,piiswrong,xlvector,piiswrong",2016-11-06 05:51:29,2016-11-11 06:13:05
IS,how to make amalgamation to work with feature extraction,I have made an amalgamation according to the instructions but when I try to do feature extraction with the libmxnet predict so it failed Errors say home hu MXNet cpp include mxnet cpp executor hpp 59 undefined reference to MXExecutorBindEX' and there are 50 such errors in total Has somebody met similar problems How to fix them Thanks,,,2016-11-11 05:56:17,2016-11-11 08:20:54
PR,Update AMI ids,I have updated the template to use the latest AMI ids that has fixed some issues w r t setting persistence mode of gpus at launch,,nswamy,2016-11-11 08:40:22,2016-11-11 17:30:13
PR,Tutorial page small fix,Change the style of anchors on tutorial page,,kevinthesun,2016-11-11 18:33:24,2016-11-11 18:49:36
PR,Making mxnet introduction more detailed,Adding updated description to mxnet introduction section Minor update to set up guide overview section,,sandeep-krishnamurthy,2016-11-11 14:04:42,2016-11-11 21:59:20
PR,doc changes MKL readme edits fixing 404s,Fixed some formatting issues with MKL README md and reordered two steps that were out of order Fixed some URLs that were redirecting to 404s Caveat I could not find the correct documentation page that corresponds to so I just removed it completely,,andremoeller,2016-11-11 21:28:44,2016-11-12 00:41:01
PR,Fix heading in intro page Fix broken link Bring Python main API refrence headings to top header under API,Fix corrupted heading in intro page Fix broken link in Brought all high level Python API options Module Model Symbol to high level heading under python This helps users to easily access commonly used API references in one click Without this changes users first click on API Python then choose topic of interest,,sandeep-krishnamurthy,2016-11-12 12:31:01,2016-11-12 19:13:41
PR,Add missing initializer,,,"piiswrong,zhreshold",2016-11-12 18:23:46,2016-11-12 19:20:58
PR,Add missing layers used in the SSD example,,,,2016-11-12 17:55:11,2016-11-12 19:21:02
IS,cpp example image classification predict cc can not be debug in vs2013 on win7,I try the cpp example image classification predict cc when I add MXPredCreate to cpp main function the program can not run into first step with a break point How to solve this problem I want to use the c interface rather than python Following is test code,,,2016-11-04 08:49:30,2016-11-12 20:31:41
PR,Less redundant Reshape spec,The core ability this gives is to allow certain kinds of reshape without requiring you to know all the dimensions up front in a more flexible way than the current codes 0 and 1 This make it easier for humans to express certain kinds of ops but also makes it possible for machines to express certain reshapes without having to know exact sizes of irrelevant parts of the input That is what I'm using this for our fork helping our neural network framework unroll recurrent networks more easily and efficiently You typically can prove ahead of time that an unrolling algorithm is correct and for speed reasons you do NOT want to track tensor dimensions during unrolling at run time But Reshape layer demands to know all tensor sizes even when the desired resize operation does not actually depend on them So this PR adds 3 more codes to Reshape that let it figure out more things itself 2 copy all remaining dimensions from input shape to output shape 3 merge two adjacent dimensions in input shape 4 split a single dimension in input shape into two where either one can be inferred if given the other It should be pretty straightforward to add more codes in future like maybe a code for merging all remaining dimensions These are documented formally in the description of target shape with proper indices etc the previous documentation was very vague Here is an example that illustrates all three let is say you want to 'broadcast' an operation across a sequence by simply merging the sequence and batch dimensions together and then doing an ordinary batched op That kind of merge op can be expressed as a Reshape with target 3 2 merge the first two dimensions then copy the rest After you apply the op you will want to do an unmerge which is 4 slen 1 2 Notice that the only static information we needed to know here was the sequence length to unmerge properly The dimensions and rank of the inner tensors was not needed Another example any existing code will be able to replace any sequence of 0s with a single 2,,"taliesinb,piiswrong,taliesinb,taliesinb,piiswrong,taliesinb,sxjscience,taliesinb,sxjscience,taliesinb,piiswrong,taliesinb,sxjscience,taliesinb",2016-11-09 19:29:41,2016-11-12 21:13:58
PR,Scala add rtc,follows Could you help to review this pr,,"Ldpe2G,yzhliu",2016-11-10 09:13:44,2016-11-13 06:21:59
IS,Can not run the example of train cifar10 py on centos6 5,Hi when I run the train cifar10 py I getting the following error 2016 10 31 20 58 27 297 Node 0 start with arguments Namespace batch size 128 data dir 'cifar10 ' gpus None kv store 'local' load epoch None lr 0 05 lr factor 1 lr factor epoch 1 model prefix None network 'inception bn 28 small' num epochs 20 num examples 60000 save model prefix None 20 58 27 src io iter image recordio cc 219 ImageRecordIOParser cifar10 train rec use 4 threads for decoding 20 58 27 src io iter normalize h 218 Cannot find cifar10 mean bin create mean image this will take some time libpng warning Application was compiled with png h from libpng 1 6 17libpng warning Application was compiled with png h from libpng 1 6 17 libpng warning Application is running with png c from libpng 1 2 49 libpng warning Application was compiled with png h from libpng 1 6 17 libpng warning Application is running with png c from libpng 1 2 49 libpng warning Application is running with png c from libpng 1 2 49 libpng error Incompatible libpng version in application and library libpng error Incompatible libpng version in application and library libpng error Incompatible libpng version in application and library 20 58 27 home wjz MXNet mxnet dmlc core include dmlc logging h libpng warning Application was compiled with png h from libpng 1 6 17 libpng warning Application is running with png c from libpng 1 2 49 libpng error Incompatible libpng version in application and library 20 58 27 home wjz MXNet mxnet dmlc core include dmlc logging h 235 20 58 27 src io image aug default cc 254 Check failed static cast index t res rows param data shape 1 static cast index t res cols param data shape 2 input image size smaller than input shape terminate called after throwing an instance of wouldmlc Error' what 20 58 27 src io image aug default cc 254 Check failed static cast index t res rows param data shape 1 static cast index t res cols param data shape 2 input image size smaller than input shape Aborted core dumped How can I solve this problem,,piiswrong,2016-10-31 13:04:59,2016-11-13 07:16:35
IS,error occur in the procedure of 'make' cuda compile generated matrix op,cuda 7 0 gcc 4 8 some error detected when the operation 'make' is executed usr include c 4 8 cmath 356 error expected a usr include c 4 8 cmath 375 error inline specifier allowed on function declarations only usr include c 4 8 cmath 375 error variable std constexpr has already been defined usr include c 4 8 cmath 375 error expected a usr include c 4 8 cmath 406 error inline specifier allowed on function declarations only usr include c 4 8 cmath 406 error variable std constexpr has already been defined usr include c 4 8 cmath 406 error expected a usr include c 4 8 cmath 443 error inline specifier allowed on function declarations only usr include c 4 8 cmath 443 error variable std constexpr has already been defined usr include c 4 8 cmath 443 error expected a Error limit reached 100 errors detected in the compilation of tmp tmpxft 0000572c 00000000 7 matrix op cpp1 ii Compilation terminated CMake Error at cuda compile generated matrix op cu o cmake 264 message Error generating file home tangjianbo Documents mxnet CMakeFiles cuda compile dir src operator cuda compile generated matrix op cu o make 2 CMakeFiles cuda compile dir src operator cuda compile generated matrix op cu o Error 1 make 1 CMakeFiles mxnet dir all Error 2 make all Error 2 how should I fix this problem,,,2016-11-13 04:57:11,2016-11-13 09:38:46
IS,Windows 10 Pro MXNET GPU For R Environment,If someone here has successfully installed the GPU version of MXNET gpu version for R could please post instructions the instructions for install do not work,,,2016-11-08 02:15:34,2016-11-13 16:19:15
IS,Where is the declaration of ctcComputeInfo,When I rebuild mxnet there is an error occurred plugin warpctc warpctc inl h 129 5 error 'ctcComputeInfo' was not declared in this scope So I need someone give me a little help thx,,terrychenism,2016-11-07 03:45:52,2016-11-14 02:50:09
PR,spelling typo fixes,Some of these diffs are from adding newline characters to the last line of the file,,andremoeller,2016-11-14 02:29:18,2016-11-14 05:14:10
IS,Symbol slicing with symbolic slice index,Is there a way to slice a symbolic input where the slice index is also symbolic I am aware of the following method mxnet symbol slice axis args kwargs but this requires the slice index to be an integer I have a situation where the slice index is not known a priori Theano has this feature where the slice indices can also be symbolic variables Any existing methods tricks I can use to do this in mxnet Thanks,,"piiswrong,sxjscience",2016-10-19 23:49:48,2016-11-14 11:08:54
IS,How to predict with dropout on to simulate ensemble for uncertainty,Hi all I would like to make predictions with dropout turned on this will allow repeat predictions to build up a conditional probability distribution providing information on prediction uncertainty This would be extremely useful It seems that dropout is turned off for prediction predict MXFeedForwardModel by default How can I turn it on for prediction I'm using MXNet R package on windows Many thanks,,"sxjscience,sxjscience",2016-11-14 10:13:15,2016-11-14 13:55:42
IS,Comparison of routine in 1 GPU vs 4 GPUs,I'm working with the ImageNet dataset in R I was comparing the performance with 1 GPU vs 4 GPUs assuming the rest of the parameters are the same Using 1 GPU 1 iteration takes 6 30h while with 4 GPUs 1 iteration takes 2h Therefore using n GPU is not n times faster I would be interested in understanding what are the botlenecks in the training function L94 Can somebody give some clarity on this,,"miguelgfierro,piiswrong,miguelgfierro,piiswrong,miguelgfierro,miguelgfierro",2016-11-07 18:00:05,2016-11-14 16:50:54
IS,How to enable GPU P2P,When I train with several GPUs I get this warning How can I enable the GPUs to have direct access,,"miguelgfierro,miguelgfierro,piiswrong,miguelgfierro,kli-nlpr",2016-11-14 13:46:26,2016-11-14 18:41:48
PR,Update new AMI Ids,,,"nswamy,piiswrong,mli,piiswrong,piiswrong,nswamy",2016-11-14 18:27:19,2016-11-14 22:24:44
PR,Added links recent examples and notebook,added links to recent examples and notebooks corrected the author is github info for license plate recognition sorry my bad,,phunterlau,2016-11-14 23:47:53,2016-11-15 02:34:52
PR,reorded cv tutorials and added descriptions that reflect content and,Reordered examples and added descriptions to the CV examples relaying their content and difficulty,,"arank,leopd,arank,arank,leopd",2016-11-14 18:06:37,2016-11-15 02:42:27
IS,some problem to write a dataiter like this,Goal Every column of csv file is a variable So i read data from csv file and write a iterator so as to assign every variable which is every column Problem When i run the program it cores dump Code,,,2016-11-14 08:53:51,2016-11-15 07:39:25
IS,i encounted an issue when building mxnet on ubuntu 16 04 and here is the error information,home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from include mxnet mxrtc h 9 from src common mxrtc cc 7 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from src resource cc 9 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from include mxnet io h 15 from src io iter mnist cc 6 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from include mxnet io h 15 from src io iter image recordio cc 6 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from include mxnet io h 15 from src io iter csv cc 6 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from src io image aug default cc 6 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated In file included from home keithyin Downloads mxnet1 mshadow mshadow tensor h 16 0 from include mxnet base h 13 from include mxnet io h 15 from src io io cc 3 home keithyin Downloads mxnet1 mshadow mshadow base h 146 20 fatal error cuda h No such file or directory include cuda h compilation terminated Makefile 151 recipe for target 'build src ndarray ndarray function o' failed make build src ndarray ndarray function o Error 1 make Waiting for unfinished jobs Makefile 151 recipe for target 'build src io iter csv o' failed make build src io iter csv o Error 1 Makefile 151 recipe for target 'build src common mxrtc o' failed make build src common mxrtc o Error 1 Makefile 151 recipe for target 'build src io iter image recordio o' failed make build src io iter image recordio o Error 1 Makefile 151 recipe for target 'build src resource o' failed make build src resource o Error 1 Makefile 151 recipe for target 'build src io iter mnist o' failed make build src io iter mnist o Error 1 Makefile 151 recipe for target 'build src io image aug default o' failed make build src io image aug default o Error 1 Makefile 151 recipe for target 'build src io io o' failed make build src io io o Error 1,,"piiswrong,miguelgfierro",2016-09-14 07:34:27,2016-11-15 11:20:35
PR,Docs tutorials page reformatting TOC,Changing the CV examples to not be headers so they do not pollute the table of contents Also adding a couple explanations and academic citations,,leopd,2016-11-15 17:56:47,2016-11-15 18:29:32
PR,fix im2rec index,,,piiswrong,2016-11-15 18:51:16,2016-11-15 18:51:28
PR,Improve Python API reference,WHatever system renders the python class docstrings into HTML is failing in some cases because there is not a blank line between the class description and the line that says Parameters n The result is e g mxnet io NDArrayIter where the description writes out the list of parameters in paragraph form like Taking NDArray or numpy array to get dataiter param data NDArrayIter supports single or This change just adds blank lines to a bunch of docstrings and should fix all these issues,,leopd,2016-11-15 22:50:46,2016-11-15 23:26:59
PR,update url,,,piiswrong,2016-11-15 22:51:04,2016-11-15 23:39:52
PR,R doc changes,Small changes Removing useless lines from R vignettes Fixing link for r tutorials Bigger change Adding an R reference manual This reference manual was generated from R package using rd2pdf The Makefile has a line to copy the pdf into the HTML tree so it is hosted at mxnet io api r mxnet r reference manual pdf Ideally the PDF would be built in the sphinx doc build rather than being added to the repo but I'm not sure how to manage the R dependencies to do this with Sphinx At any rate this seems like a decent interim measure so R has some API doc support in the short run better than nothing I suppose Alternative solutions welcome,,"andremoeller,piiswrong,andremoeller,andremoeller",2016-11-15 23:24:25,2016-11-15 23:43:24
PR,Fixing bug in NDArrayIter provide data,One word bug fix The NDArrayIter class which iterates over data loaded in memory was returning a descriptor of the label set when asked for a description of the data set,,leopd,2016-11-16 00:12:31,2016-11-16 00:16:02
PR,doc build mxnet notebooks into mxnet io,tested on our doc serving machine i will leave to to organize the index page,,mli,2016-11-16 00:29:05,2016-11-16 04:12:16
PR,New callback interface for training visualization,Neural style transfer notebook example of refactored live notebook charting Trivial notebook to run train cifar10 working notebook structure in place for live charting Needs module cb change Module callbacks almost working Epoch numbers NaN Module CNN example for notebook charts CIFAR10 training notebook for module works again Matching callback code to notebook Add new notebook Small fix Change notebook name Clean merge,,"kevinthesun,piiswrong,piiswrong,piiswrong,kevinthesun,leopd,piiswrong,piiswrong,kevinthesun,leopd,leopd,kevinthesun",2016-11-11 17:25:43,2016-11-16 07:20:47
PR,Only enable SSE2 flags when detected,SSE2 flags were always being enabled making the feature test irrelevant This should make it so SSE2 is enabled when detected Also reformatted the OpenMP codeblock to fit the double space styling of the rest of the file,,alextnewman,2016-11-16 15:23:02,2016-11-16 17:57:42
PR,fix example ssd variance vector initialization,,,zhreshold,2016-11-15 20:04:30,2016-11-16 18:00:18
PR,Making grammar correction spell checks and language tone improvement,Making grammar correction spell check and language tone improvement across doc pages under getting started tutorials how to community,,"sandeep-krishnamurthy,piiswrong,sandeep-krishnamurthy",2016-11-15 13:29:58,2016-11-16 18:08:09
PR,Cleaned up tutorial index linked new content moved language specific feature walkthru to the how to section,,,"arank,mli",2016-11-16 20:04:23,2016-11-16 22:24:44
PR,Adding link to R reference manual,Builds the PDF from mxnet R package puts it in api r links to it from api r index I checked that I can get to the pdf from 8008 api r mxnet r pdf Dependencies needed on build server are R and LaTeX If you are using apt you can apt get install r base no other R packages are required And also a LaTeX distribution is required I did not change the Dockerfile in mxnet docs since that would require installing LaTeX which would greatly increase the image size 3 GB for basically no benefit Also fixed the r tutorials link to r tutorials,,"andremoeller,mli",2016-11-16 23:21:51,2016-11-16 23:39:57
PR,MXNet Benchmarking script,,,"nswamy,piiswrong,piiswrong,nswamy",2016-11-16 01:57:22,2016-11-17 00:11:18
PR,fixing links,Fixing various links Also removing one TODO from kvstore md the link just links back to the same page and another TODO for installation on Amazon Linux from setup md better to include it when it is done or otherwise it makes the docs seem unfinished,,andremoeller,2016-11-17 01:06:16,2016-11-17 01:32:35
IS,Access NDArray beyond axis 0,I got segmentation fault when accessing entries in 2D nd array but could not figure out why Can anyone help on this Many thanks a mx nd ones 100 100 a 2 3 NDArray 0 a 2 3 asnumpy zsh segmentation fault python2 7 a mx nd ones 100 100 b a reshape 10000 b 200 NDArray 0 b 200 asnumpy zsh segmentation fault python2 7 Thanks,,"chaoyuaw,piiswrong,chaoyuaw",2016-11-16 19:27:15,2016-11-17 01:54:41
PR,Pass CMAKE C FLAGS to CMAKE CXX FLAGS,This patch passes CMAKE C FLAGS to CMAKE CXX FLAGS Thus the debug build works on c source files,,EricFisher,2016-11-16 08:35:43,2016-11-17 03:29:49
PR,Expandable toc,Build expandable sidebar,,"kevinthesun,piiswrong,kevinthesun,kevinthesun,sandeep-krishnamurthy,andremoeller",2016-11-17 05:31:05,2016-11-17 05:35:42
PR,Merge pull request 1 from dmlc master,new,,,2016-11-17 07:05:43,2016-11-17 07:11:06
PR,Add me to contributors,,,EricFisher,2016-11-17 06:47:10,2016-11-17 09:06:25
PR,Fix decoder for speech demo,The module wo not support empty list for label Need to set it to None,,"yzhang87,yzhang87,pluskid",2016-11-17 04:01:06,2016-11-17 15:36:18
PR,Grammar and language correction to pages under architecture Broken link fix Avoid 404s for old packages and system sections,Grammar and language correction to pages under architecture Fixing broken links Adding redirection place holders for old packages and system sections to redirect to api and architecture sections,,sandeep-krishnamurthy,2016-11-17 16:57:46,2016-11-17 18:55:04
PR,spelling fixes on tutorials page removing bad link to mxnet notebooks,The echo in build notebooks sh results in a bad link as well as n nThis showing up on the page the n s are not converted to newline characters,,andremoeller,2016-11-17 19:41:26,2016-11-17 19:49:47
IS,Use python six library,I just ran into an incompatibility between python 2 3 In this case it was easy enough to work around without any performance issues But in general these kinds of things are easier to deal with by using the python library six which gives a bunch of simple helper functions that work in both py2 py3 Any concerns with taking a dependency on six It is another library that people will need Or because it is a single file we could conceivably just be copy it into our codebase It is MIT licensed,,"leopd,piiswrong,leopd,tqchen,leopd",2016-11-16 18:03:43,2016-11-17 22:54:10
PR,url fixes,,,andremoeller,2016-11-17 23:14:16,2016-11-18 03:56:21
IS,SSD example make error,When I compiled mxnet with the ssd example I got erro example ssd operator multibox target inl h 129 179 VarsInfo brace enclosed initializer list DMLC DECLARE FIELD variances set default VarsInfo 0 1 0 1 0 2 0 2 It looks like it is in my machine std vector initialization method is not correct I put the code into the following float my 0 1 0 1 0 2 0 2 std vector float four my my sizeof my sizeof float DMLC DECLARE FIELD variances set default VarsInfo four If I'm wrong please let me know My machine g version g GCC 4 8 5 20150623 Red Hat 4 8 5 4,,"piiswrong,zhreshold,zhreshold,piiswrong,zhreshold",2016-11-15 04:44:26,2016-11-18 03:59:32
PR,refactor test util,,,piiswrong,2016-11-16 22:22:56,2016-11-18 06:19:38
PR,Glog,,,piiswrong,2016-11-18 06:21:37,2016-11-18 08:26:06
PR,Updated URL,,,miguelgfierro,2016-11-18 12:39:44,2016-11-18 18:34:04
PR,Grammar and language correction under API section Restructuring Getting Started Page,Grammar spell check and language correction of all pages under API section Restructuring getting started page to be more easy for users to get started Removed MNIST tutorial from the page and added links to 2 tutorials one for beginner and one for advanced,,sandeep-krishnamurthy,2016-11-18 16:44:53,2016-11-18 19:24:29
PR,module grad req fix,,,piiswrong,2016-11-18 06:21:22,2016-11-18 20:06:18
PR,add aws logo in the front page,,,mli,2016-11-18 18:25:44,2016-11-18 20:06:30
PR,R tutorials link to tutorials for generated tutorials,,,andremoeller,2016-11-18 20:19:06,2016-11-18 20:28:03
PR,adding scala docs,scaladoc needs to be installed on the docs build server I think that is installed alongside scala In any case please ensure that you can run scaladoc on the build server,,"andremoeller,piiswrong,mli,andremoeller,mli,andremoeller,Ldpe2G",2016-11-18 05:22:49,2016-11-18 20:48:28
PR,Torch and Performance how tos reformatted,Reformatting these pages since they are particularly ugly Also readding some changes that were clobbered out by what must have been one big unselective git rebase s recursive X ours,,andremoeller,2016-11-18 22:28:57,2016-11-18 23:01:08
PR,filling out scala and r API pages,They are too bare and the API doc links are not prominent enough Moving API links to top of page adding the tensor computation code from get started Also adding Neural Style We do not have any scala tutorials Made for this,,andremoeller,2016-11-18 22:48:24,2016-11-18 23:01:33
PR,pointing pretrained models link to predict imagenet notebook,This page is underdeveloped and is not a how to despite being listed as a how to This is much better I have a PR in mxnet notebooks that updates this and links to the MXNet model gallery,,andremoeller,2016-11-19 01:43:32,2016-11-19 04:40:14
PR,mx viz plot network renders all inputs not just first,Improved graphviz rendering so that it shows all inputs to the net including the label rendering them as ovals Previously it would only show the very first input in the head e g the plain net symbol from the Matrix Factorization example used to look like this screen shot 2016 11 18 at 4 34 56 pm and now it looks like this screen shot 2016 11 18 at 4 34 34 pm I also added an option to visualize the weights as nodes on the graph too if you want but of course turned it off by default because it usually makes a big mess Also cleaned up some lint warnings while I was in there,,leopd,2016-11-19 00:52:35,2016-11-19 04:41:10
IS,How to get the output of other layers on Android,I load my own model on android but i want to get features from the model instead of the last layer I do this as following But something wrong happens A libc Fatal signal 11 SIGSEGV code 1 fault addr 0x0 in tid 23311 AsyncTask 1 I also test in project WhatThis the same error occurs Could anyone give me some suggestions,,"tqchen,leopd,tqchen,tqchen,leopd",2016-11-18 10:57:20,2016-11-19 07:35:57
PR,Scala Visualization plotNetwork renders all inputs not just first,follows 3896,,Ldpe2G,2016-11-19 09:59:04,2016-11-19 19:27:14
PR,Improve sidebar,If the page has a single big head h1 such as tutorial page and how to page the sidebar will automatically expand to level 2,,kevinthesun,2016-11-19 06:57:31,2016-11-19 19:27:36
PR,WIP json versioning,,,"piiswrong,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen",2016-11-18 23:27:25,2016-11-19 21:49:25
PR,Update setup sh,,,piiswrong,2016-11-19 21:52:13,2016-11-20 02:45:27
PR,Update setup sh,,,piiswrong,2016-11-20 02:46:34,2016-11-20 04:19:57
PR,R ImageNet training with ResNet v2 Inception resnet and others for R,Several things to notice I did not put all CNN architectures only the ones that are wroking for me The default network is ResNet 18 with it I got a top 5 accuracy of 93 using Azure VMs with 4 GPUs,,"miguelgfierro,piiswrong,thirdwing,tornadomeet,miguelgfierro,miguelgfierro",2016-11-15 10:35:53,2016-11-20 18:45:49
PR,Fix broken links Add link to stack overflow Make supported language claim consistent and generic,Another round of clean up of broken links Added link to stack overflow on community page for users to ask questions and view questions tagged with mxnet Made supported language claims to be consistent across home page getting started and set up page Made it more generic with list of supported programming language name instead of using number 7,,sandeep-krishnamurthy,2016-11-20 17:48:31,2016-11-20 21:44:16
PR,Cleaned up formatting of tutorial and added language tutorials to how to,,,"arank,piiswrong,piiswrong",2016-11-17 17:29:49,2016-11-20 21:44:33
PR,Remove over optimistic shape inference in broadcast binary op,,,"tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen,tqchen,piiswrong,tqchen,tqchen,piiswrong,tqchen,piiswrong,tqchen",2016-11-04 04:51:09,2016-11-20 23:12:17
PR,fix PrefetchingIter doc string error,I had used PrefetchingIter to load data It makes me confused since the doc string is wrong In fact the iterator must implement a next method but not read method,,vsooda,2016-11-21 03:00:46,2016-11-21 05:00:21
PR,fix 3892 adding MNIST scala tutorial,Adapted from the scala package readme,,andremoeller,2016-11-21 04:47:55,2016-11-21 05:00:59
IS,Docs no scala tutorials on tutorials page,There are Scala examples under scala packages examples but no Scala tutorials to host on the tutorials page or in mxnet notebooks that could be hosted on the tutorials page Scala tutorials would instruct Scala users how to build something with MXNet,,andremoeller,2016-11-18 22:48:09,2016-11-21 05:01:02
PR,Formalize broadcast and elemwise ops,piiswrong as per discussion,,"tqchen,tqchen",2016-11-21 00:35:53,2016-11-21 06:23:15
IS,Infer shape error in rhs argument,Hi guys I'm composing a resnet model on the basis of symbol resnet py code I added some pooling layers between the resnet blocks and now I got the following error InferShape Error in plus4 is rhs argument When I change pad option in pooling layers or change number of filters in the conv layer the number in the error message changes to another one e g plus8 With a certain number of the filters and pad option the error disappears This is very strange I do not understand why I can not choose those number of filters and padding which I want It should not be a problem Could you please help me with this issue,,piiswrong,2016-11-18 13:54:02,2016-11-21 10:49:21
PR,Adding more details about supported programming languages,Fixing issue in home page about multiple programming language support Added links in how to for how to use pre trained models with javascript and matlab,,sandeep-krishnamurthy,2016-11-21 16:57:39,2016-11-21 17:58:22
PR,Adding links to AWS blogs talking about using mxnet on AWS resources preconfigured with mxnet,Adding links to AWS blogs talking about using mxnet on AWS resources preconfigured with mxnet Minor typo fix Frequently Ask Questions Frequently Asked Questions,,sandeep-krishnamurthy,2016-11-21 18:07:01,2016-11-21 18:13:36
IS,training accuracy always be 1 00000,It is a two class task with inception v3 train from the scratch the num of 0 and 1 is 3 1 and I set the shuffle when gen the dataiter Anyone have met this problem,,"burness,tornadomeet,burness",2016-11-18 11:42:13,2016-11-21 20:37:59
PR,Regression fix,Fix index page and sidebar issues due to the change of contents,,kevinthesun,2016-11-21 20:39:04,2016-11-21 22:32:56
PR,adding link to mxnet notebooks to generated tutorials,,,andremoeller,2016-11-21 21:54:08,2016-11-21 22:35:31
PR,Fixed comments in README for R package,,,miguelgfierro,2016-11-21 22:54:52,2016-11-21 23:15:06
PR,CMake Add preprocessor defines for CUDA NVRTC,The USE CUDA flag should now enable MXNET USE CUDA and MXNET USE NVRTC preprocessor defines as both libraries are located at cmake generation time,,alextnewman,2016-11-21 23:12:43,2016-11-22 06:35:49
IS,Should I set label shapes to None or when calling mx module bind if the network does not need label,If I set label shapes to None I get an error like,,"nicklhy,piiswrong",2016-11-18 01:41:16,2016-11-22 09:41:29
IS,How do I set BucketingModule to make true that the input gradients can be computed,I implement seq2seq learning with mxnet and I found that it is difficult to get the gradients of initial h and initial c for the reason that Embedding layer does not support calculate data gradient We know the inputs of the decoder are indices of input sequences and the initial hidden states which is the last hidden states of the encoder So we want get the gradients of initial hidden states to make backpropagation for encoder I have carefully read the BucketingModule but I can not find a way to get a good solution HERE is my code def build lstm decoder self is train True bef args None def gen dec sym seq len sym lstm unroll 1 seq len len vocab num hidden self hidden size num embed self embed size num label len vocab data names wouldata' 'l0 init c' 'l0 init h' label names isoftmax label' return sym data names label names if is train decoder mx mod BucketingModule gen dec sym default bucket key self seq len 1 context self ctx decoder bind data shapes wouldata' self batch size self seq len 1 'l0 init c' self batch size self hidden size 'l0 init h' self batch size self hidden size label shapes isoftmax label' self batch size self seq len 1 for training is train inputs need grad True decoder init params initializer mx init Xavier factor type in magnitude 2 34 arg params bef args decoder init optimizer optimizer 'adam' optimizer params 'learning rate' 0 02 'wd' 0 'beta1' 0 5 and I test it by dec model build lstm decoder is train True init h init c mx nd zeros 10 150 mx nd zeros 10 150 inp mx nd ones 10 26 ctx tar mx nd ones 10 26 ctx dec forward SimpleBatch data names wouldata' 'l0 init c' 'l0 init h' data inp init c init h label names isoftmax label' label tar bucket key inp shape 1 dec backward print dec get input grads 0 asnumpy,,"piiswrong,piiswrong",2016-11-18 04:59:28,2016-11-22 09:49:02
PR,RL examples,Add DDPG and group it with DQN,,WellyZhang,2016-11-22 12:13:26,2016-11-22 12:19:10
PR,added to contributors,,,sbodenstein,2016-11-22 11:09:01,2016-11-22 12:33:46
PR,RL examples,Add DDPG and group it with DQN Add a contributor,,WellyZhang,2016-11-22 12:26:14,2016-11-22 12:46:06
PR,Image processing with scaling on r g b channels,,,,2016-11-22 13:27:09,2016-11-22 13:27:38
IS,Trying to install GPU version of mxnet on windows,I'm running into a problem when trying to install the GPU version of mxnet on windows 7 I have been following the instructions here I have installed visual studio 2013 community edition and the cuda software from Nvidia In my mxnet R package inst libs x64 directory I have the following dlls 15 08 2015 04 21 PM 23 380 600 cublas64 75 dll 15 08 2015 04 21 PM 360 736 cudart64 75 dll 07 10 2016 02 37 PM 47 820 288 cudnn64 70 dll 15 08 2015 04 21 PM 45 030 520 curand64 75 dll 20 10 2014 08 08 AM 82 432 libgcc s seh 1 dll 20 10 2014 08 08 AM 1 279 488 libgfortran 3 dll 30 05 2016 10 34 PM 27 544 576 libmxnet dll 24 03 2015 01 03 PM 38 340 126 libopenblas dll 20 10 2014 08 08 AM 331 776 libquadmath 0 dll 15 08 2015 04 21 PM 14 495 744 nvrtc64 75 dll 14 02 2005 10 03 AM 162 816 unzip32 dll 04 10 2013 07 58 AM 137 376 vcomp120 dll In a command prompt window I have added the R bin directory to the path 'where R' gives the following output c program files R r 3 3 1 bin R exe I changed the current directory to the mxnet directory and entered the command R CMD INSTALL no multiarch R package Here is the output with an error message installing to library 'C Users Robert Documents R win library 3 3' installing source package 'mxnet' libs Warning running command 'make f Makevars win f c PROGRA 1 r R 33 1 1 etc x64 Makeconf f c PROGRA 1 r R 33 1 1 share make winshlib mk SHLIB LDFLAGS ' SHLIB CXXLDFLAGS ' SHLIB LD ' SHLIB CXXLD ' SHLIB mxnet dll WIN 64 TCLBIN 6 4 OBJECTS executor o export o io o kvstore o mxnet o ndarray o symbol o ' had s tatus 127 ERROR compilation failed for package 'mxnet' removing 'C Users Robert Documents R win library 3 3 mxnet' I would appreciate any help to fix the problem Thanks,,"ameshkoff,ameshkoff,ameshkoff,ameshkoff,ameshkoff,ameshkoff",2016-11-12 19:22:41,2016-11-22 14:23:56
PR,RL examples,Add DDPG and group it with DQN Add a new contributor,,"WellyZhang,WellyZhang",2016-11-22 12:46:59,2016-11-22 15:28:00
PR,Adding quick installation script for installing MXNet with Python on Amazon Linux,Added bash script that installs mxnet with python on Amazon Linux machines Added corresponding documentations in setup docs page,,"sandeep-krishnamurthy,piiswrong,piiswrong,sandeep-krishnamurthy,sandeep-krishnamurthy,piiswrong,sandeep-krishnamurthy",2016-11-21 22:14:51,2016-11-22 18:07:21
PR,R implemented speedometer for R,Implemented speedometer to log the samples per second in the training Example with MNIST in CPU,,miguelgfierro,2016-11-22 11:47:16,2016-11-22 20:49:57
PR,Better error messaging when infer shape fails,Previously when infer shape failed it would show the difference between the provided and inferred shapes for gradients gradient arguments gradient values or aux states but NOT plain arguments I fixed that so it shows the full error message for normal arguments too So previously you would just see Also just removed an extra space in the shape inconsistent message to make it display a bit more cleanly Also I added a bit of python code to print the arguments passed in to infer shape in case of failure to further help with debugging infer shape problems,,"leopd,piiswrong,leopd,piiswrong,leopd",2016-11-16 02:00:07,2016-11-22 23:56:37
PR,Better error messaging and some docs improvements,A few changes to error messaging In several places python errors were just printing the string message of the exception not the exception class or stack trace Now it is printing the full stack trace This is mostly in python operators In a couple of the C operators I have improved the error messages when checks fail Also a few comment changes which will improve the auto generated API docs,,"leopd,piiswrong,piiswrong,leopd,piiswrong,leopd,leopd,leopd,piiswrong,piiswrong,piiswrong",2016-11-14 18:44:17,2016-11-23 04:54:14
PR,Tutorial to Notebooks,Started the process of converting existing tutorials to notebooks by converting to a runnable standalone notebook,,arank,2016-11-23 03:02:38,2016-11-23 04:56:13
PR,Better error messaging in im2rec py tool,Simple cleanup in response to SO question,,leopd,2016-11-23 01:00:46,2016-11-23 04:57:24
PR,checking for correct input to sequence layers,piiswrong Graceful handling of incorrect input to sequence layers See 3937,,sbodenstein,2016-11-22 21:51:10,2016-11-23 06:48:02
PR,Fix README in example,,,WellyZhang,2016-11-23 07:21:08,2016-11-23 07:53:35
PR,R added API info for R functions,Updated man in R package and rcppexport,,"miguelgfierro,piiswrong,miguelgfierro,thirdwing,miguelgfierro",2016-11-22 23:37:48,2016-11-23 08:42:57
PR,R added API info for R functions,Same PR as 3941 but in nnvm branch,,"miguelgfierro,piiswrong,miguelgfierro",2016-11-23 08:41:56,2016-11-23 09:24:02
PR,R Add early stop callback function,Early stop callback function with the same logic as in XGBoost stopping after N steps without improvement,,"ameshkoff,piiswrong,ameshkoff",2016-11-22 19:27:02,2016-11-23 14:20:43
PR,R Script installation for R,Quick installation of MXNet for R via a bash script Also updated python script,,"miguelgfierro,thirdwing,piiswrong,miguelgfierro,thirdwing",2016-11-12 09:22:04,2016-11-23 15:52:09
PR,Sidebar auto expand,Automatically expand child level when clicking an entry,,kevinthesun,2016-11-23 20:33:46,2016-11-23 22:43:54
IS,Suggestion for asking questions,I think StackOverflow tagged mxnet is a better place for help and support technical or algorithmic questions The Github issues should focuses on tracking requests and bugs The pros are The UP DOWN vote and Reputation mechanisms of SO encourage users developers to ask good questions and attract a wider range of experts to help answer the questions e g SO tagged tensorflow The rewards of reputation in SO can motivate users developers to help others and there is a higher chance for a question being answered The search experience of the existing answers to previous asked questions is better on SO Reduce the number of issues,,minazou,2016-08-27 07:06:09,2016-11-24 02:44:38
PR,Update copyright,,,terrytangyuan,2016-11-24 01:03:57,2016-11-24 06:35:25
PR,fix error bash cd mxnet No such file or directory,bash failed to expand ' ' in the MXNET HOME var,,lazyparser,2016-11-24 06:36:24,2016-11-24 07:05:52
PR,profiler add failure information,,,"ZihengJiang,ZihengJiang,piiswrong,ZihengJiang",2016-11-24 06:21:24,2016-11-24 07:09:04
PR,fix bugs in im2rec py,fout should be closed before doing anything if it is not WindowsError Error 32 will occur fname should be removed before renaming fname tmp to fname if it is not WindowsError Error 183 will occur But why here must rewrite the lst file PS My OS is Windows 10,,"chasonlee,mli",2016-11-24 04:51:55,2016-11-24 08:03:17
IS,Why mxnet need all of Tensor TBlob TensorContainer and NDArray,There are Tensor TBlob TensorContainer in mshadow NDArray in mxnet what used for somehow the same purpose is there anyone can explain why mxnet need all these four classes and when will we use them separately,,tqchen,2016-11-23 12:51:06,2016-11-24 09:22:34
PR,add Wei Wu to contributor list,,,lazyparser,2016-11-24 08:45:40,2016-11-24 16:10:56
PR,Added MSE in R,,,"miguelgfierro,jeremiedb,miguelgfierro",2016-11-24 15:49:40,2016-11-24 21:08:51
PR,Fixed an inconsistency between the load param return value and the received value,I guess you guys are in order to adapt train end2end py add a third return value but forget to change all the code using load param,,chasonlee,2016-11-24 14:28:19,2016-11-24 21:10:08
PR,fix python path in setup utils,typofix,,lazyparser,2016-11-24 09:34:35,2016-11-24 21:12:21
PR,enable hidden arg,refactor lr mult wd mult ctx group revert test update submodule,,"piiswrong,tqchen,tqchen,tqchen,piiswrong,piiswrong,tqchen,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong",2016-11-22 21:42:46,2016-11-24 23:31:40
PR,fix a few dependence issues in install mxnet ubuntu r sh,,,"lazyparser,miguelgfierro,lazyparser,piiswrong,miguelgfierro,thirdwing",2016-11-24 10:08:55,2016-11-24 23:53:04
PR,New callback interface for training visualization,Neural style transfer notebook example of refactored live notebook charting Trivial notebook to run train cifar10 working notebook structure in place for live charting Needs module cb change Module callbacks almost working Epoch numbers NaN Module CNN example for notebook charts CIFAR10 training notebook for module works again Matching callback code to notebook Add new notebook Small fix Change notebook name Clean merge Small bug fix Import cleaning,,"kevinthesun,piiswrong,kevinthesun,piiswrong,piiswrong,leopd,lxn2",2016-11-16 07:23:01,2016-11-25 02:06:53
PR,Support for custom mask type in SequenceMask,can give an optional float value to use as the mask value added python test for SequenceMask One possible application using Softmax with variable length sequences,,"sbodenstein,sbodenstein,piiswrong",2016-11-21 09:04:31,2016-11-25 05:03:02
PR,typo error of relu name,This may not cause error results but seems a typo error,,xlvector,2016-11-25 06:24:59,2016-11-25 06:41:35
IS,Can mxnet support data slice as caffe used Slice Slice point,data data1 fc layer1 data2 fc layer2 loss i would like to split data to 2 parts and then concat in softmaxlayer how can i split data SliceChannel did not offer the slice point like caffe,,"piiswrong,Ldpe2G",2016-11-21 10:16:54,2016-11-25 12:59:05
IS,Loading pretrained model raise error,Hi I have a CNN model converted from caffe using convert model py When I try to load it fllowing steps in this tutorial MXNet raises the following error RuntimeError layer21 label is not presented The last layer of my CNN is softmax and part of my symbol json file looks like this I compared my symbol json file to resnet and inception could not find the bug Does anyone know how to fix this Thanks,,"piiswrong,burness,ysh329",2016-11-24 18:14:41,2016-11-25 16:39:21
PR,Upgrade nnvm to use automatic gradient correspondence guessing,piiswrong,,"tqchen,piiswrong,tqchen,piiswrong,tqchen",2016-11-25 01:43:07,2016-11-25 21:38:10
PR,MXNet benchmark scripts,Scripts to run MXNet benchmark on popular image networks,,"nswamy,piiswrong,nswamy,piiswrong,nswamy,piiswrong,nswamy,chasonlee,nswamy,chasonlee,nswamy,chasonlee",2016-11-24 23:42:48,2016-11-25 23:56:40
IS,MXNET R GPU Build on Windows EASY SETUP,My friend build MXNET from scratch on Windows 10 for the GPU You can download the zip file here All you have to do is go into R Studio then point at file and install package It should work,,"ameshkoff,thirdwing,ameshkoff,thirdwing,ameshkoff,ameshkoff",2016-11-16 21:00:11,2016-11-26 03:53:39
IS,fail to load library with python support on Windows,import mxnet Traceback most recent call last File stdin line 1 in module File D Anaconda lib site packages mxnet 0 7 0 py2 7 egg mxnet init py line 7 in module from base import MXNetError File D Anaconda lib site packages mxnet 0 7 0 py2 7 egg mxnet base py line 43 in module LIB load lib File D Anaconda lib site packages mxnet 0 7 0 py2 7 egg mxnet base py line 35 in load lib lib ctypes cdll LoadLibrary lib path 0 File D Anaconda lib ctypes init py line 443 in LoadLibrary return self dlltype name File D Anaconda lib ctypes init py line 365 in init self handle dlopen self name mode WindowsError Error 126 import mxnet Traceback most recent call last File stdin line 1 in module File D Anaconda lib site packages mxnet 0 7 0 py2 7 egg mxnet init py line 7 in module from base import MXNetError File D Anaconda lib site packages mxnet 0 7 0 py2 7 egg mxnet base py line 10 in module from import libinfo ImportError cannot import name libinfo It seems whatever methods I use it still shows these two errors I set my environment all by that cmd in pre built package Then I tried to build the latest version by my own and when I used python package it is the same error It seems that it is a library loading problem But I did set all environment correctly including cuda7 5 cudnnv3 opencv2 4 3 and Anaconda And that CPP test solution runs perfectly What else should I do I have been stuck here for a long time I'm new to this field and I will be so thankful if you guys can help me out solved You should use CUDNN V4 in nightly prebuilt version Thanks to MaticsL in issue 2813,,,2016-11-26 05:42:20,2016-11-26 06:00:30
IS,error with simple linear regression,I wrote this simple code to fit simple linear regression Is not correct my specification fo the linear regression model,,,2016-11-23 19:53:37,2016-11-26 13:20:55
IS,Simple usage of Embedding layer causes error does not support calculate data gradient,Test script Based on MXNet is newest version today 2016 11 26,,"sxjscience,sxjscience,sxjscience,sxjscience,sxjscience",2016-11-26 12:09:40,2016-11-26 14:08:09
IS,model s json file load error,symbol mx symbol load json os getcwd ' model resnet 50 symbol json' then 14 40 44 home xiaomin wxm mxnet dmlc core include dmlc logging h 235 14 40 44 home xiaomin wxm mxnet dmlc core include dmlc json h 635 Check failed ch ' ' Error at Line 0 around home xiaomin wxm Code rcnn model resnet 50 symbol json Expect ' ' but get ' ' Traceback most recent call last File home xiaomin wxm Code rcnn t py line 86 in module load params File home xiaomin wxm Code rcnn t py line 81 in load params symbol mx symbol load json os getcwd ' model resnet 50 symbol json' File home xiaomin wxm mxnet python mxnet symbol py line 987 in load json check call LIB MXSymbolCreateFromJSON c str json str ctypes byref handle File home xiaomin wxm mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 14 40 44 home xiaomin wxm mxnet dmlc core include dmlc json h 635 Check failed ch ' ' Error at Line 0 around home xiaomin wxm Code rcnn model resnet 50 symbol json Expect ' ' but get ' ' I download the json file from,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2016-11-26 06:46:28,2016-11-26 14:46:00
IS,rcnn example smooth L1 loss tends to be nan,well I do not understand why the codes in helper dataset load pascal annotation py load pascal annotation need to be like this for ix obj in enumerate objs bbox obj find 'bndbox' Make pixel indexes 0 based x1 float bbox find 'xmin' text 1 y1 float bbox find 'ymin' text 1 x2 float bbox find 'xmax' text 1 y2 float bbox find 'ymax' text 1 cls class to index obj find 'name' text lower strip boxes ix x1 y1 x2 y2 gt classes ix cls overlaps ix cls 1 0 My question is why we need to 1 here In my experiments it will generate some weird 65535 as the xmin number not test the other three yet into gt roidb pkl file and then the training smooth L1 loss tends to be nan because of the codes below helper peocessing bbox transform py bbox transform ex widths ex rois 2 ex rois 0 1 0 ex heights ex rois 3 ex rois 1 1 0 ex ctr x ex rois 0 0 5 ex widths 1 0 ex ctr y ex rois 1 0 5 ex heights 1 0 gt widths gt rois 2 gt rois 0 1 0 gt heights gt rois 3 gt rois 1 1 0 gt ctr x gt rois 0 0 5 gt widths 1 0 gt ctr y gt rois 1 0 5 gt heights 1 0 targets dx gt ctr x ex ctr x ex widths 1e 14 targets dy gt ctr y ex ctr y ex heights 1e 14 targets dw np log gt widths ex widths print gt rois 2 print gt rois 0 print gt widths print ex widths print gt widths ex widths print targets dw print ' ' 60 targets dh np log gt heights ex heights So I delete 1 in the previous codes and it works but another problem is when it come to flip the dataset the assert command tiggered helper peocessing roidb py append flipped images for i in range self num images boxes roidb i 'boxes' copy oldx1 boxes 0 copy oldx2 boxes 2 copy boxes 0 widths i oldx2 1 boxes 2 widths i oldx1 1 assert boxes 2 boxes 0 all entry 'boxes' boxes 'gt classes' roidb i 'gt classes' 'gt overlaps' roidb i 'gt overlaps' 'flipped' True roidb append entry,,precedenceguo,2016-11-20 07:53:24,2016-11-26 14:46:59
PR,Fix ccAdam in nnvm add ccoptimizer testcase,,,"tqchen,tqchen",2016-11-26 07:07:45,2016-11-26 20:15:18
PR,checking if args has a benchmark attribute,The commit adding benchmarks for imagenet broke the python MNIST example due to the args benchmark check The argparse Namespace module holds its arguments as attributes so the simple 'if falsey' check does not work since there is no 'benchmark' attribute unless it is explicitly passed in which would otherwise have to be added to all python examples calling train model is fit function also adding a newline at the end of the file,,andremoeller,2016-11-26 22:39:37,2016-11-27 00:45:13
PR,only return visible outputs for imperative,sxjscience,,"piiswrong,sxjscience",2016-11-26 01:49:25,2016-11-27 02:03:16
PR,only return visible outputs for imperative,,,piiswrong,2016-11-27 02:03:31,2016-11-27 02:49:45
PR,Scala add Mixed Initializer fix some spell error,,,Ldpe2G,2016-11-27 03:01:36,2016-11-27 03:29:44
IS,A problem about parameter passing in train model py,In train model py we create a object named model using mx model FeedForward and we pass these parameters learning rate momentum wd But in model py there is no initialization of these 3 parameters in def init of class FeedForward And I cannot find the parameters anywhere in model py So how do these parameters work in model py,,,2016-11-26 12:41:23,2016-11-27 14:59:25
PR,refactor logging,,,piiswrong,2016-11-28 00:07:20,2016-11-28 02:06:58
IS,Error running train mnist py,Hey I'm new to mxnet and when I'm running mxnet example image classification train mnist py I got following error messages python train mnist py 2016 11 26 19 30 44 409 Node 0 start with arguments Namespace batch size 128 data dir 'mnist ' gpus None kv store 'local' load epoch None lr 0 1 lr factor 1 lr factor epoch 1 model prefix None network 'mlp' num epochs 10 num examples 60000 save model prefix None 19 30 46 D chhong mxnet src io iter mnist cc 91 MNISTIter load 60000 images shuffle 1 shape 128 784 19 30 47 D chhong mxnet src io iter mnist cc 91 MNISTIter load 10000 images shuffle 1 shape 128 784 Traceback most recent call last File train mnist py line 164 in module train model fit args net get iterator data shape File C mxnet gpu example image classification train model py line 92 in fit if args benchmark AttributeError 'Namespace' object has no attribute 'benchmark' So I add a line in function parse args def parse args parser add argument ' benchmark' type bool default False help 'to avoid the error' return parser parse args and it works for now python train mnist py 2016 11 26 20 53 49 938 Node 0 start with arguments Namespace batch size 128 benchmark False data dir 'mnist ' gpus None kv store 'local' load epoch None lr 0 1 lr factor 1 lr factor epoch 1 model prefix None network 'mlp' num epochs 10 num examples 60000 save model prefix None 20 53 52 D chhong mxnet src io iter mnist cc 91 MNISTIter load 60000 images shuffle 1 shape 128 784 20 53 52 D chhong mxnet src io iter mnist cc 91 MNISTIter load 10000 images shuffle 1 shape 128 784 2016 11 26 20 53 52 700 Node 0 Start training with cpu 0 2016 11 26 20 53 52 896 Node 0 Epoch 0 Batch 50 Speed 34312 04 samples sec Train accuracy 0 719063 2016 11 26 20 53 52 896 Node 0 Epoch 0 Batch 50 Speed 34312 04 samples sec Train top k accuracy 5 0 943594 2016 11 26 20 53 52 896 Node 0 Epoch 0 Batch 50 Speed 34312 04 samples sec Train top k accuracy 10 1 000000 2016 11 26 20 53 52 896 Node 0 Epoch 0 Batch 50 Speed 34312 04 samples sec Train top k accuracy 20 1 000000 2016 11 26 20 53 53 085 Node 0 Epoch 0 Batch 100 Speed 34038 16 samples sec Train accuracy 0 896406 2016 11 26 20 53 53 085 Node 0 Epoch 0 Batch 100 Speed 34038 16 samples sec Train top k accuracy 5 0 993750 2016 11 26 20 53 53 085 Node 0 Epoch 0 Batch 100 Speed 34038 16 samples sec Train top k accuracy 10 1 000000 2016 11 26 20 53 53 085 Node 0 Epoch 0 Batch 100 Speed 34038 16 samples sec Train top k accuracy 20 1 000000 And if change it to default True the result will be very bad Still I do not understand the reason behind this,,,2016-11-26 12:03:55,2016-11-28 04:46:37
PR,Fix the wrong rename in the load model,Delete this code and then we can load the trained model correctly,,"chasonlee,mli",2016-11-28 13:41:25,2016-11-28 19:56:44
IS,Flipped images in the provided Cifar10 dataset,I find that the images in were vertically flipped which are different from my generated rec data Here is my code for visualization Then outputs y e 3u54t 7gp44hlg I think it is better to use the original data for training Please help to check it,,"zhreshold,tornadomeet,tornadomeet,tornadomeet",2016-11-25 05:15:25,2016-11-29 02:29:59
PR,a major refactor of example image classification,this is a major refactor of image classification changes include 1 moved common codes such as argument data iterator and training into common 2 moved symbol definitions into symbol 3 added scripts to prepare data and download pretrained models 4 added score py and fine tune py 5 updated README 6 added resnet from all py in the root fold are tested manually some TODOs 1 refactor the R part 2 instructions to enable mirror 3 double check benchmark py on multiple machines 4 a test fold with scripts to test this example,,"mli,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,mli,mli,mli,mli,piiswrong,piiswrong,piiswrong,tornadomeet,mli,mli,piiswrong,piiswrong,piiswrong,thirdwing,piiswrong,thirdwing",2016-11-27 04:36:54,2016-11-29 02:32:59
IS,No attribute CuDNNBatchNorm,Hi I want to run the demo mxnet example gan dcgan py but an AttributeError occured module object has no attribute 'CuDNNBatchNorm' But I think the mxnet had build successfully I tried to run the demo image classification it worked How should I fixed this error tks,,"tqchen,tqchen",2016-11-28 13:32:57,2016-11-29 05:43:13
PR,fix error cd home username mxnet No such file or directory,Fix error cd home username mxnet No such file or directory Users can install the mxnet in other directories,,,2016-11-28 13:34:57,2016-11-29 05:58:08
PR,fix hard code issue of mxnet code path must be home xxx mxnet,fix the repeated adding PYTHONPATH in bashrc if executing the install script multiple times,,"mli,piiswrong",2016-11-28 15:19:50,2016-11-29 05:58:58
PR,Add example with sub word units representation for word2vec,I have implemented another word2vec example based on NCE and change the word level vector representation down to triple letter representations The original work was introduced from MSR in CIKM'14 as a part of their DSSM model Please review and feel free to discuss I put some details in README,,"zihaolucky,andremoeller,piiswrong,zihaolucky,piiswrong,zihaolucky,piiswrong",2016-11-26 12:24:58,2016-11-29 07:01:55
IS,How to use cudnn in a single layer,I have build the lib with cudnn option but I only want to use cudnn in some specified layers e g conv and pooling and I do not want to use cudnn for bn How to configure the json or code to acheive this,,piiswrong,2016-11-27 01:04:13,2016-11-29 07:17:11
IS,Referencing and Slicing Elements of NDArray,Hey guys Any way to reference a single element or slice an NDArray I got an error runing the following code,,"WellyZhang,piiswrong,sxjscience,WellyZhang,piiswrong,WellyZhang,sxjscience,WellyZhang",2016-11-29 07:23:29,2016-11-29 07:56:54
IS,how to update mxnet when there are some update of mxnet,I hava a doubt about how to update mxnet when there are some changes in mxnet Is there any anyone knows Please share me your way thanks a lot,,piiswrong,2016-11-03 13:37:07,2016-11-29 13:46:55
IS,Can Mxnet fit a regression LSTM model,Is LSTM in Mxnet applicable to a regression problem such as sentence sentiment classification Thanks,,,2016-11-03 04:07:34,2016-11-29 15:01:50
PR,Recommender system examples,A set of sample notebooks and tools for building recommender systems with sparse data Includes some custom layers binary CrossEntropy loss sparse random projections and a DataIter wrapper that implements negative sampling Notebooks use LiveLearningCurve to visualize training results in progress,,leopd,2016-11-29 08:34:51,2016-11-29 18:36:41
PR,update benchmark scripts to enable MXNET CUDNN AUTOTUNE DEFAULT,,,"nswamy,mli,piiswrong",2016-11-29 01:33:36,2016-11-29 18:46:21
IS,Website Packages Scala documentation link broken,is empty,,"piiswrong,mli",2016-09-19 22:36:09,2016-11-29 19:00:59
PR,Website search engine,Add Search Function,,"kevinthesun,mli,kevinthesun",2016-11-29 01:21:26,2016-11-29 19:28:22
PR,Fix nvidia logo,Fix collaborator logo,,kevinthesun,2016-11-29 20:24:50,2016-11-29 20:28:07
PR,Update train imagenet arguments in benchmark scripts,,,"nswamy,piiswrong",2016-11-29 20:26:23,2016-11-29 20:54:30
PR,doc update readme in example image classification,,,mli,2016-11-29 20:59:06,2016-11-29 20:59:11
PR,This is to run benchmark on 2 power number of nodes,,,"nswamy,nswamy,mli",2016-11-30 00:09:13,2016-11-30 00:15:14
PR,Add simple equal operator,sxjscience Please have a look at this simple implementation,,"jermainewang,piiswrong,jermainewang,piiswrong,piiswrong,jermainewang,sxjscience",2016-11-29 21:44:06,2016-11-30 03:20:29
IS,Jupyter notebooks from KDD2016,Where I can download the Jupyter Notebooks from KDD2016 hands on tutorial,,mli,2016-11-06 09:13:56,2016-11-30 03:43:51
PR,Numerical Gradient Improvement,,,sxjscience,2016-11-14 13:46:39,2016-11-30 07:52:52
PR,add Inception resnet v2 in example image classification,Hi I have add the inception resnet v2 in mxnet and test in the 17flowers dataset It is ok to be convergent I think many people will be interested it And In order to test it is it ok to push the 17flowers binary files to the In big model network like googlenet inception resnet v2 I think that the 32x32 or 28x28 images are not suitable to make a test And could anyone test this model in bigger DataSet like ImageNet I have no enough resources to test it Thanks for such a good DL Framework,,"burness,piiswrong,burness,burness,mli,piiswrong,burness,zhreshold,burness,burness,burness,burness,burness,mli,burness,burness,mli,burness,ysh329",2016-11-13 05:57:46,2016-11-30 08:07:48
IS,An stange error where predict with the trained model,I have train my model in my ubuntu server but it can not predict the test images howerver it is ok in my macbook pro with the same code Here is the error I have search an issue 3605 but the mxnet of my ubuntu server have fix it,,"burness,piiswrong,burness",2016-11-23 09:02:06,2016-11-30 11:49:00
PR,allow using numpy dtypes for symbol and ndarray functions,sxjscience,,"piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,sxjscience,piiswrong,tqchen,piiswrong,tqchen",2016-11-28 02:17:43,2016-11-30 12:08:15
PR,fix a missing word,Add the missing word please merge,,,2016-11-30 14:39:06,2016-11-30 17:18:40
PR,try fix jenkins test error,,,"sxjscience,sxjscience,tornadomeet",2016-11-30 12:44:49,2016-11-30 17:50:02
PR,Re add inception resnet v2,In PR 3811 I have rebase someone is commit into my pr So I close 3811 and make this PR I'm so sorry to make this action,,"burness,burness",2016-11-30 08:06:28,2016-11-30 21:42:25
IS,how to change the weight decay just in FullyConnected Layers,,,CNevd,2016-12-01 02:41:31,2016-12-01 03:01:11
PR,fix compile error with vs2015 and new dmlc core,,,"yajiedesign,piiswrong,tornadomeet",2016-11-30 09:12:33,2016-12-01 05:54:26
PR,OP Topk and arange,0 Revert the reldiff in test utils 1 Fix the argument of dot and batch dot as mentioned in 2 New operators topk sort argsort arange 3 Move operator to operator tensor BlockGrad 4 Add the argument dtype to initialization operators zeros ones arange 5 Enable gradient for argmax and argmin by adding the gradient node to zeros Some Notes The implementation of TopK is not so optimized at this stage The implementation is to first sort the data and then keep the top k indices values Also the axis argument is supported in topk and topk is equivalent to argsort and sort if we set k 1 and ret typ indices or value The arange operator has an additional repeat argument which indicates the number of times each element will be repeated in the generation process,,"sxjscience,piiswrong,piiswrong,piiswrong,piiswrong,sxjscience,sxjscience,sxjscience,piiswrong,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,sxjscience,piiswrong,sxjscience,sxjscience,piiswrong,sxjscience,piiswrong,piiswrong,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,piiswrong,sxjscience,tqchen,piiswrong,sxjscience,tqchen,sxjscience,piiswrong,sxjscience,piiswrong,piiswrong,sxjscience,tqchen,tqchen",2016-11-23 13:37:19,2016-12-01 07:36:18
IS,'module' object has no attribute 'MultiBoxPrior',When I try example ssd demo py I got this example ssd symbol common py line 164 in multibox layer anthor mx symbol MultiBoxPrior from layer AttributeError 'module' object has no attribute 'MultiBoxPrior' I just gitted the latest version of mxnet and maked it with setting 'NNPACK' 'openblas' enabled Then I entered mxnet python and run 'python setup py install' How can I solve this Oh HOW CAN I JUST FORGET TO ADD EXTRA OPERATORS Forget me I'm going to close this stupid issue,,,2016-12-01 09:36:00,2016-12-01 10:45:58
PR,Numerical Gradient Improvement,,,,2016-11-30 07:51:52,2016-12-01 13:53:10
IS,Inconsistent file name prefix for saving model and loading model,For example when using mxnet example image classification train model py for training and set args save model prefix with hello the checkpoint file name is hello epoch number param and hello symbol json But when loading model with args model prefix hello mxnet will find for file with prefix hello kv rank Error log Is this a bug or I just missed something,,,2016-10-28 16:13:23,2016-12-02 02:03:02
IS,TypeError unhashable type 'list',Hi I build a multi input and output model and write a data iterator for it For the inputs and outputs in the model I define them like this Is there something wrong with my dataiter How to fix it Thanks a lot,,,2016-12-02 01:14:01,2016-12-02 04:50:05
PR,Rename dep engine md to execution engine md,this document focus on execution engine rather than dependence engine,,"jingpengw,piiswrong",2016-12-01 14:34:59,2016-12-02 05:14:33
PR,add a fine tune link in how to,just add a missing link for fine tune how to fine tune notebook pre train fine tune,,,2016-11-30 09:20:37,2016-12-02 05:20:56
PR,Add HKUST LOGO,,,sxjscience,2016-12-02 07:33:15,2016-12-02 08:05:35
IS,what kind of GPU you are using,mli test with bandwidth tools measure py test with cpu E5 2660 v4 2 m40 8 mem 256G using cuda 7 5,,"piiswrong,piiswrong,piiswrong,mli",2016-12-02 02:44:55,2016-12-02 10:59:46
PR,add me into Contributors md,,,burness,2016-12-02 06:43:43,2016-12-02 19:04:27
PR,Fix confusions between comments in variables and their descriptions src io image aug default cc,Typo fixing and revision of unmatched descriptions,,,2016-11-15 14:31:25,2016-12-02 19:04:44
IS,Strange behavior of Validation score during training when Forward is train False,I encountered a very strange problem when training a detection network Training accuracy increase fine however validation is always like 0 01 Tried debugging into the code found that during training the output of network which is then plugged into eval metric is good however during validation the output is not correct To be more specific the output of the network is mx symbol Group cls prob loc loss cls label where cls prob generated class softmax prob loc loss bounding box regression loss cls label generated class target according to original ground truth I have multiple class targets per image So I tried to print out cls label found that it is not changing at all All validation cls label is the same as the last output in training Finally I tried change the forward mode is train False in base module score to is train True L164 Everything is fine now Validation accuracy as good as training now I do not know what is going wrong when is train False I suspect a bug in retrieving output Any help,,"zhreshold,piiswrong,zhreshold,piiswrong,pluskid,zhreshold,piiswrong,zhreshold,pluskid,zhreshold",2016-09-26 17:34:57,2016-12-02 23:25:45
PR,Issue 4051 spell error,Minor change to the code but major change for me to setup the environment and get started with contributing to mxnet,,"shivarajugowda,shivarajugowda,mli,shivarajugowda",2016-12-03 00:47:16,2016-12-03 03:27:42
PR,Fix computer vision image classification md typo,Fixed typo in image classification doc docs tutorials computer vision image classification md,,,2016-12-02 23:29:39,2016-12-03 03:28:08
PR,Added additional AMI option to Cloud Setup,Bitfusion also provides an MXNet AMI pre installed w Nvidia Drivers Cuda 7 5 Toolkit cuDNN 5 1 MXNet Python 2 3 support Enum34 h5py Matplotlib NumPy Pandas PyCuda PyDot SciPy SymPy GpuStat and Jupyter to leverage Nvidia GRID instances Also includes MXNet tutorial Jupyter notebook sample code,,,2016-12-02 16:15:41,2016-12-03 03:30:43
PR,kvstore no warning message if p2p is already enabled,the current implementation will generate warning message if trying to enable p2p access on devices if p2p is already enabled it happens when creating multiple kvstores,,mli,2016-12-03 06:51:25,2016-12-03 06:51:52
IS,cudnn compile error,I changed the USE CUDNN 1 and then make It shows mostly the same reason previous declaration and invalid conversion from int to cudnnTensorFormat t fpermissive as is shown in issue 4041 How can I fix that I have successfully installed caffe in the same cmputer cuDNN v4 CUDA 7 5,,,2016-12-03 14:10:56,2016-12-03 15:05:39
PR,Python Python2 3 Support for example image classification,Adding python2 3 compatible support for example image classification I have tested on python 3 5 2 and python 2 7 12 for train mnist py train cifar10 py train imagenet py with benchmark on off gpus on off Let me know if there any questions This is related to issue 4071,,jostep,2016-12-03 21:57:45,2016-12-03 23:30:22
PR,tools bandwidth fix measure py add test measure py,fix measure py due to 3999 add test measure py remove argparse in example image classifcation test score py automatically detect how many gpus,,mli,2016-12-03 07:14:09,2016-12-03 23:30:40
PR,fix ddpg bug,Fix the bug from ddpg,,WellyZhang,2016-12-03 12:29:07,2016-12-03 23:46:35
PR,example image classification add two pre trainede models,,,mli,2016-12-04 06:49:34,2016-12-04 06:49:40
IS,A question about learning rate multipler for parameters,In optimizer py there is a function named set lr mult it is for Seting individual learning rate multipler for parameters And whether we should set this depends on the network sym Is that means different network gives parameters different learning rate L130,,"CNevd,CNevd",2016-12-03 10:03:02,2016-12-04 08:20:08
IS,understanding of arg names param names aux names arg params and aux params,I can not understand these parameters clearly arg names param names aux names arg params aux params Is there a clear distinction among them,,,2016-12-04 08:37:47,2016-12-04 08:48:16
IS,A question about symbol and sym gen,What is the meaning of this line L762 And what is the difference between symbol and sym gen,,sxjscience,2016-12-04 08:24:39,2016-12-04 13:46:31
PR,Remove the redundant statement,,,"EricFisher,tqchen,tqchen",2016-11-30 08:45:57,2016-12-04 19:37:34
PR,example moved notebook to dmlc mxnet notebooks,,,aileli,2016-12-04 21:01:55,2016-12-04 21:03:17
IS,Assignment Shape of Tensors are not consistent with target eshape 101 3000 dshape 101 1000,It is strange that I have check shapes of all symbols using infer shape but when I actually run the program an error Assignment Shape of Tensors are not consistent with target eshape 101 3000 dshape 101 1000 occurs I assume that when the model is built use mx sym to builid net and bind it with a module and here you will give that shapes of all input data it will use infer shape to check shapes of all symbols And when the program is actually running mxnet will feed all input nodes with data and data flow into all computational graph That should be the time when the error above occurs Am I right Besides is there any way I can know in which symbol the shape inconsistent error occurs,,,2016-12-02 12:15:54,2016-12-05 01:09:11
IS,Problem with Finetune,Good Evening I need to fine tune a network for the moment I'm trying to finetune VGG16 model changing the last layer to adapt it to my case with 3 output classes To do so I try to use the finetune api from a forked repo from I clone it recursively and install it as done with standard mxnet using CUDA and CUDNN lastest versions I create a yml file similar to the examples But it fails due to a shape verification error The full stacktrace is given bellow I would really appreciate some insight in the issue I am also interested if somebody could explain me other ways of performing fine tuning in MXnet Thank you very much in advance YML FILE network vgg16 data dir media Datarecords size224 dataset size224 data shape 300 num classes 3 num examples 3060 batch size 25 lr 0 0005 lr factor 0 1 lr factor epoch 320 momentum 0 9 wd 0 0005 num epochs 800 kv store local model prefix finetune clothes inception finetune from home gabriel MXNet mxnet pretrained models VGG16 VGG FC ILSVRC 16 layers 0074 params checkpoint epoch 80 eval epoch 16 The I run MXNet mxnet finetune api example finetune api python train imagenet py solver finetuneconfig yml gpus 0 and I get the following output Namespace gpus '0' load epoch None log file 'auto' solver 'finetuneconfig yml' 2016 11 13 19 51 17 733 Node 0 Start with arguments 2016 11 13 19 51 17 733 Node 0 batch size 25 2016 11 13 19 51 17 733 Node 0 checkpoint epoch 80 2016 11 13 19 51 17 733 Node 0 clip gradient 5 0 2016 11 13 19 51 17 733 Node 0 data dir media Datarecords size224 2016 11 13 19 51 17 733 Node 0 data shape 224 2016 11 13 19 51 17 733 Node 0 dataset size224 2016 11 13 19 51 17 733 Node 0 display 50 2016 11 13 19 51 17 733 Node 0 encoding jpg 2016 11 13 19 51 17 733 Node 0 epoch size None 2016 11 13 19 51 17 733 Node 0 eval epoch 16 2016 11 13 19 51 17 733 Node 0 eval initialization True 2016 11 13 19 51 17 733 Node 0 eval metric acc ce 2016 11 13 19 51 17 733 Node 0 finetune from home gabriel MXNet mxnet finetune api pretrained models VGG16 VGG FC ILSVRC 16 layers 0074 params 2016 11 13 19 51 17 733 Node 0 gpus 0 2016 11 13 19 51 17 734 Node 0 initializer default 2016 11 13 19 51 17 734 Node 0 kv store local 2016 11 13 19 51 17 734 Node 0 load epoch None 2016 11 13 19 51 17 734 Node 0 log file auto 2016 11 13 19 51 17 734 Node 0 lr 0 0005 2016 11 13 19 51 17 734 Node 0 lr factor 0 1 2016 11 13 19 51 17 734 Node 0 lr factor epoch 320 2016 11 13 19 51 17 734 Node 0 max size 0 2016 11 13 19 51 17 734 Node 0 mean values None 2016 11 13 19 51 17 734 Node 0 min size 0 2016 11 13 19 51 17 734 Node 0 model prefix finetune vgg 2016 11 13 19 51 17 734 Node 0 momentum 0 9 2016 11 13 19 51 17 734 Node 0 monitor None 2016 11 13 19 51 17 734 Node 0 network vgg16 2016 11 13 19 51 17 734 Node 0 network kwargs 2016 11 13 19 51 17 734 Node 0 num classes 3 2016 11 13 19 51 17 734 Node 0 num epochs 800 2016 11 13 19 51 17 734 Node 0 num examples 3060 2016 11 13 19 51 17 734 Node 0 pad 0 2016 11 13 19 51 17 734 Node 0 random skip ratio 0 2016 11 13 19 51 17 734 Node 0 scale 1 0 2016 11 13 19 51 17 734 Node 0 train dataset train rec 2016 11 13 19 51 17 735 Node 0 val dataset val rec 2016 11 13 19 51 17 735 Node 0 wd 0 0005 2016 11 13 19 51 17 737 Node 0 Running on machine gabriel N751JX 2016 11 13 19 51 17 737 Node 0 finetuning from home gabriel MXNet mxnet finetune api pretrained models VGG16 VGG FC ILSVRC 16 layers 0074 params 2016 11 13 19 51 18 228 Node 0 Reinitialize parameter arg prob label 19 51 18 src io iter image recordio cc 209 ImageRecordIOParser media Datarecords size224 train rec use 3 threads for decoding 19 51 20 src io iter image recordio cc 209 ImageRecordIOParser media Datarecords size224 val rec use 3 threads for decoding 2016 11 13 19 51 23 095 Node 0 Skip parameter arg fc8 weight 2016 11 13 19 51 23 096 Node 0 Skip parameter arg fc8 bias 2016 11 13 19 51 23 096 Node 0 Reinitialize parameter arg fc8 size224 weight 2016 11 13 19 51 23 096 Node 0 Reinitialize parameter arg fc8 size224 bias 19 51 23 home gabriel MXNet mxnet finetune api dmlc core include dmlc logging h 235 19 51 23 src ndarray ndarray cc 227 Check failed from shape to shape operands shape mismatch Traceback most recent call last File train imagenet py line 77 in module train model fit args net get iterator File home gabriel MXNet mxnet finetune api example finetune api train model py line 189 in fit epoch end callback checkpoint File home gabriel MXNet mxnet finetune api python mxnet model py line 799 in fit self init params dict data provide data data provide label File home gabriel MXNet mxnet finetune api python mxnet model py line 548 in init params arg params k self arg params k File home gabriel MXNet mxnet finetune api python mxnet ndarray py line 221 in setitem value copyto self File home gabriel MXNet mxnet finetune api python mxnet ndarray py line 472 in copyto return internal copyto self out other File home gabriel MXNet mxnet finetune api python mxnet ndarray py line 1147 in unary ndarray function c array ctypes c char p c str str i for i in kwargs values File home gabriel MXNet mxnet finetune api python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 19 51 23 src ndarray ndarray cc 227 Check failed from shape to shape operands shape mismatch,,"piiswrong,taoari,mli",2016-11-13 19:04:24,2016-12-05 01:23:49
IS,Must the input variable be named with wouldata',I wrote a simple network with one hidden layer net mx sym Variable 'v1' fc1 mx sym FullyConnected data net name 'fc1' num hidden 10 act1 mx sym Activation data fc1 name 'act1' act type arelu' fc2 mx sym FullyConnected data act1 name 'fc2' num hidden 3 soft2 mx sym SoftmaxOutput data fc2 name isoft3' ctx mx gpu 1 batch size 128 data shape batch size 256 executor soft3 simple bind ctx ctx grad req 'write' data data shape However an error was raised Symbol InferShapeKKeyword argument name data not found I found out that the error derived from line 676 in isymbol py' arg shapes aux shapes self infer shape kwargs It seemed that the 'infer shape' only worked when the name of the input variable was exactly wouldata' Maybe I was wrong because fixing the name of the input was not elegant at all What if there were more input variables than one But I could not find out a way to tackle this Could someone tell me how to specify a different user defined variable name for inputs THX,,tornadomeet,2016-12-05 01:06:45,2016-12-05 01:26:57
PR,doc Update What is New in Readme,,,mli,2016-12-05 01:43:23,2016-12-05 01:43:36
IS,Question about mshadow broadcast to,I find broadcast to function in the extension dir url I want to use it like this However this code can not work Is there any other way to broadcast a Tensor with Shap1 1 to various shape,,"piiswrong,sxjscience",2016-11-25 18:43:16,2016-12-05 03:29:09
PR,Rebase EmbeddingOp on the NNVM branch,This refactors the Embedding layer to the new operator API based on NNVM according to the guide in 3248 The current new operator guide is not that easy to follow for beginners Hope we will have a more detailed document on it in the future,,"flyers,sxjscience,sxjscience,flyers,sxjscience",2016-12-03 17:13:36,2016-12-05 03:30:05
IS,How to pass parameters into CustomOp,I am trying to implement a new operator loss function in Python following the instructions here But different from the example in this page my new operator need some extra parameters like kernel stride or num filter in mx sym Convolution Is it possible to pass such parameters into a CustomOp like,,"nicklhy,CNevd,nicklhy",2016-12-05 05:46:01,2016-12-05 06:27:54
IS,nnvm lstm memory issue,The new nnvm branch is faster and cost less gpu memory when seq len is small but it become slower and cost much more gpu memory when seq len is longer seqlen 100 nnvm master speed samples s 46 3 52 5 memory MB 8272 3609 seqlen 16 nnvm master speed samples s 265 247 memory MB 2186 2315,,"tqchen,tqchen,tqchen,tqchen,tqchen,Ldpe2G,tqchen",2016-11-10 02:18:24,2016-12-05 10:48:24
PR,EXEC NNVM Enforce sum order for addto optimization,piiswrong related to issue,,"tqchen,tqchen",2016-12-04 21:42:15,2016-12-05 10:48:44
PR,change ctcComputeInfo to ctcOptions for latest warp ctc,the parameter name changed from ctcComputeInfo to ctcOption in warp ctc,,piiswrong,2016-12-04 02:49:08,2016-12-05 11:43:47
PR,refactoring EmbeddingOp based on NNVM,This refactors the Embedding layer to the new operator API based on NNVM according to the guide in 3248 The current new operator guide is not that easy to follow for beginners Hope we will have a more detailed document on it in the future,,"flyers,sxjscience",2016-12-05 03:35:51,2016-12-06 00:45:38
IS,could the webpage has a search engine,maybe I miss something but I have tried to look after the search engine search engine saves a lot of time would you mind adding a search engine to,,"piiswrong,kevinthesun,pluskid,piiswrong,kevinthesun,kevinthesun",2016-11-21 11:15:54,2016-12-06 00:54:28
IS,Simple offsetting way for image preprocess,Hi guys Heard the news that Amazon on board now well done By the way for ImageRecordIter mxnet can do scaling with scale argument Is there a simple way for offsetting ie transform data from range 0 2 to 1 1 as well Ca not find it in the tutorial,,piiswrong,2016-12-05 07:44:59,2016-12-06 00:57:56
IS,what is the definition of shape in mxnet,In caffe it is defined N C W H In mxnet it seemed different For example i just passed the data iterator 200 data dim is 200 and then it will log as 100 1 200 100 means batchsize why it is not 100 200 1 1,,antinucleon,2016-11-25 13:05:05,2016-12-06 03:23:54
IS,what is the usage of concat,Here is my network input mx symbol Variable wouldata' input1 mx symbol slice axis data input axis 2 begin 0 end 187 input2 mx symbol slice axis data input axis 2 begin 188 end 287 ip1 mx symbol FullyConnected data input1 name ip1 num hidden 200 relu1 mx symbol Activation data ip1 act type relu name relu1 ip2 mx symbol FullyConnected data relu1 name ip2 num hidden 100 relu2 mx symbol Activation data ip2 act type relu name relu2 ip3 mx symbol FullyConnected data relu2 name ip3 num hidden 16 relu3 mx symbol Activation data ip3 act type relu name relu3 concat layer concat input mx symbol Concat relu3 input2 dim 2 ip5 mx symbol FullyConnected data concat input name ip5 num hidden 2 net mx symbol SoftmaxOutput data ip5 name softmax and it report 21 19 46 search data mxnet dmlc core include dmlc logging h 235 21 19 46 src operator concat inl h 144 Check failed static cast index t param dim dshape ndim the dimension to be concated is not in the range of input is dimension how can i correct it,,piiswrong,2016-11-25 13:22:15,2016-12-06 03:24:24
IS,mxnet no conversion from 'const int' to 'void const ',windows Error 50 error C2446 ' ' no conversion from 'const int' to 'void const ' D projects mxnet src engine threaded engine cc D projects mxnet dmlc core include dmlc logging h 95 1 mxnet Error 51 error C2040 ' ' 'void const ' differs in levels of indirection from 'const int' D projects mxnet src engine threaded engine cc D projects mxnet dmlc core include dmlc logging h 95 1 mxnet Error 52 error C2446 ' ' no conversion from 'const int' to 'void const ' D projects mxnet src engine threaded engine perdevice cc D projects mxnet dmlc core include dmlc logging h 95 1 mxnet Error 53 error C2040 ' ' 'void const ' differs in levels of indirection from 'const int' D projects mxnet src engine threaded engine perdevice cc D projects mxnet dmlc core include dmlc logging h 95 1 mxnet Error 60 error C2446 ' ' no conversion from 'const int' to 'void const ' D projects mxnet src engine threaded engine pooled cc D projects mxnet dmlc core include dmlc logging h 95 1 mxnet Error 61 error C2040 ' ' 'void const ' differs in levels of indirection from 'const int' D projects mxnet src engine threaded engine pooled cc D projects mxnet dmlc core include dmlc logging h 95 1 mxnet Error 68 error C1001 An internal error has occurred in the compiler d projects mxnet src ndarray ndarray cc 175 1 mxnet Error 72 error D8040 error creating or communicating with child process D projects mxnet build cl mxnet,,"piiswrong,piiswrong,yajiedesign,yajiedesign,yajiedesign,yajiedesign,yajiedesign,yajiedesign",2016-12-02 15:01:18,2016-12-06 04:15:07
IS,neural style example run error without gpu,my os is ei 10 11 6 and no N card so install mxnet without gpu enabled how can i fix it image,,,2016-12-06 06:28:05,2016-12-06 06:34:57
IS,MXNet build error on macOS Sierra cuda 8 0 cudnn 5 0,I build the latest version of mxnet and found the following problems The nvcc versioin,,kevinthesun,2016-12-05 08:46:12,2016-12-06 07:08:59
PR,fix make error,Old version caused 404 error,,piiswrong,2016-11-30 14:08:37,2016-12-06 07:59:36
PR,Adding USE MKL2017 option to cmake build,MKL is detected and MXNET USE MKL2017 is now enabled in the cmake build This attempts to follow to usage in the makefile directly This is disabled by default,,"alextnewman,piiswrong,alextnewman",2016-11-28 15:54:57,2016-12-06 08:06:17
PR,OP Move reshape and flatten to nnvm,The FCompute directly calls IdentityCompute while the backward node uses backward copy Also nnvm Tuple int is used as the type of shape which may contain negative values,,"sxjscience,piiswrong,tqchen,piiswrong,sxjscience,sxjscience",2016-12-05 09:22:50,2016-12-06 08:34:30
PR,Update navbar html,,,jspisak,2016-12-06 19:16:05,2016-12-06 19:25:21
PR,Create modelzoo md,,,"jspisak,piiswrong,mli",2016-12-06 19:19:16,2016-12-06 19:27:03
IS,OneHot Layer,piiswrong we would really like to support a one hot encoding symbol so one can easily switch between using mx Embedding layer and one hot in ones computational graph Can I add this Or is there a reason you want to avoid using it and hence wo not merge this layer,,"sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein",2016-12-06 13:16:12,2016-12-06 20:34:34
PR,doc convert markdown tables into rst before building,,,mli,2016-12-06 23:44:11,2016-12-06 23:45:36
PR,docs also convert md sentencies in a table into rst,,,mli,2016-12-07 00:13:03,2016-12-07 00:13:09
PR,docs remove n r in pandoc converted results,,,mli,2016-12-07 00:45:54,2016-12-07 00:45:59
PR,example image classification ResNeXt symbol,ResNeXt 101 top1 error 21 73 top5 error 5 92 ResNeXt 50 top1 error 23 97 top5 error 7 13 As the RexNeXt paper I used resnet v1 to train ResNeXt 101 model ResNeXt 50 is based on resnet v2 I will retrain ResNeXt50 this week,,"terrychenism,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,terrychenism,tornadomeet,piiswrong,tornadomeet,terrychenism,piiswrong,tornadomeet,piiswrong,mli,terrychenism,tornadomeet,mli,piiswrong,mli,piiswrong,mli,piiswrong,terrychenism,mli,terrychenism",2016-12-05 20:11:50,2016-12-07 02:11:11
IS,image file name in ImageRecordIter,Hello Is there any method to get a image file name with ImageRecordIter My case is to extract features of each image with pre trained model Then I will do some image clustering with these features Now I can extract features by using these code data shape args rgb args shape args shape images mx io ImageRecordIter path imgrec args data file mean img args min file rand crop False rand mirror False data shape data shape batch size 1 model mx model FeedForward load args prefix args iter internals model symbol get internals layer args layer ' output' fea symbol internals layer feature extractor mx model FeedForward ctx mx cpu symbol fea symbol numpy batch size 1 arg params model arg params aux params model aux params allow extra params True global pooling feature feature extractor predict images a np asarray global pooling feature np savetxt ' pred ' args layer ' csv' a But I do not know each feature belongs to which image I want to create a csv file with this format filename f1 f2 fn is there a way to do this should I set the shuffle True in ImageRecordIter first,,piiswrong,2016-12-06 07:48:59,2016-12-07 02:14:40
PR,fixed the main function of convert symbol py,This error is occurred when execute tools caffe converter convert symbol py Traceback most recent call last File convert symbol py line 211 in module main File convert symbol py line 203 in main symbol string output name proto2script sys argv 1 ValueError too many values to unpack I fixed the main function of convert symbol py 4121,,,2016-12-07 02:50:52,2016-12-07 03:52:11
PR,MKL feature enhance,1 Support BM V2 API mean var 2 Pool Asymmetric padding filled feature Signed off by lingyan lingyan guo intel com,,"glingyan,piiswrong,glingyan,glingyan",2016-12-07 01:17:10,2016-12-07 03:55:16
IS,there is something wrong about the main function in mxnet tools caffe converter convert symbol py,In the main function the return of proto2script sys argv 1 should be 3 values not 2 values,,piiswrong,2016-12-06 17:02:13,2016-12-07 05:12:55
PR,fix operators and improve error message,,,"piiswrong,sxjscience",2016-12-07 04:57:37,2016-12-07 05:37:58
IS,Are there some performance benchmark examples or tutorials,piiswrong I want to run some experiments for performace benchmark to figure out and compare the speed of mxnet in different conditions e g 1 GPU 2 GPUs multi GPUs on multi machines are there some existing examples which I can repeat,,"pineking,piiswrong,pineking",2016-12-07 01:39:11,2016-12-07 05:47:44
IS,The tag of docker image should have the version information,The pre built docker images from the link are out of date and the tag of the docker images should have the version information to help users to identify the version of mxnet,,pineking,2016-12-05 10:25:31,2016-12-07 06:43:37
PR,Add CUDA 7 5 and 8 0 Dockerfiles,Fix 4102,,"pineking,piiswrong,piiswrong,pineking,piiswrong,pineking,pineking,piiswrong,pineking",2016-12-06 07:34:03,2016-12-07 06:43:37
PR,Fix amalgamation scipt,Fix amalgamation scipt and document that all other dependencies shall be disabled in config mk This includes the changes proposed in issuecomment 265118194 Without these changes calling e g make MIN 1 libmxnet predict js in the amalgamation folder will fail due to unresolved dependencies e g include mkl mkl batch norm inl h,,leezu,2016-12-07 06:27:35,2016-12-07 07:48:52
PR,fix error when enable monitor for rcnn,,,howard0su,2016-12-07 06:45:46,2016-12-07 07:49:03
PR,Improve build system to avoid calling pkg config many times,,,"howard0su,piiswrong,howard0su,pineking,piiswrong,howard0su,piiswrong,howard0su,piiswrong,howard0su",2016-12-06 03:56:10,2016-12-07 07:49:40
PR,deprecate model,,,piiswrong,2016-12-07 08:11:31,2016-12-07 19:16:07
PR,Update index md,Updated the contribution section,,jspisak,2016-12-07 19:36:14,2016-12-07 19:53:22
IS,R package Minimum Source File,Currently the R package contains mxnet generated R NAMESPACE and Rd files in man folder Update of NAMESPACE need to be manually done by first installing a version then call rcppexport and roxygen Sometimes installation can fail when the exported function changes and no longer exists in the namespace I would like to automate this process by the follows Remove mxnet genenrated R and Rd files in the man folder Have a minimum NAMESPACE file in R package say NAMESPACE init This namespace only exports the rcpp export function in make rpkg script first call rcppexport either by minimum install or some other way without installation then roxygen finally build the r package can you look into this,,"tqchen,thirdwing,tqchen,tqchen,thirdwing",2016-09-21 01:51:23,2016-12-07 21:23:13
IS,broken link to MXNet R Package Document,This page does not exist yet,,"thirdwing,piiswrong,thirdwing",2016-10-18 15:39:07,2016-12-07 22:08:05
PR,fix compatibility,leopd,,piiswrong,2016-12-07 23:18:23,2016-12-08 03:47:46
PR,Add add stn parameter to avoid error,,,howard0su,2016-12-08 03:49:03,2016-12-08 04:15:21
IS,A question about kvstore and optimizer,Is optimizer py only executed on server node It seems the server node gives every node the same learning rate How do other nodes get the learning rate from the server node From kvstore If we want different node training with different learning rate how to do it,,mli,2016-12-04 13:46:04,2016-12-08 08:53:01
IS,A question about parameter updating,In model py what is the difference between these 2 functions def update params on kvstore param arrays grad arrays kvstore def update params param arrays grad arrays updater num device kvstore None L87 Both of these 2 functions have kvstore push kvstore pull,,"tornadomeet,tornadomeet",2016-12-04 13:37:44,2016-12-08 08:53:40
IS,A question about optimizers of all nodes,Every node create an object optimizer in model py but we only use the optimizer of the server node Then what about other optimizers,,,2016-12-08 09:28:13,2016-12-08 09:52:22
PR,close 2185 and close 4095,,,thirdwing,2016-12-08 15:41:22,2016-12-09 00:16:02
IS,Question about multiple parameter servers,when using multiple parameter servers how does the worker choose which parameter push to which parameter server where is the code thx,,mli,2016-12-06 08:57:16,2016-12-09 01:13:12
PR,example image classification add distributed scalability results,,,mli,2016-12-09 03:40:38,2016-12-09 03:40:50
IS,How to add attributes to an existing symbol,I am using a python version of MXNet on Ubuntu 16 04 OS I wanted to fine tune the VGG 16 model The symbols loaded from vgg16 symbol json did not have the 'lr mult' attributes I know how to set the attributes when creating the variables v1 mx sym Variable 'v1' attr 'attr1' 'value1' But how to add an attribute to an existing variable THX,,kevinthesun,2016-12-09 02:49:00,2016-12-09 06:17:51
IS,'fixed param names' and 'lr mult' behave differently,I was using the python version of MXNet on a 64 bit Ubuntu 16 04 OS I changed the fully connected layers in the VGG 16 network and I wanted to fine tune it At first I used the 'fixed param names' in mx mod Module class and it seemed to be working very well The accuracy was getting better and better with a 75 accuracy at 10th epoch Then I saved a checkpoint at the 10th epoch changed the module by removing the 'fixed param names' parameters set the 'lr mult' to 0 by calling 'opt set lr mult ' and finally loaded the checkpoint file to continue the training process However the accuracy rapidly dropped to about 50 2 classes were included in the training set Here is the code segment for using 'fixed param names' net arg params aux params mx model load checkpoint ' model vgg16' 0 name list k for k in arg params if not 'fc' in k mod mx module Module net context ctx work load list wl fixed param names name list Here is the codes for iset lr mult' method opt mx optimizer Adam learning rate 0 001 mult dict k 0 0 for k in arg params if not 'fc' in k opt set lr mult mult dict mod init optimizer optimizer opt I understand that the 'fixed param names' parameter is related to 'grad req' when calling the executor and no memory is allocated for gradients in this way For 'lr mult' case the gradients are calculated but are not added to the weights as the learning rate was 0 But I guess this should only make difference in memory occupation and computation speed The result should have been the same as the weights are the same in both cases Why were they different Maybe I was wrong in understanding the matters Could someone help me with that By the way I cannot understand the difference between 'fixed param names' and the 'BlockGrad' operator I guess both of them save the memory cosuming only that 'BlockGrad' cut off the backpropagation completely I guess this could be implemented with 'fixed param names' by specifying all layers before the blocking node right Furthermore if I wanted to freeze the middle part of a network while keep the rest parts trainable trainable parts frozen parts trainable parts deviation I guess that 'BlockGrad' would not help but 'fixed param names' would function As the results of my training was not correct I guess there must have been something wrong in my understanding Would someone please help me THX,,"precedenceguo,II-Matto",2016-12-09 01:33:39,2016-12-09 06:24:53
PR,change default quality from 80 to 95 for bigger and deeper network,,,tornadomeet,2016-12-09 02:14:38,2016-12-09 07:44:23
PR,fix segmentation faults in c api,We tried to use the c api to create recordio files and observed segmentation faults This should fix it The root of the problem was the usage of void instead of void in the api interface,,jemaw,2016-12-08 14:26:27,2016-12-09 07:48:06
PR,split tensor operator files,,,piiswrong,2016-12-08 07:31:49,2016-12-09 07:50:56
IS,Invalid URL in R Package mxnet PDF Manual,thirdwing Hi Kou Qiang thanks a lot for your great works so that we can try mxnet within R I found that the URL included in the PDF manual is invalid At the moment it is and it does not work Maybe it should be instead Thanks,,thirdwing,2016-12-05 01:29:41,2016-12-09 15:10:22
IS,GPU memory consumption,Hi guys could you please clarify the following question concerning gpu memory consumption by mxnet The thing is that it looks like when I train a model written on Lasagne Theano the gpu memory consumption is much smaller then when I use mxnet E g I can train the model written on lasagne with around 30 millions of trainable parameters with input size of the images 3x512x512 and batch size 128 on 4GB GPU On the other hand on the same GPU when I use mxnet I can train a model based on symbol resnet py with 10 millions of parameters with the same input size of the images and batch size 45 So it looks like mxnet consumes much memory then lasagne Is it normal Thank you in advance,,,2016-12-08 16:39:48,2016-12-09 15:12:40
PR,R add example of using pipe,szilard,,thirdwing,2016-12-09 22:06:49,2016-12-09 22:15:56
PR,Fixed bug in ReduceAxesBackwardUseInOut NNVM reduce ops backward pass,Fixed a bug in ReduceAxesBackwardUseInOut that caused some NNVM reduce ops to fail specifically min and max I have added test cases which now exercise this code path PS is nnvm the right branch to commit this to,,"alex-weaver,piiswrong,piiswrong,piiswrong",2016-12-09 13:45:20,2016-12-09 22:19:49
PR,Python Python2 3 Support for examples autoencoder bayesian bi lst,python compatibility support autoencoder bayesian bi lstm cnn text classification dec This PR is related to 4071,,"jostep,piiswrong,piiswrong,piiswrong,jostep,jostep,jostep,piiswrong,piiswrong,piiswrong,piiswrong,jostep,piiswrong,jostep,sxjscience,jostep,piiswrong",2016-12-05 17:02:15,2016-12-09 22:24:28
PR,rename conv6 conv7 to fc6 fc7,in vgg16 reduced model conv6 and conv7 is actual named as fc6 fc7 Following the convention so the weight in the pre trained model is used,,"howard0su,piiswrong,howard0su,piiswrong,zhreshold",2016-12-10 01:49:01,2016-12-10 05:50:14
PR,Model zoo style,Fix model zoo related style issues,,"kevinthesun,piiswrong,kevinthesun,piiswrong,kevinthesun,mli,mli",2016-12-09 20:15:45,2016-12-10 05:51:52
IS,Error running Profiler in NNVM branch,Hey I'm checking out the new profiler in the NNVM branch however when I run python profiler executor py I get the following error I had compiled with USE PROFILER 1 flag,,"ZihengJiang,piiswrong,ZihengJiang",2016-11-25 00:50:52,2016-12-10 08:34:06
IS,R Data augmentation with basic MNIST example,I want to create a custom iterator in R As an initial example I will use the MNIST data What I would like to do is an iterator that gets as input the raw data that would be a matrix of 60000 examples times 784 which is each image expanded in a vector Any help appreciated,,"miguelgfierro,thirdwing,miguelgfierro,thirdwing,miguelgfierro,thirdwing,miguelgfierro,thirdwing",2016-12-05 15:55:31,2016-12-10 08:51:52
IS,R CSVIter parsing strings in R,I found that CSVIter does not take strings I have fake csv Is there a workaround to parse strings This issue is related to 3079,,"miguelgfierro,miguelgfierro",2016-08-28 08:45:59,2016-12-10 08:53:02
PR,OP Topk and arange,Continuing of 1 Fix the argument of dot and batch dot 2 New operators topk sort argsort arange Also the previous internal operators zeros and ones have been added to the python script as mx sym zeros and mx sym ones 3 Move operator to operator tensor BlockGrad 4 Add the argument dtype to initialization operators zeros ones arange 5 Enable gradient for argmax and argmin by adding the gradient node to zeros Some Notes The implementation of TopK is not so optimized at this stage The implementation is to first sort the data and then keep the top k indices values Also the axis argument is supported in topk and topk is equivalent to argsort and sort if we set k 1 and ret typ indices or value The arange operator has an additional repeat argument which indicates the number of times each element will be repeated in the generation process,,"sxjscience,piiswrong,sxjscience,piiswrong,piiswrong,sxjscience,piiswrong",2016-12-01 07:36:14,2016-12-10 10:32:45
PR,Fixed bug in MSE,,,miguelgfierro,2016-12-10 09:00:24,2016-12-10 16:32:10
PR,Fix typo,,,"piiswrong,thirdwing,thirdwing",2016-12-08 14:05:52,2016-12-11 00:59:11
PR,DOC R simple docs for using own loss function,Just a simple example for using makeloss in R This should close 3368 2922 and 4166,,thirdwing,2016-12-11 05:01:20,2016-12-11 05:29:49
PR,Add a note for warpctc is example 'python lstm ocr py',We have to modify some code before run demo So add a note to point out what is should be noticed,,BobLiu20,2016-12-07 05:01:38,2016-12-11 05:43:18
IS,How to use mxnet model predict when the network has multi input,,,,2016-12-07 07:43:27,2016-12-11 07:33:32
IS,How can I make a predict with a lstm ctc checkpoint,I following example warpctc lstm ocr py to training a model Now I had saved a checkpoint mymodel 0100 params and mymodel symbol json So How can I make a predict with this checkpoint use only one image I had tired to use Predictor interface code below But data shape always raise error and I do not know how to set this value Anybody help me thank you,,"BobLiu20,BobLiu20,BobLiu20,BobLiu20,BobLiu20,BobLiu20",2016-12-09 13:27:44,2016-12-11 10:25:56
PR,fixed error in as matrix MXNDArray,,,miguelgfierro,2016-12-11 09:08:43,2016-12-11 16:03:07
IS,error in as array MXNDArray in R,When converting form array to MXNDArray I get the following error Any idea,,"miguelgfierro,thirdwing,thirdwing,miguelgfierro,thirdwing,miguelgfierro",2016-12-11 08:50:24,2016-12-11 19:23:28
IS,mx nd array can not be asnumpy in the evaluation metric,Hi I build a multi input output net and write a dataiter for it Here is the dataiter code I use the Multi Accuracy in the example multi task example multi task py as the eva metric But it was stuck in this line L22 And when I tried to transform the preds to np it also get stuck But the label labels i asnumpy astype 'int32' runs normally So I think there is something wrong with my dataiter Could you help me find out what is wrong with the dataiter Many thanks,,piiswrong,2016-12-02 07:09:24,2016-12-12 04:45:29
PR,OP Topk and arange,Use optional continuing,,"sxjscience,sxjscience",2016-12-10 10:32:38,2016-12-12 05:07:51
PR,Old Keras Add support for element wise comparison operators 4182,Added element wise and comparison operators,,"shivarajugowda,piiswrong,piiswrong,piiswrong,shivarajugowda,shivarajugowda,shivarajugowda,shivarajugowda,piiswrong,shivarajugowda",2016-12-11 08:40:10,2016-12-12 05:28:03
IS,problem with bucketing lstm and cnn,Hi everyone I want to use lstm and cnn to implement text recognition from image for example recognize 'good' from a text image bounding box and I use warp ctc as loss function but I find my softmax out are always the same like 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 0 01282051 dtype float32 0 01282051 1 78 as I have 78 different chars to classify my dataiter is based on bucket io py in rnn example dir and lstm part is based on warp ctc example I'm confused about this problem do you have any ideas Thank a lot,,piiswrong,2016-12-12 05:49:56,2016-12-12 07:12:47
IS,no error but install failed,I am a student who want to study dl and mxnet I followed the documents of MXNET install on Mac option1 No errors but warnings In the end i cannot import mxnet Environment info Operating System macos 10 12 Package used Python python2 7 12 MXNet version latest Error Message No errors just warning 1 install Last login Mon Dec 12 20 16 38 on ttys000 namedeMacBook Pro name git clone recursive Cloning into 'mxnet' remote Counting objects 32038 done remote Total 32038 delta 0 reused 0 delta 0 pack reused 32037 Receiving objects 100 32038 32038 12 29 MiB 1 00 MiB s done Resolving deltas 100 19481 19481 done Submodule wouldmlc core' registered for path wouldmlc core' Submodule 'mshadow' registered for path 'mshadow' Submodule 'ps lite' registered for path 'ps lite' Cloning into ' Users name mxnet dmlc core' remote Counting objects 3879 done remote Total 3879 delta 0 reused 0 delta 0 pack reused 3878 Receiving objects 100 3879 3879 864 87 KiB 264 00 KiB s done Resolving deltas 100 2293 2293 done Cloning into ' Users name mxnet mshadow' remote Counting objects 4048 done remote Total 4048 delta 0 reused 0 delta 0 pack reused 4048 Receiving objects 100 4048 4048 1 30 MiB 511 00 KiB s done Resolving deltas 100 2782 2782 done Cloning into ' Users name mxnet ps lite' remote Counting objects 1877 done remote Total 1877 delta 0 reused 0 delta 0 pack reused 1877 Receiving objects 100 1877 1877 576 01 KiB 336 00 KiB s done Resolving deltas 100 1196 1196 done Submodule path wouldmlc core' checked out '15cfc3dc3e911db4b83c585305ed4c5b8ba4bf9c' Submodule path 'mshadow' checked out 'bb0f8c7b8e5411299b01cf888970be2230fdf762' Submodule path 'ps lite' checked out '4a060e4e8aa40c3a931a0f8af9211279e012f8a2' namedeMacBook Pro name cd mxnet namedeMacBook Pro mxnet name cp make osx mk config mk namedeMacBook Pro mxnet name make j sysctl n hw ncpu g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src initialize o src initialize cc build src initialize d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src resource o src resource cc build src resource d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src c api c api o src c api c api cc build src c api c api d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src c api c api error o src c api c api error cc build src c api c api error d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src c api c api error cc o build src c api c api error o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src initialize cc o build src initialize o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src resource cc o build src resource o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src c api c api cc o build src c api c api o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src c api c predict api o src c api c predict api cc build src c api c predict api d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src c api c predict api cc o build src c api c predict api o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src common mxrtc o src common mxrtc cc build src common mxrtc d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src common mxrtc cc o build src common mxrtc o src c api c api cc 666 7 warning moving a temporary object prevents copy elision Wpessimizing move std move shalow s ListAttrShallow s ListAttr src c api c api cc 666 7 note remove std move call here std move shalow s ListAttrShallow s ListAttr src c api c api cc 702 22 warning moving a temporary object prevents copy elision Wpessimizing move ret ret vec str std move s ListArguments src c api c api cc 702 22 note remove std move call here ret ret vec str std move s ListArguments src c api c api cc 718 22 warning moving a temporary object prevents copy elision Wpessimizing move ret ret vec str std move s ListOutputs src c api c api cc 718 22 note remove std move call here ret ret vec str std move s ListOutputs src c api c api cc 734 22 warning moving a temporary object prevents copy elision Wpessimizing move ret ret vec str std move s ListAuxiliaryStates src c api c api cc 734 22 note remove std move call here ret ret vec str std move s ListAuxiliaryStates g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src engine engine o src engine engine cc build src engine engine d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src engine engine cc o build src engine engine o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src engine naive engine o src engine naive engine cc build src engine naive engine d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src engine naive engine cc o build src engine naive engine o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src engine threaded engine o src engine threaded engine cc build src engine threaded engine d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src engine threaded engine cc o build src engine threaded engine o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src engine threaded engine perdevice o src engine threaded engine perdevice cc build src engine threaded engine perdevice d In file included from src engine threaded engine perdevice cc 7 Users name mxnet dmlc core include dmlc omp h 13 9 warning Warning OpenMP is not available project will be compiled into single thread code Use OpenMP enabled compiler to get benefit of multi threading W pragma messages pragma message Warning OpenMP is not available 1 warning generated g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src engine threaded engine perdevice cc o build src engine threaded engine perdevice o In file included from src engine threaded engine perdevice cc 7 Users name mxnet dmlc core include dmlc omp h 13 9 warning Warning OpenMP is not available project will be compiled into single thread code Use OpenMP enabled compiler to get benefit of multi threading W pragma messages pragma message Warning OpenMP is not available In file included from src engine threaded engine perdevice cc 10 Users name mxnet dmlc core include dmlc concurrency h 30 22 warning braces around scalar initializer Wbraced scalar init Spinlock lock ATOMIC FLAG INIT Library Developer CommandLineTools usr bin include c v1 atomic 1779 26 note expanded from macro 'ATOMIC FLAG INIT' define ATOMIC FLAG INIT false g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src engine threaded engine pooled o src engine threaded engine pooled cc build src engine threaded engine pooled d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src engine threaded engine pooled cc o build src engine threaded engine pooled o In file included from src engine threaded engine pooled cc 9 Users name mxnet dmlc core include dmlc concurrency h 30 22 warning braces around scalar initializer Wbraced scalar init Spinlock lock ATOMIC FLAG INIT Library Developer CommandLineTools usr bin include c v1 atomic 1779 26 note expanded from macro 'ATOMIC FLAG INIT' define ATOMIC FLAG INIT false g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src io image aug default o src io image aug default cc build src io image aug default d 4 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src io io o src io io cc build src io io d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src io image aug default cc o build src io image aug default o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src io io cc o build src io io o 2 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src io iter csv o src io iter csv cc build src io iter csv d 1 warning generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src io iter image recordio o src io iter image recordio cc build src io iter image recordio d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src io iter csv cc o build src io iter csv o In file included from src io iter image recordio cc 9 Users name mxnet dmlc core include dmlc omp h 13 9 warning Warning OpenMP is not available project will be compiled into single thread code Use OpenMP enabled compiler to get benefit of multi threading W pragma messages pragma message Warning OpenMP is not available 1 warning generated g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src io iter image recordio cc o build src io iter image recordio o In file included from src io image aug default cc 11 In file included from src io image augmenter h 12 In file included from usr local Cellar opencv 2 4 13 1 include opencv2 opencv hpp 73 usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 961 18 warning 'CvForestTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData const CvMat subsa usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 1149 18 warning 'CvBoostTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag In file included from src io io cc 5 In file included from src io image augmenter h 12 In file included from usr local Cellar opencv 2 4 13 1 include opencv2 opencv hpp 73 usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 961 18 warning 'CvForestTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData const CvMat subsa usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 1149 18 warning 'CvBoostTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag In file included from src io iter image recordio cc 9 Users name mxnet dmlc core include dmlc omp h 13 9 warning Warning OpenMP is not available project will be compiled into single thread code Use OpenMP enabled compiler to get benefit of multi threading W pragma messages pragma message Warning OpenMP is not available In file included from src io iter image recordio cc 21 In file included from src io image augmenter h 12 In file included from usr local Cellar opencv 2 4 13 1 include opencv2 opencv hpp 73 usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 961 18 warning 'CvForestTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData const CvMat subsa usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 1149 18 warning 'CvBoostTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag 2 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src io iter mnist o src io iter mnist cc build src io iter mnist d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src io iter mnist cc o build src io iter mnist o 2 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src kvstore kvstore o src kvstore kvstore cc build src kvstore kvstore d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src kvstore kvstore cc o build src kvstore kvstore o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src ndarray ndarray o src ndarray ndarray cc build src ndarray ndarray d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src ndarray ndarray cc o build src ndarray ndarray o In file included from src ndarray ndarray cc 16 In file included from usr local Cellar opencv 2 4 13 1 include opencv2 opencv hpp 73 usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 961 18 warning 'CvForestTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData const CvMat subsa usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 1149 18 warning 'CvBoostTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src ndarray ndarray function o src ndarray ndarray function cc build src ndarray ndarray function d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src ndarray ndarray function cc o build src ndarray ndarray function o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator activation o src operator activation cc build src operator activation d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator activation cc o build src operator activation o 3 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator batch norm o src operator batch norm cc build src operator batch norm d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator batch norm cc o build src operator batch norm o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator block grad o src operator block grad cc build src operator block grad d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator block grad cc o build src operator block grad o 2 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator broadcast mask op o src operator broadcast mask op cc build src operator broadcast mask op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator broadcast mask op cc o build src operator broadcast mask op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator broadcast reduce op o src operator broadcast reduce op cc build src operator broadcast reduce op d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator cast o src operator cast cc build src operator cast d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator broadcast reduce op cc o build src operator broadcast reduce op o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator cast cc o build src operator cast o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator concat o src operator concat cc build src operator concat d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator concat cc o build src operator concat o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator convolution o src operator convolution cc build src operator convolution d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator convolution cc o build src operator convolution o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator correlation o src operator correlation cc build src operator correlation d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator crop o src operator crop cc build src operator crop d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator correlation cc o build src operator correlation o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator crop cc o build src operator crop o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator cross device copy o src operator cross device copy cc build src operator cross device copy d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator cross device copy cc o build src operator cross device copy o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator cudnn batch norm o src operator cudnn batch norm cc build src operator cudnn batch norm d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator cudnn batch norm cc o build src operator cudnn batch norm o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator cudnn convolution o src operator cudnn convolution cc build src operator cudnn convolution d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator cudnn convolution cc o build src operator cudnn convolution o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator custom o src operator custom cc build src operator custom d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator custom cc o build src operator custom o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator deconvolution o src operator deconvolution cc build src operator deconvolution d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator deconvolution cc o build src operator deconvolution o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator dropout o src operator dropout cc build src operator dropout d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator dropout cc o build src operator dropout o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator elementwise binary broadcast op o src operator elementwise binary broadcast op cc build src operator elementwise binary broadcast op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator elementwise binary broadcast op cc o build src operator elementwise binary broadcast op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator elementwise binary op o src operator elementwise binary op cc build src operator elementwise binary op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator elementwise binary op cc o build src operator elementwise binary op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator elementwise binary scalar op o src operator elementwise binary scalar op cc build src operator elementwise binary scalar op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator elementwise binary scalar op cc o build src operator elementwise binary scalar op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator elementwise sum o src operator elementwise sum cc build src operator elementwise sum d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator elementwise sum cc o build src operator elementwise sum o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator elementwise unary op o src operator elementwise unary op cc build src operator elementwise unary op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator elementwise unary op cc o build src operator elementwise unary op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator embedding o src operator embedding cc build src operator embedding d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator embedding cc o build src operator embedding o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator fully connected o src operator fully connected cc build src operator fully connected d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator fully connected cc o build src operator fully connected o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator identity attach KL sparse reg o src operator identity attach KL sparse reg cc build src operator identity attach KL sparse reg d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator instance norm o src operator instance norm cc build src operator instance norm d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator identity attach KL sparse reg cc o build src operator identity attach KL sparse reg o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator instance norm cc o build src operator instance norm o In file included from src operator identity attach KL sparse reg cc 6 src operator identity attach KL sparse reg inl h 150 39 warning 'BackwardInplaceOption' overrides a member function but is not marked 'override' Winconsistent missing override std vector std pair int void BackwardInplaceOption include mxnet operator h 424 47 note overridden virtual function is here virtual std vector std pair int void BackwardInplaceOption 1 warning generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator l2 normalization o src operator l2 normalization cc build src operator l2 normalization d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator leaky relu o src operator leaky relu cc build src operator leaky relu d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator l2 normalization cc o build src operator l2 normalization o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator leaky relu cc o build src operator leaky relu o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator loss binary op o src operator loss binary op cc build src operator loss binary op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator loss binary op cc o build src operator loss binary op o In file included from src operator loss binary op cc 6 src operator loss binary op inl h 68 52 warning implicit conversion from 'float' to 'unsigned char' changes value from 9 9999999E 9 to 0 Wliteral conversion scalar DType 1e 8f Users name mxnet mshadow mshadow packet base h 624 8 note expanded from macro 'MSHADOW TYPE SWITCH' VA ARGS src operator loss binary op inl h 103 30 note in instantiation of function template specialization 'mxnet op SoftmaxCrossEntropyForward mshadow cpu ' requested here set function XPU kDevMask SoftmaxCrossEntropyForward XPU kNoInplace src operator loss binary op inl h 68 52 warning implicit conversion from 'float' to 'int' changes value from 9 9999999E 9 to 0 Wliteral conversion scalar DType 1e 8f Users name mxnet mshadow mshadow packet base h 630 8 note expanded from macro 'MSHADOW TYPE SWITCH' VA ARGS 2 warnings generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator lrn o src operator lrn cc build src operator lrn d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator lrn cc o build src operator lrn o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator make loss o src operator make loss cc build src operator make loss d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator make loss cc o build src operator make loss o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator matrix op o src operator matrix op cc build src operator matrix op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator matrix op cc o build src operator matrix op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator native op o src operator native op cc build src operator native op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator native op cc o build src operator native op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator ndarray op o src operator ndarray op cc build src operator ndarray op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator ndarray op cc o build src operator ndarray op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator operator o src operator operator cc build src operator operator d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator operator cc o build src operator operator o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator operator util o src operator operator util cc build src operator operator util d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator operator util cc o build src operator operator util o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator pad o src operator pad cc build src operator pad d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator pad cc o build src operator pad o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator pooling o src operator pooling cc build src operator pooling d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator pooling cc o build src operator pooling o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator regression output o src operator regression output cc build src operator regression output d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator regression output cc o build src operator regression output o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator reshape o src operator reshape cc build src operator reshape d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator reshape cc o build src operator reshape o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator rnn o src operator rnn cc build src operator rnn d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator rnn cc o build src operator rnn o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator roi pooling o src operator roi pooling cc build src operator roi pooling d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator roi pooling cc o build src operator roi pooling o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator sample op o src operator sample op cc build src operator sample op d g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator sequence last o src operator sequence last cc build src operator sequence last d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator sample op cc o build src operator sample op o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator sequence last cc o build src operator sequence last o In file included from src operator sequence last cc 7 In file included from src operator sequence last inl h 21 src operator sequence op common h 20 7 warning unused variable 'max seq len' Wunused variable int max seq len data shape Size g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator sequence mask o src operator sequence mask cc build src operator sequence mask d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator sequence mask cc o build src operator sequence mask o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator sequence reverse o src operator sequence reverse cc build src operator sequence reverse d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator sequence reverse cc o build src operator sequence reverse o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator slice channel o src operator slice channel cc build src operator slice channel d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator slice channel cc o build src operator slice channel o In file included from src operator sequence reverse cc 7 In file included from src operator sequence reverse inl h 20 src operator sequence op common h 20 7 warning unused variable 'max seq len' Wunused variable int max seq len data shape Size 1 warning generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator smooth l1 unary o src operator smooth l1 unary cc build src operator smooth l1 unary d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator smooth l1 unary cc o build src operator smooth l1 unary o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator softmax activation o src operator softmax activation cc build src operator softmax activation d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator softmax activation cc o build src operator softmax activation o 1 warning generated g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator softmax output o src operator softmax output cc build src operator softmax output d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator softmax output cc o build src operator softmax output o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator spatial transformer o src operator spatial transformer cc build src operator spatial transformer d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator spatial transformer cc o build src operator spatial transformer o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator svm output o src operator svm output cc build src operator svm output d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator svm output cc o build src operator svm output o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator swapaxis o src operator swapaxis cc build src operator swapaxis d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator swapaxis cc o build src operator swapaxis o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator upsampling o src operator upsampling cc build src operator upsampling d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator upsampling cc o build src operator upsampling o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src optimizer optimizer o src optimizer optimizer cc build src optimizer optimizer d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src optimizer optimizer cc o build src optimizer optimizer o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src optimizer sgd o src optimizer sgd cc build src optimizer sgd d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src optimizer sgd cc o build src optimizer sgd o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src storage storage o src storage storage cc build src storage storage d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src storage storage cc o build src storage storage o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src symbol graph executor o src symbol graph executor cc build src symbol graph executor d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src symbol graph executor cc o build src symbol graph executor o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src symbol graph memory allocator o src symbol graph memory allocator cc build src symbol graph memory allocator d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src symbol graph memory allocator cc o build src symbol graph memory allocator o In file included from src symbol graph executor cc 13 src symbol graph executor h 35 8 warning 'SetMonitorCallback' overrides a member function but is not marked 'override' Winconsistent missing override void SetMonitorCallback const MonitorCallback callback include mxnet symbolic h 390 16 note overridden virtual function is here virtual void SetMonitorCallback const MonitorCallback callback g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src symbol static graph o src symbol static graph cc build src symbol static graph d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src symbol static graph cc o build src symbol static graph o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src symbol symbol o src symbol symbol cc build src symbol symbol d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src symbol symbol cc o build src symbol symbol o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator mkl mkl cppwrapper o src operator mkl mkl cppwrapper cc build src operator mkl mkl cppwrapper d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator mkl mkl cppwrapper cc o build src operator mkl mkl cppwrapper o g std c 11 DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 MM MT build src operator mkl mkl memory o src operator mkl mkl memory cc build src operator mkl mkl memory d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 c src operator mkl mkl memory cc o build src operator mkl mkl memory o cd Users name mxnet dmlc core make libdmlc a USE SSE 1 config Users name mxnet config mk cd Users name mxnet g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o line split o src io line split cc g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o recordio split o src io recordio split cc g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o input split base o src io input split base cc 1 warning generated g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o io o src io cc g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o local filesys o src io local filesys cc g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o data o src data cc g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o recordio o src recordio cc In file included from src data cc 12 In file included from src data disk row iter h 19 In file included from src data libsvm parser h 13 In file included from src data text parser h 11 include dmlc omp h 13 9 warning Warning OpenMP is not available project will be compiled into single thread code Use OpenMP enabled compiler to get benefit of multi threading W pragma messages pragma message Warning OpenMP is not available g c O3 Wall Wno unknown pragmas Iinclude std c 0x fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o config o src config cc 1 warning generated ar cr libdmlc a line split o recordio split o input split base o io o local filesys o data o recordio o config o ar crv lib libmxnet a build src initialize o build src resource o build src c api c api o build src c api c api error o build src c api c predict api o build src common mxrtc o build src engine engine o build src engine naive engine o build src engine threaded engine o build src engine threaded engine perdevice o build src engine threaded engine pooled o build src io image aug default o build src io io o build src io iter csv o build src io iter image recordio o build src io iter mnist o build src kvstore kvstore o build src ndarray ndarray o build src ndarray ndarray function o build src operator activation o build src operator batch norm o build src operator block grad o build src operator broadcast mask op o build src operator broadcast reduce op o build src operator cast o build src operator concat o build src operator convolution o build src operator correlation o build src operator crop o build src operator cross device copy o build src operator cudnn batch norm o build src operator cudnn convolution o build src operator custom o build src operator deconvolution o build src operator dropout o build src operator elementwise binary broadcast op o build src operator elementwise binary op o build src operator elementwise binary scalar op o build src operator elementwise sum o build src operator elementwise unary op o build src operator embedding o build src operator fully connected o build src operator identity attach KL sparse reg o build src operator instance norm o build src operator l2 normalization o build src operator leaky relu o build src operator loss binary op o build src operator lrn o build src operator make loss o build src operator matrix op o build src operator native op o build src operator ndarray op o build src operator operator o build src operator operator util o build src operator pad o build src operator pooling o build src operator regression output o build src operator reshape o build src operator rnn o build src operator roi pooling o build src operator sample op o build src operator sequence last o build src operator sequence mask o build src operator sequence reverse o build src operator slice channel o build src operator smooth l1 unary o build src operator softmax activation o build src operator softmax output o build src operator spatial transformer o build src operator svm output o build src operator swapaxis o build src operator upsampling o build src optimizer optimizer o build src optimizer sgd o build src storage storage o build src symbol graph executor o build src symbol graph memory allocator o build src symbol static graph o build src symbol symbol o build src operator mkl mkl cppwrapper o build src operator mkl mkl memory o a build src initialize o a build src resource o a build src c api c api o a build src c api c api error o a build src c api c predict api o a build src common mxrtc o a build src engine engine o a build src engine naive engine o a build src engine threaded engine o a build src engine threaded engine perdevice o a build src engine threaded engine pooled o a build src io image aug default o a build src io io o a build src io iter csv o a build src io iter image recordio o a build src io iter mnist o a build src kvstore kvstore o a build src ndarray ndarray o g DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 shared o lib libmxnet so build src initialize o build src resource o build src c api c api o build src c api c api error o build src c api c predict api o build src common mxrtc o build src engine engine o build src engine naive engine o build src engine threaded engine o build src engine threaded engine perdevice o build src engine threaded engine pooled o build src io image aug default o build src io io o build src io iter csv o build src io iter image recordio o build src io iter mnist o build src kvstore kvstore o build src ndarray ndarray o build src ndarray ndarray function o build src operator activation o build src operator batch norm o build src operator block grad o build src operator broadcast mask op o build src operator broadcast reduce op o build src operator cast o build src operator concat o build src operator convolution o build src operator correlation o build src operator crop o build src operator cross device copy o build src operator cudnn batch norm o build src operator cudnn convolution o build src operator custom o build src operator deconvolution o build src operator dropout o build src operator elementwise binary broadcast op o build src operator elementwise binary op o build src operator elementwise binary scalar op o build src operator elementwise sum o build src operator elementwise unary op o build src operator embedding o build src operator fully connected o build src operator identity attach KL sparse reg o build src operator instance norm o build src operator l2 normalization o build src operator leaky relu o build src operator loss binary op o build src operator lrn o build src operator make loss o build src operator matrix op o build src operator native op o build src operator ndarray op o build src operator operator o build src operator operator util o build src operator pad o build src operator pooling o build src operator regression output o build src operator reshape o build src operator rnn o build src operator roi pooling o build src operator sample op o build src operator sequence last o build src operator sequence mask o build src operator sequence reverse o build src operator slice channel o build src operator smooth l1 unary o build src operator softmax activation o build src operator softmax output o build src operator spatial transformer o build src operator svm output o build src operator swapaxis o build src operator upsampling o build src optimizer optimizer o build src optimizer sgd o build src storage storage o build src symbol graph executor o build src symbol graph memory allocator o build src symbol static graph o build src symbol symbol o build src operator mkl mkl cppwrapper o build src operator mkl mkl memory o Users name mxnet dmlc core libdmlc a pthread lm framework Accelerate L usr local Cellar opencv 2 4 13 1 lib lopencv calib3d lopencv contrib lopencv core lopencv features2d lopencv flann lopencv gpu lopencv highgui lopencv imgproc lopencv legacy lopencv ml lopencv nonfree lopencv objdetect lopencv ocl lopencv photo lopencv stitching lopencv superres lopencv ts lopencv video lopencv videostab a build src ndarray ndarray function o a build src operator activation o a build src operator batch norm o a build src operator block grad o a build src operator broadcast mask op o a build src operator broadcast reduce op o a build src operator cast o a build src operator concat o a build src operator convolution o a build src operator correlation o a build src operator crop o a build src operator cross device copy o a build src operator cudnn batch norm o a build src operator cudnn convolution o a build src operator custom o a build src operator deconvolution o a build src operator dropout o a build src operator elementwise binary broadcast op o a build src operator elementwise binary op o a build src operator elementwise binary scalar op o a build src operator elementwise sum o a build src operator elementwise unary op o a build src operator embedding o a build src operator fully connected o a build src operator identity attach KL sparse reg o a build src operator instance norm o a build src operator l2 normalization o a build src operator leaky relu o a build src operator loss binary op o a build src operator lrn o a build src operator make loss o a build src operator matrix op o a build src operator native op o a build src operator ndarray op o a build src operator operator o a build src operator operator util o a build src operator pad o a build src operator pooling o g DMSHADOW FORCE STREAM Wall O3 I Users name mxnet mshadow I Users name mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 1 include opencv I usr local Cellar opencv 2 4 13 1 include DMXNET USE NVRTC 0 std c 11 o bin im2rec tools im2rec cc build src initialize o build src resource o build src c api c api o build src c api c api error o build src c api c predict api o build src common mxrtc o build src engine engine o build src engine naive engine o build src engine threaded engine o build src engine threaded engine perdevice o build src engine threaded engine pooled o build src io image aug default o build src io io a build src operator regression output o a build src operator reshape o a build src operator rnn o a build src operator roi pooling o a build src operator sample op o a build src operator sequence last o a build src operator sequence mask o a build src operator sequence reverse o a build src operator slice channel o a build src operator smooth l1 unary o a build src operator softmax activation o a build src operator softmax output o o build src io iter csv o build src io iter image recordio o build src io iter mnist o build src kvstore kvstore o build src ndarray ndarray o build src ndarray ndarray function o build src operator activation o build src operator batch norm o build src operator block grad o build src operator broadcast mask op o build src operator broadcast reduce op o build src operator cast o build src operator concat o build src operator convolution o build src operator correlation o build src operator crop o build src operator cross device copy o build src operator cudnn batch norm o build src operator cudnn convolution o build src operator custom o build src operator deconvolution o build src operator dropout o build src operator elementwise binary broadcast op o build src operator elementwise binary op o build src operator elementwise binary scalar op o build src operator elementwise sum o build src operator elementwise unary op o build src operator embedding o build src operator fully connected o build src operator identity attach KL sparse reg o build src operator instance norm o build src operator l2 normalization o build src operator leaky relu o build src operator loss binary op o build src operator lrn o build src operator make loss o build src operator matrix op o build src operator native op o build src operator ndarray op o build src operator operator o build src operator operator util o build src operator pad o build src operator pooling o build src operator regression output o build src operator reshape o build src operator rnn o build src operator roi pooling o build src operator sample op o build src operator sequence last o build src operator sequence mask o build src operator sequence reverse o build src operator slice channel o build src operator smooth l1 unary o build src operator softmax activation o build src operator softmax output o build src operator spatial transformer o build src operator svm output o build src operator swapaxis o build src operator upsampling o build src optimizer optimizer o build src optimizer sgd o build src storage storage o build src symbol graph executor o build src symbol graph memory allocator o build src symbol static graph o build src symbol symbol o build src operator mkl mkl cppwrapper o build src operator mkl mkl memory o Users name mxnet dmlc core libdmlc a pthread lm framework Accelerate L usr local Cellar opencv 2 4 13 1 lib lopencv calib3d lopencv contrib lopencv core lopencv features2d lopencv flann lopencv gpu lopencv highgui lopencv imgproc lopencv legacy lopencv ml lopencv nonfree lopencv objdetect lopencv ocl lopencv photo lopencv stitching lopencv superres lopencv ts lopencv video lopencv videostab a build src operator spatial transformer o a build src operator svm output o a build src operator swapaxis o a build src operator upsampling o a build src optimizer optimizer o a build src optimizer sgd o a build src storage storage o a build src symbol graph executor o a build src symbol graph memory allocator o a build src symbol static graph o a build src symbol symbol o a build src operator mkl mkl cppwrapper o a build src operator mkl mkl memory o clang warning argument unused during compilation ' pthread' Library Developer CommandLineTools usr bin ranlib file lib libmxnet a initialize o has no symbols Library Developer CommandLineTools usr bin ranlib file lib libmxnet a mxrtc o has no symbols Library Developer CommandLineTools usr bin ranlib file lib libmxnet a cudnn batch norm o has no symbols Library Developer CommandLineTools usr bin ranlib file lib libmxnet a cudnn convolution o has no symbols Library Developer CommandLineTools usr bin ranlib file lib libmxnet a mkl cppwrapper o has no symbols Library Developer CommandLineTools usr bin ranlib file lib libmxnet a mkl memory o has no symbols In file included from tools im2rec cc 22 In file included from usr local Cellar opencv 2 4 13 1 include opencv2 opencv hpp 73 usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 961 18 warning 'CvForestTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData const CvMat subsa usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 1149 18 warning 'CvBoostTree train' hides overloaded virtual functions Woverloaded virtual virtual bool train CvDTreeTrainData trainData usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 867 18 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 2 vs 3 virtual bool train CvMLData trainData CvDTreeParams params CvDTre usr local Cellar opencv 2 4 13 1 include opencv2 ml ml hpp 877 26 note hidden overloaded virtual function 'CvDTree train' declared here different number of parameters 8 vs 3 CV WRAP virtual bool train const cv Mat trainData int tflag 2 warnings generated 2 since it dose not show me the erros so I tried to import mxnet in python namedeMacBook Pro mxnet name python Python 2 7 12 default Oct 11 2016 05 20 59 GCC 4 2 1 Compatible Apple LLVM 8 0 0 clang 800 0 38 on darwin Type help copyright credits or license for more information import mxnet Traceback most recent call last File stdin line 1 in module ImportError No module named mxnet Steps to reproduce 1 brew update brew install pkg config brew install git brew tap homebrew science brew info opencv brew install opencv 2 git clone recursive cd mxnet cp make osx mk config mk make j sysctl n hw ncpu,,,2016-12-12 12:33:44,2016-12-12 13:11:32
PR,Keras Add support for element wise comparison operators 4182,Add support for element wise comparison operators 4182 Added elementwise and comparison operators Signed off by Shiv Gowda shivaraju gowda gmail com,,"shivarajugowda,piiswrong,shivarajugowda,piiswrong,shivarajugowda,piiswrong,shivarajugowda,piiswrong,shivarajugowda,shivarajugowda,piiswrong,piiswrong,piiswrong,shivarajugowda,piiswrong,shivarajugowda,shivarajugowda,shivarajugowda,piiswrong,shivarajugowda",2016-12-12 05:26:48,2016-12-12 19:23:22
PR,Custom iterator tutorial for R,Tutorial on how to create a custom iterator in R,,miguelgfierro,2016-12-12 13:17:13,2016-12-12 20:10:24
IS,Keras Add support for element wise comparison operators,Add support for comparison operators is already present,,shivarajugowda,2016-12-11 07:40:04,2016-12-13 00:41:07
PR,update BM to use MKL BM V2 API,only compilable for MKL release after 1115 Signed off by lingyan lingyan guo intel com,,"glingyan,piiswrong,piiswrong,glingyan,piiswrong,glingyan,piiswrong,piiswrong,glingyan,piiswrong,glingyan,glingyan,piiswrong,glingyan,glingyan,glingyan,piiswrong,piiswrong,piiswrong,glingyan,glingyan,glingyan,glingyan,piiswrong,glingyan,glingyan",2016-11-23 13:54:15,2016-12-13 08:11:33
PR,Reorganizing setup and installation guide by OS and Programming Language,1 Created separate page with instruction set for each OS type Amazon Linux ubuntu Mac Windows Also added separate set up guide for mxnet on Docker Cloud 2 Re organized content in separate section for building mxnet library python r julia scala interface package Above to re structuring will greatly improve user experience in finding right installation guide to set up 3 Fixed issue with installation steps of Mac OS,,sandeep-krishnamurthy,2016-12-13 07:26:59,2016-12-13 08:12:46
IS,Deconstruction of ThreadLocalStore T when program exits leads program crashed,Environment info Operating System Windows 10 GPU is GTX850M CUDA version 8 0 Compiler VS 2015 Package used Python R Scala Julia MXNet cpp MXNet version Or if installed from source yes Error Message The program throws exception in main Minimum reproducible example following 4 lines C code makes program crash include cuda runtime h include curand h include mxnet cpp MxNetCpp h int main int argc char argv Context ctx dev DeviceType kGPU 0 NDArray rand Shape 512 28 ctx dev false this 'NDArray' is MXNet cpp NDArray NDArray SampleGaussian 0 1 rand NDArray WaitAll What have you tried to solve it 1 NDArray SampleGaussian use cudaMallocPitch apply GPU memory 2 When 4 lines code executed the program 'prepared' to exit some global static objects were going to be released 3 CUDA runtime is also unloading since the program prepared to exit 4 dmlc ThreadLocalStore mxnet resource ResourceManagerImpl global static object was released thread local h line 57 5 The registered pointers were released thread local h line 52 6 Eventually cudaFree was called to release the memory applied in step 1 tensor gpu int h line 54 7 cudaErrorCudartUnloading was returned by cudaFree since the reason of step 3 8 dmlc Error was thrown base h line 219 9 Windows reports the program crashed dmlc ThreadLocalStore mxnet resource ResourceManagerImpl deconstruction was called by OS and the sequence of unloading mxnet variables and CUDA runtime cannot be controlled Maybe some design should be changed a little,,,2016-12-13 09:37:42,2016-12-13 15:35:38
IS,MXNDArray for strings in R,I'm trying to implement a custom iterator that uses text data In one of the parts I need to create a MXNDArray of strings However I get an error Is there a way to do this,,"miguelgfierro,piiswrong,thirdwing,miguelgfierro",2016-12-13 12:40:01,2016-12-13 19:13:10
PR,fix executor destruction,,,piiswrong,2016-12-12 20:19:31,2016-12-13 20:34:46
IS,how to train image with correct augmentation,I wanted to train ILSVRC data set preprocessed the data set with i debug around found this is because in the dataset there is picture s pixels less 227 x 227 or y 227 or so i want to ask is there a argument to make these picture with the size i want,,,2016-12-13 06:07:18,2016-12-14 00:55:21
IS,Run error status CURAND STATUS SUCCESS CURAND Gen Uniform float failed,When I run python train py with gpu this error will come out as follows 16 27 45 home cliu mxnet dmlc core include dmlc logging h 235 16 27 45 home cliu mxnet mshadow mshadow random h 344 Check failed status CURAND STATUS SUCCESS CURAND Gen Uniform float failed size 4096 16 27 45 home cliu mxnet dmlc core include dmlc logging h 235 16 27 45 src engine threaded engine h 306 16 27 45 home cliu mxnet mshadow mshadow random h 344 Check failed status CURAND STATUS SUCCESS CURAND Gen Uniform float failed size 4096 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 16 27 45 src engine threaded engine h 306 16 27 45 home cliu mxnet mshadow mshadow random h 344 Check failed status CURAND STATUS SUCCESS CURAND Gen Uniform float failed size 4096 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging I have updated the cuda version from 7 5 to 8 0 but still remain this problem When I ran the train mnist py in example image classification with gpu it works fine But when I ran my own train file with gpu the error above occured while with cpu works ok I wonder if this has something to do with cuda Any help will be appreciated PS GTX1070 ubuntu16 04,,,2016-12-12 08:39:16,2016-12-14 03:44:03
IS,how to print values of a tensor,i want to print some values of specified locations of a tensor in forward or backward op is there some function to call or how to implement this function,,"kevinthesun,zihaolucky",2016-12-13 14:52:34,2016-12-14 09:05:06
PR,Bring TensorBoard to MXNet,Hi Thanks to the community we have made some progress in bringing TensorBoard to MXNet As mentioned in 4003 we have finished the first step and now we can use TensorBoard without relying on TF so I open this PR and please review the relevant code Please check the coding style and relevant docs anything is welcomed as it is my first time to join an open source project Before I move to next step build TensorBoard alone I will add a notebook example to show its usage,,"zihaolucky,piiswrong,piiswrong,zihaolucky,zihaolucky,piiswrong,zihaolucky,piiswrong,piiswrong,piiswrong,zihaolucky,zihaolucky,piiswrong,mli,zihaolucky",2016-12-10 06:54:05,2016-12-14 19:50:28
IS,did not find the train lst from the package,No such file or directory 'VOCdevkit VOC2012 train lst',,"tornadomeet,miguelgfierro",2016-11-27 21:48:16,2016-12-14 22:31:28
PR,Model zoo page table,Change Model zoo page table style,,kevinthesun,2016-12-15 00:08:02,2016-12-15 00:09:55
PR,Change model zoo table style,Change model zoo table style,,kevinthesun,2016-12-15 00:28:56,2016-12-15 00:55:21
IS,CNN on 6x8 data in R,Dear team I am trying to use a CNN on a dataset with 6x8 data I am working in R The dataset has the following header feat1 01 feat1 02 feat1 03 feat1 04 feat1 05 feat1 06 feat1 07 feat1 08 feat2 01 feat2 02 feat2 03 feat2 04 feat2 05 feat2 06 feat2 07 feat2 08 feat3 01 feat3 02 feat3 03 feat3 04 feat3 05 feat3 06 feat3 07 feat3 08 feat4 01 feat4 02 feat4 03 feat4 04 feat4 05 feat4 06 feat4 07 feat4 08 feat5 01 feat5 02 feat5 03 feat5 04 feat5 05 feat5 06 feat5 07 feat5 08 feat6 01 feat6 02 feat6 03 feat6 04 feat6 05 feat6 06 feat6 07 feat6 08 objective 0 74629 0 17357 0 03206 0 02836 0 01762 0 26987 0 99520 0 04293 1 00000 0 97841 0 68888 0 79276 0 48054 0 67120 0 55006 0 55094 0 18272 0 66293 0 04991 0 23771 0 69086 0 01079 0 50588 0 53587 0 51685 0 51547 0 28356 0 48355 0 23543 0 45552 0 31626 0 22796 0 63986 0 61482 0 40867 0 57193 0 31578 0 53067 0 41354 0 37138 0 99783 0 94881 0 77554 0 89344 0 69781 0 85418 0 75107 0 71303 1 0 0 43378 0 74629 0 17357 0 03206 0 02836 0 01762 0 26987 0 99520 0 88242 1 00000 0 97841 0 68888 0 79276 0 48054 0 67120 0 55006 0 65165 0 18272 0 66293 0 04991 0 23771 0 69086 0 01079 0 50588 0 27281 0 51685 0 51547 0 28356 0 48355 0 23543 0 45552 0 31626 0 48579 0 63986 0 61482 0 40867 0 57193 0 31578 0 53067 0 41354 0 90396 0 99783 0 94881 0 77554 0 89344 0 69781 0 85418 0 75107 1 0 I am attaching the full dataset csv dataset zip the code I am using is the following library mxnet Data preparation d read csv wouldataset csv' sep header T train d 100 1000 test d 1001 1100 train data matrix train test data matrix test train x train ncol train train y train ncol train train x t train x test org test test test ncol test test t test table train y Convolutional NN data mx symbol Variable wouldata' first conv conv1 mx symbol Convolution data data kernel c 4 4 num filter 20 tanh1 mx symbol Activation data conv1 act type tanh pool1 mx symbol Pooling data tanh1 pool type max kernel c 2 2 stride c 2 2 second conv conv2 mx symbol Convolution data pool1 kernel c 4 4 num filter 50 tanh2 mx symbol Activation data conv2 act type tanh pool2 mx symbol Pooling data tanh2 pool type max kernel c 2 2 stride c 2 2 first fullc flatten mx symbol Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 50 tanh3 mx symbol Activation data fc1 act type tanh second fullc fc2 mx symbol FullyConnected data tanh3 num hidden 10 loss lenet mx symbol SoftmaxOutput data fc2 train array train x dim train array c 6 8 1 ncol train x test array test dim test array c 6 8 1 ncol test mx set seed 0 tic proc time model mx model FeedForward create lenet X train array y train y ctx device cpu num round 20 array batch size 100 learning rate 0 05 momentum 0 9 wd 0 00001 eval metric mx metric accuracy epoch end callback mx callback log train metric 100 When I run the R code I get the following 22 30 31 root mxnet mxnet dmlc core include dmlc logging h 235 22 30 31 src operator convolution inl h 373 Check failed ksize y dshape 2 2 param pad 0 ksize x dshape 3 2 param pad 1 kernel size exceed input Error in eval substitute expr envir enclos InferShape Error in convolution1 22 30 31 src operator convolution inl h 373 Check failed ksize y dshape 2 2 param pad 0 ksize x dshape 3 2 param pad 1 kernel size exceed input Calls mx model FeedForward create mx model init params mx symbol infer shape Anonymous External but I do not understand what is the reason May I ask you if you see any discrepancy in my code Thanks Cheers,,"piiswrong,thirdwing",2016-10-24 20:38:26,2016-12-15 03:29:17
IS,Low accuracy,Hi I just train my own data with alexnet 9 classes but got low validation accuracy around 0 1 I have use shuffle True in data iterator anyone can help,,"kevinthesun,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet",2016-12-13 10:44:46,2016-12-15 06:09:29
PR,Adding How To for visualizing network graphs Updating install guide to install graphviz and jupyter,Adding How To page for visualizing network graphs This will help new users to visualize their computation graph Install guide was missing install steps to get graphviz and jupyter Updated Quick install scripts for Ubuntu Amazon Linux and also updated standard install guide for other OSes We need to add below image to img width 300 alt samplenetworkvisualization src,,"sandeep-krishnamurthy,sandeep-krishnamurthy",2016-12-15 01:57:09,2016-12-15 07:00:27
PR,Add test case for RHS and LHS scalar comparison operators,The comparison operators and for scalar inputs are not complete 4215,,"shivarajugowda,jermainewang,shivarajugowda",2016-12-14 05:41:25,2016-12-15 07:10:56
IS,go binding compiling problem,Environment info Operating System Steps to reproduce 1 modify mxnet pc make the prefix be your own mxnet source code dir 2 cp mxnet pc usr lib pkgconfig 3 go build state go,,,2016-12-14 07:09:51,2016-12-15 07:57:13
IS,Is graphviz part of mxnet library or should we need to install it ourselves,I am trying to visualize networks using mx viz plot network on notebook I got import error for graphviz However after doing pip install graphviz I was able to import graphviz Should we ask users to install graphviz library and add it to system path I get below error when I try to plot graph in notebook RuntimeError failed to execute wouldot' ' Tsvg' make sure the Graphviz executables are on your systems' path Should we add a step in installation guide to add path to graphviz lib in ADD LDFLAGS,,"sandeep-krishnamurthy,piiswrong,sandeep-krishnamurthy",2016-12-13 20:49:22,2016-12-15 08:01:42
IS,GPU is not enabled error on mac bookPro when load faster RCNN model,I follow the rcnn tutorial and run python demo py prefix final epoch 0 image myimage jpg gpu 0 command on my mac bookPro Then i encountered GPU not enabled error I notice that maybe this model needs the mxnet built with gpu However mac bookPro did not have a NVIDA card Is that means that there is no way I can use the pre trained model on my macbook The tutorial link The error rcnn git master python demo py prefix final epoch 0 image myimage jpg gpu 0 15 53 17 Users universe Program mxnet dmlc core include dmlc logging h 235 15 53 17 src ndarray ndarray cc 287 GPU is not enabled Traceback most recent call last File demo py line 81 in module detector get net args prefix args epoch ctx File demo py line 16 in get net args auxs num class load param prefix epoch convert True ctx ctx File Users universe Program mxnet example rcnn utils load model py line 92 in load param arg params convert context arg params ctx File Users universe Program mxnet example rcnn utils load model py line 64 in convert context new params k v as in context ctx File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 568 in as in context return self copyto context File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 536 in copyto return internal copyto self out hret File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 1225 in unary ndarray function c array ctypes c char p c str str i for i in kwargs values File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 15 53 17 src ndarray ndarray cc 287 GPU is not enabled,,,2016-12-15 08:01:43,2016-12-15 09:09:57
IS,Fix the default CI build,After every checkin there seems to be 3 sets of tests triggered One of them default always fails It looks most of the developers are aware of it But for a new comer it is confusing I guess it is confusing even for others This is task to fix it or remove the test set if appropriate,,"shivarajugowda,piiswrong,shivarajugowda,shivarajugowda",2016-12-12 06:13:13,2016-12-15 16:46:47
PR,Scala add CustomOp support,follows Could you help to review the code Or anyone else help to review this code will be appreciated I am not sure whether I am implementating it right for me this is the reasonable way I can come up with,,"Ldpe2G,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,Ldpe2G,Ldpe2G,Ldpe2G,yzhliu,yzhliu,yzhliu,yzhliu,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,yzhliu,Ldpe2G",2016-12-06 09:13:37,2016-12-15 16:47:37
IS,The comparison operators and for scalar inputs are not complete,If RHS input is a scalar we might get incorrect results since the input for non commutative operations is not set,,"shivarajugowda,jermainewang,shivarajugowda",2016-12-13 03:15:56,2016-12-15 16:48:08
PR,Improve Navbar style,Adjust navigation bar style to show larger size navigation items,,"kevinthesun,mli,kevinthesun,mli,mli,kevinthesun",2016-12-15 20:47:41,2016-12-15 20:52:33
PR,example image classification scripts to benchmark scoring,added example image classification benchmark score py to benchmark various CNNs with different batch size on both CPU and GPU,,"mli,piiswrong",2016-12-04 04:19:35,2016-12-15 21:10:13
IS,coredump for resnet v2 with USE MKL2017 EXPERIMENTAL 1,Steps to reproduce this bug 1 merge 4084 2 modify Line51 to networks aresnet 50' in benchmark score py 3 compile with USE MKL2017 EXPERIMENTAL 1 and USE MKL2017 1 4 run python benchmark score py Note set USE MKL2017 EXPERIMENTAL 0 can solve this problem but significantly reduces the speed,,"mli,glingyan",2016-12-04 04:24:12,2016-12-15 21:21:11
PR,More modification for navbar,Redesign navigation bar layout to make it cleaner,,kevinthesun,2016-12-16 00:00:41,2016-12-16 00:55:39
PR,DOC R add example code to read the original MNIST data set,,,thirdwing,2016-12-16 03:32:28,2016-12-16 04:14:46
PR,Repository is missing,The repository field in the package description is missing This is needed so that we can deploy shiny packages on shinyapp io Currently the deployApp function can not detect where to grab the money packe from Please add the missing field,,"Abusnina,piiswrong,thirdwing,thirdwing",2016-12-05 22:30:55,2016-12-16 15:41:20
PR,Fix MXNet path in bashrc file,MXNET HOME is HOME mxnet The old version added double in the path,,piiswrong,2016-12-16 14:10:54,2016-12-16 19:20:22
PR,Adjust navbar,Small fix for navbar height,,kevinthesun,2016-12-16 22:19:38,2016-12-17 00:02:31
PR,example image classification update info for resnext50,resnext 50 log file,,"terrychenism,piiswrong",2016-12-17 01:22:01,2016-12-17 06:20:11
PR,Fixing LDFLAGS issue in Mac setup,Fixing issue Splitting LDFLAGS path in to two lines,,"sandeep-krishnamurthy,piiswrong,sandeep-krishnamurthy",2016-12-17 00:56:51,2016-12-17 06:55:00
IS,Update Windows R package,I have tried installing the windows R package using install packages drat repos drat addRepo dmlc install packages mxnet The result is an error that R does not know what repository to use for the final line There may be an issue with the drat package However I can install mxnet directly from the repository at but the package there is older and no longer works with the trained models on github Would it be possible to update the package posted on the CRAN repository,,"winstywang,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing",2016-10-26 12:33:24,2016-12-17 18:48:10
IS,error in Classify Real World Images with Pre trained Model,After running the code from the above link I am getting the following error 19 43 58 D chhong mxnet dmlc core include dmlc logging h 235 19 43 58 d chhong mxnet src operator concat inl h 152 Check failed dshape j tmp j Incorrect shape 2 1 320 15 15 first input shape 1 576 14 14 Error InferShape Error in ch concat 3c chconcat 19 43 58 d chhong mxnet src operator concat inl h 152 Check failed dshape j tmp j Incorrect shape 2 1 320 15 15 first input shape 1 576 14 14,,"piiswrong,miguelgfierro,winstywang,thirdwing,miguelgfierro,thirdwing,thirdwing,thirdwing",2016-09-18 14:28:40,2016-12-17 18:48:35
IS,R Install latest version on Windows with GPU onLoad failed,Trying to install latest version of MXNet R Package on Windows I get en error onLoad failed in loadNamespace for 'mxnet' on the final step testing if installed package can be loaded I read some older issues here and set environment variable LD LIBRARY PATH but it did not help I use this source code binaries my CUDA CUDnn files cudnn library and some others Configuration Windows 10 x64 Home The exact error message with some my translation to English c mxnet 20161125 R CMD INSTALL no multiarch R package installing to library 'C Users home dir Documents R win library 3 3' installing source package 'mxnet' libs make Nothing to be done for all' installing to C Users home dir Documents R win library 3 3 mxnet libs x64 R demo inst preparing package for lazy loading help installing help indices building package indices installing vignettes testing if installed package can be loaded Error onLoad failed in loadNamespace for 'mxnet' details call inDL x as logical local as logical now error cannot load shared object 'C Users home dir Documents R win library 3 3 mxnet libs x64 libmxnet dll' LoadLibrary failure Not found Error loading failed Execution stopped ERROR loading failed removing 'C Users home dir Documents R win library 3 3 mxnet' restoring previous 'C Users home dir Documents R win library 3 3 mxnet' Previously I successfully compiled and installed R Package stable version from 31 May 2016 but with newest versions I for some reasons run in troubles Thanks for advance,,"ameshkoff,piiswrong,miguelgfierro,ameshkoff,miguelgfierro,ameshkoff,thirdwing,ameshkoff,ameshkoff",2016-11-26 18:26:01,2016-12-17 21:12:14
PR,Fix memory leak bug in Monitor,NDArray allocated in src symbol graph executor cc wo not be freed if monitor pattern not match,,Answeror,2016-12-17 12:50:26,2016-12-18 00:14:21
PR,fix a typo in docs tutorials computer vision image classification md,,,lazyparser,2016-12-17 12:47:01,2016-12-18 00:15:40
PR,add path checking in example image classification common util py,The download file function in example image classification common util py does't check whether there is directory in the path of local fname directly create the file by open local fname 'wb' it will cause error message when end user run example image classification train mnist py in a directory which does not have data directory in it Path checking code were added in this pull request to avoid that issue,,DamonDeng,2016-12-17 12:52:54,2016-12-18 00:17:35
PR,fix cython ndarray error handling,,,piiswrong,2016-12-12 21:24:48,2016-12-18 05:35:00
PR,Explicitly exported the call to the context so it can be easily modi,fied for use on machines without GPU support e g OSX,,"andreaolgiati,piiswrong",2016-12-16 21:01:00,2016-12-18 05:37:51
IS,Mac OS setup config tutorial has incorrect code,Environment info Operating System Mac OS X EI Capitan 10 11 6 Compiler clang Description Mac os setup,,"thirdwing,piiswrong,sandeep-krishnamurthy",2016-12-16 13:33:49,2016-12-18 06:49:28
IS,im2rec problem resize parameter takes no effect,Dear all I have tried im2rec to resize images but it seems that it always keep the original size of images When I tried a different format like im2rec resize 480 the result is the same This might have caused many troubles as I searched on the issues Programme mxnet bin im2rec train lst ImageNetDatasets train 480 q80 resize 480 quality 80 15 29 21 tools im2rec cc 110 Keep origin image size 15 29 21 tools im2rec cc 121 Encoding is jpg 15 29 21 tools im2rec cc 167 Write to output train 480 q80 15 29 21 tools im2rec cc 169 Output train 480 q80 15 29 21 tools im2rec cc 182 JPEG encoding quality 80,,,2016-12-18 15:33:11,2016-12-18 15:35:50
PR,Adding description to How To links and Tutorials,Adding 1 to 2 line descriptions for each How Tos Adding 1 to 2 line description to each section in How Tos and Tutorials Above 2 changes helps readers to know what to expect in a given section,,sandeep-krishnamurthy,2016-12-18 23:21:43,2016-12-19 00:08:44
IS,On prediction mxnet requires same batch size as on training,I want to predict from trained model import mxnet as mx sym arg params aux params mx model load checkpoint net path 9 mod mx mod Module symbol sym context mx gpu And after i put this mod bind for training False data shapes wouldata' 1 3 384 384 It fails with error MXNetError InferShape Error in reshape0 03 12 50 src operator reshape inl h 288 Check failed new size proposed dim 0 Illegal dim setting can not be divided If i set batch size to 96 3 384 384 which i used for training it works I want to predict several images not 96 how can i do this,,"piiswrong,piiswrong",2016-12-18 03:22:34,2016-12-19 02:17:12
IS,IOError of train mnist py,I run the example train mnist py I get some errors Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File home burning mxnet example image classification common fit py line 103 in fit train val data loader args kv File train mnist py line 39 in get mnist iter 'train labels idx1 ubyte gz' 'train images idx3 ubyte gz' File train mnist py line 24 in read data image np fromstring fimg read dtype np uint8 reshape len label rows cols File usr lib python2 7 gzip py line 261 in read self read readsize File usr lib python2 7 gzip py line 315 in read self read eof File usr lib python2 7 gzip py line 354 in read eof hex self crc IOError CRC check failed 0x90c352d9 0x91e3c8c6L I am just a newbie in MXNet so I do not know how to fix it Could someone help me Thanks a lot,,"howard0su,ysh329",2016-12-19 02:09:36,2016-12-19 03:12:28
IS,train mnist py with 4 GPUs is slower than itself with 1GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu Compiler g Package used Python R Scala Julia Python MXNet version 0 7 0 Or if installed from source Yes MXNet commit hash git rev parse HEAD 1036277b96baf96418385a5f722d989f18a040bf If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace No error message Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Run the following two command to run the train mnist py with 1 GPU and 4 GPU the one with 4 GPU is slower than the one with 1 GPU Command python mxnet install mxnet example image classification train mnist py gpus 0 python mxnet install mxnet example image classification train mnist py gpus 0 1 2 3 out put with 1 GPU python mxnet install mxnet example image classification train mnist py gpus 0 INFO root start with arguments Namespace batch size 64 disp batches 100 gpus '0' kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epochs '1 0' model prefix None mom 0 9 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 000 1 INFO root Start training with gpu 0 INFO root Epoch 0 Batch 100 Speed 36736 40 samples sec Train accuracy 0 785937 INFO root Epoch 0 Batch 200 Speed 36514 88 samples sec Train accuracy 0 893906 INFO root Epoch 0 Batch 300 Speed 36543 26 samples sec Train accuracy 0 916562 INFO root Epoch 0 Batch 400 Speed 36632 78 samples sec Train accuracy 0 935625 INFO root Epoch 0 Batch 500 Speed 36715 75 samples sec Train accuracy 0 936875 INFO root Epoch 0 Batch 600 Speed 37090 91 samples sec Train accuracy 0 939688 INFO root Epoch 0 Batch 700 Speed 37159 15 samples sec Train accuracy 0 947812 INFO root Epoch 0 Batch 800 Speed 37115 01 samples sec Train accuracy 0 947656 INFO root Epoch 0 Batch 900 Speed 37071 55 samples sec Train accuracy 0 950625 INFO root Epoch 0 Resetting Data Iterator INFO root Epoch 0 Time cost 1 945 INFO root Epoch 0 Validation accuracy 0 953324 out put of 4 GPUs python mxnet install mxnet example image classification train mnist py gpus 0 1 2 3 INFO root start with arguments Namespace batch size 64 disp batches 100 gpus '0 1 2 3' kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epo chs '10' model prefix None mom 0 9 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Start training with gpu 0 gpu 1 gpu 2 gpu 3 INFO root Epoch 0 Batch 100 Speed 17725 99 samples sec Train accuracy 0 782813 INFO root Epoch 0 Batch 200 Speed 17591 45 samples sec Train accuracy 0 907500 INFO root Epoch 0 Batch 300 Speed 17856 25 samples sec Train accuracy 0 930312 INFO root Epoch 0 Batch 400 Speed 18414 99 samples sec Train accuracy 0 934219 INFO root Epoch 0 Batch 500 Speed 17935 61 samples sec Train accuracy 0 935469 INFO root Epoch 0 Batch 600 Speed 18190 92 samples sec Train accuracy 0 942031 INFO root Epoch 0 Batch 700 Speed 18301 91 samples sec Train accuracy 0 944375 INFO root Epoch 0 Batch 800 Speed 17891 83 samples sec Train accuracy 0 944688 INFO root Epoch 0 Batch 900 Speed 17640 18 samples sec Train accuracy 0 946562 INFO root Epoch 0 Resetting Data Iterator INFO root Epoch 0 Time cost 3 803 INFO root Epoch 0 Validation accuracy 0 954618 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Launch AWS G2 8x large 2 Install MXNet 3 Run the training with 1 GPU and 4 GPU What have you tried to solve it 1 I tried to set export MXNET ENABLE GPU P2P 0 then re run the training but does not help 2 I tried to search in the issue list of this repository did not find duplicate case 3,,"DamonDeng,pineking,DamonDeng",2016-12-19 05:25:58,2016-12-19 08:21:04
IS,Can mxnet be compiled by gcc xc,like gcc xc,,,2016-12-14 11:05:40,2016-12-19 08:29:32
PR,R package Fixed a typo in document of function mx symbol Activation Refined the format a bit,A very MINOR change to fix the typo in the document of function mx symbol Activation Also tried to make the format nicer,,,2016-12-19 09:14:25,2016-12-19 10:16:18
IS,non friendly log info when using MKLDNN in make,set repeat N times with the make j N,,"tornadomeet,tornadomeet",2016-12-19 09:21:34,2016-12-19 11:44:39
PR,Docs for NNVM new operator,,,piiswrong,2016-12-18 08:43:04,2016-12-19 17:08:20
PR,Add some Julia docs in Getting Started,,,ranjanan,2016-12-19 11:44:43,2016-12-19 18:37:48
PR,example image classification add convergence rate,,,mli,2016-12-19 19:23:03,2016-12-19 19:50:49
IS,Pull comparison operator tests to a separate file,Pull comparison operator tests to a separate file to parallelize the tests,,shivarajugowda,2016-12-13 03:13:04,2016-12-20 00:07:44
IS,Possibility of a new release,Is it possible to make a new release since the current release version is 6 months old and does not have some new functions that we are looking at e g MXSymbolGetAtomicSymbolName If not what is the earliest possible date when a new release can be made I could use the nightly version but it is missing the dependent libraries,,miguelgfierro,2016-12-09 11:27:36,2016-12-20 00:09:13
PR,image classification default use min random scale 1 add data image,net1k val sh,,mli,2016-12-20 01:16:17,2016-12-20 01:17:19
IS,How to use fastpoornet params and json file from WhatIsThis,Environment info Operating System Ubuntu desktop 14 04 64bit Compiler g MXNet version v0 7 Python version and distribution python2 7 Error Message I took the params and symbol json files of fastpoornet from WhatIsThis project and loaded them for a simple image classification but the result is incorrect I'm sure that the code works since I tried other network files Should I modify something in the code or the files BTW I set allow missing True to avoid RuntimeError softmax0 label is not presented Minimum reproducible example import os import mxnet as mx f open isynset txt' 'r' synsets l rstrip for l in f sym arg params aux params mx model load checkpoint os path join os getcwd 'model' 'fastpoornet' 0 mod mx mod Module symbol sym context mx cpu mod bind for training False data shapes wouldata' 1 3 224 224 mod set params arg params aux params allow missing True import matplotlib matplotlib rc savefig dpi 100 import matplotlib pyplot as plt import cv2 import numpy as np from collections import namedtuple from tools image processing import transform Batch namedtuple 'Batch' wouldata' def predict filename mod synsets img cv2 cvtColor cv2 imread filename cv2 COLOR BGR2RGB if img is None return None img cv2 resize img 224 224 data transform img 117 0 117 0 117 0 img np swapaxes img 0 2 img np swapaxes img 1 2 img img np newaxis mod forward Batch mx nd array img prob mod get outputs 0 asnumpy prob np squeeze prob a np argsort prob 1 for i in a 0 5 print i print 'probability f class s' prob i synsets i print ready predict cat jpg mod synsets What have you tried to solve it I import transform from tools image processing to apply mean pixels 117 117 117 but it does not work,,"thirdwing,thirdwing",2016-12-14 07:09:15,2016-12-20 04:11:23
IS,how to find the max value of a tensor,i want to find the max value of a tensor i think of a method as follows Tensor xpu 2 DType data in data activation kData FlatTo2D xpu DType s Shape 1 maxshape Shape 1 1 Tensor xpu 1 DType out max NewTensor xpu DType 2 maxshape Dtype 0 false out max pool mshadow red maximum pad data 0 0 Shape 1 data shape 0 data shape 1 1 1 Are there any concise way And if i want to use the max value to the following calculation in gpu i can not access it using out max 0 in cpu kernal so how to solve it,,jermainewang,2016-12-14 11:40:10,2016-12-20 04:45:22
IS,Always Check failed in the 7th Epoch with latest version MXNET,Hi I have installed the latest version of MXNET I found that whether use image,,,2016-12-20 03:39:30,2016-12-20 04:49:12
PR,Doc Add MXNET BACKWARD DO MIRROR in MXNet environment description,,,"tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet",2016-12-20 06:23:51,2016-12-20 07:35:13
PR,refactor elementwise ops,,,piiswrong,2016-12-13 20:08:05,2016-12-20 08:57:27
IS,any plan to make tools like TensorBoard,is there any plan to make tools like TensorBoard,,philipskokoh,2016-12-20 02:33:33,2016-12-20 09:57:02
IS,The program get stuck in check call func when run rcnn demo py on macbook Pro,I use macbook Pro so i modify the demo py to use cpu Then i run python demo py prefix final epoch 0 image myimage jpg However the program got stuck in detector py line 89 scores executor output dict 'cls prob reshape output' asnumpy 0 i print the executor output dict and the output seems to be ok 'cls prob reshape output' NDArray 1x300x21 0 'bbox pred reshape output' NDArray 1x300x84 0 'rois output' NDArray 300x5 0 How can i solve this problem The tutorial link It gets stuck when run asnumpy function Further I find it gets stuck in the check call function in asnumpy func I create a ndarray in python command console However the code runs correctly I do not know why it get stuck in Pycharm Here is my test code in console import mxnet as mx a mx nd ones 1 300 21 a NDArray 1x300x21 0 print a asnumpy 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1,,jermainewang,2016-12-15 09:14:41,2016-12-20 10:04:49
PR,OP Implementations for prod nansum and nanprod,Added ops and test cases prod nansum and nanprod see issue 3201 Added test utility function for checking array equality almost equal combo because reldiff does not work well when all the elements of both arrays are close to zero For global reductions of prod the results are often in the order of ground truth 1e 20 op output 0 For this case reldiff gives a result of 1 0 Instead almost equal combo uses an element wise threshold in addition to reldiff if either of them indicate the op output is close to ground truth then the test passes Additionally almost equal combo ignores cells in both arrays if the corresponding cell in either array is NaN This allows it to be used to verify the gradients of nansum and nanprod where what we care about is getting the gradient of non NaN inputs correct,,"alex-weaver,piiswrong,piiswrong,alex-weaver,alex-weaver,alex-weaver,alex-weaver,alex-weaver,piiswrong",2016-12-12 19:09:39,2016-12-20 20:56:31
PR,fix resnet v2 segmenet fault problem,Signed off by lingyan lingyan guo intel com,,"glingyan,mli,piiswrong,glingyan,glingyan,piiswrong,mli,mli,mli,glingyan,glingyan,zhenlinluo,mli,glingyan",2016-12-14 14:45:02,2016-12-20 21:12:51
PR,image classification add resnet into benchmark score py,thanks to 4237,,mli,2016-12-20 21:50:16,2016-12-20 21:50:32
PR,fix awesome link and typo,,,tornadomeet,2016-12-21 01:00:09,2016-12-21 01:02:53
IS,Poor performance of mxnet LinearRegressionOutput,I have been unable to get reasonable performance using mxnet LinearRegressionOutput layer Full details of the problem including self contained example I have given in the following SO question The question may seen rather broad I'm getting poor performance so the answer should perhaps be the obvious do some hyper parameter tuning However given the simplicity of the regression problem considered and the much better performance of other neural net libraries out of the box I thought this might be of general interest,,,2016-12-19 07:26:24,2016-12-21 01:45:12
IS,NDArrayIter hasNext,test pad the scala code IOSuite scala NDArrayIter hasNext is false but the size is 32,,"Ldpe2G,Ldpe2G,Ldpe2G",2016-12-20 11:01:54,2016-12-21 03:31:48
PR,Scala NDArrayIter constructor default for label,Change default value for label from null to IndexedSeq empty in NDArraIter constructor In the long run the type of label should be changed to Option IndexedSeq with a default value of None However this would change the API,,"benqua,yzhliu,benqua,yzhliu,Ldpe2G,benqua,yzhliu",2016-12-18 13:15:56,2016-12-21 03:36:11
PR,fix example,,,"piiswrong,mli,mli,piiswrong",2016-12-12 21:25:11,2016-12-21 05:29:33
PR,Fix two grad,The tanh grad and square root grad seem wrong because img width 178 alt screen shot 2016 12 20 at 11 58 18 pm src,,"thirdwing,piiswrong,piiswrong,thirdwing",2016-12-21 05:01:21,2016-12-21 05:36:12
IS,How can I compile mxnet with nnpack support,I saw that mxnet supports nnpack in the make config file but there is not documentation to help how to compile them together Could you please help me to do so,,,2016-12-17 16:09:10,2016-12-21 12:43:54
IS,Mxnet support random backpropagation,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2016-12-21 03:53:21,2016-12-21 15:35:20
IS,train mnist give different result between two runs,Environment info Operating System Ubuntu 14 Windows 10 Compiler gcc visual studio 2015 Package used Python R Scala Julia Python MXNet version MXNet commit hash git rev parse HEAD 962271410059156180ab1d5e79b805e687512be9 If you are using python package please provide Python version and distribution 2 7 6 Minimum reproducible example python train mnist py network lenet gpus 1 The accuracy does not make any sense Interesting I repo this on Linux after i first noticed this on Windows like issue 1228 Steps to reproduce I tried twice with the same command howardsu DPHost mxnet example image classification python train mnist py network lenet gpus 1 INFO root start with arguments Namespace batch size 64 disp batches 100 gpus '1' kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 network 'lenet' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Start training with gpu 1 INFO root Epoch 0 Batch 100 Speed 21407 75 samples sec Train accuracy 0 116250 INFO root Epoch 0 Batch 200 Speed 22608 54 samples sec Train accuracy 0 101250 INFO root Epoch 0 Batch 300 Speed 24133 26 samples sec Train accuracy 0 101562 INFO root Epoch 0 Batch 400 Speed 27369 95 samples sec Train accuracy 0 102813 INFO root Epoch 0 Batch 500 Speed 27600 49 samples sec Train accuracy 0 102344 INFO root Epoch 0 Batch 600 Speed 27567 07 samples sec Train accuracy 0 099687 INFO root Epoch 0 Batch 700 Speed 27607 53 samples sec Train accuracy 0 102344 INFO root Epoch 0 Batch 800 Speed 27500 14 samples sec Train accuracy 0 099844 INFO root Epoch 0 Batch 900 Speed 27448 59 samples sec Train accuracy 0 103594 howardsu DPHost mxnet example image classification python train mnist py network lenet gpus 1 INFO root start with arguments Namespace batch size 64 disp batches 100 gpus '1' kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 network 'lenet' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Start training with gpu 1 INFO root Epoch 0 Batch 100 Speed 21860 08 samples sec Train accuracy 0 465625 INFO root Epoch 0 Batch 200 Speed 23576 40 samples sec Train accuracy 0 757188 INFO root Epoch 0 Batch 300 Speed 22909 50 samples sec Train accuracy 0 813438 INFO root Epoch 0 Batch 400 Speed 23619 98 samples sec Train accuracy 0 831406 INFO root Epoch 0 Batch 500 Speed 23110 21 samples sec Train accuracy 0 852344 INFO root Epoch 0 Batch 600 Speed 22845 57 samples sec Train accuracy 0 865781 INFO root Epoch 0 Batch 700 Speed 23093 27 samples sec Train accuracy 0 871563 INFO root Epoch 0 Batch 800 Speed 23533 74 samples sec Train accuracy 0 888437 INFO root Epoch 0 Batch 900 Speed 22846 45 samples sec Train accuracy 0 898906,,"howard0su,jermainewang,mli,howard0su",2016-12-20 13:34:26,2016-12-21 15:43:45
PR,Fix link to C example,,,larroy,2016-12-21 14:40:36,2016-12-21 18:17:24
PR,fix print summary and add one test case,,,howard0su,2016-12-21 15:29:47,2016-12-21 20:06:58
PR,Convert image classification to new programing API,The old FeedForward module API is deprecated I also added monitor to help debug,,"howard0su,piiswrong,howard0su,howard0su,piiswrong,mli,howard0su",2016-12-20 15:16:43,2016-12-21 20:08:44
PR,Add get started backto nav,,,kevinthesun,2016-12-21 21:53:00,2016-12-21 22:11:56
IS,use simple bind can not make continuous predictions,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Package used Python R Scala Julia Python MXNet version installed from source branch master If you are using python package please provide Anaconda 4 2 0 64 bit Python version and distribution Python 2 7 12 Error I train a OCR model use CNN Multi label net when I predict a pic one time the result is correct but run twice the first is correct and the second is wrong code def TestRecognizeOne file batch size 1 sym arg params aug params mx model load checkpoint ocr 0 data shape data batch size 3 50 200 input shapes dict data shape sym getnet executor sym simple bind ctx mx gpu input shapes for key in executor arg dict keys if key in arg params arg params key copyto executor arg dict key executor forward is train False data mx nd array img probs executor outputs 0 asnumpy a np argmax probs 0 b np argmax probs 2 print a b and def getnet data mx symbol Variable wouldata' label mx symbol Variable 'label' conv1 mx symbol Convolution data data kernel 5 5 pad 2 2 num filter 96 relu1 mx symbol Activation data conv1 act type relu pool1 mx symbol Pooling data relu1 pool type max kernel 3 3 stride 2 2 conv2 mx symbol Convolution data pool1 kernel 3 3 pad 1 1 num filter 128 relu2 mx symbol Activation data conv2 act type relu pool2 mx symbol Pooling data relu2 pool type max kernel 3 3 stride 2 2 flatten mx symbol Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 2048 relufc1 mx symbol Activation data fc1 act type relu dropout1 mx symbol Dropout data relufc1 p 0 5 fc2 mx symbol FullyConnected data dropout1 num hidden 1024 relufc2 mx symbol Activation data fc2 act type relu dropout2 mx symbol Dropout data relufc2 p 0 5 fc21 mx symbol FullyConnected data dropout2 num hidden 35 fc22 mx symbol FullyConnected data dropout2 num hidden 35 fc3 mx symbol Concat fc21 fc22 dim 0 label mx symbol transpose data label label mx symbol Reshape data label target shape 0 return mx symbol SoftmaxOutput data fc3 name softmax when run testRecognizeOne 0 jpg testRecognizeOne 0 jpg the output 2 3 0 0 the probs 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 In addition when I use simple bind in lstm ctc cnn ctc I found the same problem,,,2016-12-22 03:01:20,2016-12-22 03:03:28
IS,im2rec py is parameter made me confusing,when I try to use im2rec py to make rec it confused me 1 build the list I need to use I think it is better to make them the same just in my opinion,,piiswrong,2016-12-12 08:02:32,2016-12-22 03:27:21
IS,Why ndarray does not support matrix operations,What have you tried to solve it mxnet is ndarray likes numpy is ndarray But I find that mxnet is ndarray does not support matrix operations But I think that it is necessary for machine learning,,,2016-12-22 02:01:37,2016-12-22 03:28:13
IS,Example train mnist py failed to run,When I run I remember this worked three weeks ago And I saw there is a large refactor in train mnist py is that related Thanks,,jostep,2016-12-08 04:00:54,2016-12-22 07:30:47
PR,fix cmake build with vs2015 cuda8 0 win7,related to win7 vs2015 cuda8 0 cuDNN5 1 cpu build is ok log is 1 the error happens in in CMakeList txt it will build ok,,"tornadomeet,yajiedesign",2016-12-22 06:26:16,2016-12-22 07:40:57
PR,sync aux params across devices after each epoch,Currently only batchnorm has aux state and it only use it for evaluation However if we have ops that use aux during training it might make sense to sync aux every iteration,,piiswrong,2016-12-21 23:55:27,2016-12-22 08:28:55
IS,PLZ remove go from support list,PLZ remove go from support list which is described in repository description and README md Because it is not true The outdated repo gomxnet is just wasting our time,,mli,2016-12-19 09:50:59,2016-12-22 11:11:03
IS,Installation file removed from repository,When trying to install mxnet as an R package Warning in install packages URL '' status was '404 Not Found' Error in download file url destfile method mode wb R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 apple darwin15 6 0 64 bit Running under macOS Sierra 10 12 1 locale 1 en US UTF 8 en US UTF 8 en US UTF 8 C en US UTF 8 en US UTF 8 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 drat 0 1 2 RevoUtils 10 0 2 tools 3 3 2,,"thirdwing,thirdwing,thirdwing",2016-12-22 13:30:22,2016-12-22 15:38:31
IS,Scala bad default value in NDArrayIter constructor,In the NDArrayIter constructor L26 the default value for label is null label IndexedSeq NDArray null If a user does not set label and keep the default value then a scala MatchError exception will be thrown at ml dmlc mxnet io NDArrayIter init NDArrayIter scala 33 L33 because null cannot be cast to an IndexedSeq Default value for IndexedSeq could be IndexedSeq empty Or maybe better label could be an Option IndexedSeq with default value set to None To reproduce just try to intialize a NDArrayIter with def iterFromNDArray nda NDArray NDArrayIter new NDArrayIter IndexedSeq aNDArray It will fail with error run main 2 scala MatchError Vector ml dmlc mxnet NDArray dfb083e0 null of class scala Tuple2 scala MatchError Vector ml dmlc mxnet NDArray dfb083e0 null of class scala Tuple2 at ml dmlc mxnet io NDArrayIter init NDArrayIter scala 33 at name benq ImageUtils iterFromNDArray ImageUtils scala 30 For prediction a user should not need to set the label,,"benqua,Ldpe2G,benqua,Ldpe2G,benqua,benqua,benqua",2016-12-16 20:25:27,2016-12-22 16:53:33
PR,Scala NDArrayIter constructor fix for null,Change default value for label from null to IndexedSeq empty Fix 4216 In the long run the type of label could be changed to Option IndexedSeq and the default value could be None,,"benqua,yzhliu,benqua",2016-12-20 21:39:42,2016-12-22 16:53:33
PR,Add go binding project reference,1 Fix typos in include mxnet c predict api h 2 Add go mxnet predictor project reference to READMEs go mxnet predictor features more api bindings more comments and details in api dependency less no need Amalgamation offering docker file to build mxnet go dev env So it is easy to build and run inference examples I am working at cloud service provider company Our tech stack is based on go It is painful to do machine learning in golang with mxnet even with tensorflow I am trying to fill the gap The prior project gomxnet is outdated without maintaining PLZ review,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2016-12-22 11:03:16,2016-12-23 04:59:30
IS,Bug again API END HANDLE ERROR delete ret in MXPredCreatePartialOut,it is obviously wrong to,,piiswrong,2016-12-23 06:58:17,2016-12-23 06:59:56
IS,mxnet example in go,There is no go example in mxnet examples directory Why go has not been support yet,,zhreshold,2016-12-08 10:30:19,2016-12-23 08:26:40
IS,memory leak when using c api,the following code is in the most c api functions is global variable so the resources will only be released after the program exited it will caused the memory increased in the runtime of program how to solve it,,,2016-12-22 15:42:38,2016-12-23 08:35:50
PR,CALL FOR TESTING Single script to build and install mxnet for Windows with Python,This script is only one script to build and install mxnet with minimal dependency 1 Visual Studio 2015 Community Edition 2 Conda with Python 3 5 All the other dependencies are pulled via conda like openblas opencv The script does not require modifying the files in Visual studio installation The script is largely copied from torch distro and modified for mxnet It supports OpenBlas MKL and CUDA The script is tested only on a Windows 10 x64 version box with one single NVidia Cuda card So far I do not have plan to support 32bit OS x Support cudnn x Test MKL build x Test on various configuration x Code Cleanup Document update for Windows installation,,"howard0su,howard0su,howard0su,piiswrong,piiswrong",2016-12-16 12:00:18,2016-12-23 08:43:01
PR,fix variable initialization,Fix sym arg params aux params mx model load checkpoint aresnet 50' 0 all layers sym get internals,,piiswrong,2016-12-13 00:21:16,2016-12-23 09:18:35
PR,fix compilation under gcc 5,,,tqchen,2016-12-23 09:43:41,2016-12-23 09:44:07
PR,Variable patch,,,piiswrong,2016-12-23 09:21:47,2016-12-23 09:44:20
PR,'lst' ' lst' means format like ' rec','lst' ' lst' easy to understand it is a kind of format like ' rec',,ironyoung,2016-12-23 14:35:40,2016-12-23 18:55:36
PR,Adding a link to setup guide from How To page,Many users complained that they are unable to find set up page easily Most users mentioned they tried to see in How To page for a link to set up guidelines Hence linking setup and installation from How To,,sandeep-krishnamurthy,2016-12-21 20:40:12,2016-12-23 18:55:56
PR,Typo in new op md,,,linmx0130,2016-12-23 08:48:59,2016-12-23 19:04:41
PR,fix ndarray bcast,,,piiswrong,2016-12-23 20:20:55,2016-12-24 05:10:52
IS,Recipe for target 'build src operator batch norm o' failed,I'm follow the instruction in and got trouble at the step 2 make j2 I'm using Ubuntu 16 10 I have installed full MKL library Without using mkl2017 library in the config mk file the compilation and installation are success Any suggestion Thanks Here is config mk file And here is the log g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src initialize o src initialize cc build src initialize d g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src resource o src resource cc build src resource d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src initialize cc o build src initialize o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src io image aug default o src io image aug default cc build src io image aug default d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src resource cc o build src resource o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src io image aug default cc o build src io image aug default o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src io io o src io io cc build src io io d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src io io cc o build src io io o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src io iter csv o src io iter csv cc build src io iter csv d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src io iter csv cc o build src io iter csv o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src io iter image recordio o src io iter image recordio cc build src io iter image recordio d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src io iter image recordio cc o build src io iter image recordio o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src io iter mnist o src io iter mnist cc build src io iter mnist d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src io iter mnist cc o build src io iter mnist o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src common mxrtc o src common mxrtc cc build src common mxrtc d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src common mxrtc cc o build src common mxrtc o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src ndarray ndarray function o src ndarray ndarray function cc build src ndarray ndarray function d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src ndarray ndarray function cc o build src ndarray ndarray function o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src ndarray ndarray o src ndarray ndarray cc build src ndarray ndarray d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src ndarray ndarray cc o build src ndarray ndarray o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator instance norm o src operator instance norm cc build src operator instance norm d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator instance norm cc o build src operator instance norm o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator loss binary op o src operator loss binary op cc build src operator loss binary op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator loss binary op cc o build src operator loss binary op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator block grad o src operator block grad cc build src operator block grad d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator block grad cc o build src operator block grad o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator smooth l1 unary o src operator smooth l1 unary cc build src operator smooth l1 unary d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator smooth l1 unary cc o build src operator smooth l1 unary o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator elementwise binary broadcast op o src operator elementwise binary broadcast op cc build src operator elementwise binary broadcast op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator elementwise binary broadcast op cc o build src operator elementwise binary broadcast op o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator rnn o src operator rnn cc build src operator rnn d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator rnn cc o build src operator rnn o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator cast o src operator cast cc build src operator cast d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator cast cc o build src operator cast o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator crop o src operator crop cc build src operator crop d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator crop cc o build src operator crop o In file included from src operator crop inl h 9 0 from src operator crop cc 8 src operator crop inl h In instantiation of void mxnet op CropOp xpu Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu src operator crop cc 32 1 required from here home nam Downloads mxnet dmlc core include dmlc logging h 69 34 warning comparison between signed and unsigned integer expressions Wsign compare define CHECK EQ x y CHECK x y home nam Downloads mxnet dmlc core include dmlc logging h 62 9 note in definition of macro CHECK if x src operator crop inl h 84 5 note in expansion of macro CHECK EQ CHECK EQ in grad size param num args in grad size src operator crop inl h In instantiation of std vector int mxnet op CropOp xpu InferCropOfferset const mshadow Shape 4 const mshadow Shape 4 with xpu mshadow cpu src operator crop inl h 68 35 required from void mxnet op CropOp xpu Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu src operator crop cc 32 1 required from here home nam Downloads mxnet dmlc core include dmlc logging h 67 34 warning comparison between signed and unsigned integer expressions Wsign compare define CHECK LE x y CHECK x y home nam Downloads mxnet dmlc core include dmlc logging h 62 9 note in definition of macro CHECK if x src operator crop inl h 117 9 note in expansion of macro CHECK LE CHECK LE static cast int param offset 0 data shape 2 out shape 2 home nam Downloads mxnet dmlc core include dmlc logging h 67 34 warning comparison between signed and unsigned integer expressions Wsign compare define CHECK LE x y CHECK x y home nam Downloads mxnet dmlc core include dmlc logging h 62 9 note in definition of macro CHECK if x src operator crop inl h 121 9 note in expansion of macro CHECK LE CHECK LE static cast int param offset 1 data shape 3 out shape 3 g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator spatial transformer o src operator spatial transformer cc build src operator spatial transformer d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator spatial transformer cc o build src operator spatial transformer o src operator spatial transformer cc In instantiation of void mshadow BilinearSamplingForward const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 3 DType with DType mshadow half half t src operator spatial transformer inl h 89 30 required from void mxnet op SpatialTransformerOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType mshadow half half t src operator spatial transformer cc 138 1 required from here src operator spatial transformer cc 20 25 warning comparison between signed and unsigned integer expressions Wsign compare for index t n 0 n o n n src operator spatial transformer cc 21 27 warning comparison between signed and unsigned integer expressions Wsign compare for index t c 0 c o c c src operator spatial transformer cc 22 29 warning comparison between signed and unsigned integer expressions Wsign compare for index t h 0 h o h h src operator spatial transformer cc 23 31 warning comparison between signed and unsigned integer expressions Wsign compare for index t w 0 w o w w src operator spatial transformer cc In instantiation of void mshadow BilinearSamplingBackward const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 3 DType const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 4 DType with DType mshadow half half t src operator spatial transformer inl h 114 31 required from void mxnet op SpatialTransformerOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType mshadow half half t src operator spatial transformer cc 138 1 required from here src operator spatial transformer cc 59 25 warning comparison between signed and unsigned integer expressions Wsign compare for index t n 0 n o n n src operator spatial transformer cc 60 28 warning comparison between signed and unsigned integer expressions Wsign compare for index t h 0 h o h h src operator spatial transformer cc 61 31 warning comparison between signed and unsigned integer expressions Wsign compare for index t w 0 w o w w src operator spatial transformer cc 71 33 warning comparison between signed and unsigned integer expressions Wsign compare for index t c 0 c o c c src operator spatial transformer cc In instantiation of void mshadow BilinearSamplingForward const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 3 DType with DType double src operator spatial transformer inl h 89 30 required from void mxnet op SpatialTransformerOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType double src operator spatial transformer cc 138 1 required from here src operator spatial transformer cc 20 25 warning comparison between signed and unsigned integer expressions Wsign compare for index t n 0 n o n n src operator spatial transformer cc 21 27 warning comparison between signed and unsigned integer expressions Wsign compare for index t c 0 c o c c src operator spatial transformer cc 22 29 warning comparison between signed and unsigned integer expressions Wsign compare for index t h 0 h o h h src operator spatial transformer cc 23 31 warning comparison between signed and unsigned integer expressions Wsign compare for index t w 0 w o w w src operator spatial transformer cc In instantiation of void mshadow BilinearSamplingBackward const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 3 DType const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 4 DType with DType double src operator spatial transformer inl h 114 31 required from void mxnet op SpatialTransformerOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType double src operator spatial transformer cc 138 1 required from here src operator spatial transformer cc 59 25 warning comparison between signed and unsigned integer expressions Wsign compare for index t n 0 n o n n src operator spatial transformer cc 60 28 warning comparison between signed and unsigned integer expressions Wsign compare for index t h 0 h o h h src operator spatial transformer cc 61 31 warning comparison between signed and unsigned integer expressions Wsign compare for index t w 0 w o w w src operator spatial transformer cc 71 33 warning comparison between signed and unsigned integer expressions Wsign compare for index t c 0 c o c c src operator spatial transformer cc In instantiation of void mshadow BilinearSamplingForward const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 3 DType with DType float src operator spatial transformer inl h 89 30 required from void mxnet op SpatialTransformerOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType float src operator spatial transformer cc 138 1 required from here src operator spatial transformer cc 20 25 warning comparison between signed and unsigned integer expressions Wsign compare for index t n 0 n o n n src operator spatial transformer cc 21 27 warning comparison between signed and unsigned integer expressions Wsign compare for index t c 0 c o c c src operator spatial transformer cc 22 29 warning comparison between signed and unsigned integer expressions Wsign compare for index t h 0 h o h h src operator spatial transformer cc 23 31 warning comparison between signed and unsigned integer expressions Wsign compare for index t w 0 w o w w src operator spatial transformer cc In instantiation of void mshadow BilinearSamplingBackward const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 3 DType const mshadow Tensor mshadow cpu 4 DType const mshadow Tensor mshadow cpu 4 DType with DType float src operator spatial transformer inl h 114 31 required from void mxnet op SpatialTransformerOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType float src operator spatial transformer cc 138 1 required from here src operator spatial transformer cc 59 25 warning comparison between signed and unsigned integer expressions Wsign compare for index t n 0 n o n n src operator spatial transformer cc 60 28 warning comparison between signed and unsigned integer expressions Wsign compare for index t h 0 h o h h src operator spatial transformer cc 61 31 warning comparison between signed and unsigned integer expressions Wsign compare for index t w 0 w o w w src operator spatial transformer cc 71 33 warning comparison between signed and unsigned integer expressions Wsign compare for index t c 0 c o c c g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator cudnn convolution o src operator cudnn convolution cc build src operator cudnn convolution d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator cudnn convolution cc o build src operator cudnn convolution o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator swapaxis o src operator swapaxis cc build src operator swapaxis d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator swapaxis cc o build src operator swapaxis o g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator pad o src operator pad cc build src operator pad d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator pad cc o build src operator pad o In file included from src operator pad cc 7 0 src operator pad inl h In member function virtual bool mxnet op PadProp InferShape std vector mxnet TShape std vector mxnet TShape std vector mxnet TShape const src operator pad inl h 166 23 warning comparison between signed and unsigned integer expressions Wsign compare for int i 0 i dshape ndim i src operator pad cc In instantiation of void mshadow single image constant const mshadow Tensor mshadow cpu 3 DType mshadow Tensor mshadow cpu 3 DType mxnet TShape DType with DType mshadow half half t src operator pad cc 357 30 required from void mshadow pad image const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int DType with int dim 4 DType mshadow half half t src operator pad inl h 83 16 required from void mxnet op PadOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType mshadow half half t src operator pad cc 411 1 required from here src operator pad cc 125 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c dst size 0 c src operator pad cc 126 19 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h dst size 1 h src operator pad cc 127 21 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w dst size 2 w src operator pad cc 128 46 warning comparison between signed and unsigned integer expressions Wsign compare if w pad l h pad t h src size 1 pad t src operator pad cc 129 16 warning comparison between signed and unsigned integer expressions Wsign compare w src size 2 pad l src operator pad cc In instantiation of void mshadow single image constant const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 4 DType mxnet TShape DType with DType mshadow half half t src operator pad cc 357 30 required from void mshadow pad image const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int DType with int dim 5 DType mshadow half half t src operator pad inl h 89 16 required from void mxnet op PadOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType mshadow half half t src operator pad cc 411 1 required from here src operator pad cc 307 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c dst size 0 c src operator pad cc 308 19 warning comparison between signed and unsigned integer expressions Wsign compare for d 0 d dst size 1 d src operator pad cc 309 21 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h dst size 2 h src operator pad cc 310 23 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w dst size 3 w src operator pad cc 312 18 warning comparison between signed and unsigned integer expressions Wsign compare d src size 1 pad f h src size 2 pad t src operator pad cc 312 50 warning comparison between signed and unsigned integer expressions Wsign compare d src size 1 pad f h src size 2 pad t src operator pad cc 313 18 warning comparison between signed and unsigned integer expressions Wsign compare w src size 3 pad l src operator pad cc In instantiation of void mshadow single image constant grad const mshadow Tensor mshadow cpu 3 DType mshadow Tensor mshadow cpu 3 DType mxnet TShape with DType mshadow half half t src operator pad cc 373 35 required from void mshadow pad image grad const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int with int dim 4 DType mshadow half half t src operator pad inl h 118 21 required from void mxnet op PadOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType mshadow half half t src operator pad cc 411 1 required from here src operator pad cc 147 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c in grad size 0 c src operator pad cc 148 19 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h in grad size 1 h src operator pad cc 149 21 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w in grad size 2 w src operator pad cc In instantiation of void mshadow single image constant grad const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 4 DType mxnet TShape with DType mshadow half half t src operator pad cc 373 35 required from void mshadow pad image grad const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int with int dim 5 DType mshadow half half t src operator pad inl h 124 21 required from void mxnet op PadOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType mshadow half half t src operator pad cc 411 1 required from here src operator pad cc 333 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c in grad size 0 c src operator pad cc 334 19 warning comparison between signed and unsigned integer expressions Wsign compare for d 0 d in grad size 1 d src operator pad cc 335 21 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h in grad size 2 h src operator pad cc 336 23 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w in grad size 3 w src operator pad cc In instantiation of void mshadow single image constant const mshadow Tensor mshadow cpu 3 DType mshadow Tensor mshadow cpu 3 DType mxnet TShape DType with DType double src operator pad cc 357 30 required from void mshadow pad image const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int DType with int dim 4 DType double src operator pad inl h 83 16 required from void mxnet op PadOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType double src operator pad cc 411 1 required from here src operator pad cc 125 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c dst size 0 c src operator pad cc 126 19 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h dst size 1 h src operator pad cc 127 21 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w dst size 2 w src operator pad cc 128 46 warning comparison between signed and unsigned integer expressions Wsign compare if w pad l h pad t h src size 1 pad t src operator pad cc 129 16 warning comparison between signed and unsigned integer expressions Wsign compare w src size 2 pad l src operator pad cc In instantiation of void mshadow single image constant const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 4 DType mxnet TShape DType with DType double src operator pad cc 357 30 required from void mshadow pad image const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int DType with int dim 5 DType double src operator pad inl h 89 16 required from void mxnet op PadOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType double src operator pad cc 411 1 required from here src operator pad cc 307 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c dst size 0 c src operator pad cc 308 19 warning comparison between signed and unsigned integer expressions Wsign compare for d 0 d dst size 1 d src operator pad cc 309 21 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h dst size 2 h src operator pad cc 310 23 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w dst size 3 w src operator pad cc 312 18 warning comparison between signed and unsigned integer expressions Wsign compare d src size 1 pad f h src size 2 pad t src operator pad cc 312 50 warning comparison between signed and unsigned integer expressions Wsign compare d src size 1 pad f h src size 2 pad t src operator pad cc 313 18 warning comparison between signed and unsigned integer expressions Wsign compare w src size 3 pad l src operator pad cc In instantiation of void mshadow single image constant grad const mshadow Tensor mshadow cpu 3 DType mshadow Tensor mshadow cpu 3 DType mxnet TShape with DType double src operator pad cc 373 35 required from void mshadow pad image grad const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int with int dim 4 DType double src operator pad inl h 118 21 required from void mxnet op PadOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType double src operator pad cc 411 1 required from here src operator pad cc 147 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c in grad size 0 c src operator pad cc 148 19 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h in grad size 1 h src operator pad cc 149 21 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w in grad size 2 w src operator pad cc In instantiation of void mshadow single image constant grad const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 4 DType mxnet TShape with DType double src operator pad cc 373 35 required from void mshadow pad image grad const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int with int dim 5 DType double src operator pad inl h 124 21 required from void mxnet op PadOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType double src operator pad cc 411 1 required from here src operator pad cc 333 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c in grad size 0 c src operator pad cc 334 19 warning comparison between signed and unsigned integer expressions Wsign compare for d 0 d in grad size 1 d src operator pad cc 335 21 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h in grad size 2 h src operator pad cc 336 23 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w in grad size 3 w src operator pad cc In instantiation of void mshadow single image constant const mshadow Tensor mshadow cpu 3 DType mshadow Tensor mshadow cpu 3 DType mxnet TShape DType with DType float src operator pad cc 357 30 required from void mshadow pad image const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int DType with int dim 4 DType float src operator pad inl h 83 16 required from void mxnet op PadOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType float src operator pad cc 411 1 required from here src operator pad cc 125 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c dst size 0 c src operator pad cc 126 19 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h dst size 1 h src operator pad cc 127 21 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w dst size 2 w src operator pad cc 128 46 warning comparison between signed and unsigned integer expressions Wsign compare if w pad l h pad t h src size 1 pad t src operator pad cc 129 16 warning comparison between signed and unsigned integer expressions Wsign compare w src size 2 pad l src operator pad cc In instantiation of void mshadow single image constant const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 4 DType mxnet TShape DType with DType float src operator pad cc 357 30 required from void mshadow pad image const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int DType with int dim 5 DType float src operator pad inl h 89 16 required from void mxnet op PadOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType float src operator pad cc 411 1 required from here src operator pad cc 307 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c dst size 0 c src operator pad cc 308 19 warning comparison between signed and unsigned integer expressions Wsign compare for d 0 d dst size 1 d src operator pad cc 309 21 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h dst size 2 h src operator pad cc 310 23 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w dst size 3 w src operator pad cc 312 18 warning comparison between signed and unsigned integer expressions Wsign compare d src size 1 pad f h src size 2 pad t src operator pad cc 312 50 warning comparison between signed and unsigned integer expressions Wsign compare d src size 1 pad f h src size 2 pad t src operator pad cc 313 18 warning comparison between signed and unsigned integer expressions Wsign compare w src size 3 pad l src operator pad cc In instantiation of void mshadow single image constant grad const mshadow Tensor mshadow cpu 3 DType mshadow Tensor mshadow cpu 3 DType mxnet TShape with DType float src operator pad cc 373 35 required from void mshadow pad image grad const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int with int dim 4 DType float src operator pad inl h 118 21 required from void mxnet op PadOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType float src operator pad cc 411 1 required from here src operator pad cc 147 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c in grad size 0 c src operator pad cc 148 19 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h in grad size 1 h src operator pad cc 149 21 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w in grad size 2 w src operator pad cc In instantiation of void mshadow single image constant grad const mshadow Tensor mshadow cpu 4 DType mshadow Tensor mshadow cpu 4 DType mxnet TShape with DType float src operator pad cc 373 35 required from void mshadow pad image grad const mshadow Tensor mshadow cpu dim DType mshadow Tensor mshadow cpu dim DType mxnet TShape int with int dim 5 DType float src operator pad inl h 124 21 required from void mxnet op PadOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob with xpu mshadow cpu DType float src operator pad cc 411 1 required from here src operator pad cc 333 17 warning comparison between signed and unsigned integer expressions Wsign compare for c 0 c in grad size 0 c src operator pad cc 334 19 warning comparison between signed and unsigned integer expressions Wsign compare for d 0 d in grad size 1 d src operator pad cc 335 21 warning comparison between signed and unsigned integer expressions Wsign compare for h 0 h in grad size 2 h src operator pad cc 336 23 warning comparison between signed and unsigned integer expressions Wsign compare for w 0 w in grad size 3 w g std c 11 DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 MM MT build src operator batch norm o src operator batch norm cc build src operator batch norm d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nam Downloads mxnet mshadow I home nam Downloads mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 I opt intel compilers and libraries 2017 0 098 linux mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 0 fopenmp DMXNET USE MKL2017 1 DUSE MKL 1 I home nam Downloads mxnet src operator mkl DMKL EXPERIMENTAL 1 DMXNET USE NVRTC 0 c src operator batch norm cc o build src operator batch norm o In file included from src operator mkl mkl memory inl h 29 0 from src operator batch norm cc 11 src operator mkl mkl cppwrapper h In function dnnError t dnnBatchNormalizationCreateForward v2 uniPrimitive s dnnPrimitiveAttributes t dnnLayout t float int with Dtype float dnnPrimitive t uniPrimitive s dnnPrimitiveAttributes t void dnnLayout t dnnLayout s src operator mkl mkl cppwrapper h 835 38 error dnnBatchNormalizationCreateForward v2 F32 was not declared in this scope dataLayout eps flags src operator mkl mkl cppwrapper h In function dnnError t dnnBatchNormalizationCreateForward v2 uniPrimitive s dnnPrimitiveAttributes t dnnLayout t float int with Dtype double dnnPrimitive t uniPrimitive s dnnPrimitiveAttributes t void dnnLayout t dnnLayout s src operator mkl mkl cppwrapper h 845 38 error dnnBatchNormalizationCreateForward v2 F64 was not declared in this scope dataLayout eps flags src operator mkl mkl cppwrapper h In function dnnError t dnnBatchNormalizationCreateBackward v2 uniPrimitive s dnnPrimitiveAttributes t dnnLayout t float int with Dtype float dnnPrimitive t uniPrimitive s dnnPrimitiveAttributes t void dnnLayout t dnnLayout s src operator mkl mkl cppwrapper h 863 38 error dnnBatchNormalizationCreateBackward v2 F32 was not declared in this scope dataLayout eps flags src operator mkl mkl cppwrapper h In function dnnError t dnnBatchNormalizationCreateBackward v2 uniPrimitive s dnnPrimitiveAttributes t dnnLayout t float int with Dtype double dnnPrimitive t uniPrimitive s dnnPrimitiveAttributes t void dnnLayout t dnnLayout s src operator mkl mkl cppwrapper h 874 38 error dnnBatchNormalizationCreateBackward v2 F64 was not declared in this scope dataLayout eps flags In file included from src operator batch norm cc 12 0 src operator mkl mkl batch norm inl h In member function virtual void mxnet op MKLBatchNormOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob src operator mkl mkl batch norm inl h 174 74 error dnnUseScaleShift was not declared in this scope batchNormFwdInference NULL mem descr layout int eps dnnUseScaleShift src operator mkl mkl batch norm inl h 178 15 error dnnUseInputMeanVariance was not declared in this scope dnnUseInputMeanVariance dnnUseScaleShift src operator mkl mkl batch norm inl h 193 63 error dnnUseScaleShift was not declared in this scope batchNormFwdTraining NULL layout usr eps dnnUseScaleShift src operator mkl mkl batch norm inl h 198 15 error dnnUseInputMeanVariance was not declared in this scope dnnUseInputMeanVariance dnnUseScaleShift src operator mkl mkl batch norm inl h 234 21 error dnnResourceMean was not declared in this scope BatchNorm res dnnResourceMean mean dptr src operator mkl mkl batch norm inl h 235 21 error dnnResourceVariance was not declared in this scope BatchNorm res dnnResourceVariance var dptr src operator mkl mkl batch norm inl h 239 21 error dnnResourceMean was not declared in this scope BatchNorm res dnnResourceMean moving mean dptr src operator mkl mkl batch norm inl h 240 21 error dnnResourceVariance was not declared in this scope BatchNorm res dnnResourceVariance moving var dptr src operator mkl mkl batch norm inl h In member function virtual void mxnet op MKLBatchNormOp xpu DType Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob src operator mkl mkl batch norm inl h 310 19 error dnnResourceMean was not declared in this scope BatchNorm res dnnResourceMean mean dptr src operator mkl mkl batch norm inl h 311 19 error dnnResourceVariance was not declared in this scope BatchNorm res dnnResourceVariance var dptr Makefile 180 recipe for target 'build src operator batch norm o' failed make build src operator batch norm o Error 1 make Waiting for unfinished jobs,,"piiswrong,glingyan,glingyan,glingyan,glingyan,glingyan",2016-12-22 07:19:54,2016-12-24 06:27:56
PR,0 9 release note,tqchen See if you have any thing to add,,"piiswrong,mli,piiswrong,mli,yzhliu",2016-12-19 23:36:10,2016-12-24 07:56:12
PR,Recordio patch,,,piiswrong,2016-12-23 09:20:41,2016-12-24 08:52:28
IS,mx io ImageRecordIter seed does not change after resetting the iterator,Hi I'm having some issues with the seed of mx io ImageRecordIter set for default at zero because doing some tries I have noticed that after reading the whole rec file and resetting the iterator I obtain in the second reading cycle exactly the same sequence of images with the exactly the same crop rotation etc for each one Does this happen also when I run the model with the CNN training every epoch on the same identical sequence of images Is there a possibility to set the seed to random,,"piiswrong,piiswrong",2016-12-23 10:20:24,2016-12-24 11:56:11
IS,Memory Leak Problem in c api MXAPIThreadLocalEntry ret MXAPIThreadLocalStore Get,piiswrong I noticed that the memory will leak when I use c api functions the reason is the the upper result verifies my opinion,,tqchen,2016-12-24 11:35:18,2016-12-24 12:02:00
PR,Fix Underflow Bug in GPUPooledStorageManager Alloc,The two variables free and total are of size t type i e they are unsigned integers When performing subtraction if the value of free is smaller than that of total reserve 100 underflow happens and the result will be an extremely large integer which will prevent the memory release but it is actually required This causes out of memory problems in the FCN example,,II-Matto,2016-12-24 11:14:23,2016-12-24 19:59:08
PR,test do not merge,,,piiswrong,2016-12-24 20:58:22,2016-12-24 23:46:06
IS,Table of contents redundantly displays title,Example Tutorials is under the table of contents which is also the title of the page making the Table of Contents crowded This looks especially weird on pages with a sparse Table of Contents,,"andremoeller,piiswrong,andremoeller",2016-11-21 04:44:16,2016-12-25 22:59:24
IS,Why the picture of ilsvrc12 val dataset does not match its' label MXNet provided,Why the picture of ilsvrc12 val dataset does not match its' label MXNet provided val dataset link label link thx,,,2016-12-26 03:21:46,2016-12-26 03:45:10
IS,gradient clip with l2norm in mxnet,I'm trying to implement a gradient clip with l2norm using the c optimizer The standard gradient clip needs to calculate the l2norm value over all the parameters and use a global scale for every parameter like follows default However in mxnet is optimizer I can only access one parameter Currently I calculate a scale for each parameter but it seems the model can not converge as good as the theano baseline Is there any good idea to implement a standard gradient clip over all parameters,,"sxjscience,shivarajugowda,sxjscience,sxjscience,sxjscience,shivarajugowda,sxjscience,shivarajugowda",2016-12-10 03:27:48,2016-12-26 05:06:01
IS,ImageRecordIter on multi label rec file failed,hi I'm trying to follow the multi task example using my own data I use im2rec to make the multi label rec file like im2rec train txt train imgs data imgrec rec label width 2 and get the rec file successfully but when I am trying to use mx io ImageRecordIter to load the recfile and make a dataIter will return 04 58 12 src io iter image recordio cc 318 Check failed param label width 1 label width must be 1 unless an imglist is provided or the rec file is packed with multi dimensional label who knows how to solve this problem,,piiswrong,2016-12-23 10:01:50,2016-12-26 06:09:41
IS,how to deal with the unlabel data with the dataiter,hi I want to load some labeled and unlabeled data into mxnet I follow the example L96 url to create a dataiter how to implement the provide label method for unlabel data Thanks for your attention,,"winstywang,winstywang,winstywang",2016-11-03 06:51:31,2016-12-26 08:00:31
IS,Multi task cnn model for sentences classification,Hi thanks for your attention I use cnn to do some sentences classification The model structure is the same as example cnn text classification I try to modify it for multi task classification following the 647 There are two inputs and outputs of the model and share the hidden layers The acc is always around 50 for both train and test but it is nearly 77 when I use it for single task 3776 Under the guidance of paper A Sensitivity Analysis of and Practitioners Guide to Convolutional Neural Networks for Sentence Classification I change some params mentioned in it But the result almost has no change So I think whether the way I use the multi task is wrong Here is the code could you please help me find out what is wrong with it Many thanks,,zihaolucky,2016-11-21 02:59:42,2016-12-26 08:02:41
PR,Update index md,Julia is wrong it should be C,,,2016-12-26 12:26:08,2016-12-26 18:30:11
PR,Add mxnet notebook into setup py,Sub module mxnet notebook is missing in setup py the pull request will resolve that issue by adding mxnet notebook in packages 'mxnet' 'mxnet module',,DamonDeng,2016-12-26 09:38:25,2016-12-26 18:30:45
IS,Predict using GPU,How can I predict on my validation dataset using GPU after loading a model I have trained my model on GPU and then I saved it from terminal but when I load the model for doing predictions only CPU is used model loaded mx model FeedForward load prefix iteration preds model loaded predict validation dataset How can I predict with GPU instead of CPU since there are no ctx parameters in the predict function Thanks,,,2016-12-26 10:43:39,2016-12-26 22:51:12
PR,doc update how to perf md,ZihengJiang can you provide some instructions for how to use profiler,,"mli,piiswrong,mli,howard0su,howard0su,howard0su,howard0su,howard0su,howard0su,mli",2016-12-23 22:27:28,2016-12-27 00:55:39
IS,Fail to Run FCN Example with FCN8s or with retrain Flag On,tqchen I tried to train an FCN8s with the FCN example provided in example fcn xs but it always failed with OOM after 1 or 2 epochs The error message is like below INFO root Epoch 1 Batch 1980 Speed 2 75 samples sec Train accuracy 0 692035 17 38 03 home user name lib mxnet dmlc core include dmlc logging h 235 17 38 03 src storage pooled storage manager h 79 cudaMalloc failed out of memory Traceback most recent call last File home user name lib mxnet example fcn xs fcn xs py line 78 in module main File home user name lib mxnet example fcn xs fcn xs py line 62 in main epoch end callback mx callback do checkpoint fcnxs model prefix File home user name lib mxnet example fcn xs solver py line 73 in fit aux states self aux params File home user name lib mxnet python mxnet symbol py line 852 in bind ctypes byref handle File home user name lib mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 17 38 03 src storage pooled storage manager h 79 cudaMalloc failed out of memory I have tried mxnet memonger and called the search plan function before creating the train dataiter in example fcn xs fcn xs py The inserted line is given below fcnxs memonger search plan fcnxs data 1 3 2000 2000 Data shapes of 1000 1200 1600 and 2000 were tried but none of them worked The mxnet memonger solved the problem with FCN32s with data shape of 1000 FCN32s trained with over 40 epochs without reporting error During all experiments the cut off size is not set Some relevant information I posted few days ago can be found in issue issuecomment 265673009 Another Problem is that I cannot train an FCN with retrain flag set on It reported error of operands context mismatch When I set ctx to gpu 3 L14 the program reported mxnet base MXNetError 10 37 36 src operator operator util cc 886 Check failed lhs ctx rhs ctx operands context mismatch 2 3 vs 1 0 More information posted by me can be found in issue issuecomment 265920113 I used the latest MXNet on Ubuntu 14 04 with 12G memory Titan X,,"II-Matto,II-Matto,II-Matto,tornadomeet,II-Matto",2016-12-17 02:24:28,2016-12-27 08:11:54
PR,add support to monitor aux params,Recently I implemented an Operator using aux parameters It can be helpful if the monitor can also watch these parameters,,"luoyetx,piiswrong",2016-12-25 07:05:23,2016-12-27 08:21:26
IS,some error about mx sym RNN symbol,I got an error 14 50 47 src operator cudnn rnn inl h 191 Check failed req rnn enum kState kAddTo AddTo is not supported for state However I did not set grad req as 'add',,dsqx71,2016-12-12 07:01:52,2016-12-27 13:31:35
PR,Fix classification example,add back lr scheduler in add allow missing so that fine tune py can work,,howard0su,2016-12-27 23:34:42,2016-12-27 23:46:22
PR,fix cpu ctx,sxjscience,,piiswrong,2016-12-28 00:27:28,2016-12-28 01:49:10
IS,Development based on v0 9rc1,We are currently developing based on MXNet and can be benefitted from severals commits on v0 9 rc1 such as MXProfiler and optimizer changes However the difference between the branch and master branch is extremely large Therefore in order to leverage those advantages for development I am wondering if there is any potential issue based on this branch In addition do you have any updated schedule to release next stable release,,piiswrong,2016-12-27 10:40:46,2016-12-28 02:29:29
IS,ERROR when I train train mnist py using 2 gpus,I tried to train mnist example with train mnist py and the parameters are setting like this System Info ubuntu 16 04lts 64 bit GPU 4 x Titan X CUDA 8 CUDNN v5 1,,howard0su,2016-12-28 05:08:07,2016-12-28 07:12:40
PR,GPU installation for mac and ubuntu,,,"kevinthesun,piiswrong,howard0su,howard0su,howard0su,howard0su,kevinthesun",2016-12-21 20:47:18,2016-12-28 07:23:23
IS,search box on mxnet io does not work,it seems that there is a transparent object over the box,,"mli,kevinthesun,mli",2016-12-28 05:23:23,2016-12-28 07:31:03
PR,Topk and arange,Continue after the rebase of NNVM,,"sxjscience,piiswrong,thirdwing,sxjscience,piiswrong,sxjscience,piiswrong,piiswrong,sxjscience,piiswrong,piiswrong,piiswrong,piiswrong",2016-12-12 05:08:42,2016-12-28 11:08:27
IS,What is the right and 100 working configuration under Ubuntu 16 04LTS,Hi anyone can tell me the right TESTED and 100 WORKING configuration under Ubuntu for deep3d This config is work Ubuntu 16 04LTS 64bit Cuda 8 DNN 5 1 Nvidia 1080 8gb mxnet from git deep3d from git Thank You So So much and Merry Christmas Best Moty,,andremoeller,2016-12-24 15:31:44,2016-12-28 16:00:41
PR,Update windows setup md,add instruction on how to use script to build and install mxnet with python on Windows,,"howard0su,howard0su,piiswrong",2016-12-28 08:43:34,2016-12-28 19:13:38
PR,Fix the index check in ndarray,N length array should only have index from 0 to N 1,,howard0su,2016-12-28 07:42:00,2016-12-28 19:15:11
PR,Introduce a developer mode option in Makefile,this option will enable two addtionals for now 1 Create symbol even for optimized build 2 Enforce warning as error while I am here clean up all warnings so that I can get a clean build in developer mode,,"howard0su,piiswrong,howard0su,howard0su",2016-12-25 08:21:52,2016-12-28 19:18:10
PR,reduce default lr to 0 05,This issue pops up in community for couple times Reduce this to avoid confusion for new users,,"howard0su,piiswrong",2016-12-28 08:31:09,2016-12-28 20:18:29
PR,Checkpoint optimizer states,Currently mxnet only checkpoint the model In practice the computation states are not only model but also states of the optimizer For SGD with momentum enabled momentum is the state of the SGD optimizer and it saves something like history of gradient So when checkpointing the states of optimizer should be saved as well If the optimizer state is not saved when load a checkpoint and continue training there will be a sharp decline for both training accuracy and validation accuracy This PR saves optimizer states when checkpointing,,piiswrong,2016-12-06 22:43:05,2016-12-28 20:37:31
PR,RELEASE v0 8 Release Last release before NNVM refactor,,,piiswrong,2016-12-28 21:46:02,2016-12-28 23:27:28
PR,Module add save load optimizer states,zorksylar,,piiswrong,2016-12-28 20:11:49,2016-12-28 23:29:57
PR,Add instruction on new setup script,Add the detailed documents about how to use setup utils install mxnet windows python bat,,howard0su,2016-12-24 16:12:22,2016-12-29 02:25:38
PR,fix hidden keys,,,piiswrong,2016-12-29 02:57:56,2016-12-29 03:51:03
PR,Fix EncodeKey may occur error on linux x86 64,Fix issue,,"wzl12356,piiswrong",2016-12-29 02:21:18,2016-12-29 04:01:00
IS,There is no output after adding an output statement into model py,I add an output statement print into model py but there is no according output while running What is the problem,,andremoeller,2016-12-25 05:12:52,2016-12-29 05:31:29
PR,Fix segment fault when profiling,,,"howard0su,piiswrong,zihaolucky,piiswrong,piiswrong,howard0su,piiswrong,howard0su,howard0su",2016-12-28 14:00:50,2016-12-29 06:29:57
PR,WIP Fix amalgamation,x Compile x Add support for MKLDNN x Add support for NNPACK Test with pretrained model Add test to jekins,,"antinucleon,mli,antinucleon,antinucleon,tqchen,piiswrong",2016-12-12 06:22:56,2016-12-29 07:24:48
PR,v0 9 1 pre release,,,piiswrong,2016-12-29 08:59:45,2016-12-29 09:37:48
PR,2 step amalgamation,This temporary amalgamation fix try to avoid macro and dmlc conflict between nnvm and mxnet The future pr will merge nnvm all cc and mxnet predict all cc into a single file,,"antinucleon,piiswrong",2016-12-29 07:48:10,2016-12-29 20:17:31
PR,Adding warning if autotuner chooses suboptimal cuDNN algorithm,If the either default or specified by user value of workspace parameter to Convolution is too low autotuner silently omits cuDNN algorithms which may have performance impact This PR introduces a warning and advice message when this happens so that the user is aware of possible performance improvement,,ptrendx,2016-12-16 23:24:42,2016-12-29 20:17:31
PR,Improve profiler,1 Reduce lock scope and avoid do memory allocation inside lock when possible Limit symbol name to 32 chars 2 Add two environment variables to control profiler This enables profiling without change any code MXNET PROFILER MODE 0 1 to select 'all' or isymbolic' MXNET PROFILER AUTOSTART 1 to enable profiler automatically 3 Output pid tid as unsigned int 4 Get number of GPUs during runtime 5 Add Profiler support to cmake build and enable it by default,,"howard0su,piiswrong",2016-12-29 12:44:02,2016-12-29 20:17:31
PR,Improve test utils,Continuing 1 Use central difference 2 Use sqrt data 2 sum eps for calculating the L2Normalization Previously we use sqrt data 2 sum eps 3 A new reldiff calculation method We use relative difference if the absolute difference is larger than 1E 3 and fall back to absolute difference if it is smaller than 1E 3 The maximum difference is returned 4 Revise the threshold when testing some operators,,"sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,piiswrong",2016-12-01 13:52:46,2016-12-29 20:17:31
PR,Profiler support native engine,we need pass flag profiling to PushAsync but interface of PushAsync is non changable so we pass a lamda function to PushAsync and it needs to see NaiveEngine OnComplete in capture list so use there now we can use profiler to analyze native engine here is the one example image,,"tornadomeet,ZihengJiang,ZihengJiang,tornadomeet,piiswrong,piiswrong,tornadomeet,tornadomeet,piiswrong,ZihengJiang,tornadomeet,piiswrong,piiswrong,piiswrong,tornadomeet,tornadomeet,piiswrong",2016-12-07 12:02:57,2016-12-29 20:17:31
PR,Topk and arange rebase,,,"sxjscience,piiswrong,piiswrong",2016-12-28 11:08:10,2016-12-29 20:17:31
PR,WIP Convert iterator to mx img API,The change bring me 3x performance improvement in single GPU Please check if this is what you suggested TODO x Fix crop when xmin ymin 0 Directly convert from np code does not work with ndarray x Figure out why SmoothL1 nan when multiGPUs is using,,"howard0su,piiswrong,howard0su,piiswrong,piiswrong,piiswrong,howard0su,howard0su,howard0su,piiswrong,howard0su,howard0su,howard0su,howard0su,zhreshold,howard0su,piiswrong,howard0su,howard0su,piiswrong,zhreshold,piiswrong,howard0su,howard0su,howard0su,piiswrong",2016-12-26 06:30:15,2016-12-29 20:17:31
PR,WIP more powerful of NNPACK,currently NNPACK only support convolution operator with batch size 1 and this does'n t utilize fully performance of NNPACK when do inference so we want to make it more powerful x set number of threads by environment var x update convolution of using NNPACK set MXNET CPU NNPACK NTHREADS 4 before this pr of using NNPACK run example image classification benchmark score py here is log x document of using NNPACK,,"tornadomeet,tornadomeet,mli,tornadomeet,tornadomeet,xlvector,piiswrong",2016-12-26 05:58:12,2016-12-29 20:17:31
PR,add svm output gpu code,,,"yajiedesign,piiswrong,piiswrong,piiswrong,piiswrong,yajiedesign,piiswrong,piiswrong,yajiedesign,piiswrong,yajiedesign,piiswrong,yajiedesign",2016-11-25 06:07:38,2016-12-29 20:17:31
PR,mkl fast path patch update,1 enable fast path in nnvm branch 2 fix mkl v2 private format problem 3 disable mkl info Signed off by lingyan lingyan guo intel com,,"glingyan,glingyan,glingyan,glingyan,glingyan,glingyan,piiswrong,glingyan,piiswrong,glingyan",2016-12-29 13:37:22,2016-12-29 20:17:31
PR,fix docs,,,"piiswrong,leopd,piiswrong,piiswrong,leopd",2016-12-29 20:01:49,2016-12-30 00:27:56
PR,mkl fast path patch update,1 enable fast path in nnvm branch 2 fix mkl v2 private format problem 3 disable mkl info Signed off by lingyan lingyan guo intel com,,"glingyan,piiswrong,piiswrong,glingyan,glingyan,glingyan,howard0su,glingyan",2016-12-29 23:35:31,2016-12-30 00:58:17
PR,fixing r pdf docs generating Rd files,Generates R package man Rd files which are used to make reference manual,,andremoeller,2016-12-29 22:49:17,2016-12-30 00:58:30
PR,Scala update Visualization plotNetwork,follows the update diff 639447aa6a95cf6976f3f7fda95a3f84,,Ldpe2G,2016-12-29 13:11:33,2016-12-30 00:59:29
IS,use simple bind can not make continuous predictions,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 MXNet version installed from source branch master If you are using python package please provide Anaconda 4 2 0 64 bit Python version and distribution Python 2 7 12 Error I train a OCR model use CNN Multi label net when I predict a pic one time the result is correct but run twice the first is correct and the second is wrong code def TestRecognizeOne file batch size 1 sym arg params aug params mx model load checkpoint ocr 0 data shape data batch size 3 50 200 input shapes dict data shape sym getnet executor sym simple bind ctx mx gpu input shapes for key in executor arg dict keys if key in arg params arg params key copyto executor arg dict key executor forward is train False data mx nd array img probs executor outputs 0 asnumpy a np argmax probs 0 b np argmax probs 2 print a b and def getnet data mx symbol Variable wouldata' label mx symbol Variable 'label' conv1 mx symbol Convolution data data kernel 5 5 pad 2 2 num filter 96 relu1 mx symbol Activation data conv1 act type relu pool1 mx symbol Pooling data relu1 pool type max kernel 3 3 stride 2 2 conv2 mx symbol Convolution data pool1 kernel 3 3 pad 1 1 num filter 128 relu2 mx symbol Activation data conv2 act type relu pool2 mx symbol Pooling data relu2 pool type max kernel 3 3 stride 2 2 flatten mx symbol Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 2048 relufc1 mx symbol Activation data fc1 act type relu dropout1 mx symbol Dropout data relufc1 p 0 5 fc2 mx symbol FullyConnected data dropout1 num hidden 1024 relufc2 mx symbol Activation data fc2 act type relu dropout2 mx symbol Dropout data relufc2 p 0 5 fc21 mx symbol FullyConnected data dropout2 num hidden 35 fc22 mx symbol FullyConnected data dropout2 num hidden 35 fc3 mx symbol Concat fc21 fc22 dim 0 label mx symbol transpose data label label mx symbol Reshape data label target shape 0 return mx symbol SoftmaxOutput data fc3 name softmax when run testRecognizeOne 0 jpg testRecognizeOne 0 jpg the output 2 3 0 0 the probs 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 0 02857143 In addition when I use simple bind in lstm ctc cnn ctc I found the same problem,,"piiswrong,piiswrong",2016-12-22 02:42:59,2016-12-30 01:24:48
PR,doc improvement,,,piiswrong,2016-12-30 00:53:58,2016-12-30 02:12:47
PR,adding r html docs,mli This uses so you will need to python setup py install that on the build server Adding R html docs Currently all on one page under api r html docs index html This converts the rd files to HTML then converts those to markdown and does some post processing There is not a good way to convert rd files to markdown directly as far as I can tell We need them in markdown for Sphinx to apply the CSS and JS the HTML that Rd2HTML makes is just plain HTML pages,,"andremoeller,mli,andremoeller,piiswrong,andremoeller",2016-11-29 03:29:29,2016-12-30 02:26:20
IS,infer type error when using mx sym expand dims on an internal variable,The example code below is a simple network for linear regression Weight and bias are defined as internal variables The only difference between get symbol1 and get symbol2 is the shape of sym w I am not sure if it is a bug or something The symbol from get symbol2 is not able to execute infer type successfully The error message is mxnet base MXNetError InferType Error in expand dims0 16 48 25 src operator operator util cc 402 At least one input type needs to be specified Change mx sym expand dims to mx sym Reshape does not work either,,"nicklhy,piiswrong,nicklhy,piiswrong,nicklhy,tornadomeet,nicklhy",2016-12-28 09:01:01,2016-12-30 03:03:33
PR,Improve profiler,1 Reduce lock scope and avoid do memory allocation inside lock when possible Limit symbol name to 32 chars 2 Add two enviroment variables to control profiler This enables profiling without change any code MXNET PROFILER MODE 0 1 to select 'all' or isymbolic' MXNET PROFILER AUTOSTART 1 to enable profiler automatically 3 Output pid tid as unsigned int 4 Get number of GPUs during runtime 5 Add Profiler support to cmake build and disabled by default,,"howard0su,tornadomeet,piiswrong,howard0su,tornadomeet",2016-12-29 23:42:53,2016-12-30 04:55:42
IS,Variable with dtype,It seems mxnet does not support a varaible with specified dtype Is that difficult to add this function l,,piiswrong,2016-12-30 03:52:18,2016-12-30 04:59:34
PR,Improve OSX install experience,Added a semiautomated install script for MacOSX,,"larroy,piiswrong,howard0su,howard0su,howard0su,howard0su,larroy,larroy,larroy,piiswrong,howard0su,larroy,larroy",2016-12-23 16:04:17,2016-12-30 05:33:18
IS,symbol set attribute error,I just updated the code to the latest version and changed it to the nnvm branch Since my model is a large network I need to set some attributes and use memonger to save GPU memory cost If I set the attribute when creating a symbol y mx sym BatchNorm data mx sym Variable wouldata' attr 'mirror stage' 'True' it will incur an error like everything works fine Since mx sym load uses the first method to set attributes it is not possible to load a json file with 'mirror stage' attributes now,,"nicklhy,piiswrong",2016-12-30 03:03:07,2016-12-30 05:36:38
IS,Error Message GPU is not enabled,Hi I am trying to execute the foloowing statment python demo py prefix final epoch 0 image myimage jpg gpu 0 The demo py file is at example rcnn Error Message mxnet base MXNetError 16 33 57 src ndarray ndarray cc 296 GPU is not enabled I want to to execute on a CPU as I am not sure My graphics card supports Cuda or not Environment info Operating System Mac OS sierra 10 12 1 my graphics card is Intel Iris Pro 1536 MB Package used Python MXNet version 0 9 Python version and distribution 2 7 Thanks,,"piiswrong,piiswrong",2016-12-29 21:41:51,2016-12-30 05:39:27
PR,fixing typo in the R manual command,,,andremoeller,2016-12-30 06:27:41,2016-12-30 06:48:13
IS,compile error of new nnvm version,i compile the new version updated this morning it appears some cudnn errors as follows image i always use cuda7 5 and cudnn7 5 v5 0 i do not know if new version of mxnet do not support it again,,piiswrong,2016-12-30 03:31:01,2016-12-30 06:51:18
IS,Lots of API references missing from the docs site,What happened to the docs site I could not find lots of API references such as Deconvolution or FullyConnected there as they used to be Are they missing or moved to somewhere else,,"kevinthesun,andremoeller,piiswrong,mli,piiswrong",2016-12-29 17:06:27,2016-12-30 12:25:20
IS,Proposal on new data augmenter pipeline,Current implementation is implemented augmenter as a add on of certain iterator implementation This approach is very close to what Caffe is implementing From my point of view this is not the best way to implement a flexible data augmenter In MXNet iterator implementation it is common that we stack one iterator over another For example put a prefetch iterator over recordio iterator I would suggest to implement data augmenter as a set of iterators We can implement scale crop color distort padding rotate as a set of iterators Benefits 1 more flexible pipeline This can satisfied more different requirement on data augmenter 2 Reuse the pipe between image classification and detection In order to data augment the data for detection label have to do the transform related to image is transform for example scale current implementation will require this either using a option to state detection or classification or implement another data augmenter for detection If we go with the new approach we can just write one additional data augmenter for label this data augmenter can do the corresponding transform for label each image data augmenter need expose the actual argument to do the image transform through an additional variable Love to hear your suggestions or options before I do actual coding,,"howard0su,piiswrong,howard0su,piiswrong,howard0su",2016-12-25 11:32:51,2016-12-30 12:37:03
PR,fix unordered map,According to this issue,,xlvector,2016-12-30 05:01:04,2016-12-30 12:51:30
PR,fix memmonger,,,piiswrong,2016-12-30 04:43:23,2016-12-30 12:52:36
IS,mx nd Reshape instead of mx nd reshape,This is sort of inconsistency to use upper case for the name of operator shall we convert it to low case,,howard0su,2016-12-30 13:32:10,2016-12-30 13:59:58
PR,Fix dropout block8 inception resnet v2,Infer to 4032 As metioned I have some error in block8 and dropout p should be 0 2 instead of 0 8 Thanks for I'm so careless,,burness,2016-12-30 17:06:24,2016-12-30 17:20:04
PR,fix build warning,Signed off by lingyan lingyan guo intel com,,glingyan,2016-12-30 07:23:49,2016-12-30 17:26:48
PR,fix PythonInterp,Need cooperation,,yajiedesign,2016-12-30 03:36:26,2016-12-30 17:27:20
PR,fix dropout and block8,Infer to 4032 As metioned I have some error in block8 and dropout p should be 0 2 instead of 0 8 Thanks for I'm sorry about it,,burness,2016-12-30 17:25:45,2016-12-30 17:28:58
PR,Document the enviroment variabls of the profiler,,,howard0su,2016-12-30 12:40:06,2016-12-30 17:29:26
PR,fix complie error with vs2013 and cuda7 5,,,yajiedesign,2016-12-30 11:07:49,2016-12-30 17:30:08
PR,merge from new,,,"wzl12356,mli,wzl12356",2016-12-29 02:00:23,2016-12-30 17:31:21
IS,Build failed with unordered map init,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System linux debian Compiler nvcc Package used Python R Scala Julia python MXNet version master Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message g std c 11 DMSHADOW FORCE STREAM Wall O3 I home tiger mxnet mshadow I home tiger mxnet dmlc core include fPIC I home tiger mxnet nnvm include Iinclude funroll loops Wno unused parameter Wno unknown pragmas msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE DIST KVSTORE I home tiger mxnet ps lite include I home tiger mxnet deps include fopenmp DMXNET USE NVRTC 0 MM MT build src operator tensor elemwise unary op o src operator tensor elemwise unary op cc build src operator tensor elemwise unary op d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home tiger mxnet mshadow I home tiger mxnet dmlc core include fPIC I home tiger mxnet nnvm include Iinclude funroll loops Wno unused parameter Wno unknown pragmas msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE DIST KVSTORE I home tiger mxnet ps lite include I home tiger mxnet deps include fopenmp DMXNET USE NVRTC 0 c src operator tensor elemwise unary op cc o build src operator tensor elemwise unary op o src operator tensor elemwise unary op cc In lambda function src operator tensor elemwise unary op cc 41 62 error converting to std unordered map std basic string char std basic string char from initializer list would use explicit constructor std unordered map Key Tp Hash Pred Alloc unordered map std unordered map Key Tp Hash Pred Alloc size type const hasher const key equal const allocator type with Key std basic string char Tp std basic string char Hash std hash std basic string char Pred std equal to std basic string char Alloc std allocator std pair const std basic string char std basic string char std unordered map Key Tp Hash Pred Alloc size type long unsigned int std unordered map Key Tp Hash Pred Alloc hasher std hash std basic string char std unordered map Key Tp Hash Pred Alloc key equal std equal to std basic string char std unordered map Key Tp Hash Pred Alloc allocator type std allocator std pair const std basic string char std basic string char auto lhs MakeGradNode backward copy n ograds,,"xlvector,xlvector,piiswrong",2016-12-29 14:13:12,2016-12-30 17:35:24
IS,Is Symbol grad implemented,Hello I was trying to compute some gradients in the model In the reference manual I found a function called mxnet symbol Symbol grad wrt It seemed to be pretty like the auto differentiation function in THEANO However when I tried it a not implemented error was raised By tracing the codes I found the following codes in src c api c api symbolic cc However I still believe the symbolic auto differentiation must have been implemented in MXNet Could someone help me with this THX in advance,,shivarajugowda,2016-12-30 13:23:51,2016-12-30 22:29:40
PR,Update index md,Update to include what pre trained vision models we have already Will open to the community to contribute the rest,,jspisak,2016-12-30 21:47:01,2016-12-31 04:03:02
PR,Update index md,sorry missed a link,,jspisak,2016-12-31 04:26:05,2016-12-31 05:26:27
PR,update submodule,,,piiswrong,2016-12-30 17:36:54,2016-12-31 05:29:31
IS,mx model load checkpoint gives LocalFileSystem fail to open error,This is the first time I use mxnet I download the all latest files from github I use ubuntu 16 04 and python 2 7 mx model load checkpoint gives error even though the file is there Please see also attachmnet image Thank you error code prefix r' root share mxnet models mxnet gallery inception bn Inception BN' sym arg params aux params mx model load checkpoint prefix 216 error mxnet base MXNetError 11 13 36 src io local filesys cc 154 Check failed allow null LocalFileSystem fail to open root share mxnet models mxnet gallery inception bn Inception BN 0216 params,,,2016-12-31 11:27:24,2016-12-31 11:59:30
IS,Amalgamation for Android Changes Model Behavior,I am running amalgamation for Android and noticed some strange behavior I wrote a unit test for Leliana WhatsThis and discovered that the libmxnet predict so amalgamated from the most recent master branch changes the behavior of the model For the same model params and input feedforward results in different output using the freshly amalgamated so I am using an image of a guinea pig which resulted in 'hamster' using the WhatsThis model but now outputs 'nematode In fact updating the so for WhatsThis outputs 'nematode' for most images master branch amalgamated 1ff820a03846b84fc370850ed3770ce52d8169dd Can anyone help me find out if this is my issue an amalgamation issue or simply an artifact of the WhatsThis model I can upload my test if needed Thanks,,piiswrong,2016-12-26 17:47:50,2016-12-31 12:45:40
PR,Fix syntax error with C compiler in mxnet c api h,Declarations and definitions in mxnet c api h should be available for C compiler without C features This patch fixes some errors about the compatibility with C compiler Include 1 Incomplete MXNET EXTERN C macro 2 stddef h should be included for size t 3 bool is not a valid C data type 4 In C language default arguments would be syntax error,,"piiswrong,piiswrong,piiswrong,piiswrong,tqchen,piiswrong,tqchen,tqchen,Ldpe2G,Ldpe2G,piiswrong,howard0su,howard0su",2016-11-25 15:55:49,2016-12-31 16:52:18
PR,Update index md,Fix tutorial link for Neural Network Graphs,,rravu3,2016-12-31 15:12:13,2016-12-31 19:28:34
PR,add inst src o src so to clean rule for R package,The clean rule in Makefile does not remove all temporary files in R package inst and some shared objects files were missed This PR adds them to the Makefile,,bwilbertz,2016-12-31 15:10:41,2016-12-31 19:29:19
PR,Enable warning as error,Also enable profiler code into the build,,"howard0su,piiswrong,howard0su",2016-12-30 11:39:43,2016-12-31 23:37:41
PR,Use mean data for prediction,It use mean data from pre trained model and get a slightly better accuracy than a static mean value,,"howard0su,piiswrong,piiswrong",2016-12-31 16:44:30,2017-01-01 06:43:19
PR,Fix syntax error with C compiler in mxnet c api h,Continue the work in 3981 Make a new pull request and the diff looks more clear,,"howard0su,piiswrong",2016-12-31 16:51:03,2017-01-01 06:47:46
PR,add mxnet install script for fedora python,Add install script of mxnet python for fedora machine it tested for the Fedora 21 0,,"piiswrong,piiswrong,zihaolucky",2017-01-01 04:32:02,2017-01-01 08:11:43
PR,add access to attributes of R symbols,The R package does not offer any access to attributes of a symbol This PR adds a Rcpp property 'attributes' and allows getting and setting of the internal attribute values as it is possible in Python Brief usage example can be found in test symbol R,,bwilbertz,2016-12-31 15:16:17,2017-01-01 18:33:27
PR,add profiler to R Interface,This PR adds profiling support to R Interface along the Python lines Example,,bwilbertz,2017-01-01 14:01:51,2017-01-01 20:23:16
PR,add new Chinese translation wouldocs zh tutorials computer vision image classification md',add new Chinese translation wouldocs zh tutorials computer vision image classification md' which is from wouldocs tutorials computer vision image classification md',,ironyoung,2017-01-01 13:07:57,2017-01-01 20:24:30
IS,src zmq van h 122 there is no socket to node 9,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler gcc 4 9 4 Package used Python R Scala Julia python python2 7 MXNet commit hash git rev parse HEAD e036c24632fa6ed66b0f152df8902e3e2d997293 Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error I use 3 computer to train my example I reboot the 3 computer and train it again the error occur What have you tried to solve it 1 I make clean all the mxnet 2 cd into the ps lite and use the command make 3 I try the standard example the error message is the same,,mli,2016-12-08 13:06:12,2017-01-01 20:24:55
PR,update ps lite,fix 4156 see explanation in L18,,mli,2016-12-30 19:19:06,2017-01-01 20:24:55
PR,committing Rd files until these can be reliably generated,This is a stopgap These should really be generated but something is going wrong on the build server This will be easier to fix once the build server opens up to a wider audience,,"andremoeller,andremoeller",2016-12-31 01:37:14,2017-01-02 01:01:10
IS,docs R packages man not generated,The R reference manual is missing most of its content This is where the R docs are generated L19 The files in R packages man are generated using roxygen2 roxygenize which are then used to make the reference manual You may need to install the packages roxygen2 on the docs build host but without access to the host or logs I'm not sure,,"andremoeller,piiswrong,andremoeller,piiswrong,andremoeller,andremoeller",2016-12-30 16:58:13,2017-01-02 01:01:37
IS,Cython compilation Cannot declare pointer to void,Hello I tried to install MXnet on an amazon ubuntu The installation fails on sudo python setup py install with Cannot declare pointer to void I followed the script install mxnet ubuntu python sh Can someone guide me on the instructions to follow to fix this Thanks in advance Environment info Operating System ubuntu 14 04 3 Compiler gcc version 4 8 4 Ubuntu 4 8 4 2ubuntu1 14 04 3 Package used Python 2 7 6 from system Cython version 0 22 1 MXNet version master b5988bdb45d2bbcd19ef5bdb11e7c6a0dc6d5f45 latest Error Message Compiling mxnet cython ndarray pyx because it changed Compiling mxnet cython symbol pyx because it changed Cythonizing mxnet cython ndarray pyx Cythonizing mxnet cython symbol pyx running install running bdist egg running egg info writing requirements to mxnet egg info requires txt writing mxnet egg info PKG INFO writing top level names to mxnet egg info top level txt writing dependency links to mxnet egg info dependency links txt reading manifest file 'mxnet egg info SOURCES txt' writing manifest file 'mxnet egg info SOURCES txt' installing library code to build bdist linux x86 64 egg running install lib running build py running build ext building 'mxnet cy2 ndarray' extension x86 64 linux gnu gcc pthread fno strict aliasing DNDEBUG g fwrapv O2 Wall Wstrict prototypes fPIC I include I nnvm include I usr include python2 7 c mxnet cython ndarray cpp o build temp linux x86 64 2 7 mxnet cython ndarray o cc1plus warning command line option Wstrict prototypes is valid for C ObjC but not for C enabled by default mxnet cython ndarray cpp In function PyObject pyx pf 7ndarray 22 make ndarray function generic ndarray function PyObject PyObject PyObject mxnet cython ndarray cpp 2512 36 error cannot declare pointer to void pyx t 7ndarray NDArrayHandle pyx t 15 GCC config COLLECT GCC x86 64 linux gnu gcc COLLECT LTO WRAPPER usr lib gcc x86 64 linux gnu 4 8 lto wrapper Target x86 64 linux gnu Configured with src configure v with pkgversion 'Ubuntu 4 8 4 2ubuntu1 14 04 3' with bugurl file enable languages c c java go d fortran objc obj c prefix usr program suffix 4 8 enable shared enable linker build id libexecdir usr lib without included gettext enable threads posix with gxx include dir usr include c 4 8 libdir usr lib enable nls with sysroot enable clocale gnu enable libstdcxx debug enable libstdcxx time yes enable gnu unique object disable libmudflap enable plugin with system zlib disable browser plugin enable java awt gtk enable gtk cairo with java home usr lib jvm java 1 5 0 gcj 4 8 amd64 jre enable java home with jvm root dir usr lib jvm java 1 5 0 gcj 4 8 amd64 with jvm jar dir usr lib jvm exports java 1 5 0 gcj 4 8 amd64 with arch directory amd64 with ecj jar usr share java eclipse ecj jar enable objc gc enable multiarch disable werror with arch 32 i686 with abi m64 with multilib list m32 m64 mx32 with tune generic enable checking release build x86 64 linux gnu host x86 64 linux gnu target x86 64 linux gnu Thread model posix,,,2017-01-02 10:52:23,2017-01-02 20:12:24
IS,Model not training properly when using ImageRecordIter,Hi I'm using R and MXNET to predict 7 classes from a certain dataset I decided to change the way my model receives the training data from directly loading a data frame from the original dataset file to creating the corresponding ImageRecordIter However when the model is trained using this iterator instead of the dataset array I always get the same predicted label when using predict The dataset is made of 35887 images of 48x48x1 each The following code exemplifies the creation of the lst and rec for the iterator Images are already written on disk Also the naming for the validation set instead of test set is just for convenience After training the model with these iterators I have run predict with either the val array and the val iterator obtaining the same result so my intuition is that something is going on when the training is performed using these iterators Also the training accuracy is much lower when the training is done with iterators should not it be more or less the same and it even gets stuck at the same accuracy which may be a consequence of always predicting the same label afterwards I would appreciate any insights or any help you could provide to further debug this behaviour Thank you for reading,,,2016-12-15 09:47:11,2017-01-02 21:31:30
PR,Build SSD operators in jenkins,,,howard0su,2017-01-01 10:36:29,2017-01-02 23:59:19
IS,A problem of RMSProp,Hi contributors I used mxnet recently It is very cool There is a problem of RMSProp Maybe you can take into consideration In mxnet torch and caffe all have a parameter epsilon to avoid division by zero But there is a difference In torch and caffe the formula like this w is weight dfdx is gradient decay is the param of RMSProp default of eps is 1e 8 delta 1 decay dfdx 2 decay delta w lr dfdx sqrt delta eps In mxnet eps is equal to 1e 4 w lr dfdx sqrt delta eps At the beginning of training delta is initialized as zero It is possible that dfdx 2 eps which leads to that the denominator depends on eps Maybe you can set a new parameter for eps or replace sqrt delta eps with sqrt delta eps In my case the input is 0 1 the label is 0 1 and loss is L2 Norm There is a big gap between them,,piiswrong,2017-01-02 14:04:10,2017-01-03 01:14:39
PR,Fix JQuery version,Sphinx 1 5 1 will update JQuery to 3 1 0 while Bootstrap requires JQuery 1 9 1 to 3 0 0 I fixed this issue by using JQuery version 1 11 1 which is the version for current website,,kevinthesun,2017-01-03 04:34:55,2017-01-03 04:48:46
PR,bug in scala package scalar times symbol,A bug found in Symbol scala,,"piiswrong,Ldpe2G",2017-01-02 21:56:09,2017-01-03 04:49:22
PR,Fix Jquery 1 11 1,Create static query 1 11 1 js file,,kevinthesun,2017-01-03 05:30:49,2017-01-03 05:31:55
IS,Some hard time compiling mxnet on mac,Operating System macOS Sierra Following the installation steps goes well Finally when executing make j sysctl n hw ncpu I get the following errors clang error no such file or directory ' usr local lib libopencv calib3d dylib' clang error no such file or directory ' usr local lib libopencv contrib dylib' clang error no such file or directory ' usr local lib libopencv core dylib' clang error no such file or directory ' usr local lib libopencv features2d dylib' clang error no such file or directory ' usr local lib libopencv flann dylib' clang error no such file or directory ' usr local lib libopencv gpu dylib' clang error no such file or directory ' usr local lib libopencv highgui dylib' clang error no such file or directory ' usr local lib libopencv imgproc dylib' clang error no such file or directory ' usr local lib libopencv legacy dylib' clang error no such file or directory ' usr local lib libopencv ml dylib' clang error no such file or directory ' usr local lib libopencv nonfree dylib' clang error no such file or directory ' usr local lib libopencv objdetect dylib' clang error no such file or directory ' usr local lib libopencv ocl dylib' clang error no such file or directory ' usr local lib libopencv photo dylib' clang error no such file or directory ' usr local lib libopencv stitching dylib' clang error no such file or directory ' usr local lib libopencv superres dylib' clang error no such file or directory ' usr local lib libopencv video dylib' clang error no such file or directory ' usr local lib libopencv videostab dylib' if looking in my directory I see my libs all have the 2 4 7 suffix such as libopencv calib3d 2 4 7 dylib Any thought Either changing the make or the names of the libs would work Many thanks,,"kevinthesun,piiswrong",2016-12-22 03:52:24,2017-01-03 07:13:35
IS,shape 0 idx 1 vs 1 index out of range error when passing group symbol output,I have been trying DCGAN sample in the example I used Group symbol to group two symbols together then return it Then I use this group symbol to create a module In each iteration I called one of them as output ndarray to pass through a discriminator If I returned only the called symbol in my function it worked just fine by this Traceback most recent call last File d newwork workspace gan test with mnist py line 284 in module modD forward mx io DataBatch outG label is train True File D Anaconda lib site packages mxnet module module py line 448 in forward self exec group forward data batch is train File D Anaconda lib site packages mxnet module executor group py line 313 in forward load data data batch self data arrays self data layouts File D Anaconda lib site packages mxnet module executor group py line 43 in load data load general batch data targets major axis File D Anaconda lib site packages mxnet module executor group py line 17 in load general for d src d targets axis in zip data targets major axis File D Anaconda lib site packages mxnet ndarray py line 315 in getitem return self at in slice File D Anaconda lib site packages mxnet ndarray py line 374 in at self handle idx ctypes byref handle File D Anaconda lib site packages mxnet base py line 75 in check call raise MXNetError py str LIB MXGetLastError MXNetError 13 23 09 D Program Files x86 Jenkins workspace mxnet mxnet include mxnet ndarray h 276 Check failed shape 0 idx 1 vs 1 index out of range the latest version called different error from the previous ones the previous versions said dshape mismatch And is there any good way to print out one of the internal symbol value in an iteration instead of defining a Group symbol in the function I have been searching of documents of module API Is there any good way to get internal ndarray value after each forward backward procedure,,"piiswrong,piiswrong",2017-01-03 05:46:21,2017-01-03 07:50:03
IS,API menu item does not work in mxnet io,It cannot be opened It was working two days ago,,"piiswrong,kevinthesun,kevinthesun,piiswrong,kevinthesun,kevinthesun,kevinthesun",2017-01-02 10:26:10,2017-01-03 08:48:32
IS,Need Advice Bernoulli Sampling operator for RBM,Hi I am trying to dive into mxnet and was thinking about contributing an RBM example As a result I am thinking of adding a Bernoulli operator to sample visible hidden layers So I am seeking your advice on the design part Should I just add Bernoulli operator or more generic Sample operator or maybe extend operator tensor sample op,,piiswrong,2017-01-03 01:57:25,2017-01-03 15:33:26
IS,How should I use NDArray tanh and related functions in the scala package,NDArray tanh when passed in an NDArray fails,,,2017-01-03 05:29:52,2017-01-03 17:18:24
PR,Revise gitignore,I find that the current gitignore will also ignore the src nnvm This should fix the problem,,sxjscience,2017-01-03 09:16:04,2017-01-03 18:04:33
PR,test rmsprop,If you have any questions you can contact me Related to Issue 4493,,,2017-01-03 11:40:48,2017-01-03 18:06:10
PR,Wrote Amazon Linux CPU CI setup,Wrote Dockerfile for Amazon Linux CPU tests environment dependency installation scripts and fix for lint failure,,"lxn2,piiswrong,lxn2,piiswrong,lxn2",2016-12-29 20:51:49,2017-01-03 19:03:53
PR,R docs html2text,Thanks to work we have R documentation completed I modified some style for this page Note that MXNet for R needs to be installed on the build server installing the mxnet package for r for the R script in R package man to find the mxnet package,,"kevinthesun,mli,thirdwing,mli",2016-11-29 23:51:57,2017-01-03 23:09:22
IS,Makefile 259 recipe for target 'bin im2rec' failed,Config mk attached renamed to config mk txt to upload config mk txt Environment info Operating System Kubuntu16 04LTS Compiler export CC gcc 5 4 9 export CXX g 5 4 9 export NVCC nvcc Package used Python R Scala Julia shared Library MXNet commit hash git rev parse HEAD c3c21715bb1f603865608dd903d60a0ee0be3610 Error Message For export CC gcc 4 9 im2rec cc text startup 0x1ac5 undefined reference to cv imencode std string const cv InputArray const std vector unsigned char std allocator unsigned char std vector int std allocator int const ' collect2 error ld returned 1 exit status Makefile 259 recipe for target 'bin im2rec' failed seems gcc 4 9 can not find libopencv dev which is installed through apt where pkg config cflags opencv I usr include opencv For export CC gcc 5 build src operator tensor indexing op gpu o In function nnvm Op nnvm Op set attr std function void nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob std allocator mxnet TBlob const std vector mxnet OpReqType std allocator mxnet OpReqType const std vector mxnet TBlob std allocator mxnet TBlob const std string const std function void nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob std allocator mxnet TBlob const std vector mxnet OpReqType std allocator mxnet OpReqType const std vector mxnet TBlob std allocator mxnet TBlob const const int ' home nova mxnet nnvm include nnvm op h 426 undefined reference to nnvm Op UpdateAttrMap std string const std function void dmlc any ' collect2 error ld returned 1 exit status Makefile 259 recipe for target 'bin im2rec' failed this time the nnvm became the problem Steps to reproduce make B j8 What have you tried to solve it change gcc 5 to gcc 4 9 failed for some other errors,,"piiswrong,thirdwing",2017-01-01 07:57:23,2017-01-04 00:00:59
PR,R docs html2text,,,kevinthesun,2017-01-04 00:36:15,2017-01-04 00:36:25
PR,Automatically download data,Automatically download data Automatically download data Create checkpoint,,mzhang001,2017-01-03 19:16:17,2017-01-04 00:39:04
PR,Add transpose kwargs for dot,Also added the unittest,,"jermainewang,piiswrong,jermainewang,jermainewang,piiswrong,jermainewang",2017-01-03 00:36:28,2017-01-04 00:45:39
IS,How the weight is initialized when using Caffe plugin,The code in this line L111 will reset Caffe is blob by mxnet is data structure in the first forward But in the training phase and there is no initialized model whether this line will cover the weight initialization in Caffe is SetUp function,,"piiswrong,jermainewang,HrWangChengdu",2016-12-29 07:22:40,2017-01-04 02:05:51
IS,mxnet support grained operation matrix is math operation,If factorization machine and other network structure are united to train it need matrix is operations for example pow So how to implement it Can you help me,,"piiswrong,sxjscience,sxjscience",2017-01-04 02:46:41,2017-01-04 06:20:11
PR,A3C example,winstywang,,"piiswrong,sxjscience,loofahcus",2017-01-04 06:11:05,2017-01-04 06:58:33
PR,Update index md,,,jspisak,2017-01-04 06:48:46,2017-01-04 06:58:55
PR,fix image file in example ssd,It seems that in example sdd L133 the var image file should be label file It may be a typo error,,burness,2017-01-04 09:35:13,2017-01-04 17:11:51
PR,Pop up version number in ubuntu r setup script,,,"hetong007,piiswrong,hetong007,piiswrong,piiswrong,hetong007,hetong007,piiswrong,thirdwing,piiswrong,hetong007,piiswrong,hetong007,piiswrong",2016-12-31 00:12:44,2017-01-04 17:12:50
IS,PS worker,mxnet worker ps,,mli,2017-01-04 12:57:51,2017-01-04 18:20:52
IS,data compression in distributed training,In a multi machine distributed Does the worker do compression operation before sending data to the parameter server,,mli,2017-01-04 12:40:15,2017-01-04 18:23:09
PR,fix custom op,fix issue See if this fixes it,,"piiswrong,lyttonhao",2017-01-04 20:13:38,2017-01-04 21:21:00
PR,Merge dmlc,,,mzhang001,2017-01-04 21:55:04,2017-01-04 21:55:47
PR,Convert iterator to mx img API,,,"howard0su,howard0su,howard0su,howard0su,piiswrong,piiswrong,piiswrong,howard0su,piiswrong,howard0su,piiswrong,howard0su,piiswrong,howard0su",2016-12-30 04:43:47,2017-01-04 22:03:24
IS,MakeFile Error USE MKL2017 for performance undefined reference to dnnBatchNormalizationCreateBackward v2 F32',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 04 4 Compiler It seems that wouldnnBatchNormalizationCreateBackward v2 F32' is not included Someone met this before Thx,,"piiswrong,glingyan,glingyan,glingyan,glingyan,glingyan,glingyan,glingyan,glingyan",2017-01-03 08:01:31,2017-01-05 02:46:08
IS,Custom Op Bug when using multiple custom ops,I found that when using multiple output custom ops the program got stuck It seems that the engine is suffering from the deadlock This problem will occur when the custom op contains the codes like mx nd xx xx asnumpy ' This problem does not occur when using NaiveEngine I have written an example to reproduce this bug You can put this file on the path of 'exmple numpy ops' and then run it If we add line 15 file test custom py L15 the program will get stuck Otherwise it works fine MXNet version test two versions 1 the newest master ceb9f0187a31d528e5566f810d933cf4834d3282 2 an older master 01cde15b5611d3add9a103abd7979c3272693625,,"lyttonhao,sxjscience,piiswrong,lyttonhao",2017-01-04 12:00:22,2017-01-05 04:48:47
IS,upgrade mxnet to 0 9 can not print train is logs,I upgrade mxnet to 0 9 When i ran old program logs training error information for instance were not print Why In mxnet version 0 9 it need to setup other configure tk u,,piiswrong,2017-01-05 04:22:56,2017-01-05 06:18:36
IS,is there some where using Auto Differentiation,in the home page there is Auto Differentiation feature but i can not find it in the source code,,andremoeller,2016-12-26 06:29:12,2017-01-05 07:11:01
PR,Add new op of take,,,WellyZhang,2017-01-05 07:45:36,2017-01-05 08:20:46
PR,OP Add new op of take,,,WellyZhang,2017-01-05 08:54:01,2017-01-05 09:33:33
IS,the big code expansion after 0 9 1,after 0 9 1 the full gpu complie cuda arch is all dll size more than 300mb compression also more 100mb with vs2013 and vs2015 befor 0 9 1 it only 100mb and after compression 8mb Is there a problem,,"yajiedesign,howard0su,howard0su,yajiedesign,yajiedesign",2016-12-30 16:00:36,2017-01-05 11:21:43
IS,How to get online GPU memory usage with python for linux,Hi guys want to get gpu memory info to adjust batch size Is there any python api to get gpu memory info for linux system Ubuntu 14 04,,"piiswrong,jermainewang",2016-12-29 02:14:20,2017-01-05 14:15:16
PR,RFC Operator priority based on pushed order,DO NOT MERGE This branch is for engine refactor Here is the change needed to fix the memory consumption problem mentioned in 4294 This is the current solution though not ideal More details and thinkings on this problem are in the document I sent to you Basic idea Assign a decreasing priority to operators based on their pushed order In such way if pushed operators could be paralleled their execution order will be close to the sequential order as much as possible For following example Previous engine will raise OOM error This change will use 500 MB memory I want to let MinPy use this version of MXNet in the future to make everything more stable Please have a look Thanks,,"jermainewang,piiswrong,howard0su,jermainewang,tqchen,howard0su,jermainewang,jermainewang,howard0su,jermainewang",2017-01-02 16:17:01,2017-01-05 14:47:41
IS,MXNet for R on Windows installation fails due to latest update in DiagremmeR,More details here on stackoverflow Temp fix Update installation instruction to reflect this issue This alteast helps new users to know the cause of issue and downgrade their DiagremmeR package Further We need to work on the compatibility issue We should check if we need to add mxnet as reverse dependency to DiagremmeR,,"sandeep-krishnamurthy,piiswrong,thirdwing,Roshrini,thirdwing,thirdwing",2017-01-04 19:25:40,2017-01-05 17:07:59
PR,R close 4527,This should fix 4527 However the visualization result looks ugly,,"thirdwing,thirdwing",2017-01-05 16:12:19,2017-01-05 17:07:59
PR,logging when save params and optimizer states,print information like model save checkpoint,,luoyetx,2017-01-05 07:06:24,2017-01-05 20:40:52
IS,install error on ubuntu,dear team i want to know the install guide on your home page for mxnet 0 7 0 holds for mxnet 0 8 0 when i download the mxnet on github the file of mshadow is empty thank you very much best,,luoyetx,2017-01-05 06:20:04,2017-01-05 21:02:01
IS,MXNET on PASCAL GPUS,Hi All Does MXNET work on PASCAL nvidia GPUs GP100GL When I try a CIFAR test I get the following segfault Is there any flags I can enable to debug this I already tried DEBUG 1 bash 4 2 python example image classification train cifar10 py kv store device gpus 0 INFO root start with arguments Namespace batch size 128 benchmark 0 data nthreads 4 data train wouldata cifar10 train rec' data val wouldata cifar10 val rec' disp batches 20 gpus '0' image shape '3 28 28' kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epochs '200 250' max random aspect ratio 0 max random h 36 max random l 50 max random rotate angle 0 max random s 50 max random scale 1 max random shear ratio 0 min random scale 1 model prefix None mom 0 9 network aresnet' num classes 10 num epochs 300 num examples 50000 num layers 110 optimizer isgd' pad size 4 random crop 1 random mirror 1 rgb mean '123 68 116 779 103 939' test io 0 top k 0 wd 0 0001 10 29 41 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 train rec use 4 threads for decoding 10 29 42 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 val rec use 4 threads for decoding INFO root in get lr scheduler epoch size 390 INFO root Start training with gpu 0 Segmentation fault,,mli,2017-01-04 15:31:25,2017-01-05 21:02:11
IS,,mxnet,,piiswrong,2017-01-02 07:27:55,2017-01-05 21:02:26
IS,EncodeKey may occur error on linux x86 64,In below code source file in src kvstore kvstore dist h when size 53804734 num servers 90 static cast size t static cast double size 90 90 53804733 this would lead to static cast size t pskv size size,,"wzl12356,piiswrong,mli,wzl12356,mli",2016-12-27 10:00:16,2017-01-05 21:11:53
IS,Scala example on website does not compile,I do not know if this is the best place to communicate this but I tried copy pasting the only Scala example in the website and it does not compile I got it to compile by doing what is described below I'm not sure whether that was the right fix later I get a core dumped that I will post in a separate issue This is just a heads up for you guys to be aware of the problem with the website Environment info Operating System Ubuntu 16 10 amd64 Compiler sbt Package used Python R Scala Julia Scala package scalaVersion 2 11 8 MXNet version ml dmlc mxnet mxnet full 2 10 linux x86 64 gpu 0 1 1 Error Message and got it to compile,,"Ldpe2G,yzhliu,yzhliu",2017-01-05 10:09:36,2017-01-06 02:56:45
IS,nnvm c predict example failed with msg check failed op nullptr,MXNet version 0 9 1 MXNet commit hash git rev parse HEAD ceb9f01 Error Message src core op cc 55 check failed op nullptr Operator Convolution is not registered Minimum reproducible example example image classification predict cpp Phenomenon 1 The nnvm version c predict api failed when load json file L87 2 The python api is ok when loading the last epoch model and json file to continue training like python train imagnet py load epoch 1 3 I add the following debug code in L55 Using c predict api only operators refactoring to nnvm are in Registry When using python api all operators can be printed,,"howard0su,piiswrong,howard0su",2017-01-04 12:02:20,2017-01-06 03:22:24
IS,cudnnBatchnorm cannot be used correctly,Hi all I installed mxnet version 0 8 0 with cuda 7 5 and cudnn v5 When using cudnnBatchNorm it has a trouble on cudnnBatchNormalizationForwardTraining The feedback indicates that asynchronous engine operation leads to this error image I just use a small network to test this which contains a conv layer followed by bn and a L2 loss image conv can work correctly on both cpu and gpu bn only works correctly on cpu Could you help me to solve this problem,,,2017-01-05 15:27:31,2017-01-06 03:50:03
PR,kvstore use CPUPinned only if there is more than one GPUs,based on from,,"mli,piiswrong,mli,howard0su,mli",2017-01-05 21:10:53,2017-01-06 04:58:40
PR,Fix predict c api,Fix the regression in predict api While I am here update and cleanup the C image classification example,,"howard0su,piiswrong,howard0su,Piyush3dB,Piyush3dB",2017-01-05 09:17:37,2017-01-06 09:03:42
PR,Adding missing commands to pull MXNet source code in installation steps of OSX Ubuntu and Amazon Linux,Major issue Installation guide did not have steps for users to pull source code of mxnet Added these steps in all 3 os osx amazon linux ubuntu There were issues in quick install script of osx I have removed quick install script details from docs I will fix issue with the script and submit corresponding installation guide documentation in a separate PR,,"sandeep-krishnamurthy,howard0su,sandeep-krishnamurthy",2017-01-06 00:41:45,2017-01-06 17:54:29
PR,PythonInterp,,,yajiedesign,2017-01-05 11:44:01,2017-01-06 17:59:12
IS,Bucketing error after upgrade from v0 8 0,I have a model using a bucketing module that worked in v0 8 0 but now crashes after updating MXNet it is isolated in the script below which runs fine on the v0 8 0 tag but results in the error below on latest 16d0cde9633ebaf846745b9d12ccc1f25aa1e1a9 I'm not sure from the error message and wonder which features changed since v0 8 0 that result in the error Thanks Environment info Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 16d0cde9633ebaf846745b9d12ccc1f25aa1e1a9 Python version and distribution 2 7 10 Error Message,,piiswrong,2017-01-06 14:55:46,2017-01-06 18:56:23
IS,MXNet Installation fails on Ubuntu 14 04 4 LTS,Environment info Operating System If you could help me understand the error I would really appreciate it Thanks Besir,,"piiswrong,xlvector",2016-12-31 01:31:02,2017-01-06 19:55:26
PR,Reduce the overhead of profiler,,,"howard0su,jermainewang,tqchen,howard0su,jermainewang,howard0su",2017-01-05 12:07:08,2017-01-06 22:13:37
PR,Docs fix Recursive recurrent in RNN Downplaying model zoo until,more content Took Model Zoo out of header of main website because it has a lot of missing content that seems likely to confuse new users more than help them,,"leopd,leopd",2017-01-05 23:57:43,2017-01-06 22:15:53
IS,No rule to make target 'install',A Brief question How to install Cheers,,tqchen,2017-01-06 10:04:24,2017-01-07 00:23:44
PR,EXEC Specify external memory directly in memory planning,,,"tqchen,tqchen",2017-01-06 03:45:52,2017-01-07 02:05:32
PR,fix warpctc plugin work with new version warpctc,,,"yajiedesign,piiswrong,yajiedesign,piiswrong,yajiedesign,piiswrong",2017-01-05 02:45:32,2017-01-07 06:55:25
PR,Fix search function,Since updating sphinx to 1 5 1 breaks search function I link searchtool js to a static file copied from sphinx 1 3 5 to resolve this problem,,kevinthesun,2017-01-07 00:37:23,2017-01-07 07:23:18
IS,In the mxnet how to implement a column vector with a matrix dot multiplication,for example a colum vector,,"sxjscience,sxjscience,sxjscience",2017-01-07 03:44:07,2017-01-07 07:35:59
PR,OP Topk and arange Update submodules,Continuing of 1 Fix the argument of batch dot 2 New operators topk sort argsort arange Also the previous internal operators zeros and ones have been added to the python script as mx sym zeros and mx sym ones 3 Move operator to operator tensor BlockGrad 4 Add the argument dtype to initialization operators zeros ones arange 5 Enable gradient for argmax and argmin by adding the gradient node to zeros Some Notes 1 The implementation of TopK is not so optimized at this stage The implementation is to first sort the data and then keep the top k indices values In the future we will try to refactor the implementation using CUB 2 The axis argument is supported in topk and topk is equivalent to argsort and sort if we set k 1 and ret typ indices or value 3 The arange operator has an additional repeat argument which indicates the number of times each element will be repeated in the generation process This can be helpful for implementing the meshgrid,,"sxjscience,piiswrong,piiswrong",2017-01-06 15:50:21,2017-01-07 15:07:43
PR,Fix search for sphinx 1 5 1,Linking searchtools js to static file will result in incorrect search result link According to this post Search function breaking in sphinx 1 5 1 is caused by theme issue After fixing layout html it works,,kevinthesun,2017-01-07 09:52:42,2017-01-07 20:35:38
IS,MXNet doc website search function issue,After updating sphinx to version 1 5 1 some errors will happen when using search function Open the web console it shows the following information Uncaught TypeError Cannot read property 'length' of undefined at displayNextItem searchtools js 543 at Object query searchtools js 574 at Object setIndex searchtools js 362 at Users wayao Documents MXNetDoc mxnet docs build html searchindex js 1 It seems that searchtools js has been updated and there are errors in it,,"kevinthesun,piiswrong,kevinthesun",2017-01-03 22:47:18,2017-01-07 21:34:44
IS,Any reason that can cause BN output to go NAN,as for 'is train True' before my last BN symbol conv and after my last BN symbol qq 20170107120250 as for 'is train False' It all goes zeros I set eps 1e 5 1e 12 I tried both fixed gamma and unfixed one The problem still exits It seems it only occurred in my last BN symbol the previous 4 are all right though Any solution or just swipe this BN symbol out,,,2017-01-07 04:07:27,2017-01-08 01:30:51
IS,how to use mx sym broadcast plus in the lstm py I find element mask output result is not right,The Follow is element mask test result code,,sxjscience,2017-01-06 11:33:58,2017-01-08 05:10:40
IS,Can mxnet add np argsort operation,I want to sort the elements of the matrix However I have not found the sort operation in mx symbol Can mxnet add the np argsort operation or how can I easily implement the sort operation by python interface directly use the np argsort maybe too slow,,"sxjscience,sxjscience",2017-01-02 08:25:33,2017-01-08 05:13:43
IS,argsort operator support,Will mxnet support argsort operator It is very useful when training NMT tasks,,"sxjscience,sxjscience",2016-11-14 05:14:13,2017-01-08 05:14:32
IS,OP request argmax,Hi I am implementing Pointer Networks it seems that an argmax OP will help a lot Thanks,,"sxjscience,sxjscience",2016-09-29 11:16:04,2017-01-08 05:16:02
PR,nnpack update support more op,this pr is moved from,,"tornadomeet,piiswrong,piiswrong,piiswrong,tornadomeet,tornadomeet,tornadomeet,piiswrong,tornadomeet,tornadomeet,tornadomeet,piiswrong,tornadomeet,tornadomeet,tornadomeet",2017-01-04 10:08:52,2017-01-08 09:16:22
PR,fix 4584,When I build the project using g 4 9 2 I get error converting to std unordered map std basic string char std basic string char from initializer list would use explicit constructor As in c 11 the definition of the default constructor of unordered map is explicit,,sxjscience,2017-01-08 09:26:24,2017-01-08 11:49:38
PR,fix compile error,fix,,"sxjscience,sxjscience",2017-01-08 09:33:52,2017-01-08 12:09:58
IS,KVStore API in CUDA enabled MxNet does not work without GPU,I'm not sure if this is a bug Maybe intentional After I compiled MxNet source with USE CUDA 1 I tried KVStore API on a Linux machine without GPU When I use the example in Mxnet docs CUDA error below is raised after kv init src storage storage cc 38 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Example kv mx kv create 'local' create a local kv store shape 2 3 kv init 3 mx nd ones shape 2 CUDA error is raised after this line a mx nd zeros shape kv pull 3 out a For USE CUDA 0 there is no problem,,"howard0su,mli",2016-12-25 19:06:17,2017-01-08 13:47:34
PR,OP take,,,"WellyZhang,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,WellyZhang,piiswrong,piiswrong,sxjscience,WellyZhang,piiswrong,sxjscience",2017-01-05 09:38:53,2017-01-08 14:59:33
PR,enable transpose flags for batch dot,Like dot we can avoid the unnecessary transpose by setting the flags in batch dot It could simplify the code accelerate the speed,,sxjscience,2017-01-08 15:22:39,2017-01-08 20:29:26
PR,Usage example for module,Usage example for module Same methods in subclass are skipped,,"kevinthesun,piiswrong,kevinthesun",2016-12-29 19:40:36,2017-01-08 21:52:37
IS,Suggestions for MKL README,1st suggestion 4 Set LD LIBRARY PATH LD LIBRARY PATH MKLML ROOT lib 2nd suggestion If MKL2017 full package is already installed and MKL2017 ML will be used for MxNet compilation do not execute compilervars sh script before MxNet compilation Otherwise MKL2017 may conflict with MKL2017 ML and MxNet may not be compiled,,"piiswrong,glingyan",2017-01-08 14:03:42,2017-01-09 01:43:41
IS,MXNET gpu memory usage issue,there is one net definition below the gpu memory required i computed is different from that from memonger i can not figure out why below is net definition num class 19 data mx sym Variable wouldata' ctx conv1 1 mx sym Convolution data data kernel 3 3 pad 1 1 num filter num class name 'ctx conv1 1' workspace 2048 ctx relu1 1 mx sym Activation data ctx conv1 1 act type arelu' name 'ctx relu1 1' ctx conv1 2 mx sym Convolution data ctx relu1 1 kernel 3 3 pad 1 1 num filter num class name 'ctx conv1 2' workspace 2048 ctx relu1 2 mx sym Activation data ctx conv1 2 act type arelu' name 'ctx relu1 2' ctx conv2 1 mx sym Convolution data ctx relu1 2 kernel 3 3 pad 2 2 num filter num class dilate 2 2 name 'ctx conv2 1' workspace 2048 ctx relu2 1 mx sym Activation data ctx conv2 1 act type arelu' name 'ctx relu2 1' ctx conv3 1 mx sym Convolution data ctx relu2 1 kernel 3 3 pad 4 4 num filter num class dilate 4 4 name 'ctx conv3 1' workspace 2048 ctx relu3 1 mx sym Activation data ctx conv3 1 act type arelu' name 'ctx relu3 1' ctx conv4 1 mx sym Convolution data ctx relu3 1 kernel 3 3 pad 8 8 num filter num class dilate 8 8 name 'ctx conv4 1' workspace 2048 ctx relu4 1 mx sym Activation data ctx conv4 1 act type arelu' name 'ctx relu4 1' ctx conv5 1 mx sym Convolution data ctx relu4 1 kernel 3 3 pad 16 16 num filter num class dilate 16 16 name 'ctx conv5 1' workspace 2048 ctx relu5 1 mx sym Activation data ctx conv5 1 act type arelu' name 'ctx relu5 1' ctx conv6 1 mx sym Convolution data ctx relu5 1 kernel 3 3 pad 32 32 num filter num class dilate 32 32 name 'ctx conv6 1' workspace 2048 ctx relu6 1 mx sym Activation data ctx conv6 1 act type arelu' name 'ctx relu6 1' ctx conv7 1 mx sym Convolution data ctx relu6 1 kernel 3 3 pad 64 64 num filter num class dilate 64 64 name 'ctx conv7 1' workspace 2048 ctx relu7 1 mx sym Activation data ctx conv7 1 act type arelu' name 'ctx relu7 1' ctx fc1 mx sym Convolution data ctx relu7 1 kernel 3 3 pad 1 1 num filter num class name 'ctx fc1' workspace 2048 ctx fc1 relu1 mx sym Activation data ctx fc1 act type arelu' name 'ctx fc1 relu1' ctx final mx sym Convolution data ctx fc1 relu1 kernel 1 1 num filter num class name 'ctx final' workspace 2048 softmax mx sym SoftmaxOutput data ctx final multi output True use ignore True ignore label 255 name softmax when the input shape is 1 3 1024 1024 i use memonger get cost to compute the memory used for feature map as below cost memonger get cost softmax data 1 3 1024 1024 cost is 1596MB then i compute that by hand as below input 3 1024 1024 4 bytes 12MB all ctx output and label 19 1024 1024 11 1024 1024 4 bytes 840MB 12 840 852 MB 852MB is much less than 1596MB which confuses me a lot i hope someone can help me figure out why thanks,,tqchen,2017-01-07 14:04:52,2017-01-09 02:27:40
IS,GraphExecutor Print output INFO,Where does this '11' come from Is this a typo L60,,"Godricly,tqchen",2017-01-09 02:15:20,2017-01-09 03:18:01
PR,Update new op md,Update link issue 4603,,Godricly,2017-01-09 03:46:33,2017-01-09 04:57:45
PR,fix typo as mentioned in 4588,,,WellyZhang,2017-01-09 04:18:20,2017-01-09 04:58:00
IS,Outdated Link,The second link under the List of basic attributes of C implementation is outdated,,"Godricly,WellyZhang",2017-01-09 03:23:13,2017-01-09 05:39:13
IS,how to output the value of the eval data for each epoch,I output the eval batch in this line L299 but it prints like this image it shows the address of the data batch so how to output the value of the eval data,,piiswrong,2017-01-08 13:33:06,2017-01-09 06:03:44
PR,fix imageiter and add profiling,,,"piiswrong,howard0su",2017-01-09 00:54:28,2017-01-09 07:21:30
PR,support profier for native engine,this pr is moved from profiler is more important in deploy environment which is always in NativeEngine pls review again,,"tornadomeet,piiswrong,tornadomeet,tornadomeet,tqchen,piiswrong,piiswrong,tornadomeet,tornadomeet,tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet,tqchen,tornadomeet,tornadomeet",2017-01-04 01:13:38,2017-01-09 17:08:37
PR,Enabled loading of saved NDArray with GPU context in CPU only environ,To resolve issue 4597 Enabled NDArray loading method to load saved NDArray file with GPU context in a CPU only environment,,DamonDeng,2017-01-09 06:45:04,2017-01-09 17:10:03
IS,'weight' should be 'width',L207 'weight' should be 'width',,,2017-01-08 04:17:15,2017-01-09 17:18:29
PR,Scala docs,Hi I created Scala API examples pages Model API Symbolic API IO Data Loading API NDArray API KVStore API exactly similar to Python Did not change lot of description These small examples of how to use Scala API for MXNet will be helpful for lots of Scala and Spark developers Also fixed some of the not working urls in Python API pages Please let me know if I need to provide any more information Thanks Roshani,,"Roshrini,piiswrong,yzhliu",2017-01-09 18:29:17,2017-01-10 00:22:07
IS,Fix conv layers and train only FC layers,What I would to do is loading a pre trained model on imagenet and train only its last fully connected layers on a new dataset maintaining fixed all the weights of the convolutional layers in order to use the first part of the network as a feature extractor Can someone explain me how can I do this in particular fix the conv layers Thanks,,"howard0su,jrosebr1",2017-01-09 22:36:22,2017-01-10 15:04:52
PR,fix channels not convert from BGR to RGB when len layer blobs 0 shap,e dim 0,,ae86208,2017-01-10 13:38:20,2017-01-10 17:41:47
PR,update docs fix typos,,,tornadomeet,2017-01-10 11:39:52,2017-01-10 17:42:22
PR,Fixing missing XML R package and installation issue for R on Ubuntu,Installation script for MXNet with R on Ubuntu was failing due to missing XML related package Updated script to include this package Minimum R version to run installation script is 3 2 0 Updated installation guide to cover this aspect and give commands to users to upgrade R,,"sandeep-krishnamurthy,sandeep-krishnamurthy,thirdwing",2017-01-10 04:34:38,2017-01-10 17:43:20
PR,Added epsilon to CrossEntropy loss to avoid divison by zero errors,When using the CrossEntropy loss the prob i e probabilities variable can potentially have zeros in the array When this happens taking the log results in a division by zero error similar to the one seen below pre home adrian virtualenvs dlbook local lib python2 7 site packages mxnet metric py 298 RuntimeWarning divide by zero encountered in log self sum metric numpy log prob sum pre By adding a small epsilon to prob we can avoid this division by zero error,,"jrosebr1,piiswrong,jrosebr1",2017-01-08 15:07:10,2017-01-10 17:48:53
PR,fix custom op for naive engine,,,piiswrong,2017-01-07 21:16:15,2017-01-10 17:49:25
IS,element mask is removed who know how to use mask reshape mask size 1 1 1,element mask is removed Please use src mask reshape mask size 1 1 1 directly as binary ops now support broadcasting I update the mxnet version to 0 9 1 in the window I play the lstm but 'module' object has no attribute 'element mask' please help me for a hint and code example in python to use mask reshape mask size 1 1 1 replace the element mask,,,2017-01-07 04:31:39,2017-01-11 04:16:53
PR,Python scala topic broken links fixed,Fixed broken topics links for Python and Scala API docs,,Roshrini,2017-01-10 23:22:27,2017-01-11 07:31:26
PR,Revert OP take 4538,This reverts commit 8159ad641fd4ff96337886984559e0267618a0a8 revert due to 1 incomplete implementation 2 not consistent with numpy take interface Please re submit PR later,,piiswrong,2017-01-10 21:08:43,2017-01-11 07:31:45
PR,fix issues when testing and a bug in l2 norm,,,"WellyZhang,piiswrong,piiswrong,WellyZhang",2017-01-09 07:42:45,2017-01-11 12:30:42
PR,fix when image encode failed in im2rec py,When use multiprocessing to pack images if some images are missing the count wo not increase due to the missing index,,luoyetx,2017-01-11 07:45:07,2017-01-11 16:47:35
PR,fix cpu gpu copy,,,"piiswrong,tqchen,piiswrong,tqchen,tqchen,tqchen,tqchen,piiswrong,piiswrong,tqchen",2017-01-10 19:06:26,2017-01-11 17:01:12
PR,Fix operator arguments,1 argument data for Activation is missing 2 Old operators use Symbol but new operators change to NDArray 3 nnvm Tuple int lacks a type name 4 Pad use an argument typed with double I'm not sure if it is intended,,"lx75249,sxjscience,lx75249,sxjscience,lx75249,piiswrong,sxjscience,lx75249,piiswrong,lx75249,lx75249,lx75249",2017-01-07 06:55:30,2017-01-11 17:06:33
PR,Add predict sample code for lstm ctc ocr Also update it is README md,I think a predict sample code for lstm ctc ocr is very useful Related issue point issue,,"BobLiu20,piiswrong,BobLiu20,BobLiu20",2016-12-25 10:02:40,2017-01-11 17:09:42
PR,Add prelu layer support for caffe convert tool,I add the support for prelu layer for the caffe convert tool and use the code to convert the center loss model to mxnet model which use prelu as its nonlinear activation function And get the same output between mxnet and caffe,,"piiswrong,piiswrong",2016-12-18 08:34:28,2017-01-11 17:10:42
PR,Refactor initializer,tqchen,,"piiswrong,yzhliu",2017-01-03 01:23:15,2017-01-11 23:17:35
PR,fix error,,,piiswrong,2017-01-11 18:43:14,2017-01-12 04:56:16
PR,Adding Mac quick install script for mxnet with Python,Script Authored by Bhavin Thaker Quick install script for Mac OS users This scripts install MXNet for Python users on Mac Tested on Sierra and ElCapitan versions,,"sandeep-krishnamurthy,piiswrong,thirdwing,piiswrong",2017-01-11 18:37:17,2017-01-12 05:04:06
PR,Minor CMake build changes and fix a couple of signed unsigned warnings,,,cjolivier01,2017-01-11 18:01:39,2017-01-12 05:43:06
IS,Example of ndarray Convolution,Hi guys Always got segmentation error when using ndarray Convolution operator especially when providing kwargs Is there any official guide or code for its usage Thks,,"piiswrong,piiswrong",2017-01-11 02:05:53,2017-01-12 13:23:39
IS,Reshape failed to use memonger,We try to use memonger to save memory in LSTM network with NNVM version but when set attribute force mirroring to Reshape symbol there throws an error cannot find argument force mirroring Other operators like FullyConnected seem ok I found that Reshape operator has been moved to src tensor matrix op cc and register switch to NNVM REGISTER OP does this cause the problem,,piiswrong,2017-01-11 03:38:36,2017-01-12 18:33:39
IS,Train accuracy high low validation accuracy,I am trying do the inception bn full on my own dataset near 1400w and 5190 classes python train imagenet py network inception bn full batch size 100 lr 0 05 lr factor 0 9 gpus 2 3 num epoch 60 data dir data5 rd xiajizhong parts 17kClasses train dataset small rec val dataset small rec val num examples 14040000 and when use the latest mxnet I get bad validation accuracy 2016 09 24 02 49 24 212 Node 0 Epoch 0 Batch 140400 Speed 47 30 samples sec Train accuracy 0 546400 2016 09 24 02 49 24 213 Node 0 Epoch 0 Batch 140400 Speed 47 30 samples sec Train top k accuracy 5 0 787400 2016 09 24 02 49 24 213 Node 0 Epoch 0 Batch 140400 Speed 47 30 samples sec Train top k accuracy 10 0 842400 2016 09 24 02 49 24 213 Node 0 Epoch 0 Batch 140400 Speed 47 30 samples sec Train top k accuracy 20 0 884800 2016 09 24 02 49 25 074 Node 0 Update 140401 Change learning rate to 4 50000e 02 2016 09 24 02 49 31 389 Node 0 Epoch 0 Resetting Data Iterator 2016 09 24 02 49 31 390 Node 0 Epoch 0 Time cost 301141 343 2016 09 24 02 49 31 645 Node 0 Saved checkpoint to model inception bn full Vdian 0001 params 2016 09 24 05 15 15 472 Node 0 Epoch 0 Validation accuracy 0 142758 2016 09 24 05 15 15 472 Node 0 Epoch 0 Validation top k accuracy 5 0 280551 2016 09 24 05 15 15 472 Node 0 Epoch 0 Validation top k accuracy 10 0 350004 2016 09 24 05 15 15 472 Node 0 Epoch 0 Validation top k accuracy 20 0 418618 But I was doing the experiment well in the old version mxnet which the validation accuracy is also very high near top1 50 after one epoch So I am wondering why this happened I am not very clear about different changed in the new version mxnet Is it caused by the Pooling layer behavior changed and on the old version the result below 2016 06 10 10 26 15 591 Node 0 Epoch 0 Batch 140350 Speed 50 40 samples sec Train accuracy 0 406846 2016 06 10 10 27 53 871 Node 0 Epoch 0 Batch 140400 Speed 50 88 samples sec Train accuracy 0 406867 2016 06 10 10 27 53 875 Node 0 Update 140401 Change learning rate to 4 05000e 01 2016 06 10 10 27 59 645 Node 0 Epoch 0 Resetting Data Iterator 2016 06 10 10 27 59 647 Node 0 Epoch 0 Train accuracy 0 406869 2016 06 10 10 27 59 647 Node 0 Epoch 0 Time cost 258948 773 2016 06 10 10 28 01 107 Node 0 Saved checkpoint to model vdian5190 0 0001 params 2016 06 10 12 50 10 700 Node 0 Epoch 0 Validation accuracy 0 457665,,"winstywang,winstywang,winstywang",2016-09-27 03:20:36,2017-01-13 01:43:26
IS,Though I trained cnn and lstm successfully but predicted failed,Here is my code It raised an error mxnet base MXNetError 19 28 07 src symbol graph executor cc 732 Check failed info type kInternalAllocated How can I fix it,,"piiswrong,piiswrong",2017-01-12 11:29:42,2017-01-13 09:36:04
IS,mxnet base MXNetError 10 20 17 src ndarray ndarray cc 231 Check failed from shape to shape operands shape mismatch,I use GoogleNet model for my data it is my code import mxnet as mx import os import sys import numpy as np import logging def get googlenet data mx symbol Variable data conv1 7x7 s2 mx symbol Convolution name 'conv1 7x7 s2' data data num filter 64 pad 3 3 kernel 7 7 stride 2 2 no bias False conv1 relu 7x7 mx symbol Activation name 'conv1 relu 7x7' data conv1 7x7 s2 act type arelu' pool1 3x3 s2 mx symbol Pooling name 'pool1 3x3 s2' data conv1 relu 7x7 pooling convention 'full' pad 0 0 kernel 3 3 stride 2 2 pool type 'max' pool1 norm1 mx symbol LRN name 'pool1 norm1' data pool1 3x3 s2 alpha 0 000100 beta 0 750000 knorm 1 000000 nsize 5 conv2 3x3 reduce mx symbol Convolution name 'conv2 3x3 reduce' data pool1 norm1 num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False conv2 relu 3x3 reduce mx symbol Activation name 'conv2 relu 3x3 reduce' data conv2 3x3 reduce act type arelu' conv2 3x3 mx symbol Convolution name 'conv2 3x3' data conv2 relu 3x3 reduce num filter 192 pad 1 1 kernel 3 3 stride 1 1 no bias False conv2 relu 3x3 mx symbol Activation name 'conv2 relu 3x3' data conv2 3x3 act type arelu' conv2 norm2 mx symbol LRN name 'conv2 norm2' data conv2 relu 3x3 alpha 0 000100 beta 0 750000 knorm 1 000000 nsize 5 pool2 3x3 s2 mx symbol Pooling name 'pool2 3x3 s2' data conv2 norm2 pooling convention 'full' pad 0 0 kernel 3 3 stride 2 2 pool type 'max' inception 3a 1x1 mx symbol Convolution name 'inception 3a 1x1' data pool2 3x3 s2 num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3a relu 1x1 mx symbol Activation name 'inception 3a relu 1x1' data inception 3a 1x1 act type arelu' inception 3a 3x3 reduce mx symbol Convolution name 'inception 3a 3x3 reduce' data pool2 3x3 s2 num filter 96 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3a relu 3x3 reduce mx symbol Activation name 'inception 3a relu 3x3 reduce' data inception 3a 3x3 reduce act type arelu' inception 3a 3x3 mx symbol Convolution name 'inception 3a 3x3' data inception 3a relu 3x3 reduce num filter 128 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 3a relu 3x3 mx symbol Activation name 'inception 3a relu 3x3' data inception 3a 3x3 act type arelu' inception 3a 5x5 reduce mx symbol Convolution name 'inception 3a 5x5 reduce' data pool2 3x3 s2 num filter 16 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3a relu 5x5 reduce mx symbol Activation name 'inception 3a relu 5x5 reduce' data inception 3a 5x5 reduce act type arelu' inception 3a 5x5 mx symbol Convolution name 'inception 3a 5x5' data inception 3a relu 5x5 reduce num filter 32 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 3a relu 5x5 mx symbol Activation name 'inception 3a relu 5x5' data inception 3a 5x5 act type arelu' inception 3a pool mx symbol Pooling name 'inception 3a pool' data pool2 3x3 s2 pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 3a pool proj mx symbol Convolution name 'inception 3a pool proj' data inception 3a pool num filter 32 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3a relu pool proj mx symbol Activation name 'inception 3a relu pool proj' data inception 3a pool proj act type arelu' inception 3a output mx symbol Concat name 'inception 3a output' inception 3a relu 1x1 inception 3a relu 3x3 inception 3a relu 5x5 inception 3a relu pool proj inception 3b 1x1 mx symbol Convolution name 'inception 3b 1x1' data inception 3a output num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3b relu 1x1 mx symbol Activation name 'inception 3b relu 1x1' data inception 3b 1x1 act type arelu' inception 3b 3x3 reduce mx symbol Convolution name 'inception 3b 3x3 reduce' data inception 3a output num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3b relu 3x3 reduce mx symbol Activation name 'inception 3b relu 3x3 reduce' data inception 3b 3x3 reduce act type arelu' inception 3b 3x3 mx symbol Convolution name 'inception 3b 3x3' data inception 3b relu 3x3 reduce num filter 192 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 3b relu 3x3 mx symbol Activation name 'inception 3b relu 3x3' data inception 3b 3x3 act type arelu' inception 3b 5x5 reduce mx symbol Convolution name 'inception 3b 5x5 reduce' data inception 3a output num filter 32 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3b relu 5x5 reduce mx symbol Activation name 'inception 3b relu 5x5 reduce' data inception 3b 5x5 reduce act type arelu' inception 3b 5x5 mx symbol Convolution name 'inception 3b 5x5' data inception 3b relu 5x5 reduce num filter 96 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 3b relu 5x5 mx symbol Activation name 'inception 3b relu 5x5' data inception 3b 5x5 act type arelu' inception 3b pool mx symbol Pooling name 'inception 3b pool' data inception 3a output pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 3b pool proj mx symbol Convolution name 'inception 3b pool proj' data inception 3b pool num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 3b relu pool proj mx symbol Activation name 'inception 3b relu pool proj' data inception 3b pool proj act type arelu' inception 3b output mx symbol Concat name 'inception 3b output' inception 3b relu 1x1 inception 3b relu 3x3 inception 3b relu 5x5 inception 3b relu pool proj pool3 3x3 s2 mx symbol Pooling name 'pool3 3x3 s2' data inception 3b output pooling convention 'full' pad 0 0 kernel 3 3 stride 2 2 pool type 'max' inception 4a 1x1 mx symbol Convolution name 'inception 4a 1x1' data pool3 3x3 s2 num filter 192 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4a relu 1x1 mx symbol Activation name 'inception 4a relu 1x1' data inception 4a 1x1 act type arelu' inception 4a 3x3 reduce mx symbol Convolution name 'inception 4a 3x3 reduce' data pool3 3x3 s2 num filter 96 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4a relu 3x3 reduce mx symbol Activation name 'inception 4a relu 3x3 reduce' data inception 4a 3x3 reduce act type arelu' inception 4a 3x3 mx symbol Convolution name 'inception 4a 3x3' data inception 4a relu 3x3 reduce num filter 208 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 4a relu 3x3 mx symbol Activation name 'inception 4a relu 3x3' data inception 4a 3x3 act type arelu' inception 4a 5x5 reduce mx symbol Convolution name 'inception 4a 5x5 reduce' data pool3 3x3 s2 num filter 16 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4a relu 5x5 reduce mx symbol Activation name 'inception 4a relu 5x5 reduce' data inception 4a 5x5 reduce act type arelu' inception 4a 5x5 mx symbol Convolution name 'inception 4a 5x5' data inception 4a relu 5x5 reduce num filter 48 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 4a relu 5x5 mx symbol Activation name 'inception 4a relu 5x5' data inception 4a 5x5 act type arelu' inception 4a pool mx symbol Pooling name 'inception 4a pool' data pool3 3x3 s2 pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 4a pool proj mx symbol Convolution name 'inception 4a pool proj' data inception 4a pool num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4a relu pool proj mx symbol Activation name 'inception 4a relu pool proj' data inception 4a pool proj act type arelu' inception 4a output mx symbol Concat name 'inception 4a output' inception 4a relu 1x1 inception 4a relu 3x3 inception 4a relu 5x5 inception 4a relu pool proj inception 4b 1x1 mx symbol Convolution name 'inception 4b 1x1' data inception 4a output num filter 160 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4b relu 1x1 mx symbol Activation name 'inception 4b relu 1x1' data inception 4b 1x1 act type arelu' inception 4b 3x3 reduce mx symbol Convolution name 'inception 4b 3x3 reduce' data inception 4a output num filter 112 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4b relu 3x3 reduce mx symbol Activation name 'inception 4b relu 3x3 reduce' data inception 4b 3x3 reduce act type arelu' inception 4b 3x3 mx symbol Convolution name 'inception 4b 3x3' data inception 4b relu 3x3 reduce num filter 224 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 4b relu 3x3 mx symbol Activation name 'inception 4b relu 3x3' data inception 4b 3x3 act type arelu' inception 4b 5x5 reduce mx symbol Convolution name 'inception 4b 5x5 reduce' data inception 4a output num filter 24 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4b relu 5x5 reduce mx symbol Activation name 'inception 4b relu 5x5 reduce' data inception 4b 5x5 reduce act type arelu' inception 4b 5x5 mx symbol Convolution name 'inception 4b 5x5' data inception 4b relu 5x5 reduce num filter 64 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 4b relu 5x5 mx symbol Activation name 'inception 4b relu 5x5' data inception 4b 5x5 act type arelu' inception 4b pool mx symbol Pooling name 'inception 4b pool' data inception 4a output pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 4b pool proj mx symbol Convolution name 'inception 4b pool proj' data inception 4b pool num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4b relu pool proj mx symbol Activation name 'inception 4b relu pool proj' data inception 4b pool proj act type arelu' inception 4b output mx symbol Concat name 'inception 4b output' inception 4b relu 1x1 inception 4b relu 3x3 inception 4b relu 5x5 inception 4b relu pool proj inception 4c 1x1 mx symbol Convolution name 'inception 4c 1x1' data inception 4b output num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4c relu 1x1 mx symbol Activation name 'inception 4c relu 1x1' data inception 4c 1x1 act type arelu' inception 4c 3x3 reduce mx symbol Convolution name 'inception 4c 3x3 reduce' data inception 4b output num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4c relu 3x3 reduce mx symbol Activation name 'inception 4c relu 3x3 reduce' data inception 4c 3x3 reduce act type arelu' inception 4c 3x3 mx symbol Convolution name 'inception 4c 3x3' data inception 4c relu 3x3 reduce num filter 256 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 4c relu 3x3 mx symbol Activation name 'inception 4c relu 3x3' data inception 4c 3x3 act type arelu' inception 4c 5x5 reduce mx symbol Convolution name 'inception 4c 5x5 reduce' data inception 4b output num filter 24 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4c relu 5x5 reduce mx symbol Activation name 'inception 4c relu 5x5 reduce' data inception 4c 5x5 reduce act type arelu' inception 4c 5x5 mx symbol Convolution name 'inception 4c 5x5' data inception 4c relu 5x5 reduce num filter 64 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 4c relu 5x5 mx symbol Activation name 'inception 4c relu 5x5' data inception 4c 5x5 act type arelu' inception 4c pool mx symbol Pooling name 'inception 4c pool' data inception 4b output pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 4c pool proj mx symbol Convolution name 'inception 4c pool proj' data inception 4c pool num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4c relu pool proj mx symbol Activation name 'inception 4c relu pool proj' data inception 4c pool proj act type arelu' inception 4c output mx symbol Concat name 'inception 4c output' inception 4c relu 1x1 inception 4c relu 3x3 inception 4c relu 5x5 inception 4c relu pool proj inception 4d 1x1 mx symbol Convolution name 'inception 4d 1x1' data inception 4c output num filter 112 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4d relu 1x1 mx symbol Activation name 'inception 4d relu 1x1' data inception 4d 1x1 act type arelu' inception 4d 3x3 reduce mx symbol Convolution name 'inception 4d 3x3 reduce' data inception 4c output num filter 144 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4d relu 3x3 reduce mx symbol Activation name 'inception 4d relu 3x3 reduce' data inception 4d 3x3 reduce act type arelu' inception 4d 3x3 mx symbol Convolution name 'inception 4d 3x3' data inception 4d relu 3x3 reduce num filter 288 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 4d relu 3x3 mx symbol Activation name 'inception 4d relu 3x3' data inception 4d 3x3 act type arelu' inception 4d 5x5 reduce mx symbol Convolution name 'inception 4d 5x5 reduce' data inception 4c output num filter 32 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4d relu 5x5 reduce mx symbol Activation name 'inception 4d relu 5x5 reduce' data inception 4d 5x5 reduce act type arelu' inception 4d 5x5 mx symbol Convolution name 'inception 4d 5x5' data inception 4d relu 5x5 reduce num filter 64 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 4d relu 5x5 mx symbol Activation name 'inception 4d relu 5x5' data inception 4d 5x5 act type arelu' inception 4d pool mx symbol Pooling name 'inception 4d pool' data inception 4c output pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 4d pool proj mx symbol Convolution name 'inception 4d pool proj' data inception 4d pool num filter 64 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4d relu pool proj mx symbol Activation name 'inception 4d relu pool proj' data inception 4d pool proj act type arelu' inception 4d output mx symbol Concat name 'inception 4d output' inception 4d relu 1x1 inception 4d relu 3x3 inception 4d relu 5x5 inception 4d relu pool proj inception 4e 1x1 mx symbol Convolution name 'inception 4e 1x1' data inception 4d output num filter 256 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4e relu 1x1 mx symbol Activation name 'inception 4e relu 1x1' data inception 4e 1x1 act type arelu' inception 4e 3x3 reduce mx symbol Convolution name 'inception 4e 3x3 reduce' data inception 4d output num filter 160 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4e relu 3x3 reduce mx symbol Activation name 'inception 4e relu 3x3 reduce' data inception 4e 3x3 reduce act type arelu' inception 4e 3x3 mx symbol Convolution name 'inception 4e 3x3' data inception 4e relu 3x3 reduce num filter 320 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 4e relu 3x3 mx symbol Activation name 'inception 4e relu 3x3' data inception 4e 3x3 act type arelu' inception 4e 5x5 reduce mx symbol Convolution name 'inception 4e 5x5 reduce' data inception 4d output num filter 32 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4e relu 5x5 reduce mx symbol Activation name 'inception 4e relu 5x5 reduce' data inception 4e 5x5 reduce act type arelu' inception 4e 5x5 mx symbol Convolution name 'inception 4e 5x5' data inception 4e relu 5x5 reduce num filter 128 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 4e relu 5x5 mx symbol Activation name 'inception 4e relu 5x5' data inception 4e 5x5 act type arelu' inception 4e pool mx symbol Pooling name 'inception 4e pool' data inception 4d output pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 4e pool proj mx symbol Convolution name 'inception 4e pool proj' data inception 4e pool num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 4e relu pool proj mx symbol Activation name 'inception 4e relu pool proj' data inception 4e pool proj act type arelu' inception 4e output mx symbol Concat name 'inception 4e output' inception 4e relu 1x1 inception 4e relu 3x3 inception 4e relu 5x5 inception 4e relu pool proj pool4 3x3 s2 mx symbol Pooling name 'pool4 3x3 s2' data inception 4e output pooling convention 'full' pad 0 0 kernel 3 3 stride 2 2 pool type 'max' inception 5a 1x1 mx symbol Convolution name 'inception 5a 1x1' data pool4 3x3 s2 num filter 256 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5a relu 1x1 mx symbol Activation name 'inception 5a relu 1x1' data inception 5a 1x1 act type arelu' inception 5a 3x3 reduce mx symbol Convolution name 'inception 5a 3x3 reduce' data pool4 3x3 s2 num filter 160 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5a relu 3x3 reduce mx symbol Activation name 'inception 5a relu 3x3 reduce' data inception 5a 3x3 reduce act type arelu' inception 5a 3x3 mx symbol Convolution name 'inception 5a 3x3' data inception 5a relu 3x3 reduce num filter 320 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 5a relu 3x3 mx symbol Activation name 'inception 5a relu 3x3' data inception 5a 3x3 act type arelu' inception 5a 5x5 reduce mx symbol Convolution name 'inception 5a 5x5 reduce' data pool4 3x3 s2 num filter 32 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5a relu 5x5 reduce mx symbol Activation name 'inception 5a relu 5x5 reduce' data inception 5a 5x5 reduce act type arelu' inception 5a 5x5 mx symbol Convolution name 'inception 5a 5x5' data inception 5a relu 5x5 reduce num filter 128 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 5a relu 5x5 mx symbol Activation name 'inception 5a relu 5x5' data inception 5a 5x5 act type arelu' inception 5a pool mx symbol Pooling name 'inception 5a pool' data pool4 3x3 s2 pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 5a pool proj mx symbol Convolution name 'inception 5a pool proj' data inception 5a pool num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5a relu pool proj mx symbol Activation name 'inception 5a relu pool proj' data inception 5a pool proj act type arelu' inception 5a output mx symbol Concat name 'inception 5a output' inception 5a relu 1x1 inception 5a relu 3x3 inception 5a relu 5x5 inception 5a relu pool proj inception 5b 1x1 mx symbol Convolution name 'inception 5b 1x1' data inception 5a output num filter 384 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5b relu 1x1 mx symbol Activation name 'inception 5b relu 1x1' data inception 5b 1x1 act type arelu' inception 5b 3x3 reduce mx symbol Convolution name 'inception 5b 3x3 reduce' data inception 5a output num filter 192 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5b relu 3x3 reduce mx symbol Activation name 'inception 5b relu 3x3 reduce' data inception 5b 3x3 reduce act type arelu' inception 5b 3x3 mx symbol Convolution name 'inception 5b 3x3' data inception 5b relu 3x3 reduce num filter 384 pad 1 1 kernel 3 3 stride 1 1 no bias False inception 5b relu 3x3 mx symbol Activation name 'inception 5b relu 3x3' data inception 5b 3x3 act type arelu' inception 5b 5x5 reduce mx symbol Convolution name 'inception 5b 5x5 reduce' data inception 5a output num filter 48 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5b relu 5x5 reduce mx symbol Activation name 'inception 5b relu 5x5 reduce' data inception 5b 5x5 reduce act type arelu' inception 5b 5x5 mx symbol Convolution name 'inception 5b 5x5' data inception 5b relu 5x5 reduce num filter 128 pad 2 2 kernel 5 5 stride 1 1 no bias False inception 5b relu 5x5 mx symbol Activation name 'inception 5b relu 5x5' data inception 5b 5x5 act type arelu' inception 5b pool mx symbol Pooling name 'inception 5b pool' data inception 5a output pooling convention 'full' pad 1 1 kernel 3 3 stride 1 1 pool type 'max' inception 5b pool proj mx symbol Convolution name 'inception 5b pool proj' data inception 5b pool num filter 128 pad 0 0 kernel 1 1 stride 1 1 no bias False inception 5b relu pool proj mx symbol Activation name 'inception 5b relu pool proj' data inception 5b pool proj act type arelu' inception 5b output mx symbol Concat name 'inception 5b output' inception 5b relu 1x1 inception 5b relu 3x3 inception 5b relu 5x5 inception 5b relu pool proj pool5 7x7 s1 mx symbol Pooling name 'pool5 7x7 s1' data inception 5b output pooling convention 'full' pad 0 0 kernel 7 7 stride 1 1 pool type 'avg' pool5 drop 7x7 s1 mx symbol Dropout name 'pool5 drop 7x7 s1' data pool5 7x7 s1 p 0 400000 flatten 0 mx symbol Flatten name 'flatten 0' data pool5 drop 7x7 s1 loss3 classifier mx symbol FullyConnected name 'loss3 classifier' data flatten 0 num hidden 397 no bias False prob mx symbol SoftmaxOutput name isoftmax' data loss3 classifier return prob def get iterator batch size 128 data shape 3 227 227 data iter train mx io ImageRecordIter path imgrec data iter sun227 train rec mean img data iter mean bin data shape data shape batch size batch size rand crop True rand mirror True preprocess threads 8 val mx io ImageRecordIter path imgrec data iter sun227 val rec mean img data iter mean bin data shape data shape batch size batch size rand crop False rand mirror False preprocess threads 8 return train val if name main head ' asctime 15s message s' logging basicConfig level logging DEBUG format head batch size input batch size dev mx gpu int i for i in raw input 'gpus ' split ' ' old model mx model FeedForward load googlenet 1 model mx model FeedForward ctx dev symbol get googlenet num epoch 200 learning rate 0 001 wd 0 00001 arg params old model arg params aux params old model aux params allow extra params True data train data test get iterator batch size model fit X data train eval data data test kvstore 'local allreduce device' batch end callback mx callback Speedometer batch size 50 epoch end callback mx callback do checkpoint sun and i get this error mxnet base MXNetError 10 20 17 src ndarray ndarray cc 231 Check failed from shape to shape operands shape mismatch Please help me,,piiswrong,2017-01-12 02:34:24,2017-01-13 09:52:05
PR,Update model py,,,piiswrong,2017-01-14 06:57:24,2017-01-14 07:51:29
IS,how to use embeding layer in this case,Hi I have some questions about the mx sym Embedding I have two document sets written in English and Japanese and I want to feed them to a 2 inputs net The net is like this After the reshape layer the shape is batch size 2 sentence size num embed Asumming the vocab size is 9000 English words are mapping to 1 5000 Japanese words are mapping to 5001 9000 and both of them use 0 for padding I'm not sure about it because the embedding api said All the input values should be integers in the range 0 input dim 1 Can I directly set the input dim 5000 for embed1 and input dim 4000 for embed2 if I want to use these two embedding layers What will happen if the input values of embed2 the value of input dim in this case Or should I build a new vocab for Japanese and map the word to 1 4000 and 0 for padding If they are wrong should I use only one embeding layer input dim 9000 which contains words in 2 languages 2 How two get the word vectors from a pre trained model Does the row index of the embed weight correspond to the word index e g the first row of the embed weight is the word embedding of the word which is mapped to 1 Thanks a lot,,,2016-12-15 05:42:36,2017-01-14 09:12:54
PR,unplug a warning under VC compile,The const number is too long for a double add f postfix to force compiler to treat it as a float,,howard0su,2017-01-13 05:59:45,2017-01-14 21:23:55
IS,ssd example segmentation fault,Did anyone use the latest ssd example I almost not touch the code except change pixel mean from tuple to list However it throw out a segmentation fault at the end of training I can not figure out where this problem occurs I set end epoch 1 for debug some logs INFO root Epoch 0 Batch 140 Speed 54 16 samples sec Train Acc 0 743964 INFO root Epoch 0 Batch 140 Speed 54 16 samples sec Train ObjectAcc 0 010814 INFO root Epoch 0 Batch 140 Speed 54 16 samples sec Train SmoothL1 92 468682 INFO root Epoch 0 Train Acc 0 751502 INFO root Epoch 0 Train ObjectAcc 0 006965 INFO root Epoch 0 Train SmoothL1 97 715778 INFO root Epoch 0 Time cost 90 291 INFO root Saved checkpoint to data2 obj detect ssd mx ssd model ssd 300 0001 params Segmentation fault,,howard0su,2017-01-14 03:11:55,2017-01-15 03:22:08
PR,Update docs how to cloud md,Fix a bug in docs how to cloud md which may cause import error package common not found This bug is caused by copying all python scripts without copying common dir Fix a typo in python mxnet kvstore py,,qiyuangong,2017-01-15 14:24:58,2017-01-15 19:30:19
PR,Update neural art md,3849 renamed run py to nstyle py,,kj-ki,2017-01-15 11:33:43,2017-01-15 19:30:34
PR,Create ImageIter from a in memory list,this allows me easy to consume this API from ipython,,"howard0su,piiswrong",2017-01-09 14:29:43,2017-01-15 19:31:32
PR,Update graph executor cc,,,piiswrong,2017-01-15 19:44:28,2017-01-15 22:30:42
IS,GAN example guide,Hello I'm interested in example gan but the only script I found was a dcgan py Is there anyone could bring a README for this example Several hard codes are found in the dcgan py script and I found it hard to run with Thanks very much,,,2017-01-11 03:44:27,2017-01-16 07:10:12
PR,fix inception resnet v2 example,remove unused import fix error,,"minazou,ysh329",2017-01-16 14:01:29,2017-01-16 17:08:23
PR,Fixed reshaping of input was erroring out,,,andreaolgiati,2017-01-16 17:22:06,2017-01-16 18:41:30
IS,Scala FeedForward should not symbol by public Or how to build one net from an existing one,I would like to use the convolution par of a pre trained VGG 16 network and retrained the fully connected classification layers However after importing the VGG model from I see no way in FeedForward scala to built a network with only the CNN layers with their trained weights and a new set of top fully connected layers FeedForward symbol is necessary to build a new network but it is private Is there a way to fulfill this quite common use case with mxnet scala Did I miss something,,"benqua,Ldpe2G,benqua,yzhliu,benqua,yzhliu,benqua",2017-01-15 09:04:38,2017-01-16 19:42:05
IS,error when runing rcnn example train end2end py,Hi I'm trying to run the rcnn example train end2end and It report an error File home lilhope mxnet example rcnn rcnn rpn proposal py line 59 in forward raise ValueError there is nan in input scores I print the rpn cls prob reshape rpn bbox pred and find it was nan Then I test the input of the network data it was a mx ndarry NDArray 1x3x600x800 0 NDArray 1x3 0 It seems nothing wrong with the network input I use the pretrain model VGG16 get it from the script run sh in mxnet tools caffe converter and put the file vgg16 0001 params vgg16 symbol json in the model folder I modify the training parse load epoch 1 could anyone help me to figure out which cause the error Thanks,,,2017-01-16 05:13:40,2017-01-17 01:07:35
IS,Crash with latest master,Error pure virtual method called terminate called without an active exception the crash stack 0 0x00007ffff6d26c37 in GI raise sig sig entry 6 at nptl sysdeps unix sysv linux raise c 56 1 0x00007ffff6d2a028 in GI abort at abort c 89 2 0x00007fffd206cb05 in gnu cxx verbose terminate handler from home howardsu anaconda2 bin lib libstdc so 6 3 0x00007fffd206aca6 in from home howardsu anaconda2 bin lib libstdc so 6 4 0x00007fffd206acd3 in std terminate from home howardsu anaconda2 bin lib libstdc so 6 5 0x00007fffd206b78f in cxa pure virtual from home howardsu anaconda2 bin lib libstdc so 6 6 0x00007fffe8382cc0 in mxnet Engine PushSync std function void mxnet RunContext mxnet Context std vector mxnet engine Var std allocator mxnet engine Var const std vector mxnet engine Var std allocator mxnet engine Var const mxnet FnProperty int char const this 0x6c4550 exec fn exec ctx exec ctx entry const vars mutable vars prop prop entry mxnet kNormal priority priority entry 0 opr name opr name entry 0x7fffe927dfbe SyncCopyCPU2CPU at include mxnet engine h 213 7 0x00007fffe8421a82 in mxnet NDArray SyncCopyFromCPU this 0x7fffa40026c0 data optimized out size optimized out at src ndarray ndarray cc 698 8 0x00007fffe832d6fe in MXNDArraySyncCopyFromCPU handle optimized out data optimized out size optimized out at src c api c api cc 181 9 0x00007ffff661831c in ffi call unix64 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes libffi src x86 unix64 S 76 10 0x00007ffff6617a75 in ffi call cif optimized out fn 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t rvalue optimized out avalue 0x7fffb3527160 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes libffi src x86 ffi64 c 525 11 0x00007ffff660f126 in call function pointer argcount 3 resmem 0x7fffb3527190 restype optimized out atypes optimized out avalues 0x7fffb3527160 pProc 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t flags 4353 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes callproc c 837 0 0x00007ffff6d26c37 in GI raise sig sig entry 6 at nptl sysdeps unix sysv linux raise c 56 1 0x00007ffff6d2a028 in GI abort at abort c 89 2 0x00007fffd206cb05 in gnu cxx verbose terminate handler from home howardsu anaconda2 bin lib libstdc so 6 3 0x00007fffd206aca6 in from home howardsu anaconda2 bin lib libstdc so 6 4 0x00007fffd206acd3 in std terminate from home howardsu anaconda2 bin lib libstdc so 6 5 0x00007fffd206b78f in cxa pure virtual from home howardsu anaconda2 bin lib libstdc so 6 6 0x00007fffe8382cc0 in mxnet Engine PushSync std function void mxnet RunContext mxnet Context std vector mxnet engine Var std allocator mxnet engine Var const std vector mxnet engine Var std allocator mxnet engine Var const mxnet FnProperty int char const this 0x6c4550 exec fn exec ctx exec ctx entry const vars mutable vars prop prop entry mxnet kNormal priority priority entry 0 opr name opr name entry 0x7fffe927dfbe SyncCopyCPU2CPU at include mxnet engine h 213 7 0x00007fffe8421a82 in mxnet NDArray SyncCopyFromCPU this 0x7fffa40026c0 data optimized out size optimized out at src ndarray ndarray cc 698 8 0x00007fffe832d6fe in MXNDArraySyncCopyFromCPU handle optimized out data optimized out size optimized out at src c api c api cc 181 9 0x00007ffff661831c in ffi call unix64 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes libffi src x86 unix64 S 76 10 0x00007ffff6617a75 in ffi call cif optimized out fn 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t rvalue optimized out avalue 0x7fffb3527160 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes libffi src x86 ffi64 c 525 11 0x00007ffff660f126 in call function pointer argcount 3 resmem 0x7fffb3527190 restype optimized out atypes optimized out avalues 0x7fffb3527160 pProc 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t flags 4353 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes callproc c 837 10 0x00007ffff6617a75 in ffi call cif optimized out fn 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t rvalue optimized out avalue 0x7fffb3527160 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes libffi src x86 ffi64 c 525 11 0x00007ffff660f126 in call function pointer argcount 3 resmem 0x7fffb3527190 restype optimized out atypes optimized out avalues 0x7fffb3527160 pProc 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t flags 4353 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes callproc c 837 12 ctypes callproc pProc 0x7fffe832d6f0 MXNDArraySyncCopyFromCPU NDArrayHandle void const size t argtuple 0x7fffb3527280 flags 4353 argtypes optimized out restype 0x6d6430 checker 0x0 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes callproc c 1180 13 0x00007ffff6606ce3 in PyCFuncPtr call self optimized out inargs optimized out kwds 0x0 at home ilan minonda conda bld work Python 2 7 12 Modules ctypes ctypes c 3954 14 0x00007ffff7a29dc3 in PyObject Call func 0x7fffb3e86120 arg optimized out kw optimized out at Objects abstract c 2546 15 0x00007ffff7adb6c7 in do call nk optimized out na optimized out pp stack 0x7fffb3527508 func 0x7fffb3e86120 at Python ceval c 4567 16 call function oparg optimized out pp stack 0x7fffb3527508 at Python ceval c 4372 17 PyEval EvalFrameEx f optimized out throwflag optimized out at Python ceval c 2987 18 0x00007ffff7ade1ce in PyEval EvalCodeEx co 0x7fffee0de730 globals optimized out locals optimized out args optimized out argcount 2 kws 0x7fffa80a8580 kwcount 0 defs 0x0 defcount 0 closure 0x0 at Python ceval c 3582 19 0x00007ffff7add1f6 in fast function nk optimized out na 2 n optimized out pp stack 0x7fffb3527728 func 0x7fffee0ece60 at Python ceval c 4445 20 call function oparg optimized out pp stack 0x7fffb3527728 at Python ceval c 4370 21 PyEval EvalFrameEx f optimized out throwflag optimized out at Python ceval c 2987 22 0x00007ffff7ade1ce in PyEval EvalCodeEx co 0x7fffee0de630 globals optimized out locals optimized out args optimized out argcount 3 kws 0x0 kwcount 0 defs 0x0 defcount 0 closure 0x0 at Python ceval c 3582 23 0x00007ffff7a597e1 in function call func 0x7fffee0ecd70 arg 0x7ffff7fc9cd0 kw 0x0 at Objects funcobject c 523 24 0x00007ffff7a29dc3 in PyObject Call func 0x7fffee0ecd70 arg optimized out kw optimized out at Objects abstract c 2546 25 0x00007ffff7a3c54f in instancemethod call func 0x7fffee0ecd70 arg 0x7ffff7fc9cd0 kw 0x0 at Objects classobject c 2602 26 0x00007ffff7a29dc3 in PyObject Call func 0x7ffff7ea8dc0 arg optimized out kw optimized out at Objects abstract c 2546 27 0x00007ffff7a938c5 in call method o optimized out name optimized out nameobj 0x7ffff7dbb168 setitem str format 0x7ffff7b31288 OO at Objects typeobject c 1281 28 0x00007ffff7a93c2b in slot mp ass subscript self optimized out key optimized out value optimized out at Objects typeobject c 5192 29 0x00007ffff7ad44e5 in assign slice u 0x7ffff7eb4090 v optimized out w optimized out x 0x7ffff7ea8e40 at Python ceval c 4763,,"howard0su,piiswrong,piiswrong,ysh329",2017-01-13 09:17:26,2017-01-17 03:23:41
IS,gcc cuda,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 10 Compiler gcc Ubuntu 6 2 0 5ubuntu12 6 2 0 20161005 Package used Python R Scala Julia python MXNet version latest Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 Anaconda 4 2 0 64 bit If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace usr local cuda include host config h 119 2 error error unsupported GNU version gcc versions later than 5 are not supported error unsupported GNU version gcc versions later than 5 are not supported usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES g O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 50 code compute 50 Xcompiler DMSHADOW FORCE STREAM Wall O3 I home kkk mxnet mshadow I home kkk mxnet dmlc core include fPIC I home kkk mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE NVRTC 0 M MT build src operator tensor elemwise binary scalar op extended gpu o src operator tensor elemwise binary scalar op extended cu build src operator tensor elemwise binary scalar op extended gpu d In file included from usr local cuda include cuda runtime h 78 0 from command line 0 usr local cuda include host config h 119 2 error error unsupported GNU version gcc versions later than 5 are not supported error unsupported GNU version gcc versions later than 5 are not supported Makefile 205 recipe for target 'build src operator tensor elemwise binary op extended gpu o' failed make build src operator tensor elemwise binary op extended gpu o Error 1 make Waiting for unfinished jobs g c O3 Wall Wno unknown pragmas Iinclude std c 0x fopenmp fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o input split base o src io input split base cc Makefile 205 recipe for target 'build src operator tensor elemwise binary scalar op extended gpu o' failed make build src operator tensor elemwise binary scalar op extended gpu o Error 1 g c O3 Wall Wno unknown pragmas Iinclude std c 0x fopenmp fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o io o src io cc Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it I have specified the gcc version in config mk to gcc 4 9 but not work I installed cuda8 with override option because it occurs error while my gcc is too fresh for cuda is that the problem dont konw how to solve it if I dont use that option I cant install cuda 1 2 3,,,2017-01-16 19:21:47,2017-01-17 05:03:54
PR,Copy3,tqchen,,"piiswrong,tqchen",2017-01-15 22:31:44,2017-01-17 07:28:10
IS,Accuracy of MXNet training statistics,Hi I am using MXNet evaluation metrics to acquire training statistics I notice that the accuracy produced by mxnet metric Accuracy always contains 6 digits behind decimal point I am using 5000 samples as validation data Therefore in validation accuracy there should be at most 4 meaningful digits behind decimal point 1 5000 0 0002 and the last 2 digits if present should be 0 However in that case the last 2 digits are usually not 0 e g 0 887891 0 890039 0 888281 etc I am using MXNet in a research project Therefore I might be slightly particular about details Could anyone explain why the last 2 digits are not 0,,piiswrong,2017-01-17 04:38:06,2017-01-17 09:10:40
PR,Fix Amazon Linux Installation Title,,,kevinthesun,2017-01-17 20:46:22,2017-01-18 00:55:03
PR,Reorganzie get started page content,1 Remove redundant contant in introduction session 2 Move MXNet Open Source Community to bottom 3 Modify the introduction of Starting with the Basics session,,kevinthesun,2017-01-17 23:07:31,2017-01-18 01:01:20
IS,ssd example evaluate py problem,Environment info Operating System ubuntu 14 04 Compiler gcc 4 8 Package used Python R Scala Julia python MXNet version 0 9 1 latest Python version and distribution anaconda 2 7 11 Error Message Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce 1 mxnet example ssd python evaluate py gpus 0 1 batch size 128 epoch 100 What have you tried to solve it 1 update to the latest mxnet,,piiswrong,2017-01-18 14:31:06,2017-01-18 17:28:25
IS,example ssd error Operator Scale is not registered,zhreshold For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler g 4 8 4 gcc 4 8 4 Package used Python R Scala Julia Python MXNet version 0 9 2 download 2017 1 17 CUDA 0 only for cpu Python version and distribution 2 7 EXTRA OPERATORS example ssd operator Error Message python demo py epoch 0 images data demo dog jpg thresh 0 5 cpu 22 24 20 include dmlc logging h 300 22 24 20 src core op cc 55 Check failed op nullptr Operator Scale is not registered Stack trace returned 34 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm2Op3GetERKSs 0x376 0x7fd1584c2c06 bt 1 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so 0x1859608 0x7fd15850a608 bt 2 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x110 0x7fd1585111c0 bt 3 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so 0x1857881 0x7fd158508881 bt 4 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7fd157884dbf bt 5 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x703 0x7fd1584c8fc3 bt 6 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN5mxnet18LoadLegacyJSONPassEN4nnvm5GraphE 0x18b 0x7fd15788118b bt 7 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7fd157884dbf bt 8 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x703 0x7fd1584c8fc3 bt 9 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fd15778e01e bt 10 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so MXSymbolCreateFromFile 0x471 0x7fd1577873c1 bt 11 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7fd15c558adc bt 12 usr lib x86 64 linux gnu libffi so 6 ffi call 0x1fc 0x7fd15c55840c bt 13 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48e 0x7fd15c76f5fe bt 14 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x15f9e 0x7fd15c770f9e bt 15 python PyEval EvalFrameEx 0x965 0x499be5 bt 16 python PyEval EvalCodeEx 0x2ac 0x4a090c bt 17 python PyEval EvalFrameEx 0x18c5 0x49ab45 bt 18 python PyEval EvalCodeEx 0x2ac 0x4a090c bt 19 python PyEval EvalFrameEx 0x18c5 0x49ab45 bt 20 python 0x4a1c9a bt 21 python 0x4dfe94 bt 22 python PyObject Call 0x36 0x505f96 bt 23 python 0x4de41a bt 24 python 0x5039eb bt 25 python PyEval EvalFrameEx 0x965 0x499be5 bt 26 python PyEval EvalCodeEx 0x2ac 0x4a090c bt 27 python PyEval EvalFrameEx 0x7d2 0x499a52 bt 28 python 0x4a1634 bt 29 python PyRun FileExFlags 0x92 0x44e4a5 bt 30 python PyRun SimpleFileExFlags 0x2ee 0x44ec9f bt 31 python Py Main 0xb5e 0x44f904 bt 32 lib x86 64 linux gnu libc so 6 libc start main 0xf5 0x7fd15da25ec5 bt 33 python 0x578c4e Traceback most recent call last File demo py line 95 in module ctx args nms thresh args force nms File demo py line 41 in get detector data shape mean pixels ctx ctx File home guorui soft code mxnet mxnet example ssd detect detector py line 34 in init args auxs mx model load checkpoint model prefix epoch File usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet model py line 372 in load checkpoint symbol sym load ' s symbol json' prefix File usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet symbol py line 1056 in load check call LIB MXSymbolCreateFromFile c str fname ctypes byref handle File usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet base py line 75 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Failed loading Op relu4 3 scale 20 of type Scale 22 24 20 src core op cc 55 Check failed op nullptr Operator Scale is not registered Stack trace returned 34 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm2Op3GetERKSs 0x376 0x7fd1584c2c06 bt 1 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so 0x1859608 0x7fd15850a608 bt 2 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x110 0x7fd1585111c0 bt 3 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so 0x1857881 0x7fd158508881 bt 4 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7fd157884dbf bt 5 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x703 0x7fd1584c8fc3 bt 6 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN5mxnet18LoadLegacyJSONPassEN4nnvm5GraphE 0x18b 0x7fd15788118b bt 7 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7fd157884dbf bt 8 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x703 0x7fd1584c8fc3 bt 9 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fd15778e01e bt 10 usr local lib python2 7 dist packages mxnet 0 9 2 py2 7 egg mxnet libmxnet so MXSymbolCreateFromFile 0x471 0x7fd1577873c1 bt 11 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7fd15c558adc bt 12 usr lib x86 64 linux gnu libffi so 6 ffi call 0x1fc 0x7fd15c55840c bt 13 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48e 0x7fd15c76f5fe bt 14 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x15f9e 0x7fd15c770f9e bt 15 python PyEval EvalFrameEx 0x965 0x499be5 bt 16 python PyEval EvalCodeEx 0x2ac 0x4a090c bt 17 python PyEval EvalFrameEx 0x18c5 0x49ab45 bt 18 python PyEval EvalCodeEx 0x2ac 0x4a090c bt 19 python PyEval EvalFrameEx 0x18c5 0x49ab45 bt 20 python 0x4a1c9a bt 21 python 0x4dfe94 bt 22 python PyObject Call 0x36 0x505f96 bt 23 python 0x4de41a bt 24 python 0x5039eb bt 25 python PyEval EvalFrameEx 0x965 0x499be5 bt 26 python PyEval EvalCodeEx 0x2ac 0x4a090c bt 27 python PyEval EvalFrameEx 0x7d2 0x499a52 bt 28 python 0x4a1634 bt 29 python PyRun FileExFlags 0x92 0x44e4a5 bt 30 python PyRun SimpleFileExFlags 0x2ee 0x44ec9f bt 31 python Py Main 0xb5e 0x44f904 bt 32 lib x86 64 linux gnu libc so 6 libc start main 0xf5 0x7fd15da25ec5 bt 33 python 0x578c4e Thank you,,"piiswrong,zhreshold,zhreshold,zhreshold,zhreshold,zhreshold",2017-01-18 14:31:19,2017-01-18 17:28:32
PR,OP take,,,"WellyZhang,sxjscience,piiswrong",2017-01-18 07:50:02,2017-01-18 17:45:50
IS,scala Error initilizing NDArrayIter with IndexedSeq when IndexedSeq size 1,I get an error when initializing a NDArrayIter with a sequence with more than one element It works perfectly with an indexedSeq with only one element What have you tried to solve it Did not find the time to investigate much more,,"benqua,benqua,yzhliu,benqua,benqua,yzhliu,benqua,yzhliu",2017-01-14 12:43:07,2017-01-18 20:11:53
PR,Update setup py,,,"piiswrong,tqchen,tqchen,piiswrong,yajiedesign,piiswrong,yajiedesign,piiswrong",2017-01-18 21:52:30,2017-01-19 02:51:33
PR,Update example fcn xs README md,Fix a import error bug caused by Pillow which is required by image segment py Add support for CPU Add more details about this example,,"qiyuangong,tornadomeet,qiyuangong",2017-01-18 01:53:56,2017-01-19 02:54:08
PR,OP add WarpOp,WarpOp is very similar to remap in OpenCV but have slight differences It will remap feature map via bilinear interpolation according to optical flow If you want to know the mathematical details of WarpOp please refer to Supplementary Material for FlowNet 2 0 Evolution of Optical Flow Estimation with Deep Networks,,"dsqx71,howard0su,howard0su,howard0su,howard0su,howard0su,piiswrong,piiswrong,piiswrong,dsqx71,dsqx71,dsqx71,sxjscience,sxjscience,sxjscience,piiswrong,piiswrong,tornadomeet,dsqx71,dsqx71,dsqx71,piiswrong,sxjscience,sxjscience,sxjscience",2016-12-27 13:26:25,2017-01-19 10:00:34
IS,R Error with multi device training with GPUs in R,There is an error when training in multi device in R The same in python works well The behavior is different if training with 1 GPU or with several When training with 1 GPU All the load goes to only one of the hosts It trains correctly but I see 2 processes in nvidia smi in one host and 0 precesses in the other Also the processes are not eliminated after the computation is finished I had to kill them manually When training with several GPUs It is not working I tried with different batch sizes and with synchronous and asynchronous distribution I get the following error,,"miguelgfierro,miguelgfierro",2016-10-26 18:44:38,2017-01-19 10:42:20
IS,R Bottle neck in metrics update,I have been doing some profiling in the training L94 function in R I realised that this line is the most expensive in terms of time The bottleneck is in the update L8 function How could we optimize this code,,"miguelgfierro,thirdwing,miguelgfierro",2016-09-06 11:06:57,2017-01-19 10:43:28
PR,Refactor zero gradient Revise test of binary logic ops,Creates a zeros node for each grad in,,"sxjscience,sxjscience,piiswrong",2017-01-19 15:11:38,2017-01-19 18:06:00
PR,Update rcnn example with accleration module resnet coco and nnvm,Issue 4713 collected all current issues with rcnn example This pr is tested on MXNet Jan 12 a week ago All VOC experiments have been verified and reported using MXNet v0 9 1 I do not think there is any need to verify the 12 commits added since last week With this pr 1 training and testing speed is accelerated 2 module testing replaced executor reshape 3 added resnet support 4 added coco dataset where the performance with resnet will be tuned in the future 5 minor adjustments for nnvm 6 solved many issues collected Relationship with past pr on rcnn example in short I have included or used equivalent implementation 3173 wont fix because numpy 1 8 2 supports blank indexing i e a b where b 3327 changes are equivalent example rcnn demo py load param returns num class changed to config controlled behavior example rcnn helper processing bbox transform py clip is not necessary to stablize training 3495 windows string template support included executor reshape ignored replaced by module predict symbol BlockGrad included remove warmup scheduler ignored because it is not necessary at first 3641 remove an argument ignored not necessary 4136 install monitor included 4221 ignored need more experiments on zfnet to merge Remaining issues all of the following will not work in v0 8 either 1 Python3 Windows environment unknown 2 No mx image usage 3 4G memory for VGG py faster rcnn caffe uses 3G May have to solve it later All in all At least everything works if they worked before,,"precedenceguo,sxjscience,precedenceguo,piiswrong,piiswrong,zhreshold,precedenceguo",2017-01-19 12:43:22,2017-01-19 18:16:01
PR,added fix for nonsquare stride during backprop,The fix was added for patch2col but not for col2patch,,"samster25,piiswrong,samster25",2017-01-18 00:00:11,2017-01-19 18:45:31
PR,Scala fix error when run example ExampleCustomOpWithRtc scala,relates to the issue 4699 I figure out a way to solve the problem but I think the solution may be a bit ugly what is your opinion,,"Ldpe2G,yzhliu",2017-01-18 14:22:27,2017-01-19 18:46:14
IS,AttributeError 'module' object has no attribute 'Proposal' when running RCNN train end2end py,,,"krosaen,krosaen,piiswrong,krosaen,krosaen,precedenceguo,krosaen,precedenceguo,liangfu",2017-01-19 22:07:08,2017-01-19 22:37:36
IS,Scala fail to run the example ExampleCustomOpWithRtc scala,Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 Package used Python R Scala Julia Scala MXNet version v0 9 Or if installed from source MXNet commit hash git rev parse HEAD 6d05979cce53041f356204b17db2effb09371328 Error Message What have you tried to solve it I have locate the commit which cause the problem it is the pr 4528 after I roll back to the commit 50a3a3184e3034a98b2d4ad82f186d035803ab9b before 4528 the problem have solved And I also check the CUDA doc error code 201 means CUDA ERROR INVALID CONTEXT 201 This most frequently indicates that there is no context bound to the current thread This can also be returned if the context passed to an API call is not a valid handle such as a context that has had cuCtxDestroy invoked on it This can also be returned if a user mixes different API versions i e 3010 context with 3020 API calls See cuCtxGetApiVersion for more details,,"Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G",2017-01-17 11:08:20,2017-01-20 00:07:06
IS,Compilation error error identifier 'mask' is undefined,I'm trying to compile mxnet on an Ubuntu 16 04 machine and I have been getting multiple instances of this error when compiling I pulled earlier today mxnet src operator broadcast mask op inl h 50 error identifier mask is undefined This is when it is trying to compile the broadcast mask op cu file when compiling it for CUDA It is happening during the macro expansions mentioned below Line 50 ElementwiseMaskForward I have had a look around the source code myself but I'm unable to determine what this 'mask' may be I presume it is meant to be a function Any ideas help would be appreciated,,piiswrong,2016-12-16 14:16:15,2017-01-20 00:48:43
PR,Add Index2d,tqchen,,"piiswrong,tqchen,mli,tqchen,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,piiswrong,sxjscience",2017-01-17 20:49:16,2017-01-20 05:26:42
PR,Improve index layout,Improve navigation bar layout 1 For medium screen nav bar will collapse each entry one by one instead of a single burger button 2 For small screen search box is replaced by a search button When user clicks it a search box will pop up,,kevinthesun,2017-01-20 01:32:53,2017-01-20 05:30:27
IS,Convolution lstm,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler Pycharm Package used Python MXNet version v0 8 Or if installed from source If you are using python package please provide Python version and distribution 2 7 I want to implement Convolution LSTM in mxnet But I have to do convolution between two mxnet symbols how could I achieve it For instance I am supposed to do convolution between H t 1 and weight W hi In mxnet both of them are represented as mxnet symbol And I think mxnet symbol convolution would only allow me to do convolution with only one symbol The formula is in this paper 02798800 d8fa 11e5 901b 9b4480b029d6 As stated on the paper ' ' denotes convolutional operator and 'o' denotes element wise multiplication,,"piiswrong,sxjscience",2017-01-20 01:49:11,2017-01-20 06:18:12
PR,Add other cc projects into CMakefile,this helps reduce the complexity to build these on Windows,,"howard0su,yajiedesign",2017-01-13 05:58:44,2017-01-20 06:28:08
PR,Prototype A new dispatcher to scale CPU dispatching better,,,"howard0su,howard0su",2017-01-09 05:23:43,2017-01-20 14:54:50
PR,Fixes mxnet additonal deps,A couple of small fixes python matplotlib is an apt package not a pip package pip package for scimage is scikit image not python skimage,,"krosaen,precedenceguo",2017-01-20 16:42:28,2017-01-20 17:00:39
PR,fix profiler cc gpu num in cpu only no initialization error,,,yajiedesign,2017-01-20 16:02:19,2017-01-20 17:01:23
PR,fix image classification import error with python3,fix import error with python3,,"yajiedesign,piiswrong,piiswrong,yajiedesign,yajiedesign,mli",2017-01-16 01:35:20,2017-01-21 00:04:52
PR,fix a replicated beta params bug in cudnn convolution,rt,,"gengyifeng,piiswrong,gengyifeng",2017-01-12 10:53:01,2017-01-21 00:05:38
PR,Fix cuda error when multi process is used,do not be too smart to detect of GPU which cause a GPU context created in very early stage Due to CUDA limitation fork process cannot run well if we create GPU context before fork,,"howard0su,piiswrong,piiswrong",2017-01-17 06:01:00,2017-01-21 00:08:21
PR,update MKL README,explain the usage of MKL2017 MKL2017 ML for the request of mcxkarakoc Signed off by lingyan lingyan guo intel com,,glingyan,2017-01-09 01:17:15,2017-01-21 00:10:50
PR,Correct CTC plugin compilation error,Fix of plugin warpctc warpctc inl h 129 5 error 'ctcComputeInfo' was not declared in this scope according to issue 3747,,"piiswrong,xlvector,xlvector,xlvector,xlvector,xlvector,piiswrong,xlvector,piiswrong,BobLiu20,BobLiu20,piiswrong,piiswrong",2016-11-16 09:58:49,2017-01-21 00:15:09
PR,Fix warning when CPU only,,,howard0su,2017-01-21 01:02:33,2017-01-21 01:35:09
PR,add symbol clip,add symbol clip remove old clip,,"yajiedesign,howard0su,howard0su,yajiedesign,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,sxjscience",2017-01-20 11:36:23,2017-01-21 05:12:33
PR,disable cython to prevent strange import error,issuecomment 274229165 strange error cannot reproduce but seems to be common for a lot of users but looks like this will fix it,,piiswrong,2017-01-21 05:51:09,2017-01-21 05:59:06
PR,WIP Fix amalgamation,Do not merge now,,"antinucleon,piiswrong,piiswrong",2017-01-13 17:30:22,2017-01-21 23:28:14
PR,Fix Amalgamation,,,"piiswrong,howard0su",2017-01-21 23:18:31,2017-01-22 00:41:26
PR,pip package starter,szha next step 1 statically link opencv and openblas Dynamically link cuda and cudnn 5 1 2 make sure pip install works on different platforms 3 upload to PyPI 4 setup automatic build on CI and upload,,"piiswrong,phunterlau,zihaolucky,zihaolucky",2017-01-21 08:44:27,2017-01-22 08:16:46
IS,Makefile 291 recipe for target 'rpkg' failed,I got an error when install mxnet for R in ubuntu 16 04 64bits I followed quick install in install mxnet for r,,"piiswrong,thirdwing,thirdwing",2017-01-16 12:15:27,2017-01-22 15:51:26
PR,Enhencement on build,Move macro to build command line to avoid the issues causes by order of macro defined Also reduce the duplicated logic in two places Avoid compile the code when generate d file to reduce compile significant,,"howard0su,piiswrong,piiswrong,howard0su",2017-01-22 03:07:25,2017-01-22 18:10:34
PR,Fix launcher log message,Fix message typo,,kj-ki,2017-01-22 10:32:29,2017-01-22 18:10:48
PR,Fix build on OSX,lrt is not needed as well as on Linux,,howard0su,2017-01-22 01:16:11,2017-01-22 18:11:12
PR,RELEASE v0 9 3 official release,,,piiswrong,2017-01-22 18:22:02,2017-01-22 19:07:10
IS,Error when apply mx sym simple bind on gpu,Hi I was following the tutorial about mx sym simple bind at mxnet notebook Everything was fine until I changed the device context from mx cpu to mx gpu By the way everything was fine on gpu when I work with mx mod Module The error information is as follows,,,2017-01-22 13:21:04,2017-01-23 04:59:29
IS,Weired memory allocation in mxnet python module,I built mxnet 949300d15957299d99d7c32fedecdede5619cc26 from source on a virtual machine running ubuntu 16 04 without gpu support When I tried import mxnet in python it throws a std bad alloc exception and aborted Using strace I found out that this was caused by some weired memory allocations the mmap calls It seems the python process was trying to allocate 176258011136 bytes about 164GB of memory when importing the mxnet module Why is this happening mmap NULL 176258011136 PROT READ PROT WRITE MAP PRIVATE MAP ANONYMOUS 1 0 1 ENOMEM Cannot allocate memory brk 0x290b2b0000 0x15da000 mmap NULL 176258142208 PROT READ PROT WRITE MAP PRIVATE MAP ANONYMOUS 1 0 1 ENOMEM Cannot allocate memory mmap NULL 134217728 PROT NONE MAP PRIVATE MAP ANONYMOUS MAP NORESERVE 1 0 0x7f035910c000 munmap 0x7f035910c000 49233920 0 munmap 0x7f0360000000 17874944 0 mprotect 0x7f035c000000 135168 PROT READ PROT WRITE 0 mmap NULL 176258011136 PROT READ PROT WRITE MAP PRIVATE MAP ANONYMOUS 1 0 1 ENOMEM Cannot allocate memory Environment info Operating System Ubuntu 16 04 Compiler gcc 5 4 0 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 949300d15957299d99d7c32fedecdede5619cc26 Python version python 2 7 12 Error Message terminate called after throwing an instance of istd bad alloc' what std bad alloc Aborted core dumped Minimum reproducible example import mxnet What have you tried to solve it,,"piiswrong,piiswrong,antinucleon,piiswrong,antinucleon,howard0su,piiswrong",2017-01-19 11:23:18,2017-01-23 05:48:40
IS,MXNet on Mobile Device,Environment info Operating System MacOS iOS Compiler gcc 6 Package used Python R Scala Julia Python MXNet version 0 7 0 8 0 9 2 Python version and distribution Python 2 7 Error Message I tried to use MXNet on iOS but found that the 'amalgamation' folder is not suitable for current version It is because the whole project structure has changed a lot and some files are removed I tried to fix it but encountered endless problems such as I also tried to use CMake to make libmxnet a but failed to build it on xcode on error target specifies product type 'com apple product type tool' but there is no such product type for the 'iphoneos' platform Some new operations and parameters can not be parsed using the single cc file on So is there any solution Could anyone update the amalgamation module please,,"xlvector,piiswrong",2017-01-18 07:20:57,2017-01-23 05:49:11
IS,Amalgamation does not include SwapAxis,I'm not sure if this is intended functionality or a bug We are hopeful we can do what we need without it using reshape Is there a list of so supported operations,,"antinucleon,piiswrong",2017-01-06 18:16:26,2017-01-23 05:49:18
IS,v0 8 0 Amalgamation broken for Android ARM 64 bit,Amalgamation does not work out of the box for Android 64 bit ARM architectures I finally got it working but I'm not sure I'm doing it right Can anyone help First I recompiled OpenBLAS and exported a standalone toolchain both for arm64 Here are the following changes I had to make Use the following values for CC and CXX QUESTION mhard float and lm hard are not supported by the gcc g compilers for arm64 Is removing them the correct approach or should they be replaced by something I'm not aware of,,"piiswrong,antinucleon,piiswrong",2017-01-05 23:48:01,2017-01-23 05:49:34
IS,Amalgamation for Android broken in v0 9 1pre,I tested out amalgamation on v0 9 1pre and could not get it to work My environment amalgamates perfectly in v0 8 0 The initial error I get is mxnet amalgamation dmlc core include dmlc logging h 18 22 fatal error execinfo h No such file or directory After commenting that out I tried to manually change paths and comment out headers not located in the source One issue I had is that nnvm files are linked incorrectly e g include nnvm op h include nnvm include nnvm op h And several elementwise operators were missing from mxnet predict0 cc Eventually amalgamation simply fails without explanation make mxnet predict0 d Error 1 Has something significant change in the requirements for amalgamation Environment info Operating System Android amalgamation MXNet commit hash git rev parse HEAD 84e5155fc26563ef8bebc08f201e8a88d4c3845e,,"piiswrong,piiswrong,piiswrong",2016-12-30 02:26:30,2017-01-23 05:49:53
PR,Keras support,howard0su you need to pull my nnvm fork,,piiswrong,2017-01-17 00:27:33,2017-01-23 06:49:46
PR,Making spelling and general grammar corrections,Did a pass through across all pages of docs site and corrected spelling errors and obvious grammatical issue Spelling issues were commonly condemned by many users,,sandeep-krishnamurthy,2017-01-23 05:18:10,2017-01-23 06:50:34
IS,what is wrong with the cudnn when my code run on mxnet compiled with USE CUDNN 1,When i compile mxnet with USE CUDNN 0 i ran program on gpu successfully but when i compile mxnet with USE CUDNN 1 the same program failed to run In the meantime the same program ran successfully on cpu cuda version 7 5 18 cudnn version 5 0 5 mxnet version 0 7 What is wrong with the cudnn can u help me,,,2017-01-23 04:08:28,2017-01-23 08:49:44
PR,Add TensorBoard tutorial link to index,piiswrong Add the TensorBoard is link here,,zihaolucky,2017-01-24 02:36:49,2017-01-24 05:24:45
IS,how can i use neural style on ios,I am run an mxnet example on ios but it can only use predict The API has four functions could you tell me how to add new APIs and use backward on ios,,,2017-01-20 09:47:37,2017-01-24 06:41:01
IS,Unknown CMake command detect cuDNN,,,,2017-01-24 08:23:14,2017-01-24 08:47:54
PR,Additional shape info should generate warning,it is harmless,,"howard0su,piiswrong",2017-01-24 09:27:56,2017-01-24 17:17:54
PR,add docs get started index zh md,Add a docs get started index zh md which index md translated into Chinese,,dkleikesa,2017-01-24 06:58:02,2017-01-24 17:18:35
PR,fix,,,"piiswrong,howard0su,piiswrong,howard0su",2017-01-24 21:32:01,2017-01-24 21:32:26
PR,Fix voc ap issue in rcnn test path,Fixes issue in voc ap where np concatenate expects a tuple I encountered an issue when running python test py and this fixes the issue allowing me to evaluate my trained model,,"krosaen,piiswrong,precedenceguo",2017-01-24 20:23:49,2017-01-25 05:39:57
IS,scala src c api c api symbolic cc 385 InferShapeKeyword argument name label not found,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux Ubuntu MXNet version latest from source Error Message My trainIter shape is 3022 3 204 204 and its data shape is data 32 3 204 204 label 32 8 From 1263 I understand that the name of the last layer should match the one from the iterator However I see no way to set the name of the labels in the NDArrayIter API I cannot change the name of the last softmax layer to label because label is automatically added to the name So if I name it label it ends in label label How can I make these two names match Is not the name parameter missing from the NDArrayiter API,,"benqua,yzhliu,benqua,yzhliu,benqua,Ldpe2G,benqua,Ldpe2G",2017-01-21 13:23:59,2017-01-25 09:54:15
PR,Scala Add default name to NDArrayIter data and labels,Add dataName and labelName as parameter to the NDArrayIter constructor Fix 4755,,"benqua,Ldpe2G,yzhliu,benqua",2017-01-23 20:51:59,2017-01-25 09:54:15
PR,OP Negative axis begin end and end None for slice axis,Support negative axis begin end and end None for slice axis Should be continuing,,"sxjscience,sxjscience,sbodenstein",2017-01-23 14:33:38,2017-01-25 14:29:29
PR,New AddTakeGradLargeBatch and SortByKey for GPUs,,,"ap-hynninen,piiswrong,ap-hynninen,ap-hynninen,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,sxjscience,ap-hynninen,ap-hynninen,sxjscience,piiswrong",2017-01-23 23:39:53,2017-01-25 17:04:07
PR,Jenkins notebook test,Jenkins nightly test for notebook It will test notebook in mid night and send report emails Currently only two notebooks are tested GPU support for docker image and notebook cleaning are required before all the notebooks can be tested,,"kevinthesun,piiswrong,kevinthesun,piiswrong",2017-01-25 01:17:27,2017-01-25 17:04:50
PR,Doc,Translate all documents in docs get started folder to chinese The translated documents are in docs zh get started,,dkleikesa,2017-01-25 11:45:13,2017-01-25 17:27:08
PR,Modify the caffe converter to use the new api,Compatible with python2 3 Specification naming,,"yajiedesign,piiswrong",2017-01-13 01:59:05,2017-01-25 18:14:39
PR,Update viz graph for R,Fix the bugs associated with DiagrammeR by using visNetwork Add the data shapes similar to python visualization MLP Conv and RNN architectures tested,,"jeremiedb,thirdwing,jeremiedb,piiswrong,thirdwing,hetong007",2017-01-23 10:39:40,2017-01-25 22:03:07
PR,Support incomplete shape in infershape,tqchen,,"piiswrong,taliesinb,piiswrong",2017-01-07 21:21:47,2017-01-26 01:13:26
PR,Remove for loop in roi pooling operator 4752,Remove the unnecessary for loop,,"piiswrong,precedenceguo,Godricly,sxjscience,Godricly,sxjscience,sxjscience",2017-01-23 09:57:09,2017-01-26 04:17:16
IS,Simplest way to get actvations of certain layer during training,Hello I would like to make a picture of the activations of a certain layer during training after every batch Let is say I would make a batch end callback and then do something and then dump the activations as an image to disk What would be the easiest way to go about this Is this even possible If not why not,,"piiswrong,sxjscience",2017-01-25 22:32:50,2017-01-26 07:38:50
PR,Use shallow submodule for cub,Follows 38895397,,"sxjscience,sxjscience,sxjscience",2017-01-26 06:52:06,2017-01-26 08:22:23
PR,Imporve two Ops,,,howard0su,2017-01-26 07:12:25,2017-01-26 10:12:59
PR,Fix cmake for cub,,,"sxjscience,sxjscience",2017-01-26 08:41:53,2017-01-26 12:14:57
PR,Add docs get started centos setup md,According to our painful experience installing MXNet on CentOS needs much more effort and time than Ubuntu So I add a doc and a FAQ list for it Add install guide for CentOS Address a issue caused by old opencv 2 4 lib during installation,,"qiyuangong,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,qiyuangong,qiyuangong,piiswrong,qiyuangong",2017-01-24 07:57:03,2017-01-26 17:20:40
PR,Keras add BatchNorm data gamma argument,fix symbol py getitem error,,"yajiedesign,piiswrong",2017-01-24 02:51:15,2017-01-26 21:13:29
PR,pip script for making python wheel for mxnet CPU only version,piiswrong This is continuation of 4754 It accomplishes static linking of opencv and openblas and their dependencies to mxnet The build is tested on ubuntu and amazon linux Installation is tested on ubuntu Cross os arch platform build is probably not gonna work Continuing from 4754 code wise potential next steps are 1 More testing on different platforms More auto detection of tools and safeguard 2 Also the currently supported package managers that wheel build can depend on are apt get and yum This can be extended if such need arises 3 looking into and see if it is possible to share builds across linux platforms In addition we can 1 Once tested upload to PyPI 2 Set up auto builds and auto uploads and get our badges for them 3 Anaconda anyone 4 Update doc,,"szha,piiswrong,piiswrong,piiswrong,szha",2017-01-26 23:53:39,2017-01-27 01:08:58
PR,OP Enable addto for deconvolution,AddTo support for deconvolution layer,,sxjscience,2017-01-26 17:40:44,2017-01-27 02:38:49
PR,Clean add test environments and scripts,Add Dockerfile for Ubuntu 14 04 GPU test environment and edited the test script for it Cleaned up Dockerfile and test script for AML Builds and tests from scripts are passing in the test environments Need to merge these in to trigger builds tests on PR merge,,"lxn2,piiswrong,sxjscience,lxn2",2017-01-26 18:28:10,2017-01-27 06:46:10
PR,OP add BilinearSamplingOp and GridGeneratorOp,BilinearSamplingOp BilinearSamplingOp has two input data and grid It apply bilinear sampling to data according to grid Mathematical detail output batch channel y dst x dst G data batch channel y src x src x dst y dst enumerate all spatial locations in output x src grid batch 0 y dst x dst y src grid batch 1 y dst x dst G denotes the bilinear interpolation kernel Users can use GridGeneratorOp to generate grid Users can also design CustomOp to manipulate grid I guess the CustomOp would not be time comsuming since it only involves 2D grid calculation GridGeneratorOp It support affine and warp If transform type is affine input data should be affine matrix batch 6 If transform type is warp input data should be optical flow batch 2 Y X,,"dsqx71,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,piiswrong,piiswrong,dsqx71,sxjscience,sxjscience,sxjscience,sxjscience,piiswrong,piiswrong,piiswrong,sxjscience,piiswrong",2017-01-19 11:09:55,2017-01-27 18:34:16
IS,training speed slow down after 1 epoch,Environment info Operating System ubuntu 14 04 Compiler gcc 4 8 Package used Python R Scala Julia Python MXNet version 0 9 2 If you are using python package please provide Python version and distribution Anaconda 2 7 11 Error Message I'm using cnn lstm ctc to do ocr task and my training dataset contains 600000 images my dataiter prefetchiter read data batch and do some transforms on ssd when training The first epoch can reach Could anyone offer me some help Is the problem with my ssd or something,,piiswrong,2017-01-25 16:36:52,2017-01-28 04:58:04
IS,Configure AttoBot for MXNet jl,Could someone with admin permission of the dmlc org help setting up the attobot for our MXNet jl repo It is a tool to help automating Julia package releasing The setup should be straightforward as stated in the README One just need to login with the dmlc managing permission and add that repo at this URL,,"pluskid,piiswrong,pluskid",2017-01-28 14:45:48,2017-01-28 22:05:52
PR,OP add BilinearSamplingOp and GridGeneratorOp,Continuing,,"sxjscience,piiswrong,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,piiswrong,piiswrong,sxjscience,sxjscience",2017-01-27 18:33:22,2017-01-29 04:07:33
IS,Amalgamation on iOS,I generate the mxnet predict all cc and add it to WhatIsThis iOS project which throws the errors Unknown type name ' m128' Use of undeclared identifier ' mm set1 ps' I can find no solution about sse2 on iOS but what should I do for this Environment info Operating System OSX MXNet version 0 9 3,,piiswrong,2017-01-28 09:26:00,2017-01-29 08:50:05
PR,scala package remove confusing warning logging,remove confusing warning log this line and exception info is actually unnecessary considering that the loader will continue working and throw an exception after all attempts fail eventually,,"CodingCat,CodingCat,CodingCat",2017-01-30 03:18:17,2017-01-30 05:25:06
PR,scala package fix typo in scala README md,while this 4m does not affect the correctness it is to be consistent with other examples like,,"CodingCat,CodingCat,CodingCat",2017-01-30 03:07:42,2017-01-30 05:26:29
PR,scala package add apache license header and corresponding scalastyle config entry,IIRC this is a must have after being incubator project,,"CodingCat,piiswrong,CodingCat,piiswrong,CodingCat,CodingCat,CodingCat",2017-01-30 05:05:26,2017-01-30 07:05:18
IS,Pretrained models are missing on the website,On the web site there are only few models VGG NIN SqueezeNet and other pretrained models were missing,,piiswrong,2017-01-25 09:47:42,2017-01-30 11:05:55
IS,Difficulty training RCNN on other datasets adhering to VOC format RPNL1Loss nan,I'm having a hard time training RCNN on other datasets besides VOC Our approach is to convert our dataset to VOC format e g the expected directory structure xml files and then map this dataset into the docker container expects the dataset to live More details about our container and how we run it here This approach works when using VOC itself but when we pass it our dataset which we had successfully trained a much older version of MXNet RCNN on we immediately get RPNL1Loss nan and the training fails to converge We also tried training on KITTI in voc format with the same issues Note that what we have been able to do is to test evaluate a VOC trained model on the KITTI in VOC format so that points to our conversion process being ok So what I'm wondering if there is anything I might look into that could make this work It would be very convenient to be able to use the existing PascalVOC image database with any VOC like dataset so as not to have to write a custom loader as with the COCO example One things we investigated we thought that having a different number of classes could have caused the issue our dataset only has car and person as classes Modifying the source file and the NUM CLASSES configuration so that it only expected these two classes did not seem to make any difference On further consideration it seems like as long as the classes in the dataset are a subset of the original VOC dataset classes it should work just fine If anyone has any tips or places to investigate please advise I will report back if we get to the bottom of the issue as I think having a reproducible way to train and evaluate on datasets in VOC like format would be useful to many,,"krosaen,krosaen",2017-01-30 17:30:09,2017-01-30 20:21:09
PR,fix reshape for 0 shape,,,piiswrong,2017-01-31 01:13:25,2017-01-31 05:50:40
PR,fix leakyrelu shape,,,piiswrong,2017-01-31 07:52:40,2017-01-31 16:59:21
IS,No module named bbox when running faster cnn exampl,trying to reproduce the faster rcnn tutorial as in running python train alternate py gpus 0 yields this error Traceback most recent call last File train alternate py line 7 in module from rcnn tools train rpn import train rpn File mnt gelu mxnet example rcnn rcnn tools train rpn py line 7 in module from symbol import File mnt gelu mxnet example rcnn rcnn symbol init py line 1 in module from symbol vgg import File mnt gelu mxnet example rcnn rcnn symbol symbol vgg py line 2 in module import proposal File mnt gelu mxnet example rcnn rcnn symbol proposal py line 11 in module from rcnn processing bbox transform import bbox pred clip boxes File mnt gelu mxnet example rcnn rcnn processing bbox transform py line 2 in module from cython bbox import bbox overlaps cython ImportError No module named bbox I have tried searching on bbox module but could not find it anywhere My mxnet is compiled in ubuntu 16 with cuda and cdnn support,,"Gelu74,piiswrong,Gelu74",2017-01-31 18:28:34,2017-01-31 20:48:38
PR,Fix RCNN pascal evaluations for float bboxes and empty classes,A couple of small fixes 1 Allow for floating point bounding box predictions when reading in evaluations The provided test rcnn function outputs results as floats perhaps only when evaluating on VOC like datasets with different image dimensions 2 Allow for zero detections of some classes during evaluation This is handy when evaluating on another dataset in VOC like format with a subset of the original VOC classes These fixes have been necessary when training and evaluating on non VOC datasets in VOC format as described here,,"krosaen,piiswrong,precedenceguo,krosaen,precedenceguo,krosaen",2017-01-31 17:40:07,2017-02-01 18:02:29
PR,Adding a tutorial template jupyter notebook to enable easy and standard contributions,Contributed by Ann Becherer Adding a tutorial template notebook This enables easy contribution by users and also make all tutorials look standard and better Next step will be to modify all existing tutorials at least most important tutorials to this format,,sandeep-krishnamurthy,2017-02-01 00:31:08,2017-02-01 18:12:54
IS,Installing MXNet on Windows 7,I have been trying to install MXNet on Windows 7 in RStudio but I keep finding new issues I can not fix I have tried to run install packages drat repos drat addRepo dmlc install packages mxnet And I get the following messages in the RStudio Console Sorry if my question is too basic I'm a newby and been searching the web for some time now and trying things with no success Thanks Ainhoa,,thirdwing,2017-02-01 09:59:56,2017-02-01 19:19:05
IS,Distributed computation requires CUDA for non CUDA MxNet version,Environment info Operating System Debian Jessie x64 Hi I have not examined the issue deeply yet but maybe some contributors may notice the problem easily When I submit a command similar to below launch py n X launcher ssh H hosts python myexperiment py kv store dist sync the error below is raised However I have compiled MxNet with USE CUDA 0 and do not use any GPU in my code Why does it try to find GPU I have tried dist async still the same problem I have tried removing nodes with GPU still the same problem I have tried removing nodes with GPU and decreasing the number of the nodes X This time it worked without any problem However when I increase X again the same problem What should I do As a note when I do not use launch py my code works without any problem Error NVIDIA no NVIDIA devices found Stack trace home mkarakoc private frameworks mxnet dmlc core include dmlc logging h 235 21 34 52 src storage storage cc 38 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Traceback most recent call last File RunDeepAutoencoder py line 179 in module main File RunDeepAutoencoder py line 153 in main ae model layerwise pretrain File autoencoder py line 315 in layerwise pretrain File local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module base module py line 362 in fit File local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet module module py line 356 in init optimizer File local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet model py line 82 in initialize kvstore kvstore init idx arg params param names idx File local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet kvstore py line 100 in init self handle mx uint len ckeys ckeys cvals File local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError,,mli,2017-01-05 15:50:32,2017-02-01 20:04:53
PR,refactor cast,ap hynninen You can do now,,"piiswrong,ap-hynninen,piiswrong",2017-01-31 19:12:23,2017-02-02 04:43:09
IS,Enable default context,For now ctx is a required argument in symbol bind How about let it use current context in default instead of passing this argument every time This will also make with syntax possible,,"ZihengJiang,jermainewang,ZihengJiang",2017-01-29 20:36:34,2017-02-02 11:14:50
PR,Fix typo in error message,incompete should be incomplete,,howard0su,2017-02-02 06:16:26,2017-02-02 17:56:47
PR,fix broken url,,,,2017-02-02 13:35:58,2017-02-02 17:57:17
IS,Installing mxnet on Windows 10 through RStudio,Hi Thanks for your reply on previous post I see it has been closed but I tried to run msnet on my personal laptop which is 64 bit Windows 10 and I get the following errors The last line is what I have tried after reading similar issues in StackOverflow As you can see it is not working for me as the function combine edges does not appear at all Can you help me I'm beginning to think I'm cursed with the mxnet package Thanks,,,2017-02-02 07:12:13,2017-02-02 22:22:43
PR,call mkdir modified,it seems this is the correct way to call os mkdir Ubuntu 16 04 python 2 7,,hyqLeonardo,2017-02-01 11:20:08,2017-02-02 23:50:35
IS,src symbol symbol cc 155 Symbol InferShapeKeyword argument name data not found,I use AWS EC2 instance p2 16xlarge with 'Deep Learning AMI Amazon Linux 1 5 MXNet with MKL Support ami 7cb9896b ' Python version and distribution Python 2 7 12 default Sep 1 2016 22 14 00 GCC 4 8 3 20140911 Red Hat 4 8 3 9 on linux2 Error Message model fit X data train eval metric mx metric np Perplexity batch end callback mx callback Speedometer batch size 20 epoch end callback mx callback do checkpoint obama 22 25 36 home ec2 user src mxnet dmlc core include dmlc logging h 235 22 25 36 src symbol symbol cc 155 Symbol InferShapeKeyword argument name data not found Candidate arguments 0 data 0 1 embed weight 2 l0 i2h weight 3 l0 i2h bias 4 l0 init h 5 l0 h2h weight 6 l0 h2h bias 7 l0 init c 8 l1 i2h weight 9 l1 i2h bias 10 l1 init h 11 l1 h2h weight 12 l1 h2h bias 13 l1 init c 14 l2 i2h weight 15 l2 i2h bias 16 l2 init h 17 l2 h2h weight 18 l2 h2h bias 19 l2 init c 20 cls weight 21 cls bias 22 label 0 23 data 1 24 label 1 25 data 2 26 label 2 27 data 3 28 label 3 29 data 4 30 label 4 31 data 5 32 label 5 33 data 6 34 label 6 35 data 7 36 label 7 37 data 8 38 label 8 39 data 9 40 label 9 41 data 10 42 label 10 43 data 11 44 label 11 45 data 12 46 label 12 47 data 13 48 label 13 49 data 14 50 label 14 51 data 15 52 label 15 53 data 16 54 label 16 55 data 17 56 label 17 57 data 18 58 label 18 59 data 19 60 label 19 61 data 20 62 label 20 63 data 21 64 label 21 65 data 22 66 label 22 67 data 23 68 label 23 69 data 24 70 label 24 71 data 25 72 label 25 73 data 26 74 label 26 75 data 27 76 label 27 77 data 28 78 label 28 79 data 29 80 label 29 81 data 30 82 label 30 83 data 31 84 label 31 85 data 32 86 label 32 87 data 33 88 label 33 89 data 34 90 label 34 91 data 35 92 label 35 93 data 36 94 label 36 95 data 37 96 label 37 97 data 38 98 label 38 99 data 39 100 label 39 101 data 40 102 label 40 103 data 41 104 label 41 105 data 42 106 label 42 107 data 43 108 label 43 109 data 44 110 label 44 111 data 45 112 label 45 113 data 46 114 label 46 115 data 47 116 label 47 117 data 48 118 label 48 119 data 49 120 label 49 121 data 50 122 label 50 123 data 51 124 label 51 125 data 52 126 label 52 127 data 53 128 label 53 129 data 54 130 label 54 131 data 55 132 label 55 133 data 56 134 label 56 135 data 57 136 label 57 137 data 58 138 label 58 139 data 59 140 label 59 141 data 60 142 label 60 143 data 61 144 label 61 145 data 62 146 label 62 147 data 63 148 label 63 149 data 64 150 label 64 151 data 65 152 label 65 153 data 66 154 label 66 155 data 67 156 label 67 157 data 68 158 label 68 159 data 69 160 label 69 161 data 70 162 label 70 163 data 71 164 label 71 165 data 72 166 label 72 167 data 73 168 label 73 169 data 74 170 label 74 171 data 75 172 label 75 173 data 76 174 label 76 175 data 77 176 label 77 177 data 78 178 label 78 179 data 79 180 label 79 181 data 80 182 label 80 183 data 81 184 label 81 185 data 82 186 label 82 187 data 83 188 label 83 189 data 84 190 label 84 191 data 85 192 label 85 193 data 86 194 label 86 195 data 87 196 label 87 197 data 88 198 label 88 199 data 89 200 label 89 201 data 90 202 label 90 203 data 91 204 label 91 205 data 92 206 label 92 207 data 93 208 label 93 209 data 94 210 label 94 211 data 95 212 label 95 213 data 96 214 label 96 215 data 97 216 label 97 217 data 98 218 label 98 219 data 99 220 label 99 221 data 100 222 label 100 223 data 101 224 label 101 225 data 102 226 label 102 227 data 103 228 label 103 229 data 104 230 label 104 231 data 105 232 label 105 233 data 106 234 label 106 235 data 107 236 label 107 237 data 108 238 label 108 239 data 109 240 label 109 241 data 110 242 label 110 243 data 111 244 label 111 245 data 112 246 label 112 247 data 113 248 label 113 249 data 114 250 label 114 251 data 115 252 label 115 253 data 116 254 label 116 255 data 117 256 label 117 257 data 118 258 label 118 259 data 119 260 label 119 261 data 120 262 label 120 263 data 121 264 label 121 265 data 122 266 label 122 267 data 123 268 label 123 269 data 124 270 label 124 271 data 125 272 label 125 273 data 126 274 label 126 275 data 127 276 label 127 277 data 128 278 label 128 infer shape error Arguments softmax label 32 129 l2 init c 32 512 l0 init c 32 512 l1 init h 32 512 l0 init h 32 512 l2 init h 32 512 data 32 129 l1 init c 32 512 Traceback most recent call last File stdin line 4 in module File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet model py line 763 in fit self init params dict data provide data data provide label File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet model py line 487 in init params arg shapes aux shapes self symbol infer shape input shapes File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet symbol py line 459 in infer shape return self infer shape impl False args kwargs File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet symbol py line 526 in infer shape impl ctypes byref complete File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 22 25 36 src symbol symbol cc 155 Symbol InferShapeKeyword argument name data not found Candidate arguments 0 data 0 1 embed weight 2 l0 i2h weight 3 l0 i2h bias 4 l0 init h 5 l0 h2h weight 6 l0 h2h bias 7 l0 init c 8 l1 i2h weight 9 l1 i2h bias 10 l1 init h 11 l1 h2h weight 12 l1 h2h bias 13 l1 init c Minimum reproducible example I used exactly the same code here Note that if I run jupter notebook the code runs without error But the code errors when I try to run the code in python Steps to reproduce Run the example code in a standalone python session The following step gives the error model fit X data train eval metric mx metric np Perplexity batch end callback mx callback Speedometer batch size 20 epoch end callback mx callback do checkpoint obama The above step generates the error,,,2017-02-02 23:25:09,2017-02-03 06:39:27
PR,added rpi setup instructions,updated NNVM makefile to remove x86 specific msse flag and added Raspberry Pi build instructions for MxNet waiting on amalgamation fixes to create build script,,"arank,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,arank,arank,arank,arank,arank,arank,arank,arank,arank,arank",2017-01-17 01:49:13,2017-02-03 20:04:16
PR,R roxygen2 is broken close 4870,,,thirdwing,2017-02-03 21:38:13,2017-02-04 03:34:37
IS,Travis CI gave out multiple FAILED result on R test preventing PR,Travis CI recently gave out multiple build failure it is very possible that Travis CI is R environment itself is not maintained well this happened in about recent 5 PRs or more,,"piiswrong,sandeep-krishnamurthy,thirdwing,thirdwing",2017-02-03 18:01:48,2017-02-04 03:34:43
IS,Ca not install least version R package in Ubuntu,Anyone have same issue I tried on two machines,,"thirdwing,thirdwing,thirdwing",2017-02-03 11:12:41,2017-02-04 03:36:34
PR,RMSProp Update as NNVM op,The PR implements RMSProp as an NNVM op making it available to all language bindings,,"sbodenstein,piiswrong,sbodenstein",2017-02-03 21:17:56,2017-02-04 07:44:01
PR,Fix lots of typos in documentation,,,jonsafari,2017-02-03 23:36:30,2017-02-04 07:44:45
PR,WIP Tentative RNN Interface,leopd,,"piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,leopd,leopd,leopd,leopd,leopd,leopd,leopd,sxjscience,piiswrong,pluskid,sxjscience,piiswrong,tdomhan,sxjscience,piiswrong,pluskid,piiswrong,pluskid,piiswrong,sxjscience,pluskid,piiswrong,sxjscience",2017-01-10 00:14:51,2017-02-04 08:23:24
IS,Unstable Validation dataset Logloss,hi all when I train network I come across a problem that the Validation dataset is logloss is sometimes large and sometimes is small The Validation dataset is logloss is as followed For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2016-12-21 03:48:06,2017-02-04 12:21:48
PR,Accuracy Function For Speeding Up LSTM CTC Convergence,When processing Sequence Tagging problems an accuracy func which is too strict is not convergence friendly This accuracy function is from the solution of a basic leetcode problem LCS Largest Common Subsequence Original All Correct Or Nothing accuracy function may takes 7x times or more long to achieve certain accuracy,,"piiswrong,xlvector,xlvector",2017-02-03 05:40:32,2017-02-04 19:23:25
IS,ImportError cannot import name rnn,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System macOS Sierra Compiler Apple LLVM version 8 0 0 clang 800 0 42 1 Package used Python R Scala Julia Python Error Message After running my program I get this error Steps to reproduce 1 git clone mxnet recursive 2 cd mxnet setup utils 3 bash install mxnet osx python sh After running the steps I know I installed mxnet because of the output SUCCESS MXNet test passed SUCCESS MXNet is successfully installed and works fine Any ideas what I'm doing wrong missing Thanks,,"fhieber,piiswrong",2017-02-04 21:21:26,2017-02-04 22:09:55
PR,Refactor initializer,Conflicts nnvm,,piiswrong,2017-01-11 22:14:21,2017-02-05 07:04:43
IS,Compare the performace with other tools on LSTM task,This link shows the benchmark of some DL tools I m wondering the reason for the performance on LSTM task which is not good as MXNet does on CNN task As has said before on Weibo one of the reason is that the kernel fusion is not implemented I also found a repo about kernel fusion Does anyone else have other comments or then we can make the roadmap of improving the performance of LSTM task,,"pineking,piiswrong,pineking,piiswrong,piiswrong",2017-02-04 02:10:42,2017-02-05 08:06:01
IS,Googlenet and alexnet symbol files have been mixed up with in the example,I think the following two files have been mixed up,,,2017-02-05 03:40:13,2017-02-05 09:05:27
IS,Amalgamation on Android failed,Environment info Operating System Ubuntu 14 04 64bit Desktop Compiler arm linux androideabi g MXNet version v0 9 3 NDK vesion android ndk r13b Description I cloned the latest version and tried amalgamation but met some troubles I run 'make ANDROID 1' directly and there are two errors First I added define fopen64 std fopen in mxnet predict all cc and fopen64 error is gone but I do not know how to solve others I think this may be my NDK is problem What should I do now I have attached my mxnet predict all cc and nnvm d mxnet predict all cc zip nnvm d zip,,arank,2017-02-05 08:00:46,2017-02-05 14:16:04
IS,Potential Problem with latest R Studio IDE,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows Compiler Package used Python R Scala Julia R MXNet version 0 7 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 7 x64 build 7601 Service Pack 1 locale 1 LC COLLATE English United States 1252 LC CTYPE English United States 1252 LC MONETARY English United States 1252 4 LC NUMERIC C LC TIME English United States 1252 attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 mlbench 2 1 1 deepnet 0 2 neuralnet 1 33 loaded via a namespace and not attached 1 igraph 1 0 1 Rcpp 0 12 9 rstudioapi 0 6 magrittr 1 5 munsell 0 4 3 colorspace 1 3 2 7 R6 2 2 0 brew 1 0 6 stringr 1 1 0 plyr 1 8 4 dplyr 0 5 0 visNetwork 1 0 3 13 Rook 1 1 1 tools 3 3 2 grid 3 3 2 gtable 0 2 0 DBI 0 5 1 influenceR 0 1 0 19 DiagrammeR 0 9 0 htmltools 0 3 5 lazyeval 0 2 0 digest 0 6 12 assertthat 0 1 tibble 1 2 25 gridExtra 2 2 1 RColorBrewer 1 1 2 ggplot2 2 2 1 htmlwidgets 0 8 viridis 0 3 4 rgexf 0 15 3 31 stringi 1 1 2 scales 0 4 1 XML 3 98 1 5 jsonlite 1 2 Error Message Loading required package mxnet Error object combine edges is not exported by 'namespace DiagrammeR' Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Tried to run on both Mac and Windows so it is not the operating system but could be revised R Studio IDE 2 3,,"thirdwing,thirdwing,thirdwing,jeremiedb,thirdwing,thirdwing,jeremiedb",2017-02-02 23:00:10,2017-02-05 18:54:07
IS,New MXNetR version for windows 10 is much more slower,Hi I'm doing a project with MXNetR version for windows 10 and I downloaded the last update version I found that this new version in much more slower than the previous ones Is possible to adress this problemis with a new update mxnet version 0 9 it will be available soon Thanks,,"thirdwing,thirdwing,thirdwing",2016-12-18 22:13:07,2017-02-05 18:56:41
PR,Refresh viz graph,Clean the function description Add back the Grahviz support through DiagrammeR in addition to Vis Additional displayed information,,"jeremiedb,piiswrong,thirdwing,jeremiedb",2017-02-05 19:35:13,2017-02-06 01:20:09
IS,Build fail on macOS Sierra with OpenMP disabled,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System macOS Sierra Compiler clang Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 9cc4d01931bb98b64cd0161be1445832ccd806e6 If you are using python package please provide Python version and distribution 3 6 0 official If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I had OpenMP turned off all the compilations can pass but when building the lib we have Then just set OPENMP 0 in config mk,,jli05,2017-01-29 16:38:16,2017-02-06 01:58:58
IS,Input node is not complete,When I use the model pretrained by myself I get the problem Input node is not complete I have examined the loaded model and the input data but cannot find the problem Does anybody know why,,,2017-02-06 07:54:04,2017-02-06 08:01:21
PR,R switch to Apache 2 0,We have discussed this over emails See L13 as a reference,,thirdwing,2017-02-06 05:42:47,2017-02-06 15:02:58
IS,Platform dependent parsing of args,Consider the symbol It would be very useful to support Inf values for the mask for example this allows SoftmaxActivation to act on variable length sequences Finite masking values are never 100 safe how would you recommend handling this and guaranteeing the same behaviour across platforms,,"sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein",2017-02-03 23:56:17,2017-02-06 19:28:48
PR,increment dmlc core commit,This resolves 4876,,sbodenstein,2017-02-06 13:45:05,2017-02-06 19:28:48
PR,bug fix for python 3 support in python mxnet module sequential module py,Rewrite the implementation for Python 3 support,,,2017-02-06 11:29:58,2017-02-06 19:29:44
PR,Remove use of shfl xor operation to keep compatiblity with sm20,,,ap-hynninen,2017-02-06 17:46:59,2017-02-06 20:03:56
PR,Add repeat and tile ops,,,"reminisce,piiswrong,piiswrong,sxjscience,reminisce",2017-02-03 02:51:40,2017-02-07 00:03:19
PR,Enhance infershape for concat dot,,,piiswrong,2017-02-06 19:53:01,2017-02-07 00:09:16
IS,Documentation for mxnet,I have seen the only documentation for mxnet is somewhere like here But this one is only for brief introduction However if I want to get more specific information such as what are the input parameters of certain operator like convolution or what layers are available where can I find these information Do I need to go check the code directly Thanks,,piiswrong,2017-02-06 22:40:57,2017-02-07 00:12:25
PR,fix amalgamation,,,piiswrong,2017-02-07 01:50:39,2017-02-07 05:36:13
PR,Fix rmsprop on GPU,RMSProp update rmsprop update,,"sxjscience,howard0su,sbodenstein",2017-02-07 02:45:48,2017-02-07 05:37:44
PR,fix ssd demo,,,"howard0su,piiswrong,howard0su,piiswrong,howard0su",2017-02-05 13:38:52,2017-02-07 05:39:24
PR,Do not permit to use gpu context in CPU only build,,,howard0su,2017-02-07 04:06:39,2017-02-07 06:51:06
IS,mxnet cannot support CUDA with computing capability 2 0,Dear All My OS is Arch linux with CUDA 8 0 installed I can use Theano Keras for the gpu computing I cannot use mxnet git for my old femi display card compute capability 2 0 If I use the default config mk I can successfully complie and install mxnet mxnet can work for cpu but not work for my femi gpu and show the following error Cuda kernel failed Error invalid device function Check failed error cudaSuccess 8 vs 0 invalid device function If I added the following code to config mk CUDA ARCH gencode arch compute 20 code sm 20 gencode arch compute 20 code sm 21 I cannnot compile mxnet successfully Please find the following config mk for my compilation Thank your for your help,,"piiswrong,ap-hynninen,taoari,ap-hynninen,taoari,ap-hynninen",2017-02-05 07:12:01,2017-02-07 08:37:57
IS,Question debug mxnet op with gdb,Question How one could debug mxnet operators using python bindings How do you test new operators while developing them Would be happy to hear some advice and best practices I have simple one op code with a single convolution I would like to step in the code of Forward pass What should be done to do that I have tried the method described bellow so far it brings no fruits If there are some mistakes or misunderstanding would be happy if you can point on them Environment info Operating System MacOS Siera Compiler gcc Package used Python MXNet version 0 9 3 Python version and distribution python2 7 Minimum reproducible example,,piiswrong,2017-02-04 13:43:56,2017-02-07 14:45:28
PR,Fix bilinear Upsampling,Also adding test cases,,howard0su,2017-02-07 14:57:26,2017-02-07 17:01:36
PR,fix bug in slice channel op,this PR fixes a bug in the ListOutputs function of the slice channel op Before this fix this function will not produce digits after output but letters and later non printable characters if the number of outputs is larger than 10,,Bartzi,2017-02-07 12:58:39,2017-02-07 17:02:26
PR,Fix resume and begin epoch off by one issue with rcnn train end2end py,When resuming training should load from resume epoch 1 so we can begin at resume epoch Per the docs at python mxnet module base module py So I think in the case of resuming we should be loading from begin epoch 1 I have verified that resuming works as I would expect with this fix in place,,"krosaen,piiswrong,precedenceguo,krosaen",2017-02-06 20:00:53,2017-02-07 20:05:02
IS,low train accuracy when train mnist with gpus,Environment info Operating System centos 7 1 MXNet version v0 9 Python versio v2 7 Error Message When I train mnist dataset with gpus Train accuracy is very low around 0 1 and Train accuracy is almost 0 9 without the gpus Steps to reproduce 1 use gpu train mnist py was not changed could anyone help me please,,"sxjscience,howard0su,sxjscience",2017-02-06 08:58:17,2017-02-08 01:45:38
IS,OSError mxnet python mxnet lib libmxnet so undefined symbol ATL ssyreflect,I installed the latest mxnet following I compiled mxnet with atlas and there is no compilation error and libmxnet so was generated Then I executed python setup py install user in the python directory when I wanted to import mxnet the error was reported OSError mxnet python mxnet lib libmxnet so undefined symbol ATL ssyreflect I have tried 1 make clean all and make again 2 remove the whole directory and git clone a new one Thanks,,piiswrong,2017-02-07 13:28:54,2017-02-08 02:52:39
IS,Initialise module with some fixed parameter,Hello I am using Python 3 5 and trying to pass a fixed non trainable variable 'weights' to a module weight matrix mx nd array 74 8 117 67 data mx sym Variable wouldata' label mx sym Variable isoftmax label' Shape 1 3 float32 weights mx sym Variable 'weights' weights mx sym Reshape data weights shape 3 1 net mx sym FullyConnected data data name 'fc1' num hidden 128 net mx sym Activation data net name arelu1' act type relu net mx sym FullyConnected data net name 'fc2' num hidden 3 net mx sym SoftmaxActivation data net name isoftmax' Shape 1 3 float32 net mx sym MakeLoss mx sym dot mx sym square label net weights mod mx mod Module net fixed param names 'weights' mod fit train dataIterator optimizer params 'learning rate' 0 01 'momentum' 0 9 num epoch 1 arg params 'weights' weight matrix allow missing True This raises the following error 19 37 44 D Program Files x86 Jenkins workspace mxnet mxnet dmlc core include dmlc logging h 300 19 37 44 d program files x86 jenkins workspace mxnet mxnet src operator tensor matrix op inl h 466 dot currently only support 2D 2D array or 1D 1D array 1 3 v s infer shape error Arguments Traceback most recent call last softmax label 1 3 data 1 1 145 10 File C Development Anaconda3 lib site packages IPython core interactiveshell py line 2885 in run code exec code obj self user global ns self user ns File ipython input 89 005b424281de line 13 in module num epoch 1 arg params 'weights' weight matrix allow missing True File C Development mxnet Python mxnet module base module py line 388 in fit for training True force rebind force rebind File C Development mxnet Python mxnet module module py line 345 in bind grad req grad req File C Development mxnet Python mxnet module executor group py line 187 in init self bind exec data shapes label shapes shared group File C Development mxnet Python mxnet module executor group py line 279 in bind exec shared group File C Development mxnet Python mxnet module executor group py line 483 in bind ith exec arg shapes aux shapes self symbol infer shape input shapes File C Development mxnet Python mxnet symbol py line 542 in infer shape return self infer shape impl False args kwargs File C Development mxnet Python mxnet symbol py line 609 in infer shape impl ctypes byref complete File C Development mxnet Python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator dot4 19 37 44 d program files x86 jenkins workspace mxnet mxnet src operator tensor matrix op inl h 466 dot currently only support 2D 2D array or 1D 1D array 1 3 v s It seems as if initialising 'weights' via 'weight matrix' does not work I have tried to initialise 'weights' manually by test mod mx mod Module weights fixed param names weights test mod bind data shapes 'weights' 1 3 1 for training False force rebind True test mod init params arg params 'weights' weight matrix and this seems to work as test mod get params yields the desired outcome 'weights matrix' Could you suggest any solution,,,2017-02-07 18:58:14,2017-02-08 08:59:08
IS,R package poor NAMESPACE missing help pages,I downloaded and compiled mxnet 0 9 4 with GPU support on a server running CentOS 7 and built and installed the R package The package loads fine and it seems it also works but there are some changes compared to earlier versions and they do not seem right or at least strange to me The NAMESPACE file only consists of two simple imports but lacks all exports which requires me to prefix all calls into the package with 'mxnet ' This is not only inconvenient it also makes all my previous code and all mxnet tutorials invalid All help pages are gone I understand that the vignettes and online tutorials still provide a lot of information but they never cover all arguments of all R functions so it would be really great if there was some quick reference I do not know whether the two points mentioned above are errors or intended In any case I am curious about the background Thanks in advance U For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux CentOS 7 Package used Python R Scala Julia R MXNet version 0 9 4 If you are using R package please provide R sessionInfo R version 3 3 0 2016 05 03 Platform x86 64 pc linux gnu x86 64 64 bit Running under CentOS Linux 7 Core locale 1 LC CTYPE en US UTF 8 LC NUMERIC C 3 LC TIME en US UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 7 LC PAPER en US UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C 11 LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 mxnet 0 9 4 loaded via a namespace and not attached 1 tools 3 3 0 Rcpp 0 12 9 codetools 0 2 14,,thirdwing,2017-02-08 07:40:25,2017-02-08 14:40:30
PR,Removing two confusing line from CustomMetric,those two line make writing two class metrics impossible such as f measure,,,2017-02-08 07:11:07,2017-02-08 17:12:58
PR,Fix gpu consistency test of upsampling,When it is bilinear the args are up data and up weight,,sxjscience,2017-02-08 04:39:24,2017-02-08 17:24:53
PR,Fix some doc typos,,,kevinthesun,2017-02-08 21:59:53,2017-02-08 22:20:30
IS,dynamical computation graph support,TensorFlow has released its dynamical computation graph library Is there any plan for mxnet to support similar lib,,piiswrong,2017-02-08 10:41:50,2017-02-09 04:37:40
IS,Build Error on Windows of the latest version,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows server 2012R2 Compiler VS 2013 Package used Python R Scala Julia Python 3 MXNet version Or if installed from source commit 30df58cae6cd707c76f09c5703e396dc8b8b2eb7 Build with CUDA 8 0 without CUDNN or OPENCV cmake config image Error Message Error 462 error C1001 An internal error has occurred in the compiler D mxnet nnvm src pass order mutation cc 114 1 full log Minimum reproducible example No idea about how to reproduce it VS reports 464 warnings and 1 error What have you tried to solve it 1 I can successfully build early versions To update to the latest I found that I cannot build it,,piiswrong,2017-02-04 05:05:11,2017-02-09 05:11:05
PR,Fix wine detector page,,,"kevinthesun,kevinthesun",2017-02-09 23:39:00,2017-02-09 23:43:37
IS,Native method not found org dmlc mxnet Predictor createPredictor,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Android MXNet version v0 8 0 Error Message 02 09 17 20 25 722 D dalvikvm 22918 Trying to load lib data app lib com fer 1 libmxnet predict so 0x41dc22a8 02 09 17 20 25 726 D dalvikvm 22918 Added shared lib data app lib com fer 1 libmxnet predict so 0x41dc22a8 02 09 17 20 25 726 D dalvikvm 22918 No JNI OnLoad found in data app lib com fer 1 libmxnet predict so 0x41dc22a8 skipping init 02 09 17 20 25 726 W dalvikvm 22918 No implementation found for native Lorg dmlc mxnet Predictor createPredictor B BII Ljava lang String I J 02 09 17 20 25 726 D AndroidRuntime 22918 Shutting down VM 02 09 17 20 25 726 W dalvikvm 22918 threadid 1 thread exiting with uncaught exception group 0x41779ba8 02 09 17 20 25 727 E AndroidRuntime 22918 FATAL EXCEPTION main 02 09 17 20 25 727 E AndroidRuntime 22918 Process com fer PID 22918 02 09 17 20 25 727 E AndroidRuntime 22918 java lang UnsatisfiedLinkError Native method not found org dmlc mxnet Predictor createPredictor B BII Ljava lang String I J 02 09 17 20 25 727 E AndroidRuntime 22918 at org dmlc mxnet Predictor createPredictor Native Method 02 09 17 20 25 727 E AndroidRuntime 22918 at org dmlc mxnet Predictor init Predictor java 43 02 09 17 20 25 727 E AndroidRuntime 22918 at com fer WhatsApplication onCreate WhatsApplication java 58 02 09 17 20 25 727 E AndroidRuntime 22918 at android app Instrumentation callApplicationOnCreate Instrumentation java 1007 02 09 17 20 25 727 E AndroidRuntime 22918 at android app ActivityThread handleBindApplication ActivityThread java 4355 02 09 17 20 25 727 E AndroidRuntime 22918 at android app ActivityThread access 1500 ActivityThread java 139 02 09 17 20 25 727 E AndroidRuntime 22918 at android app ActivityThread H handleMessage ActivityThread java 1270 02 09 17 20 25 727 E AndroidRuntime 22918 at android os Handler dispatchMessage Handler java 102 02 09 17 20 25 727 E AndroidRuntime 22918 at android os Looper loop Looper java 136 02 09 17 20 25 727 E AndroidRuntime 22918 at android app ActivityThread main ActivityThread java 5028 02 09 17 20 25 727 E AndroidRuntime 22918 at java lang reflect Method invokeNative Native Method 02 09 17 20 25 727 E AndroidRuntime 22918 at java lang reflect Method invoke Method java 515 02 09 17 20 25 727 E AndroidRuntime 22918 at com android internal os ZygoteInit MethodAndArgsCaller run ZygoteInit java 791 02 09 17 20 25 727 E AndroidRuntime 22918 at com android internal os ZygoteInit main ZygoteInit java 607 02 09 17 20 25 727 E AndroidRuntime 22918 at dalvik system NativeStart main Native Method,,,2017-02-09 09:23:51,2017-02-10 03:12:00
PR,Fix some doc grammar issues,,,kevinthesun,2017-02-10 01:40:26,2017-02-10 05:16:14
IS,dlopen failed cannot locate symbol cblas sgemm referenced by libmxnet predict so,Operating System ANDROID Compiler ANDROID NDKr13 using api 21 also 19 I tried to build the mxnet to android using 'make ANDROID 1' and finally get the 'libmxnet predict so' But when I tried to use it in an android project it coulde not run throw this EXCEPTION 'java lang UnsatisfiedLinkError dlopen failed cannot locate symbol cblas sgemm referenced by libmxnet predict so ' How can I solve this problem Did I make some mistakes in compiling MXNet version I have tried the newest vision and the 0 8 0 vision and all got that EXCEPTION,,,2017-02-06 11:07:32,2017-02-10 05:47:35
IS,How to create a symbol with multiple output,I wanted to slice a image into 2 parts and feed them to different operations to do this I have to create a symbol with multiple output but cannot find any examples about it Is there are anyone know about this Thank you very much,,Godricly,2017-02-09 16:50:27,2017-02-10 13:57:50
PR,Fixing double cloning issue in osx install script,1 There was a bug in osx install script MXNet source was cloned 2 times Fixing it 2 There was no manual python package install guide for Python in Ubuntu OSx Amazon Linux Adding it,,"sandeep-krishnamurthy,sandeep-krishnamurthy",2017-02-07 20:31:12,2017-02-10 17:32:06
PR,Add one hot op,This PR implements the operator one hot The one called onehot encode in the current code base will be deprecated,,"reminisce,sxjscience,sxjscience,reminisce,reminisce,sxjscience,sxjscience,reminisce,sxjscience,reminisce,sxjscience",2017-02-08 22:21:32,2017-02-10 18:43:33
IS,How to load ARK files in mxnet,Hi I'm attempting to load some ARK files into mxnet but I'm not exactly sure on how to go about it I did go over the speech demo example but in there there are custom io files that is loading sequences in certain format for LSTMs I'm trying to simply load an ark file for an acoustic model I was wondering if you could point to me exactly what files I can use in the speech demo or a work around on how to go about it Venkatesh,,"yzhang87,yzhang87",2017-02-06 19:18:01,2017-02-10 19:24:42
IS,Problem with mx symbol UpSampling,The document of mx symbol UpSampling was ambiguous With several attempts I found that the following code would produce a result successfully by chance By the way I realized that the weights for bilinear up sampling needs to be defined explicitly however this was not mentioned in the document The codes Is this a bug,,howard0su,2017-02-05 17:34:43,2017-02-11 01:29:04
IS,Ca not download the VGG16 Model from Mxnet model zoo,Hi I am trying to download the VGG16 Model from But I can not find the VGG16 model in the imagenet folder I use the command wget It gives a 404 ERROR,,,2017-02-08 09:17:10,2017-02-12 01:49:17
PR,API docs formatting review changes and scala tutorial added,Made code formatting and spelling correction changes to Python and Scala API docs Added one scala tutorial,,"Roshrini,piiswrong,Roshrini,piiswrong,Roshrini,Roshrini",2017-02-07 17:52:38,2017-02-12 05:46:15
IS,There is no explicit api for operations,Can you provide explicit api for operations I know the mxnet operations' api are automatically generate by code and has the document of operations but as a developer api can not provide the params and the meaning of params every time people write a operation which not saw before should to view official site document this seems a trivial thing also they look like compile errors operations like mx sym Convolution this is also a suggestion thanks,,piiswrong,2017-02-12 04:58:50,2017-02-12 06:45:42
IS,why can not change the name of input variable data and label variable softmax label,when I attempt to change the name of variable from x mx sym Variable wouldata' to x mx sym Variable wouldat' some errors ocur as below ValueError Unknown initialization pattern for dat Default initialization is now limited to weight bias gamma 1 0 and beta 0 0 Please use mx sym Variable init mx init to set initialization pattern,,piiswrong,2017-02-12 04:17:01,2017-02-12 06:46:11
PR,Use MAKE variable for recursive make invocation,piiswrong GNU make recommends the usage of MAKE variable in the case of recursive makes Details can be found here,,szha,2017-02-12 05:38:41,2017-02-12 07:12:16
PR,Scala documentation,Python Scala API docs formatting typo grammar corrections after review changes and scala tutorial added,,"Roshrini,piiswrong",2017-02-12 03:43:22,2017-02-12 07:13:49
IS,New issue with MXNet after installing Anaconda,Environment info Operating System macOS Sierra Compiler Apple LLVM version 8 0 0 clang 800 0 42 1 Package used Python R Scala Julia Python Error After I installed Anaconda MXNet is no longer recognized in my programs I get the error ImportError No module named mxnet Is there a workaround Because I really need Anaconda,,"piiswrong,piiswrong",2017-02-12 04:42:09,2017-02-12 07:17:10
IS,R Installation fails,I have just tried to install the R package and I am getting From R Studio install packages mxnet Installing package into home marco R x86 64 pc linux gnu library 3 2 as lib is unspecified 2017 02 12 11 25 47 Resolving dmlc github io dmlc github io 151 101 112 133 Connecting to dmlc github io dmlc github io 151 101 112 133 443 connected HTTP request sent awaiting response 301 Moved Permanently Location following 2017 02 12 11 25 47 Resolving dmlc ml dmlc ml 192 30 252 153 192 30 252 154 Connecting to dmlc ml dmlc ml 192 30 252 153 80 connected HTTP request sent awaiting response 404 Not Found 2017 02 12 11 25 47 ERROR 404 Not Found From R console install packages mxnet Installing package into usr local lib R site library as lib is unspecified trying URL '' Content type 'text html' length 178 bytes downloaded 9340 bytes Error in getOctD x offset len invalid octal digit The downloaded source packages are in tmp RtmpauR432 downloaded packages Warning message In install packages mxnet installation of package mxnet had non zero exit status My system is Linux Mint 18 1 Serena n l Linux inspiron 4 4 0 53 generic 74 Ubuntu SMP Fri Dec 2 15 59 10 UTC 2016 x86 64 x86 64 x86 64 GNU Linux R version is R version 3 2 3 2015 12 10 Platform x86 64 pc linux gnu 64 bit Running under Linux Mint 18 1 locale LC CTYPE en US UTF 8 LC NUMERIC C LC TIME en US UTF 8 LC COLLATE en US UTF 8 LC MONETARY de CH UTF 8 LC MESSAGES en US UTF 8 LC PAPER de CH UTF 8 LC NAME C LC ADDRESS C LC TELEPHONE C LC MEASUREMENT de CH UTF 8 LC IDENTIFICATION C attached base packages stats graphics grDevices utils datasets methods base loaded via a namespace and not attached drat 0 1 2 tools 3 2 3 SysInfo sysname release Linux 4 4 0 53 generic version nodename 74 Ubuntu SMP Fri Dec 2 15 59 10 UTC 2016 inspiron machine login x86 64 unknown user effective user marco marco R Version platform x86 64 pc linux gnu arch x86 64 os linux gnu system x86 64 linux gnu status major 3 minor 2 3 year 2015 month 12 day 10 svn rev 69752 language R version string R version 3 2 3 2015 12 10 nickname Wooden Christmas Tree Is the problem the download of the package Thanks Regards,,"thirdwing,thirdwing,thirdwing,thirdwing",2017-02-12 10:41:20,2017-02-12 16:15:08
IS,Error after replaced target shape with shape,label mx sym Reshape data label target shape 0 label mx sym Reshape data label shape 0 The error message is below Traceback most recent call last File main py line 30 in module main File main py line 25 in main output size vocab size dropout 0 0 mx ctx CTX File home ec2 user mxnet seq2seq seq2seq py line 29 in init self decoder self build lstm decoder File home ec2 user mxnet seq2seq seq2seq py line 143 in build lstm decoder for training is train File home ec2 user src mxnet python mxnet module bucketing module py line 186 in bind force rebind False shared module None grad req grad req File home ec2 user src mxnet python mxnet module module py line 345 in bind grad req grad req File home ec2 user src mxnet python mxnet module executor group py line 187 in init self bind exec data shapes label shapes shared group File home ec2 user src mxnet python mxnet module executor group py line 279 in bind exec shared group File home ec2 user src mxnet python mxnet module executor group py line 483 in bind ith exec arg shapes aux shapes self symbol infer shape input shapes File home ec2 user src mxnet python mxnet symbol py line 542 in infer shape return self infer shape impl False args kwargs File home ec2 user src mxnet python mxnet symbol py line 609 in infer shape impl ctypes byref complete File home ec2 user src mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator reshape0 22 43 52 src operator tensor matrix op inl h 151 Check failed oshape Size dshape Size 26 vs 260 Target shape size is different to source Target 26 Source 26 10 Stack trace returned 75 entries bt 0 home ec2 user src mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fb1824b14d9 bt 1 home ec2 user src mxnet python mxnet lib libmxnet so ZN5mxnet2op12ReshapeShapeERKN4nnvm9NodeAttrsEPSt6vectorINS1 6TShapeESaIS6 EES9 0x1467 0x7fb182c3c2b7 bt 2 home ec2 user src mxnet python mxnet lib libmxnet so 0x1ddecf7 0x7fb183bb4cf7 bt 3 home ec2 user src mxnet python mxnet lib libmxnet so 0x1de02d2 0x7fb183bb62d2 bt 4 home ec2 user src mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x509 0x7fb183bd0d99 bt 5 home ec2 user src mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fb182fe771e bt 6 home ec2 user src mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x240 0x7fb182fead40 bt 7 home ec2 user src mxnet python mxnet lib libmxnet so MXSymbolInferShape 0x271 0x7fb182fe2bb1 bt 8 opt python3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ffi call unix64 0x4c 0x7fb18c8bc92c bt 9 opt python3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ffi call 0x165 0x7fb18c8bba75 bt 10 opt python3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ctypes callproc 0x283 0x7fb18c8b3ee3 bt 11 opt python3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so 0x967f 0x7fb18c8ab67f bt 12 python3 PyObject FastCallDict 0xa2 0x451182 bt 13 python3 0x53ddf5 bt 14 python3 PyEval EvalFrameDefault 0x2a2c 0x541a5c bt 15 python3 0x53dca1 bt 16 python3 PyFunction FastCallDict 0x14a 0x546c7a bt 17 python3 PyObject FastCallDict 0x1ef 0x4512cf bt 18 python3 PyObject Call Prepend 0xcb 0x4513cb bt 19 python3 PyObject Call 0x60 0x450fa0 bt 20 python3 PyEval EvalFrameDefault 0x3b25 0x542b55 bt 21 python3 0x53dca1 bt 22 python3 PyFunction FastCallDict 0x14a 0x546c7a bt 23 python3 PyObject FastCallDict 0x1ef 0x4512cf bt 24 python3 PyObject Call Prepend 0xcb 0x4513cb bt 25 python3 PyObject Call 0x60 0x450fa0 bt 26 python3 PyEval EvalFrameDefault 0x3b25 0x542b55 bt 27 python3 0x53dca1 bt 28 python3 0x53df9f bt 29 python3 PyEval EvalFrameDefault 0x2a2c 0x541a5c bt 30 python3 0x53dca1 bt 31 python3 0x53df9f bt 32 python3 PyEval EvalFrameDefault 0x2a2c 0x541a5c bt 33 python3 0x53dca1 bt 34 python3 PyFunction FastCallDict 0x14a 0x546c7a bt 35 python3 PyObject FastCallDict 0x1ef 0x4512cf bt 36 python3 PyObject Call Prepend 0x74 0x451374 bt 37 python3 PyObject Call 0x60 0x450fa0 bt 38 python3 0x4c65fb bt 39 python3 0x4bde2a bt 40 python3 PyObject FastCallDict 0xa2 0x451182 bt 41 python3 PyObject FastCallKeywords 0xd4 0x451674 bt 42 python3 0x53ddf5 bt 43 python3 PyEval EvalFrameDefault 0x3c1e 0x542c4e bt 44 python3 0x53dca1 bt 45 python3 0x53df9f bt 46 python3 PyEval EvalFrameDefault 0x3c1e 0x542c4e bt 47 python3 0x53dca1 bt 48 python3 0x53df9f bt 49 python3 PyEval EvalFrameDefault 0x3c1e 0x542c4e bt 50 python3 0x53dca1 bt 51 python3 0x53df9f bt 52 python3 PyEval EvalFrameDefault 0x2a2c 0x541a5c bt 53 python3 0x53dca1 bt 54 python3 PyFunction FastCallDict 0x14a 0x546c7a bt 55 python3 PyObject FastCallDict 0x1ef 0x4512cf bt 56 python3 PyObject Call Prepend 0xcb 0x4513cb bt 57 python3 PyObject Call 0x60 0x450fa0 bt 58 python3 0x4c65fb bt 59 python3 0x4bde2a bt 60 python3 PyObject FastCallDict 0xa2 0x451182 bt 61 python3 PyObject FastCallKeywords 0xd4 0x451674 bt 62 python3 0x53ddf5 bt 63 python3 PyEval EvalFrameDefault 0x3c1e 0x542c4e bt 64 python3 0x53dca1 bt 65 python3 0x53df9f bt 66 python3 PyEval EvalFrameDefault 0x2a2c 0x541a5c bt 67 python3 0x53dca1 bt 68 python3 PyEval EvalCode 0x60 0x53eab0 bt 69 python3 PyRun FileExFlags 0x168 0x426278 bt 70 python3 PyRun SimpleFileExFlags 0xdd 0x42645d bt 71 python3 Py Main 0xd15 0x43a8d5 bt 72 python3 main 0x162 0x41d652 bt 73 lib64 libc so 6 libc start main 0xf5 0x7fb1aa551b15 bt 74 python3 0x41d711,,piiswrong,2017-02-12 22:44:44,2017-02-12 22:55:43
IS,Blocking gradients on a sample by sample basis,Hello I have a multitask network but only have labels for a subset of these tasks for each example Is it possible to block the gradient for the tasks I do not have labels for in mxnet on a sample by sample basis Thanks,,"piiswrong,piiswrong",2016-09-16 22:43:16,2017-02-13 03:38:48
IS,slowness with new code,I know this is vague question but I recently pulled the latest code and all of my models runs half as fast or worse now during training with the same code My last version came from early March I was wondering if something about training has been changed which could explain why this happened I'm using convolutional pooling batch normalization and dense layers with ReLU activations Thanks,,"tornadomeet,tornadomeet",2016-10-19 02:11:22,2017-02-13 03:39:09
IS,Is it possible to split an embedding layer across multiple gpus,I am toying around with a matrix factorization problem where one of my embedding layers has 115 million possible input values and 100 outputs Obviously this causes a memory error I saw the tutorial on how to split a model which has many layers across GPUs but is it possible to split the embedding layer itself across multiple GPUs,,"piiswrong,piiswrong",2017-02-03 20:26:48,2017-02-13 03:43:07
IS,is there a docker container supports data distributed training,,,,2017-01-25 15:32:38,2017-02-13 06:51:24
PR,Improve numerical gradient and fix tests,,,"piiswrong,piiswrong,sbodenstein,piiswrong,sxjscience,piiswrong",2017-02-12 09:15:49,2017-02-13 07:15:00
IS,How to add an attribute in a customized symbol,Imagine I'm creating a customized layer CustSoftmax in python in its forward I want it to have access to some hyper parameter aaa could someone give me an example of how to do this Thanks,,piiswrong,2017-01-26 08:16:04,2017-02-13 07:23:22
IS,Does the docker support distributed computation,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"pineking,pineking,pineking,pineking,eric-haibin-lin,szha",2017-01-22 14:06:27,2017-02-13 07:23:28
IS,lstm rnn io batch size,bucket n batches for i in range len self data bucket n batches append len self data i self batch size self data i self data i int bucket n batches i self batch size,,formath,2017-01-10 01:53:14,2017-02-13 07:23:48
IS,help when i use the bucket io py define my train iter i get this error Check failed from shape to shape operands shape mismatch,2017 01 13 08 36 14 767 Start training with cpu 0 Traceback most recent call last File ipython input 6 3b63cb5d1170 line 16 in module batch end callback mx callback Speedometer batch size 1 File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet model py line 811 in fit sym gen self sym gen File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet model py line 236 in train multi device executor manager load data batch data batch File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet executor manager py line 410 in load data batch self curr execgrp load data batch data batch File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet executor manager py line 257 in load data batch load data data batch self data arrays File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet executor manager py line 93 in load data load general batch data targets File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet executor manager py line 89 in load general d src slice idx copyto d dst File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet ndarray py line 533 in copyto return internal copyto self out other File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet ndarray py line 1225 in unary ndarray function c array ctypes c char p c str str i for i in kwargs values File D Program Files Anaconda2 lib site packages mxnet 0 8 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError MXNetError 08 36 14 D Mxnet windows mxnet master src ndarray ndarray cc 231 Check failed from shape to shape operands shape mismatch,,piiswrong,2017-01-13 00:57:13,2017-02-13 07:23:58
IS,what does the mx sym SliceChannel means,such as slice gates mx sym SliceChannel gates num outputs 4 name t d l d slice seqidx layeridx Does this function slice the gates' dim to 4 little dims such as 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7,,,2017-01-17 02:50:14,2017-02-13 07:24:10
IS,it is easy to occur an out of memory error of Mxnet,By comparison Baidu is paddle plays better at this aspect,,,2017-01-18 05:31:17,2017-02-13 07:24:23
PR,suport MKL ml 02 05 new feature,1 support MXNet including padding pooling 2 verified googlnet bn training Signed off by lingyan lingyan guo intel com,,"glingyan,piiswrong,piiswrong,glingyan,glingyan,piiswrong,glingyan,piiswrong,glingyan,piiswrong,glingyan,glingyan,glingyan,glingyan,glingyan,glingyan",2017-02-13 06:35:26,2017-02-13 17:24:49
IS,How to compile Amalgamation for android with NNPACK,Environment Info Operating System Ubuntu 14 04 64bit Desktop Compiler arm linux androideabi clang MXNet version v0 9 3 NDK vesion android ndk r13b Error Description I have successfully compiled the amalgamation for android with NNPACK The file jni libmxnet predict so is copied to WhatsThis Android project properly however I had an error when running the app dlopen failed cannot locate symbol ZN5mxnet2op16nnpackinitializeE Compiling Steps here is what I have done 1 build android standalone toolchain cd android ndk r13b build tools python make standalone toolchain py arch arm api 21 install dir home pallas android toolchain stl libc 2 compile OPENBLAS for android cd OpenBLAS make clean export PATH PATH home pallas android toolchain bin make TARGET ARMV7 HOSTCC gcc CC arm linux androideabi gcc NOFORTRAN 1 3 compile NNPACK in MY NNPACK ROOT jni Application mk set APP PLATFORM android 21 run ndk build 4 add these to the makefile in amalgamation follows export OPENBLAS ROOT MY OPENBLAS ROOT export NNPACK ROOT MY NNPACK ROOT mxnet itself CFLAGS I MXNET ROOT mxnetroot CFLAGS I MXNET ROOT dmlc core include mxnetroot dmlc core include CFLAGS I MXNET ROOT include mxnetroot include CFLAGS I MXNET ROOT mshadow mxnetroot mshadow CFLAGS I MXNET ROOT nnvm include nnpack CFLAGS DMXNET USE NNPACK 1 CFLAGS DMXNET USE NNPACK NUM THREADS 8 CFLAGS I NNPACK ROOT include LDFLAGS L NNPACK ROOT obj local armeabi v7a LDFLAGS lnnpack lpthreadpool lnnpack ukernels lnnpack reference lgtest lfp16 utils lbench utils lcpufeatures LDFLAGS lnnpack lpthreadpool lnnpack ukernels lcpufeatures nnpack dependence googletest CFLAGS I NNPACK ROOT third party gtest 1 7 0 include CFLAGS I NNPACK ROOT third party gtest 1 7 0 nnpack dependence pthreadpool CFLAGS I NNPACK ROOT third party pthreadpool include CFLAGS I NNPACK ROOT third party FXdiv include other define used CFLAGS DMSHADOW STAND ALONE 1 5 add these to Amalgamation py the line 123 if MXNET USE NNPACK 1 include src operator nnpack nnpack convolution inl h endif MXNET USE NNPACK define fopen64 std fopen 6 nnvm issues follows nnvm amalgamation Makefile line 2 change to DEFS DMSHADOW USE CUDA 0 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE SSE 0 DDMLC LOG STACK TRACE 0 DMSHADOW FORCE STREAM DMXNET USE OPENCV 0 DMXNET PREDICT ONLY 1 DDISABLE OPENMP 1 export CFLAGS std c 11 Wall O3 Wno unknown pragmas funroll loops Iinclude fPIC DEFS 7 compile amalgamation export PATH PATH my android toolchain bin export CC arm linux androideabi clang export CXX arm linux androideabi clang make clean make ANDROID 1 8 copy jni libmxnet predict so and libc shared so to android project Is this a ndk version mismatch problem The libmxnet predict so works fine without NNPACK speedup if I do not add step 3 and 4 I confirm the ndk build is run from android ndk r13b my os is Ubuntu14 04,,,2017-02-10 11:44:30,2017-02-14 00:23:16
IS,Save and evaluate mxnet model between full epochs for finetuning,There was a topic But it seems new interface mx mod Module does not have epoch size argument anymore Is there a way to save model during finetuning,,piiswrong,2017-02-13 00:46:20,2017-02-14 01:51:34
PR,Perl interface for MXNet,Hello all Let me introduce myself I am Sergey Kolychev a Perl programmer from Portland OR First and foremost thank you for writing such an impressive library I really like the design decisions that you guys made Here I am proposing a work in progress on an interface to MXNet for Perl programming language It is not yet full parity with Python interface but I plan to get to full parity in coming weeks This is not yet probably ready to be merged in I am mostly trying to gather your reaction and thoughts about my work My final view for this piece is to become integral seamless part of mxnet repository with a setup as simple as Python one and automated creation of self contained Ubuntu deb packages I am striving to keep my Perl code as close as possible to the Python code in order to be able to easy incorporate changes made in the Python interface I plan to support this project for foreseeable future I used Modern Perl techniques for writing this interface in hopes to make it more readable for a person with no prior Perl knowledge Thank you and please consider having this interface a part of your distribution,,"sergeykolychev,piiswrong,sergeykolychev,piiswrong,sergeykolychev,thirdwing,sergeykolychev,thirdwing,sergeykolychev,sergeykolychev,piiswrong,sergeykolychev",2017-02-13 08:03:38,2017-02-14 03:56:30
IS,Bucketing module prediction,I tried to load my bucketing ocr using bucketing module with saved model The prediction predicts nothing but blank in ctc for all examples even if I use training data for testing The training was doing ok with model converged and saved,,"Godricly,Godricly,tqchen,Godricly,Godricly,Godricly",2017-02-14 02:38:20,2017-02-14 06:30:41
PR,fix L2Normalization,I have not fixed all modes in this OP in my previous PR Now it should be correct,,sxjscience,2017-02-14 11:32:03,2017-02-14 12:38:02
PR,Fix bug in CPU memory allocation,This bug will cause Cannot Free space to a device you have not allocated when normalization parameter of SoftmaxOutput is valid,,"Answeror,piiswrong",2017-02-14 16:44:54,2017-02-14 18:02:03
IS,Exception AttributeError 'NoneType' object has no attribute 'MXNDArrayFree',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-02-15 08:16:46,2017-02-15 08:18:11
PR,Fix output shapes property of BucketingModule,The output shapes of the BucketingModule returns label shapes instead of output shapes One line PR to fix this typo,,fhieber,2017-02-15 08:37:03,2017-02-15 18:57:38
PR,Dmlc core update,piiswrong pls update dmlc core for CPU PHI training,,zhenlinluo,2017-02-15 04:40:16,2017-02-15 18:58:18
PR,fix index bug caused by mx img ImageIter,5016 ImageIter next generates DataBatch with pad equals to minus one which will cause mx mod Module predict break down I fix this bug by just changing one line in image py,,,2017-02-15 03:02:52,2017-02-15 18:59:00
PR,Doc Fix doc of optimizer ops fix doc of dtype,,,"sxjscience,piiswrong,sxjscience",2017-02-14 12:05:13,2017-02-15 19:07:38
PR,Remove full mkl package dependency for blas mkl,piiswrong HI Pls review the changes The purpose is to remove the annoying full mkl dependency when set blas mkl,,"zhenlinluo,piiswrong,zhenlinluo",2017-02-15 23:31:28,2017-02-16 03:54:44
IS,there appear nan in convolution weight,at batch 1 there appear nan in convolution weight at batch 1 And i set clip 5 0 i set monitor and information about weight is printed nan why how can i set extra hyperparameter,,"tornadomeet,tornadomeet",2017-02-14 14:05:23,2017-02-16 07:43:21
IS,some questions about clip and convolution update,Question 1 when computing convolution layer gradient whether gradient become nan Question 2 when gradident is nan does clip work small help welcome thks,,tornadomeet,2017-02-15 09:00:37,2017-02-16 07:44:53
IS,How to use mxnet for android with NNPACK,I'm about to compile mxnet to a single file amagamation for android and I want to use NNPACK on android too but someone told me that currently mxnet do not support NNPACK on android Is that so How can I use mxnet on android with NNPACK,,,2016-12-26 14:21:16,2017-02-16 12:15:50
IS,NDArray precision error,,,dsqx71,2017-02-16 13:26:40,2017-02-16 13:52:41
PR,Scala add Profiler support,follows 3163,,"Ldpe2G,piiswrong,yzhliu,yzhliu,Ldpe2G,yzhliu,Ldpe2G,piiswrong,yzhliu,yzhliu,Ldpe2G",2017-02-02 05:56:07,2017-02-16 15:21:34
PR,add fused rnn cell,sxjscience,,"piiswrong,piiswrong,piiswrong,sbodenstein,sbodenstein,piiswrong",2017-02-13 17:44:32,2017-02-16 19:15:27
IS,Ask Amalgamation with extra operator,Is it possible to include extra operator in amalgamation Operators from mxnet example Does it take Extra Operator path in config mk Thanks,,piiswrong,2017-02-09 10:14:18,2017-02-17 07:24:58
IS,How to create an empty model of specified structure,I have a model of matconvnet and I can get the specific parameters of the model in Python Now I want to run this model in mxnet How can I create an empty model of the same structure in mxnet such that I can fill all the parameters on my own Any help will be appreciatived,,tornadomeet,2017-02-15 02:05:32,2017-02-17 08:43:26
PR,Add where op,This PR implements the operator where in mxnet The operator always takes three arguments condition x and y x and y must have the same dtype Two situations are allowed for their shapes 1 condition x and y have the same shape 2 condition is a vector x and y have the same shape and condition is size is same as x is first dim size,,"reminisce,piiswrong,piiswrong,piiswrong,piiswrong,reminisce,reminisce,reminisce,reminisce,piiswrong,reminisce,piiswrong,piiswrong,piiswrong,piiswrong,reminisce,reminisce,reminisce,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,reminisce",2017-02-12 06:37:18,2017-02-17 18:58:45
PR,Optimized broadcast and reduce CUDA kernels,,,"ap-hynninen,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,ap-hynninen,ap-hynninen,ap-hynninen,ap-hynninen,ap-hynninen,piiswrong,piiswrong,ap-hynninen",2017-02-14 18:35:23,2017-02-17 18:59:02
PR,MKL specific CNN symbol and benchmark for better perf,mli due to MKL does not support default INCLUDE PADDING in pooling layer existing CNN symbol and benchmark will call default mshadow pooling layer for CPU scoring To get best MKL performance for benchmark temporarily add symbol to add pooling convention full to utilize MKL pooling,,"zhenlinluo,piiswrong,piiswrong,glingyan,piiswrong,glingyan,piiswrong,glingyan,piiswrong,glingyan,glingyan,piiswrong,glingyan,piiswrong,glingyan,piiswrong,glingyan,glingyan,glingyan,piiswrong,piiswrong",2017-01-09 23:36:02,2017-02-17 19:09:25
PR,Enable R tests in Jenkins,Add test script to run Jenkins tests in Ubuntu Modify Ubuntu build slave environment to support R Modify existing R test script to be non OSX specific,,"lxn2,piiswrong,piiswrong,thirdwing,lxn2,lxn2,thirdwing,lxn2,lxn2,lxn2",2017-02-10 23:57:19,2017-02-18 06:59:33
PR,update perf number on CPU,piiswrong after pooling feature and other patches merged the perf on CPU is improved a lot,,zhenlinluo,2017-02-17 19:32:59,2017-02-18 06:59:57
PR,bugfix Some valid kernel sizes do not pass 3d conv pooling infer shape checks,I think the following kernel sizes should be valid,,"ckomaki,ckomaki",2017-02-17 17:05:50,2017-02-18 07:00:37
PR,fix BilinearSampler test bug,,,"dsqx71,piiswrong,dsqx71",2017-02-17 08:46:07,2017-02-18 07:02:28
IS,Fail to compile the latest mxnet with GPU,Environment info Operating System CentOS7 Compiler gcc 4 8 cuda7 0 Package used Python R Scala Julia anaconda python 2 7 MXNet version latest Or if installed from source yes Error Message src operator tensor indexing op inl cuh error more than one instance of overloaded function max matched the argument list function max int int argument types are size t size t Is it the problem of CUDA Version My CUDA version is 7,,piiswrong,2017-02-12 15:53:09,2017-02-18 08:22:33
PR,fix speech demo add timit demo,I had fix the speech demo with the latest mxnet and kaldi I also add timit demo script It runs ok in my envirenment,,"vsooda,piiswrong,piiswrong,vsooda,vsooda,piiswrong,vsooda,pluskid,piiswrong,pluskid,vsooda,vsooda,pluskid",2017-02-09 07:04:44,2017-02-18 08:40:01
PR,Op,,,piiswrong,2017-02-18 20:36:44,2017-02-19 02:00:08
PR,scala package generate source jars correctly and a large cleanup of pom files,fix for it seems that there are a lot of conflicts among the plugins This patch is ready for the review There are four major changes here 1 Move the dependency and plugin definition in the root pom xml I found that the submodules have a lot of shared dependencies plugins so it is not necessary to define in dependencyManagement pluginManagement in root pom and reference them in submodules A more efficient structure would be define them directly in root 2 Fix the source jar generated issue It seems that the latest version of maven scala plugin is the cause and I usually use scala maven plugin in my other projects so I replace maven scala plugin with scala maven plugin and everything becomes fine 3 fix some misconfigurations some projects spark example are configured with packaging pom packaging which is incorrect 4 remove some unused duplicate plugins I published jars to my local repo refer the dependency as image and try to check source in IntelliJ image,,"CodingCat,yzhliu,CodingCat,CodingCat,yzhliu,yzhliu,CodingCat,yzhliu,CodingCat,yzhliu,CodingCat,CodingCat,CodingCat",2017-02-08 03:49:29,2017-02-19 06:43:26
PR,fix rmsprop optimizer,Add the missing brackets in RMSProp optimizer,,"loofahcus,sxjscience,loofahcus,sxjscience,sbodenstein,piiswrong,sbodenstein,sxjscience",2017-02-17 08:40:55,2017-02-19 08:22:10
PR,Add ckomaki to CONTRIBUTORS md for 5048,,,ckomaki,2017-02-18 09:01:57,2017-02-19 08:28:57
PR,fix repeat and tile error,,,"yajiedesign,piiswrong",2017-02-08 03:12:51,2017-02-19 08:32:10
PR,auto download data for the cifar10 example file for R,I found that the examples in image recognition do not work out of the box for R This is an attempt to fix part of it,,,2017-02-19 15:32:44,2017-02-19 16:53:40
IS,use the Mutable Module to avoid bucketing,Hi dmlc groups now I want to train a LSTM model for some reasons I must set the batch size 1 that means I will only provide one sentence to the network Now I want to use the MutableModule see mxnet example rcnn rcnn core module to avoid bucketing This is my code simple My question is for each training I must create a new module bind it and do forward backward update will it be extremely slow And is there an better implement I think bucketing is a good idea for create less Module but it seems do not fit my work because the shape of the training data is changing all the time not only the sentence I feed an image to the net in the same time Thanks in advance,,"Godricly,Godricly,Godricly",2017-02-14 14:29:11,2017-02-20 01:13:46
IS,question about share weights,Hi I want to share some weights in my network I tried this code but during the training time the cost GPU memory increase every batch After about 4 batch it up to 8GB and my code collapse so my question is Is it the right way to share weights my input shape changes all the time so I must create a new module bind it is it cause the memory boom Any help will be appreciated,,,2017-02-17 03:16:36,2017-02-20 01:13:55
IS,Request for install script for CentOS,Can anyone please make an install script for CentOS similar to the install scripts present in setup utils directory Thanks,,luoyetx,2017-02-20 04:09:53,2017-02-20 08:13:25
IS,Error in data train object of type 'closure' is not subsettable when trying to run train mnist R,Environment info R sessionInfo R version 3 2 3 2015 12 10 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 16 04 2 LTS locale 1 LC CTYPE en US UTF 8 LC NUMERIC C LC TIME fr FR UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY fr FR UTF 8 LC MESSAGES en US UTF 8 LC PAPER fr FR UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C LC MEASUREMENT fr FR UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 igraph 1 0 1 Rcpp 0 12 9 rstudioapi 0 6 magrittr 1 5 munsell 0 4 3 6 colorspace 1 3 2 R6 2 2 0 brew 1 0 6 stringr 1 1 0 plyr 1 8 4 11 dplyr 0 5 0 visNetwork 1 0 3 Rook 1 1 1 tools 3 2 3 grid 3 2 3 16 gtable 0 2 0 DBI 0 5 1 influenceR 0 1 0 DiagrammeR 0 9 0 htmltools 0 3 5 21 lazyeval 0 2 0 digest 0 6 12 assertthat 0 1 tibble 1 2 gridExtra 2 2 1 26 RColorBrewer 1 1 2 ggplot2 2 2 1 codetools 0 2 14 htmlwidgets 0 8 viridis 0 3 4 31 rgexf 0 15 3 stringi 1 1 2 scales 0 4 1 XML 3 98 1 5 mxnet 0 9 4 36 jsonlite 1 2 mxnet mxnet version 0 9 4 Error Message Please paste the full error message including stack trace Loading required package argparse Loading required package proto Loading required package mxnet Loading required package methods Init RcppArgumentsbatch size data dir kv store lr network num round 128 mnist local 0 05 mlp 10 Error in data train object of type 'closure' is not subsettable Calls train model fit Execution halted Steps to reproduce 1 from ubuntu console Rscript train mnist R 2 3 Notes I have run successfully other examples using the mxnet library from R so i assume it is working properly,,,2017-02-18 11:42:47,2017-02-20 19:18:45
IS,Loading multiple files for training,Hi I am attempting to load multiple ARK files for training but I noticed that the numpy dataIter only takes a single numpy feature matrix and single numpy label vector is it possible to load multiple data sets I could not figure this out so I then attempted to concantataneted all my ARK files and created a single numpy matrix and vector for both features and matrix and ended up with this error even though the shapes are the same include mxnet tensor blob h 742 Check failed this shape Size shape Size TBlob get with shape new and old shape do not match total elements Traceback most recent call last File am py line 52 in module train mx io NDArrayIter featureMat label targetMat batch size 100 File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet io py line 420 in init self data init data data allow empty False default name wouldata' File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet io py line 391 in init data should be NDArray or numpy ndarray TypeError Invalid type ' type 'numpy ndarray' ' for data should be NDArray or numpy ndarray The shapes are the same though 39673722 340 featureMat 39673722 targetVec,,,2017-02-16 02:14:11,2017-02-21 00:36:31
IS,embedding operator is not included in amalgamation java jni,Environment info Operating System OSX Compiler clang Package used Python R Scala Julia Java MXNet version clone from github very recent Error Message 00 00 24 jni mxnet predict all cc 628 00 00 24 jni mxnet predict all cc 22379 Check failed op nullptr Operator Embedding is not registered Exception in thread main org dmlc mxnet MxnetException Failed loading Op test embedding of type Embedding 00 00 24 jni mxnet predict all cc 22379 Check failed op nullptr Operator Embedding is not registered Minimum reproducible example trying to use the java jni amalgamation with the embedding layer,,"freddycct,freddycct",2017-02-20 08:09:59,2017-02-21 01:02:17
IS,Lastest LSTM version,What is the latest version of lstm implementation I'm still using the old one in warpctc example Is there any new implementation after NNVM My image iter can outputs 1600 samples per second while the lstm only able to process 100 per second The gpu is almost idle all the time,,"Godricly,piiswrong,piiswrong,piiswrong,Godricly,eric-haibin-lin,Godricly,Godricly",2017-02-16 03:40:16,2017-02-21 06:17:03
IS,How can I run train alternate py with only 4G or less of the memory of the GPU,Can I make the batch size less or do other things,,,2017-02-20 13:14:15,2017-02-21 08:16:32
PR,WIP Scala Module API Support,Here is what I got for examples scripts module mnist mlp sh,,"yzhliu,Ldpe2G,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,Ldpe2G,CodingCat,CodingCat,yzhliu,CodingCat,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,Ldpe2G,yzhliu,CodingCat,yzhliu,CodingCat,Ldpe2G,Ldpe2G,yzhliu,yzhliu,yzhliu,yzhliu",2017-01-07 16:51:58,2017-02-21 16:08:45
PR,fix test init py crash,Signed off by lingyan lingyan guo intel com,,"glingyan,szha",2017-02-21 05:58:06,2017-02-21 16:59:46
PR,Run tests in Docker as current user,Slave environments are running out of disk space due to artifacts owned by root which is the default user in Docker containers This runs tests as the current user in the host,,"lxn2,piiswrong,lxn2,piiswrong",2017-02-21 05:47:05,2017-02-21 17:18:12
PR,Perl5 interface to MXNet,piiswrong Hello I have addressed the issues raised 1 expanded tests a bit 2 made perl package testable via travis 3 changed copyright to Apache 2 0 license Let me know please if you will need additional changes Thanks,,"sergeykolychev,piiswrong,sergeykolychev,piiswrong,sergeykolychev,piiswrong,sergeykolychev,piiswrong,sergeykolychev,piiswrong,mli,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,piiswrong,sergeykolychev",2017-02-14 21:01:39,2017-02-21 17:20:51
PR,Fix to alamgamation so that JS build target works using latest emscripten,This PR resolves issue 4909 1 Improvements to amalgamation makefile to allow specification of emcc executable in command line This enables usage of the official 'emcc' docker image 2 Improvements to amalgamation python script so that unwanted header files are removed when run in minimum mode This gets rid of error seen in issue 4909 3 Updated amalgamation build instructions for JS target and added information on how to use 'emcc' docker image,,"Piyush3dB,piiswrong,Piyush3dB",2017-02-21 11:30:29,2017-02-22 00:32:02
PR,Fix mx img resize arguments order in ssd example,I found a small bug in example ssd In success case I fixed it,,"higumachan,piiswrong,howard0su",2017-02-20 12:47:31,2017-02-22 00:38:00
PR,Implement uncentered version of RMSProp Rename old version to RMSPropAlex,See issuecomment 280674628,,"leezu,sxjscience,piiswrong,leezu,sxjscience,sxjscience,leezu",2017-02-20 15:33:07,2017-02-22 04:52:48
PR,refactor slice,incomplete shape for broadcast,,"piiswrong,piiswrong,Piyush3dB,piiswrong,Piyush3dB",2017-02-10 17:15:28,2017-02-22 05:42:14
PR,Doc spell checker,MXNet documentation spelling and grammar checker,,"kevinthesun,piiswrong,kevinthesun,piiswrong",2017-02-12 08:20:50,2017-02-22 05:46:57
IS,questions about prediction API,Hi community Recently I'm playing with MXNet and some questions raised during my experiments Q1 I just wonder why size of libmxnet predict so is large than libmxnet so Is it amalgamation version target to have a compact lib Info both are compiled only support CPU openBLAS is used for libmxnet has Opencv no openmp no KVStore no hdfs no s3 platform is Mac OSX 10 12 libmxnet predict so is around 22M and libmxnet so is around 16M Q2 I'm using lstm bucketing py from rnn folder as training script and I added following two lines to save json model parameter but json string could be not parsed by MXPredCreate function of prediction library error code shows mxnet predict all cc 22737 Check failed op nullptr Operator Embedding is not registered Assertion failed predh It seems embedding OP is not compiled in prediction library My question is do we have any workaround to use Embedding layer in prediction library Or do we have any example to show how to load pre trained word embedding vectors using CPP or python Thanks K,,"piiswrong,freddycct,piiswrong,freddycct",2017-02-20 15:54:49,2017-02-22 06:18:21
IS,Amalgamation build not working for JS target,The following amalgamation command It seems emcc requires the macro SSE to be defined otherwise there is an error is reported by the line in emmintrin h The last known working version of amalgamation is in the mxnet js repo is over a year old Has anyone successfully built a more recent MXNet version for the JS target,,"Piyush3dB,Piyush3dB,SlipknotTN,Piyush3dB",2017-02-06 20:49:42,2017-02-22 08:45:03
IS,Upload the usable Android library libmxnet predict so please,Is there anyone have an usable android library which mxnet vision is after v0 8 0 I have tried compile the amalgamation many times but it broken when using in Android app,,"piiswrong,xlvector,xlvector,piiswrong,xlvector",2017-02-08 10:45:04,2017-02-22 09:29:55
IS,predict use c predict api h use the tutorial char lstm model and get error 'The shape information of is not enough to get the shapes',Environment info Operating System win10 Compiler vs2015update3 Package used Python R Scala Julia Python c MXNet version 0 8 0 Or if installed from source yes If you are using python package please provide Python version and distribution 2 7 12 Error Message model Inception obama symbol json 980395 bytes model Inception obama 0002 params 23327710 bytes 11 08 42 F mxnet0 8 mxnet 0 8 0 dmlc core include dmlc logging h 300 11 08 42 F mxnet0 8 mxnet 0 8 0 src c api c predict api cc 158 Check failed sym InferShape arg shapes out shapes aux shapes The shape information of is not enough to get the shapes Assertion failed pred hnd file my cpp line 141 Minimum reproducible example also get the error could anyone help me thx,,,2017-01-12 03:17:29,2017-02-22 09:46:26
PR,Add a note about installing MXNet for Python,Add a note about the Cython version should be 0 23 perhaps for the Python interface Lower version Cython compilers would arise syntax errors when cythonizing ndarray pyx e g closures inside cdef functions are not supported,,xioryu,2017-02-22 10:35:12,2017-02-22 11:18:47
PR,fix name creation in ListArgument functions,This PR fixes a problem similar to the problem already fixed for the slice layer with PR 4920 With this PR all occurrences of static cast char '0' i in the code should be gone and cause no more problems,,Bartzi,2017-02-10 13:50:34,2017-02-22 17:03:27
PR,Importing for all collobarators log on mxnet home page to mxnet repo,Importing all logos used on mxnet home page to dmlc We had a broken NVIDIA logo as it referenced wikipedia which was removed This is important security wise because we were referring to open wiki links Note This should be merged only after uploading attached logo images to,,sandeep-krishnamurthy,2017-02-22 19:54:51,2017-02-22 20:28:04
PR,updating aws logo reference from svg to png,,,sandeep-krishnamurthy,2017-02-22 21:58:57,2017-02-22 22:02:25
IS,multiple inputs fail on amalgamation,piiswrong can you please help me with the java amalgamation I have the following architecture defined in python,,"freddycct,Piyush3dB,freddycct,freddycct,freddycct,freddycct",2017-02-22 02:51:49,2017-02-22 23:46:37
IS,Does MXNET support to train on multi CPU cluster,Issue 185 has same question but looks like it is not resolved Link pointed to in the tracker is broken However I see a similar article at Following this I created an AWS EC2 cluster of t2 micro CPUs using the deeplearning template modified to create cluster of t2 micro CPUs However I see the following issue when trying to run distributed training Steps to Reproduce 1 Create a Cloud Stack on AWS EC2 using CPU only instances using the deeplearning template 2 cd src mxnet example image classification 3 tools launch py n 2 H hosts orig python train mnist py kv store dist sync It seems that MXNET is looking for a GPU whenever distributed training is attempted Can anyone please confirm if distributed training on multiple CPUs is supported at all using MXNET Thanks,,"glingyan,qiyuangong,glingyan,qiyuangong",2017-02-21 18:20:54,2017-02-23 01:34:37
IS,Error raise when compiles on GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System CentOS Tesla K40c Compiler gcc 4 9 2 Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source yes MXNet commit hash git rev parse HEAD bfc75a23aa2bdba7e158ec12afdcdb1658bf060b If you are using python package please provide Python version and distribution Python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 change GCC from 4 7 x to 4 9 x 2 try make it after cmake it 3 compile mxnet 0 7 successfully,,piiswrong,2017-02-22 13:49:42,2017-02-23 02:12:36
PR,Update rnn cell py,fix typo,,"Godricly,Godricly",2017-02-23 02:10:30,2017-02-23 03:20:19
IS,where can I set the environment variables for mxnet,such as MXNET CPU PRIORITY NTHREADS In which file config mk,,,2017-02-22 06:19:07,2017-02-23 05:24:07
IS,fatal error cub device device radix sort cuh No such file or directory,In file included from src operator tensor sort op h 85 0 from src operator tensor indexing op h 24 from src operator tensor indexing op cu 8 src operator tensor sort op inl cuh 10 44 fatal error cub device device radix sort cuh No such file or directory include cub device device radix sort cuh compilation terminated make build src operator tensor indexing op gpu o Error 1,,Soonhwan-Kwon,2017-02-23 02:39:30,2017-02-23 06:43:44
IS,Loading GPU Trained Model on CPU,I load the trained resnet model obtained here on a pre built MXNet 0 7 0 obtained from here 20160531 win2012 x64 gpu 7z I am using Azure Web App which runs Windows and Python 2 7 x64 Initially I can not load the model until I remove all the cudnn lines from the json However after this the model loads but takes a long time to run a prediction and always returns the same result regardless of input image probability 0 007025 class n04351699 suiting I then take the model and symbol file along with the same code and run it on a linux machine in the same way CPU model mx model FeedForward load resnet 152 0 ctx mx cpu And it seems to run fine,,piiswrong,2017-02-17 12:00:20,2017-02-23 11:23:26
IS,Error using MakeLoss or custom loss layer,I try calculate loss using a softmax cross entropy symbol with a MakeLoss symbol as the final output of my own network After start training the system returns following error info Process finished with exit code 136 interrupted by signal 8 SIGFPE However when I replace the softmax cross entropy and MakeLoss with a custom loss layer with input shape batch size num labels return loss value in shape 1 same error occurs Is there any bugs with loss output,,,2016-12-25 13:59:27,2017-02-23 12:10:36
IS,Can I create a custom activation function in the R package,As shown in fiveMinutesNeuralNetwork Rmd I see it is possible to create a custom eval function such as demo metric mae mx metric custom mae function label pred res mean abs label pred return res Is it also possible to create a custom activation function for example sigmoid2 mx activation custom sigmoid2 function w x res 1 exp w x 1 exp w x return res and then add this activation to a given layer as follows data mx symbol Variable data fc1 mx symbol FullyConnected data name fc1 num hidden 128 act1 mx symbol Activation fc1 name h1sigmoid2 act type sigmoid2 Thanks,,,2016-12-01 21:12:02,2017-02-23 16:48:36
PR,example fcn xs fix indent error in image segmentation py,fix indent error in example fcn xs image segmentation py,,,2017-02-10 08:37:24,2017-02-23 19:31:23
PR,Support incomplete shape inference for slice channel and convolution,Useful for convolutional rnn GRU,,"sxjscience,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,sxjscience,piiswrong,piiswrong,sxjscience,sxjscience,piiswrong,sxjscience",2017-02-06 07:12:17,2017-02-24 01:36:45
IS,subgraph in mxnet,I need to construct a symbol with several subgraphs some of them only need to do forward computation For the consideration of gpu memory cost is there any way to forbid a subgraph generating the gradient memory for backward Creating multiple executors for each graph and only do forward pass will be a solution but not very convenient,,piiswrong,2017-02-23 08:16:35,2017-02-24 03:18:58
PR,Fp16 training for Alexnet,Fixes alexnet py python example script introduced fp16 training in alexnet fp16 py fp16 support added to mxnet symbol LRN,,"ap-hynninen,piiswrong,piiswrong,piiswrong,ap-hynninen,ap-hynninen,ap-hynninen,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2017-02-14 18:36:52,2017-02-24 05:47:50
PR,initializer py revised to allow any shape of NDArray and built in cuDNN RNN operator example remove the LSTM bias workaround naming,bucket io py revised to be compatible with python 3 x initializer py revised to support built in RNN operator parameters which has only one shape dimension rnn cell demo py renames LSTM bias workaround name with LSTM parameters and gains better convergence speed The original example rnn cell demo py uses default Xavier as initalizer which relies on variable name cannot initialize LSTM parameters Thus it was renamed to LSTM bias which can be initialized as zero It weakens the converge speed Here I use revised initializer py and introduce class RNNXavier to initialize LSTM parameters I attached the first epoch comparison The old LSTM bias case 2017 01 28 01 56 19 164 Epoch 0 Batch 50 Speed 1081 03 samples sec Train Perplexity 4506 515945 2017 01 28 01 56 25 360 Epoch 0 Batch 100 Speed 1033 19 samples sec Train Perplexity 797 195374 2017 01 28 01 56 31 394 Epoch 0 Batch 150 Speed 1060 61 samples sec Train Perplexity 569 822128 2017 01 28 01 56 37 784 Epoch 0 Batch 200 Speed 1001 64 samples sec Train Perplexity 469 483883 2017 01 28 01 56 43 424 Epoch 0 Batch 250 Speed 1134 96 samples sec Train Perplexity 341 282116 2017 01 28 01 56 49 097 Epoch 0 Batch 300 Speed 1128 55 samples sec Train Perplexity 327 141254 2017 01 28 01 56 54 667 Epoch 0 Batch 350 Speed 1149 03 samples sec Train Perplexity 321 201624 2017 01 28 01 57 00 193 Epoch 0 Batch 400 Speed 1158 18 samples sec Train Perplexity 293 513695 2017 01 28 01 57 04 366 Epoch 0 Train Perplexity 335 999086 2017 01 28 01 57 04 366 Epoch 0 Time cost 51 624 2017 01 28 01 57 07 082 Epoch 0 Validation Perplexity 276 119522 The new LSTM parameters case based on RNNXavier and revised initializer py 2017 01 28 01 54 08 342 Epoch 0 Batch 50 Speed 1272 47 samples sec Train Perplexity 3510 636976 2017 01 28 01 54 14 230 Epoch 0 Batch 100 Speed 1087 30 samples sec Train Perplexity 849 972727 2017 01 28 01 54 20 326 Epoch 0 Batch 150 Speed 1049 99 samples sec Train Perplexity 496 757170 2017 01 28 01 54 25 690 Epoch 0 Batch 200 Speed 1193 19 samples sec Train Perplexity 324 072778 2017 01 28 01 54 31 388 Epoch 0 Batch 250 Speed 1123 19 samples sec Train Perplexity 278 631529 2017 01 28 01 54 37 856 Epoch 0 Batch 300 Speed 989 55 samples sec Train Perplexity 271 496667 2017 01 28 01 54 43 726 Epoch 0 Batch 350 Speed 1090 45 samples sec Train Perplexity 210 853686 2017 01 28 01 54 49 338 Epoch 0 Batch 400 Speed 1140 62 samples sec Train Perplexity 198 847126 2017 01 28 01 54 53 062 Epoch 0 Train Perplexity 186 354118 2017 01 28 01 54 53 063 Epoch 0 Time cost 50 277 2017 01 28 01 54 55 770 Epoch 0 Validation Perplexity 181 052411,,piiswrong,2017-01-27 18:39:34,2017-02-24 05:49:51
PR,WIP Broadcast reduce op refactor Help needed,Current broadcast and reduce ops are not efficient and takes a long time to compile This is an attempt to fix it by using kernels instead of mshadow ops I'm having trouble optimizing the kernels Help from someone with more expertise in cuda will be appriciated,,piiswrong,2016-12-31 23:30:34,2017-02-24 05:50:18
PR,Add negative indexing support to slice axis,One application of this PR is sequences you often want to take all sequence elements but the first one eg in seq2seq training with teacher forcing,,"sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,sbodenstein,piiswrong,sbodenstein,piiswrong,piiswrong,sbodenstein,sbodenstein,piiswrong,tqchen,sxjscience,pluskid,sbodenstein,pluskid,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,piiswrong",2016-11-21 16:05:30,2017-02-24 05:52:43
PR,cython friendly setup py,,,"yajiedesign,piiswrong,yajiedesign,tqchen,yajiedesign,tqchen,tqchen",2017-01-19 01:49:16,2017-02-24 06:00:05
PR,Make the range function compatible to both Python 2 and 3,Changed the xrange to compatible range I noticed this incompatibility when trying the new interface And this was also mentioned in discussion r100542992,,"yajiedesign,sxjscience,yajiedesign,piiswrong",2017-02-10 20:22:15,2017-02-24 06:06:29
PR,Amalgamation JNI bug for Java Android prediction,This bug results in a wrong index array that could cause a severe memory fault when multiple inputs are given to the createPredictor If the program does not crash due to wrong indexing of the arrays in C then the program will throw exception due to shape inference errors,,freddycct,2017-02-22 23:42:03,2017-02-24 06:07:43
IS,What does node row ptr stands for in the symbol json file,Hi does anyone know what the node row ptr list stands for in the created json when the model is saved The nodes defines 33 total nodes for me but the node row ptr contains 36 elements,,"Godricly,Godricly",2017-01-24 02:48:12,2017-02-24 08:00:13
IS,Variable batch size,Hi can we have some feature supporting varaible batch size When doing lstm validation I need to fill those buckets with duplicated examples if the number of example is not enough Alough I can bypass this issue by flating each bucket data with speical designed bucket key is there any more elegant solution,,"Godricly,piiswrong,Godricly",2017-02-10 02:48:12,2017-02-24 08:00:25
IS,LSTMCell unroll error on GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centOS Tesla K40 Compiler gcc4 9 X Package used Python R Scala Julia python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD bfc75a23aa2bdba7e158ec12afdcdb1658bf060b If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace This is part of my code I figure out the unroll inputs has three ways to precess here the rnn1 outputs is list 1 Concat it at time axis 2 input rnn1 outpus Above two things I have tryied the error raises about slicechannel whenever I do in one of the two ways Is there something different on CPU or GPU for LSTMCell,,"sxjscience,sxjscience,sxjscience,sxjscience",2017-02-24 11:58:05,2017-02-24 13:47:09
PR,MERGE MINPY add log py a more verbose customed logger,Migrated from MinPy Usage I set a global logger in mxnet log module called mxlogger and this logger will not change the global python logging module state I'm not sure whether to replace all the logging used by the other file Because in some situation they are just used to print some message like L346 But when you want to debug something in python a more verbose logger will be useful,,"ZihengJiang,piiswrong,ZihengJiang,piiswrong,ZihengJiang",2017-02-06 23:52:17,2017-02-24 17:21:33
IS,Discussion Sharing Operators between DL Frameworks,See This Link for discussion repo This discussion started from with THC is a tensor library that backs torch I open this issue in MXNet repo so more developers can see it First of all it is possible reuse operator libraries between frameworks for example Support for THC and Torch Module was done in Torch Plugin with interfacing to torch is lua library MXNet supports reuse operators from caffe It is always interesting to see interchangeability happen For example schedule pytorch operations in mxnet is async engine or run mxnet is declarative API to directly share data with pytorch is array However there is some engineering obstacles in doing so which I would like to explain what these obstacles are and hopefully this can motivate the community to move forward and make this easier Coupled Operator Data Structure Components An operator can mean many things here are some basic components on what the operators are Data structure that holds shape pointers to the array Possible memory allocator to handle run time memory allocation Resource handles if external resources is needed Scheduling related objects if array support synchronize execution Why such coupling prevents reuse There are two reasons Many systems have their own memory allocator and ways of resource handling code While having memory allocator enables runtime memory allocations sometimes memory allocation is not preferred at all e g BLAS calls where all memory are pre allocated To resolve this problem an operator library design should enable operators that accept user managed memory resources when possible not introduce allocator or resource management but give hints to the user CuDNN is workspace requirement eliminates the need to internal memory allocator From this point of view CuDNN an cuBLAS are good examples THC is nice but still encapsulate memory allocator which is needed sometimes for dynamic operators Lack of Unified Operator Interface The second obstacle is mainly lack of common operator interface This is a problem of CUDNN and THC that prevents reusing Take CuDNN for example each CuDNN API is a C function with its own interface to adopt the operator there need to be one or multiple adapting function per operator Consider instead if there is an unified operator interface the following is a mock design where each TBlob is a reference to the data fields and shape and every function gets registered to the registry with their name Then it only takes one function to extract and reuse all operators and automatically expose them to front end In MXNet it even directly generates the symbolic counterpart from the same imperative operator if gradient is provided Problem of One Unified Operator Interface There is always a flip side of the coin Assume that we go with a unified operator interface As a matter of fact that is what MXNet TensorFlow and Caffe have done The problem now becomes what the interface should look like One trap that framework designer always falls into is that we need one interface that rules them all Since one interface rules them all we want to support all possible operators what about the ones that need runtime memory allocations Maybe add memory allocator to it what about the ones that is asynchronize In the end the interface have to include memory allocator scheduling module in some way and that introduces the Coupled Operator Data Structure Components problem The operator interface become deeply coupled with the rest of the framework and not reusable A Better Solution A Few Unified Interfaces Can we get the best of both worlds having as few data structures and interfaces as possible while still not introducing coupling to allocator and scheduling as much as possible I think the answer is yes and we need to jump out from the ideal of one interface that rules all the operators I can categorize the operators roughly in three categories type1 Basic operators The ones that can do shape inference based on input shape can take memory pointer stream and go type2 Basic operators Same as basic operator but also need to declare some additional resources workspace type3 Complicated operators The ones that requires runtime memory allocator its output shape depends on content of the data If we design for general operator interface the answer will usually looks like type3 However type 1 and 2 dominates 90 of the major operators we are using If we design one operator interfaces for each type this problem is solved So that frameworks can pull and interact with each type in their own way It is much easier to do things like static memory planning if type1 and type2 are explicitly introduced This is one additional layer of wrapping on top of THC and CuDNN is is lacking so far A registry system like NNVM could come very handy to easily resgister these informations and get pull out by the libraries The Hope I have always hopped that there is a minimum set of operator interface standard in C that can be shared across libraries I think we have a good idea on what the solution looks like While most system tends to become opague and coupled I think this kind of transparent way can help evolve the community in a healthy way This being said there is always effort to make these happen This involves a open discussion on what the interfaces should be and commitment from framework builders I would really love to see this happen and that is why I spend more than one hour writing this Unfortunately most frameworks already have kinda of enough collection of operators so having a unified operator interface will contribute little to each framework in terms of usability in short term Naturally this would be given lower priority That is why commitment is needed to bring this out for longer term benefit,,"tqchen,tqchen,tqchen,tqchen,piiswrong,piiswrong,tqchen,jermainewang,tqchen,tqchen,tqchen,tqchen,piiswrong,piiswrong,tqchen,piiswrong,piiswrong,tqchen,tqchen,tqchen,tqchen,tqchen,tqchen,piiswrong,tqchen,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,tqchen,tqchen,tqchen,piiswrong,piiswrong,tqchen,piiswrong,tqchen,tqchen,tqchen,tqchen,tqchen,tqchen,mli,piiswrong,mli,jli05,tqchen,tqchen",2017-01-19 18:17:04,2017-02-24 22:23:19
IS,I find the mxnet0 9 3 need a lot of memory and mxnet0 7 0 need some memory,in example neural style mxnet0 9 3 is out of memory But in mxnet0 7 3 has a normal result in GTX650M 2G memory,,"piiswrong,eric-haibin-lin,eric-haibin-lin",2017-02-23 10:55:32,2017-02-25 08:37:43
PR,Bring Matlab binding up to date with HEAD,This PR addresses call for contribution 4924 The Matlab binding has been updated to work with the HEAD of MXNet Added script to download the example Inception BN model from the MXNet model gallery Modified and Improved Matlab scripts Improved readme documentation with information on downloading example model running the model and how the binding works Also added a record of which Matlab version binding has been tested with Fixed some typos,,"Piyush3dB,piiswrong",2017-02-25 15:34:59,2017-02-25 19:52:37
PR,fixed run the ocr predict py several times and will get different results,5047 Reference to this issue Thank you,,BobLiu20,2017-02-25 01:17:26,2017-02-25 19:54:17
PR,add api symbol get children,,,piiswrong,2017-02-25 00:05:55,2017-02-25 22:02:09
IS,R package install failing,I have installed mxnet with GPU Python package can be loaded and used When I installed R package using,,"Lchiffon,Lchiffon,thirdwing,Lchiffon",2017-02-24 02:14:54,2017-02-26 01:16:26
IS,MKL2017 Problem,glingyan I was doing some work on building a version with MKLML and discovered that mxnet python api does not seem to be working with MKL2017 ML turned on Please make sure the python unit testing is included going forward Related prs 4128 4237 4433 4599 5000 Environment info Operating System ubuntu14 04 Compiler gcc g 4 8 4 Package used Python R Scala Julia python Installed from source Python version and distribution Python 2 7 6 Error Message Please paste the full error message including stack trace And what was the thinking behind having prepare mkl sh to install to usr local by default It is by default not writeable by non root Should the users be using sudo to build L82 I would suggest that you take the auto install out and ask users to run prepare mkl sh separately in MKL README so that users do not have to run as root just to build,,"szha,glingyan,glingyan,glingyan,szha,glingyan,glingyan,szha,glingyan,glingyan,glingyan,szha,glingyan,szha,glingyan,glingyan,glingyan,glingyan,szha,glingyan,szha,glingyan,szha,piiswrong,szha,glingyan,glingyan,szha,szha,glingyan,szha,glingyan",2017-02-21 03:59:24,2017-02-26 02:32:03
IS,support variable length instances in a mini batch,1 In rnn and other sparse input examples each instance in a mini batch must have the same length I saw this is because of embedding layer just support this manner of mini batch input The solution in rnn example is to split data into buckets where each bucket has a unique instance length However if the data num in a bucket is less than batch size all data in this bucket will been discarded 2 I do not find solution that can read libsvm data directly The solution I found is to transform the original data into dense data firstly But it is absolutely not right when feature num is very very large I think this restriction is also related to current implementation of embedding layer So is there any solutions to resolve those problems like embedding lookup sparse in tensorflow Thanks,,"formath,piiswrong,pluskid,formath,pluskid",2016-12-28 08:13:29,2017-02-26 09:14:38
PR,Correctly handle weight decay in Adam 5099,See 5099,,"leezu,sergeykolychev,sxjscience,leezu",2017-02-23 12:50:01,2017-02-27 00:31:00
PR,Add Uncentered version of RMSProp,See the discussion at issuecomment 280674628 This supersedes 5075 as I erroneously created that pull request based on my master branch This pull request already contains the refactored python API for RMSProp as discussed in 5075,,"leezu,sxjscience,piiswrong,leezu,leezu",2017-02-23 03:28:01,2017-02-27 00:34:58
IS,traing inception issue validation low,as once the issue and I am afraid the problem is still exist I am trying to retry the classification using network like inception bn full as I have done in mxnet 5 1 successful but I have met some problem in mxnet latest also I have raise a issue before I have tried with fix gamma true and false both failed I found 1 the train acc is not stable as training in mxnet 5 1 which increase very small but very stable increase but in the latest increase and decrease very unstable and after one epoch increase to 16 around then quick drop back to very low for example 1 INFO root start with arguments Namespace batch size 128 benchmark 0 data nthreads 4 data train ' data5 rd xiajizhong parts 17kClasses small rec test ' data val ' data5 rd x iajizhong parts 17kClasses small rec val ' disp batches 20 gpus '2 3' image shape '3 224 224' kv store wouldevice' load epoch None lr 0 5 lr factor 0 9 lr step epochs '2 3 4 5 6 7 8 9 10 11 12 13 14 15' max random aspect ratio 0 25 max random h 36 max random l 50 max random rotate angle 10 max random s 50 max random scale 1 max random she ar ratio 0 1 min random scale 1 model prefix 'models inception bn full' mom 0 9 monitor 0 network 'inception bn full' num classes 5190 num epochs 80 num examples 140400 num layers 50 optimizer isgd' pad size 0 random crop 1 random mirror 1 rgb mean '123 68 116 779 103 939' test io 0 top k 0 wd 0 0001 2 15 40 29 src io iter image recordio cc 221 ImageRecordIOParser data5 rd xiajizhong parts 17kClasses small rec test use 4 threads for decoding 3 15 40 31 src io iter image recordio cc 221 ImageRecordIOParser data5 rd xiajizhong parts 17kClasses small rec val use 4 threads for decoding 4 INFO root Epoch 0 Batch 20 Speed 44 90 samples sec Train accuracy 0 023810 5 INFO root Epoch 0 Batch 40 Speed 53 99 samples sec Train accuracy 0 042578 6 INFO root Epoch 0 Batch 60 Speed 52 16 samples sec Train accuracy 0 062500 7 INFO root Epoch 0 Batch 80 Speed 49 40 samples sec Train accuracy 0 052734 8 INFO root Epoch 0 Batch 100 Speed 48 92 samples sec Train accuracy 0 072266 9 INFO root Epoch 0 Batch 120 Speed 49 46 samples sec Train accuracy 0 069531 10 INFO root Epoch 0 Batch 140 Speed 49 66 samples sec Train accuracy 0 071875 11 INFO root Epoch 0 Batch 160 Speed 49 06 samples sec Train accuracy 0 073047 12 INFO root Epoch 0 Batch 180 Speed 48 88 samples sec Train accuracy 0 063672 725 INFO root Epoch 1 Batch 3180 Speed 54 00 samples sec Train accuracy 0 163281 726 INFO root Epoch 1 Batch 3200 Speed 54 12 samples sec Train accuracy 0 169531 727 INFO root Epoch 1 Batch 3220 Speed 53 97 samples sec Train accuracy 0 176563 728 INFO root Epoch 1 Batch 3240 Speed 53 96 samples sec Train accuracy 0 175391 729 INFO root Epoch 1 Batch 3260 Speed 54 00 samples sec Train accuracy 0 103125 730 INFO root Update 14249 Change learning rate to 1 41215e 01 731 INFO root Epoch 1 Batch 3280 Speed 53 90 samples sec Train accuracy 0 071094 732 INFO root Epoch 1 Batch 3300 Speed 53 86 samples sec Train accuracy 0 068359 733 INFO root Epoch 1 Batch 3320 Speed 53 89 samples sec Train accuracy 0 069531 734 INFO root Epoch 1 Batch 3340 Speed 54 10 samples sec Train accuracy 0 064844 735 INFO root Epoch 1 Batch 3360 Speed 54 07 samples sec Train accuracy 0 064844 736 INFO root Epoch 1 Batch 3380 Speed 54 16 samples sec Train accuracy 0 065625 737 INFO root Epoch 1 Batch 3400 Speed 54 15 samples sec Train accuracy 0 061328 738 INFO root Epoch 1 Batch 3420 Speed 54 18 samples sec Train accuracy 0 071094 739 INFO root Epoch 1 Batch 3440 Speed 54 13 samples sec Train accuracy 0 060547 740 INFO root Epoch 1 Batch 3460 Speed 54 11 samples sec Train accuracy 0 064453 741 INFO root Epoch 1 Batch 3480 Speed 54 20 samples sec Train accuracy 0 076172 742 INFO root Epoch 1 Batch 3500 Speed 54 03 samples sec Train accuracy 0 064062 743 INFO root Epoch 1 Batch 3520 Speed 54 23 samples sec Train accuracy 0 070312 744 INFO root Epoch 1 Batch 3540 Speed 54 11 samples sec Train accuracy 0 056250 745 INFO root Epoch 1 Batch 3560 Speed 54 19 samples sec Train accuracy 0 076953 746 INFO root Epoch 1 Batch 3580 Speed 54 13 samples sec Train accuracy 0 067187 747 INFO root Epoch 1 Batch 3600 Speed 54 18 samples sec Train accuracy 0 064062 748 INFO root Epoch 1 Batch 3620 Speed 54 20 samples sec Train accuracy 0 075000 749 INFO root Epoch 1 Batch 3640 Speed 54 12 samples sec Train accuracy 0 066797,,,2017-01-20 11:36:24,2017-02-27 06:53:25
PR,Fix FusedRNNCell input to be time major if passed an input list,Fix FusedRNNCell input to be time major if passed a list of batch major tensors as inputs,,fhieber,2017-02-27 19:10:24,2017-02-27 19:35:39
PR,add states to modules,,,"piiswrong,piiswrong,pluskid,piiswrong,pluskid,piiswrong,pluskid",2017-02-22 18:41:23,2017-02-27 23:20:23
PR,Fix char RNN dataset URLs,The URLs in the comments of the shell scripts for the Scala char RNN examples appear to be outdated,,mkolod,2017-02-28 00:40:48,2017-02-28 02:10:38
PR,partial set params,,,piiswrong,2017-02-27 23:50:08,2017-02-28 04:33:05
PR,fix conv addto mode test failed,Signed off by lingyan lingyan guo intel com,,"glingyan,szha",2017-02-25 02:43:00,2017-02-28 04:33:57
PR,Allocating largest arrays first in GraphExecutor in order to save space,Before this change ndarrays were just allocated in the order they are in the graph This could lead to scenarios where large ndarrays from the shared pool get used for relatively small ndarrays Trying to allocate the largest ndarrays from the shared pool first will avoid this and save space,,"tdomhan,piiswrong,eric-haibin-lin",2017-02-27 12:28:53,2017-02-28 04:34:22
PR,remove redundancy head in storage,ps i found is no need more should we also delete it,,tornadomeet,2017-02-27 06:41:18,2017-02-28 04:35:02
IS,RNN cell with list of symbol as input,piiswrong when using list of symbol as input the layer is assumed as TNC order While the default layout input is NTC It is not properly parsed for list input case,,"Godricly,Godricly",2017-02-23 02:45:00,2017-02-28 05:02:48
IS,Problem in SliceChannel with a buckteting cnn lstm ctc model for ocr tasks,Environment info Operating System ubuntu 16 04 Compiler gcc 5 4 Package used Python R Scala Julia Python MXNet version 0 9 3 Python version and distribution Anaconda 2 7 11 Error Message Please paste the full error message including stack trace when I train my model the error occurs like this What have you tried to solve it 1 make sure I do provide bucket key and default bucket key,,"Godricly,Godricly,Godricly",2017-02-27 15:22:27,2017-02-28 09:29:21
PR,added when the symbol has multiple outputs and shape input in visualization py,When the symbol has multiple outputs and get input shape argument visualization py crashed into issues For example when I use the symbol like slicechannel name slice 1 which has multiple output for example 2 ouputs the shape dict is keys were made like islice 1 output1' and islice 1 output2' But by the difference of input nodes and shape dict it attempts to finds islice 1 output' instead of the islice 1 output1' So I added code to overcome this issue and made an additional test with sliceChannel symbol to confirm,,Soonhwan-Kwon,2017-02-24 11:53:36,2017-02-28 12:42:02
IS,SSD example CUDA error too many resources requested for launch,It is okay to run CPU version SSD exmaple but there is something wrong with CUDA The main cuda error log is example ssd operator multibox detection cu 199 Check failed error cudaSuccess 7 vs 0 too many resources requested for launch Environment info Operating System TegraX1 Ubuntu 16 04 2 LTS Linux tegra ubuntu 3 10 96 tegra Compiler gcc version 5 4 0 20160609 Ubuntu Linaro 5 4 0 6ubuntu1 16 04 4 Package used Python R Scala Julia python MXNet version 0 9 4 Or if installed from source Yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 Error Message Please paste the full error message including stack trace py mxnet SSD yuanshuai tegra ubuntu sdcard code mxnet SSD cuda openblas example ssd python demo py epoch 0 images data demo dog jpg thresh 0 5 09 58 45 src nnvm legacy json util cc 175 Loading symbol saved by previous version v0 8 0 Attempting to upgrade 09 58 53 mnt sdcard yuanshuai code mxnet SSD cuda openblas dmlc core include dmlc logging h 300 09 58 53 example ssd operator multibox detection cu 199 Check failed error cudaSuccess 7 vs 0 too many resources requested for launch Stack trace returned 7 entries bt 0 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x48 0x7f737f2da0 bt 1 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op19MultiBoxDetectionOpIN7mshadow3gpuEfE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x1b18 0x7f74a34a40 bt 2 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xf2a048 0x7f740f5048 bt 3 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x88 0x7f74095b68 bt 4 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x88 0x7f7409c6c0 bt 5 usr lib aarch64 linux gnu libstdc so 6 0xb8280 0x7f66e24280 bt 6 lib aarch64 linux gnu libpthread so 0 0x6fc4 0x7f7e02dfc4 09 58 53 mnt sdcard yuanshuai code mxnet SSD cuda openblas dmlc core include dmlc logging h 300 09 58 53 src engine threaded engine h 336 09 58 53 example ssd operator multibox detection cu 199 Check failed error cudaSuccess 7 vs 0 too many resources requested for launch Stack trace returned 7 entries bt 0 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x48 0x7f737f2da0 bt 1 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op19MultiBoxDetectionOpIN7mshadow3gpuEfE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x1b18 0x7f74a34a40 bt 2 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xf2a048 0x7f740f5048 bt 3 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x88 0x7f74095b68 bt 4 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x88 0x7f7409c6c0 bt 5 usr lib aarch64 linux gnu libstdc so 6 0xb8280 0x7f66e24280 bt 6 lib aarch64 linux gnu libpthread so 0 0x6fc4 0x7f7e02dfc4 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 5 entries bt 0 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x48 0x7f737f2da0 bt 1 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x2d0 0x7f74095db0 bt 2 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x88 0x7f7409c6c0 bt 3 usr lib aarch64 linux gnu libstdc so 6 0xb8280 0x7f66e24280 bt 4 lib aarch64 linux gnu libpthread so 0 0x6fc4 0x7f7e02dfc4 terminate called after throwing an instance of wouldmlc Error' what 09 58 53 src engine threaded engine h 336 09 58 53 example ssd operator multibox detection cu 199 Check failed error cudaSuccess 7 vs 0 too many resources requested for launch Stack trace returned 7 entries bt 0 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x48 0x7f737f2da0 bt 1 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op19MultiBoxDetectionOpIN7mshadow3gpuEfE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x1b18 0x7f74a34a40 bt 2 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xf2a048 0x7f740f5048 bt 3 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x88 0x7f74095b68 bt 4 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x88 0x7f7409c6c0 bt 5 usr lib aarch64 linux gnu libstdc so 6 0xb8280 0x7f66e24280 bt 6 lib aarch64 linux gnu libpthread so 0 0x6fc4 0x7f7e02dfc4 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 5 entries bt 0 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x48 0x7f737f2da0 bt 1 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x2d0 0x7f74095db0 bt 2 mnt sdcard yuanshuai virtualenvs py mxnet SSD local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x88 0x7f7409c6c0 bt 3 usr lib aarch64 linux gnu libstdc so 6 0xb8280 0x7f66e24280 bt 4 lib aarch64 linux gnu libpthread so 0 0x6fc4 0x7f7e02dfc4 Aborted,,"ysh329,ysh329,ysh329",2017-02-28 02:10:38,2017-02-28 14:37:06
IS,Issue closed,,,,2017-02-28 03:49:55,2017-02-28 15:00:12
IS,Higher Level API for RNN,We have created a higher level API for recurrent neural networks and have completed gradient tests forward test and speed comparison against CuDNN The class definition and key methods look like this We decide to Pull Request this feature Should we create a new directory under python mxnet like operators to store these kind of composed symbols What do you think,,"sxjscience,pluskid,sxjscience,sxjscience,xlvector,sxjscience,leezu,zhenlinluo,piiswrong,sbodenstein,sxjscience,sxjscience,zhenlinluo,sxjscience,sxjscience,zhenlinluo,zhenlinluo,zhenlinluo,sxjscience,sxjscience,sxjscience,sxjscience",2016-11-22 09:07:55,2017-02-28 15:30:36
PR,Memory allocator bug fix 5035,4795 5123 5035 Together with NNVM PR Found some inefficiency in the system and made a few changes related to memory allocation in MXNet 1 In the bucketing module curr module is passed in as the shared module but instead the module with default bucket key should be passed Link L214 2 When the memory pool of the default bucket module does not hold sufficient memory for other bucket to bind extra NDArrays are allocated But these NDArrays are not added back to the pool Link L517 3 While matching new memory required to the existing memory slots in the pool the new memory list is not sorted based on size which could result in small memory blocks occupying a big one 4 The NNVM EXEC MATCH RANGE variable has impact on the result of memory planning Instead of letting the user to choose it the backend could just try different values and choose the best one to automate the process Some users are not even aware of this variable Link L299 Fixing 1 and 2 reduce the memory quite a lot while 3 and 4 bring marginal reduction if 1 and 2 are fixed 5 10 Benchmark result on LSTM workload Version 22673b6 baseline 1 1 2 1 2 3 1 2 3 4 Memory MB 12288 Out of Memory 4297 1956 1774 1718 Benchmark result on Neural Style workload Version 22673b6 baseline 1 2 3 4 Memory MB 12288 Out of Memory 2034 LSTM configuration python lstm bucketing py gpus 0 num layers 4 num hidden 1024 num embed 512 num epochs 1 Neural style uses default configuration,,"eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin",2017-02-24 06:35:34,2017-02-28 17:25:17
IS,High memory usage with bucketing,When using the bucketing module I would expect the memory usage to be about the same as when using the normal module unrolled to the largest bucket size However we observe unusually high GPU memory usage in MxNet when using multiple buckets This can be reproduced observed with the lstm bucketing py example from the latest MXNet commit as such in examples rnn lstm bucketing py change When using multiple buckets see line 49 overall memory usage is 1419MB When changing line 49 to only use a single bucket e g 60 overall memory usage is only 1185MB It should be noted that the initial memory usage for bucketing is the same 1185MB but after a couple of batches the memory usage increases We suspect this is due to the BucketingModule binding another sub module when a new bucket size is given by the data iterator and memory sharing across modules is not working properly While for this model the difference is only 300 MB we observed much higher differences in practice making it difficult to train any reasonably sized model with bucketing Note the default bucket key is of course the largest bucket,,"tdomhan,tdomhan,piiswrong,tdomhan,tdomhan,eric-haibin-lin,tdomhan,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,tdomhan,eric-haibin-lin,piiswrong,tdomhan,eric-haibin-lin,tdomhan,tdomhan",2017-02-15 18:07:13,2017-02-28 17:25:26
PR,bugfix set correct default values to 3d pooling padding and stride,Currently I'm experiencing this weird behavior below,,"ckomaki,piiswrong,ckomaki,piiswrong,ckomaki,piiswrong,ckomaki,piiswrong,piiswrong",2017-02-22 05:41:47,2017-02-28 17:33:40
PR,solved issue with multiple outputs in visualize in python,When the symbol has multiple outputs and get input shape argument visualization py crashed into issues For example when I use the symbol like slicechannel name slice 1 which has multiple output for example 2 ouputs the shape dict is keys were made like islice 1 output1' and islice 1 output2' But by the difference of input nodes and shape dict it attempts to finds islice 1 output' instead of the islice 1 output1' So I added code to overcome this issue and made an additional test with sliceChannel symbol to confirm Related Issue is 3221,,"Soonhwan-Kwon,gurumurthys",2017-02-28 13:48:09,2017-02-28 17:34:14
PR,Fix the incorrect unit test for cpu naive storage,5174,,eric-haibin-lin,2017-02-28 06:45:04,2017-02-28 17:36:23
PR,fix Profiler timestamp when using VC12,function Profiler NowInUsec uses high resolution clock to get the time stamp However the implement of the class is not accuracy before VC15 Using high resolution clock in VC12 before Using QueryPerformanceCounter functions after More detail can be found here v vs 85 aspx and here v vs 140 aspx,,"luoyetx,ZihengJiang,luoyetx,piiswrong,ZihengJiang",2017-02-28 05:26:48,2017-02-28 17:41:49
PR,add svm output gpu code,add svm output gpu code originally saw the merger but somehow disappeared,,"yajiedesign,piiswrong,yajiedesign",2017-01-05 11:19:56,2017-02-28 17:58:33
IS,Broken unit test,The storage test cc unit test is out of date It assumes Pooled Storage for CPU which is no longer the default With Naive storage on CPU we should not check the pointer value on second allocation I guess it was passing previously because libc malloc keeps pool of memory pages so that the pointer values happen to be the same,,eric-haibin-lin,2017-02-28 06:44:53,2017-02-28 18:14:23
IS,Potential problems of exporting CFLAGS LDFLAGS in Makefiles,I'm on macOS Sierra and there is no bundled OpenSSL on macOS Sierra so I had to give the include path to ADD CFLAGS However after making NNVM the CFLAGS became std c 11 Wall O2 Iinclude fPIC for mxnet src io cc files as nnvm Makefile L13 reads As we are doing recursive make s this might be dangerous as it will override subsequent use of the environment variables,,"jli05,jli05,jli05,jli05",2017-02-28 16:05:28,2017-02-28 20:23:12
IS,BUG in mx rnn FusedRNNCell API,I use the mx rnn FusedRNNCell API as follow It cause the error src operator slice channel inl h 173 Check failed dshape real axis param num outputs 0 1 vs 0 num outputs 16 does not divide input dimension 1 1 I see the output of mx symbol RNN the shape of it is 16L 1L 128L so maybe the mx sym SliceChannel outputs axis axis num outputs length squeeze axis 1 cause this error,,piiswrong,2017-02-28 08:14:08,2017-03-01 00:54:34
IS,do predictor in amalgamation support gpu boosting,I find it it is slow when just using cpu to do inference,,"piiswrong,piiswrong",2017-02-28 03:03:23,2017-03-01 02:31:09
PR,add data shapes check,,,"piiswrong,chunyang-wen",2017-03-01 00:45:47,2017-03-01 04:39:08
IS,Exploring the graph programmatically,Something I would like to be able to do is explore the operator and variable graph from any symbol within the Python API Such a functionality enables programmatic replication of subsets of input graphs However the only way I see to acquire this information in Python is the tojson method of the Symbol class which while doable is not necessarily an ideal format It would be great if there was a python API for more directly exploring exposing the graph structure,,"howard0su,piiswrong",2017-02-03 18:23:56,2017-03-01 06:03:53
PR,Correct handling of weight decay in Adam and RMSProp uncentered version of RMSProp weight clipping for RMSProp,See 5099 on the handling of weight decay Weight clipping is necessary to train WassersteinGAN Clipping the weights directly in the optimizer speeds things up compared to iterating over the weight bias arrays in Python and performing the weight clipping there WassersteinGAN are typically trained with RMSProp This pull request obsoletes 5116 and introduces the uncentered version of RMSProp,,"leezu,sxjscience,piiswrong,leezu",2017-02-24 09:53:22,2017-03-01 12:29:37
IS,Correctly handling weight decay in the optimizer,Our objective function is like l w E x f w x lambda w 2 2 If we include the L2 regularizer the stochastic gradient should be true g rescale grad g wd weight The stochastic gradient will then be used to update the curvature and momentum In the current implementation of Adam and RMSProp the g is directly used as the stochastic gradient for updating the other state variables which is not consistent with other packages like Keras We may need to revise this Reference Adam in MXNet L196 L197 Adam in Keras L415 L446,,"sxjscience,piiswrong,leezu,sxjscience,sxjscience,sxjscience,leezu",2017-02-22 08:55:42,2017-03-01 12:30:24
PR,op keras add reverse,,,"yajiedesign,piiswrong,piiswrong,yajiedesign,yajiedesign,piiswrong,yajiedesign,piiswrong,piiswrong,piiswrong,piiswrong",2017-02-10 14:54:27,2017-03-01 17:01:05
PR,Add stacked fused RNN example,Stack multiple fused RNN cells instead of one cell with multiple layer Benchmark result Version baseline stacked Seconds per epoch 226 58 123 0 1 8x speedup cudnn lstm bucketing py gpus 0 1 2 3 4 5 6 7 8 num layers 4 num embed 512 num hidden 1024,,"eric-haibin-lin,piiswrong,eric-haibin-lin",2017-02-28 19:52:15,2017-03-01 17:08:53
IS,How to reinitialize module in ipython notebook,Is there any API to do so,,,2017-03-01 23:57:46,2017-03-01 23:58:17
IS,Why does my net keep output 'rmse nan',Environment info Operating System Windows 10 python Anaconda on CPU MXNet version 0 92 Here is my net structure a simple FC net used for Regression define net net in mx symbol Variable wouldata' net in mx symbol Flatten data net in net label mx symbol Variable isoftmax label' fc1 mx symbol FullyConnected data net in name 'fc1' num hidden 512 act1 mx symbol Activation data fc1 name arelu1' act type relu fc2 mx symbol FullyConnected data act1 name 'fc2' num hidden 512 act2 mx symbol Activation data fc2 name arelu2' act type relu fc5 mx symbol FullyConnected data act2 name 'fc5' num hidden net out size net out mx symbol LinearRegressionOutput data fc5 label net label name 'net out' But as the train starts all I get is 'rmse nan' When I use smaller net with fc layer size 256 The net gives rmse inf at the start of the training then it converges soon But when I increase the num hidden to 512 I get these nan s I can understand how multiply add computation gives 'nan' Anyone provide me some suggestion Thank you By the way May be my problem is not linear is there any better alternative to my last layer mx symbol LinearRegressionOutput,,,2017-02-26 16:11:38,2017-03-02 03:13:33
PR,add parallel actor critic algorithm,There is something wrong with symbol definition and gradients calculation in A3C here is the parallel actor critic algorithm transplanted from Minpy parallel actor critic This implementation can get relatively good results for Pong in about 30 minutes of training,,"loofahcus,piiswrong,loofahcus,sxjscience,loofahcus,sxjscience",2017-03-01 14:45:30,2017-03-02 05:46:59
PR,Update CONTRIBUTORS md,5137 4135,,leezu,2017-03-02 08:24:32,2017-03-02 09:56:29
IS,Can I parameter share batch normalization layers,I know how to share parameters between conv and fc layers by first declaring mx sym Variable is then giving them into symbols to be parameter shared But I wonder if I can do a similar stuff to BatchNorm layers since I also want to share or at least copy from pre trained one data mean and variances which are defined as outputs of BN layers If parameter sharing is not possible then at least I want to copy mean and variances from one layer to other layers before training,,piiswrong,2017-03-02 16:41:26,2017-03-02 18:15:25
PR,Add sudoers permission to docker user,This gives the docker user sudoers permissions,,"lxn2,piiswrong,lxn2,lxn2,lxn2",2017-02-28 08:24:11,2017-03-02 18:39:52
PR,Add ResNet BatchNorm Converter Support,Added important functionality to Caffe converter TL DR it now supports ResNet models from the original ResNet git repository They were verified to achieve test error within 10 4 of original Caffe model Reshape layer support was also added BatchNorm is now supported along with Scale layer in the sense that is usually used in Caffe BatchNorm normalizes and Scale takes care of the beta gamma parameters At the momemnt Scale is only supported in this manner i e it checks that it always follows a BatchNorm layer,,piiswrong,2017-02-27 18:04:33,2017-03-02 22:06:00
PR,Syncing up perl package to current state of the Python interface,piiswrong Notable additions a high level RNN API b initializers reworked to include InitDesc c new attribute for variables init holds jsonified initializer constructor parameters d miscellaneous improvements,,"sergeykolychev,sergeykolychev,sergeykolychev,piiswrong",2017-02-27 18:19:55,2017-03-03 01:50:37
PR,Caffe improve caffe converter,I create this PR to simplify others to continue working on it But currently it is not ready for merging Things are changed 1 docs how to caffe md for detailed instructions for caffe users to use mxnet 2 added convert caffe modelzoo py to download and convert caffe model zoo models 3 added test converter py which tests the accuracy and speed all converted model Issues need to be fixed 1 failed to convert bvlc alexnet 2 failed to convert resnet due to the lack of Scale layer related issues 3138 1473 3 does not support multiple inputs outputs models 4 i did not test if using a mean binaryproto for mean instead of a list of rgb mean see resnet 50 5 does not support PRelu 4139 6 more code documents such as we should add docstrings to each function 7 improve code quality in convert symbol py and convert model py e g remove import caffe because we can assume users will install protobuf first 8 optional if an operator is not supported consider to use sym CaffeOp instead of raise an exception to reproduce 1 and 2 just change to models 'bvlc alexnet' aresnet 50' in test converter py the bottomline is we should get 1 5 fixed,,"mli,piiswrong,zihaolucky,piiswrong,piiswrong,sbodenstein,piiswrong,mli",2016-12-07 22:41:55,2017-03-03 02:32:23
IS,mxnet amalgamation mxnet predict0 cc should include src operator lrn cc,piiswrong Now the library made with amalgamation cannot use the layer LRN I think mxnet amalgamation mxnet predict0 cc should include src operator lrn cc to fix this problem,,,2017-02-13 05:46:31,2017-03-03 02:52:36
IS,SequentialRNNCell simple bind raise segmentation fault,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 15 10 Compiler gcc4 9 x Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD bfc75a23aa2bdba7e158ec12afdcdb1658bf060b If you are using python package please provide Python version and distribution python2 7 Anaconda4 0 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I guess it could happen in data idx data idx should visit somewhere wrong ps this sym is a part of image caption NIC I relize,,piiswrong,2017-03-02 13:07:59,2017-03-03 03:15:17
IS,Could not build mxnet on windows,Hello I built mxnet successfully but when i opened the build folder build mxnet sln i got the following fatal error LINK fatal error LNK1104 cannot open file ' Debug libmxnet lib' Besides that this is the full output message 1 Rebuild All started Project ZERO CHECK Configuration Debug x64 1 Checking Build System 1 CMake does not need to re run because C mxnet build CMakeFiles generate stamp is up to date 1 CMake does not need to re run because C mxnet build dmlc core CMakeFiles generate stamp is up to date 1 CMake does not need to re run because C mxnet build tests cpp CMakeFiles generate stamp is up to date 1 CMake does not need to re run because C mxnet build example image classification predict cpp CMakeFiles generate stamp is up to date 2 Rebuild All started Project dmlccore Configuration Debug x64 3 Skipped Rebuild All Project dmlccore lint Configuration Debug x64 3 Project not selected to build for this solution configuration 4 Skipped Rebuild All Project mxnet lint Configuration Debug x64 4 Project not selected to build for this solution configuration 2 Building Custom Rule C mxnet dmlc core CMakeLists txt 2 CMake does not need to re run because C mxnet build dmlc core CMakeFiles generate stamp is up to date 2 config cc 2 data cc 2 io cc 2 recordio cc 2 line split cc 2 recordio split cc 2 input split base cc 2 local filesys cc 2 dmlccore vcxproj C mxnet build dmlc core Debug dmlccore lib 5 Rebuild All started Project mxnet Configuration Debug x64 5 Building NVCC Device object CMakeFiles cuda compile 1 dir src ndarray Debug cuda compile 1 generated ndarray function cu obj 5 ndarray function cu 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 20 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error expected an expression 5 5 5 5 C mxnet src ndarray ndarray function cu 33 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error no instance of overloaded function mshadow Copy matches the argument list 5 5 argument types are error type error type error type error type mshadow Stream mshadow gpu 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error class mshadow TBlob has no member FlatTo1D 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error type name is not allowed 5 5 5 5 C mxnet src ndarray ndarray function cu 46 error type name is not allowed 5 5 5 5 Error limit reached 5 5 100 errors detected in the compilation of C Users Avrahim AppData Local Temp tmpxft 000031bc 00000000 11 ndarray function cpp1 ii 5 5 Compilation terminated 5 5 ndarray function cu 5 5 CMake Error at cuda compile 1 generated ndarray function cu obj Debug cmake 282 message 5 Error generating file 5 C mxnet build CMakeFiles cuda compile 1 dir src ndarray Debug cuda compile 1 generated ndarray function cu obj 5 5 6 Rebuild All started Project image classification predict Configuration Debug x64 6 Building Custom Rule C mxnet example image classification predict cpp CMakeLists txt 6 CMake does not need to re run because C mxnet build example image classification predict cpp CMakeFiles generate stamp is up to date 6 image classification predict cc 6 LINK fatal error LNK1104 cannot open file ' Debug libmxnet lib' 7 Skipped Rebuild All Project ALL BUILD Configuration Debug x64 7 Project not selected to build for this solution configuration Rebuild All 2 succeeded 2 failed 3 skipped,,,2017-03-02 06:41:50,2017-03-03 07:19:54
PR,Fix 3d conv inter shape check failures for valid kernel sizes,A bug mentioned by in 5048,,ckomaki,2017-03-03 06:36:46,2017-03-03 07:39:30
PR,Last post we did for text classification with CNN,Last post with text classification with CNNs at character level The code is available here It is in python and R If you guys want me to push the code as one of the examples let me know We have a web app that shows the system you can play with it,,miguelgfierro,2017-03-03 13:04:14,2017-03-03 16:50:31
PR,Fixes bug in GPU broadcast sub,In broadcast sub OP was set to mshadow op minus which gives wrong results Fixed by changing the OP to identity like it should be,,"ap-hynninen,piiswrong,sxjscience",2017-03-03 00:29:50,2017-03-03 16:50:47
PR,doc refactor python ndarray api,This PR adds more comments for ndarray operators and refactors ndarray API docs A preview is available at refactor api python ndarray md The refactor consists 1 operators that should be deprecated are removed from the summary list including we can use either 1 data or data0 data1 for multi input or 2 a b to stay the same with numpy Also we used axis axes dim to refer to dimensions Suggest to use axis for single dimension and axes for multi dimensions some additional changes include ret typ ret type or rtype for topk TODOs for next PRs 1 comments for neural network layers 2 improve symbol md,,"mli,piiswrong,mli,piiswrong,jermainewang,mli,sergeykolychev,mli,piiswrong,mli",2017-02-26 00:20:46,2017-03-03 19:08:01
IS,RMSProp causing outputs to return NaN,I have noticed that RMSProp seems to cause my networks to return NaNs This does not always happen on the first training batch but the minimal example below starts returning NaNs right away In the example below changing the kwargs of RMSProp e g clip gradient 1 0 does not seem to prevent the NaNs from occurring If you uncomment line 5 and comment line 6 the script seems to run fine with SGD I also tried it with Adam and that worked alright too Minimal Example Environment info Operating System OS X Ubuntu 14 04 Package used Python R Scala Julia Python Python version and distribution Python 3 5 2 Anaconda 4 2 0 x86 64 Git commit hash b4e8743101294cf7e37510ebe4cdfd0238520156,,"piiswrong,leezu,sxjscience",2017-03-03 00:54:08,2017-03-03 20:35:49
PR,Doc Misleading statement in architecture docs,I'm reading the Architecture docs According to the context I'm afraid a NOT is missing in this statement,,,2017-03-03 19:17:02,2017-03-03 20:50:28
PR,Update nnvm submodule,piiswrong Somehow my last PR did not include the submodule update Please merge in,,eric-haibin-lin,2017-03-04 00:34:47,2017-03-04 04:06:01
PR,Update solver py,fix tiny typos,,tornadomeet,2017-03-04 08:34:28,2017-03-04 08:36:10
IS,Infer shape error I do not understand,I'm trying to write a network to solve binary classification I am using logistic regression for the loss function I followed the api and tutorial but I'm not sure what I'm doing wrong Here is my error 12 44 26 Users travis build dmlc mxnet distro mxnet build dmlc core include dmlc logging h 300 12 44 26 src c api c api symbolic cc 385 InferShapeKeyword argument name softmax label not found Candidate arguments 0 data 1 fc1 weight 2 fc1 bias 3 gt label Stack trace returned 4 entries bt 0 0 libmxnet so 0x000000010dac61b8 ZN4dmlc15LogMessageFatalD2Ev 40 bt 1 1 libmxnet so 0x000000010e0bd08f ZN5mxnet14MatchArgumentsIN4nnvm6TShapeEEEvRKNS1 12IndexedGraphERKNSt3 113unordered mapINS6 12basic stringIcNS6 11char traitsIcEENS6 9allocatorIcEEEET NS6 4hashISD EENS6 8equal toISD EENSB INS6 4pairIKSD SE EEEEEEPNS6 6vectorISE NSB ISE EEEEPKc 3519 bt 2 2 libmxnet so 0x000000010e0bb268 MXSymbolInferShape 1368 bt 3 3 ctypes so 0x00000001083adf57 ffi call unix64 79 infer shape error Arguments data 1 1L 50L softmax label 1 Traceback most recent call last File neuralnet py line 42 in module num epoch 1000 File Applications anaconda lib python2 7 site packages mxnet module base module py line 388 in fit for training True force rebind force rebind File Applications anaconda lib python2 7 site packages mxnet module module py line 345 in bind grad req grad req File Applications anaconda lib python2 7 site packages mxnet module executor group py line 187 in init self bind exec data shapes label shapes shared group File Applications anaconda lib python2 7 site packages mxnet module executor group py line 279 in bind exec shared group File Applications anaconda lib python2 7 site packages mxnet module executor group py line 483 in bind ith exec arg shapes aux shapes self symbol infer shape input shapes File Applications anaconda lib python2 7 site packages mxnet symbol py line 542 in infer shape return self infer shape impl False args kwargs File Applications anaconda lib python2 7 site packages mxnet symbol py line 609 in infer shape impl ctypes byref complete File Applications anaconda lib python2 7 site packages mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 12 44 26 src c api c api symbolic cc 385 InferShapeKeyword argument name softmax label not found Candidate arguments 0 data 1 fc1 weight 2 fc1 bias 3 gt label Stack trace returned 4 entries bt 0 0 libmxnet so 0x000000010dac61b8 ZN4dmlc15LogMessageFatalD2Ev 40 bt 1 1 libmxnet so 0x000000010e0bd08f ZN5mxnet14MatchArgumentsIN4nnvm6TShapeEEEvRKNS1 12IndexedGraphERKNSt3 113unordered mapINS6 12basic stringIcNS6 11char traitsIcEENS6 9allocatorIcEEEET NS6 4hashISD EENS6 8equal toISD EENSB INS6 4pairIKSD SE EEEEEEPNS6 6vectorISE NSB ISE EEEEPKc 3519 bt 2 2 libmxnet so 0x000000010e0bb268 MXSymbolInferShape 1368 bt 3 3 ctypes so 0x00000001083adf57 ffi call unix64 79 I'm new to MXNet so if anyone can help me thank you,,piiswrong,2017-02-28 17:51:04,2017-03-04 19:11:35
PR,Scala fix scalatest failed problem,fix scalatest fails with stod throwing invalid argument described in 5231,,Ldpe2G,2017-03-05 02:02:42,2017-03-05 04:02:33
IS,mx metric Perplexity 1 update metric raise segement fault,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu15 10 Compiler gcc 9 4 x Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source yeap MXNet commit hash git rev parse HEAD be38c5b84030a63d0ab51f19737f99a75a7feb23 If you are using python package please provide anaconda4 2 Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace For above code when it run into fit and run at the line where update metric metric it will raise segfault when I comment it and it works well My dataIter works well and I test it some times So I think the error must be related to update metric However the example lstm bucketing py runs well I dont what I should do,,,2017-03-05 03:49:18,2017-03-05 07:26:17
PR,update navbar toc and python api index,,,,2017-03-05 07:31:26,2017-03-05 07:32:19
IS,error C1083 Cannot open include file 'cub device device radix sort cuh' No such file or directory,Hello I am trying to compile the newest MxNet in Windows by using Visual Studio I am constantly getting that error and can not fix it The cub directory is included in the MxNet source and the CMake seems to have it added in the Additional Include Directories line in the project properties under C C The line which creates problem is in the sort op inl cuh file Copyright c 2017 by Contributors file sort op inl cuh brief CUDA implementations for sort op h ifndef MXNET OPERATOR TENSOR SORT OP INL CUH define MXNET OPERATOR TENSOR SORT OP INL CUH include thrust device ptr h include thrust sort h include cub device device radix sort cuh if CUDA VERSION 7000 include thrust system cuda execution policy h endif The line include cub device device radix sort cuh generates the error I double checked both the cub folder and the device radix sort cuh file both do exist and under the appropriate location pointed by the entry in Additional Include Directories What else may cause that error The file being included has the cuh extension which belongs to a CUDA kernel for example can this cause a problem with the Visual Studio while trying to include Thanks in advance,,,2017-03-04 18:06:34,2017-03-05 18:37:45
PR,fix measure py and kill mxnet py scripts in tools,,,,2017-03-05 10:10:33,2017-03-05 20:15:49
PR,Fine tune in R end to end example,This is tutorial mentioned in,,"statist-bhfz,statist-bhfz,statist-bhfz",2017-03-05 05:51:03,2017-03-05 20:20:08
PR,doc fix frontpage style,before image after image,,mli,2017-03-05 05:01:18,2017-03-05 20:22:07
PR,Fix broken amalgamation,Amalgamation build is broken due to a recent change in matrix op inl h This PR provides a fix by adding thrust device vector h to the blacklist,,Piyush3dB,2017-03-04 09:05:36,2017-03-05 20:26:40
PR,Fixed previous weight handling for DCASGD optimizer,While porting latest Python changes to the Perl interface I stumbled upon something that looks like a small logic error,,"sergeykolychev,piiswrong,piiswrong,sergeykolychev,sergeykolychev,sergeykolychev",2017-02-24 23:36:33,2017-03-05 20:36:46
IS,Embedding layer does not support calculate data gradient,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu15 10 Compiler gcc 4 9x Package used Python R Scala Julia python MXNet version 0 9 4 Or if installed from source yes MXNet commit hash git rev parse HEAD be38c5b84030a63d0ab51f19737f99a75a7feb23 If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I just use pretrain vgg to extract fc layer ouputs and then input them into caption module name image feature I create custom dataIter process them by vgg and then copy them into image feature However it raises above error all the time Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Try to pop item in grad dict about word data 2 Use Module API and bind symbol 3,,"piiswrong,sxjscience",2017-03-05 12:09:55,2017-03-06 07:11:51
PR,gcc 4 8 5 error reported by pfc and fixed by sxj,,,"FCInter,sxjscience",2017-03-06 08:10:50,2017-03-06 10:10:08
IS,Check failed e fmap at alias 0x10fc620 vs 0x103adf0 Entry add n already registered under different entry,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Centos 6 6 Compiler gcc 4 8 5 Package used Python R Scala Julia python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 Anaconda 4 2 0 64 bit If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 16 38 16 home mypath software try mxnet mxnet dmlc core include dmlc logging h 300 16 38 16 home mypath software try mxnet mxnet dmlc core include dmlc registry h 66 Check failed e fmap at alias 0x10fc620 vs 0x103adf0 Entry add n already registered under different entry Stack trace returned 4 entries bt 0 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc8RegistryIN4nnvm2OpEE8AddAliasERKSsS5 0x79b 0x7fa4c820635b bt 1 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4nnvm2Op9add aliasERKSs 0x1f 0x7fa4c820493f bt 2 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xb3cea9 0x7fa4c7073ea9 bt 3 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0x1cff226 0x7fa4c8236226 terminate called after throwing an instance of wouldmlc Error' what 16 38 16 home mypath software try mxnet mxnet dmlc core include dmlc registry h 66 Check failed e fmap at alias 0x10fc620 vs 0x103adf0 Entry add n already registered under different entry Stack trace returned 4 entries bt 0 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc8RegistryIN4nnvm2OpEE8AddAliasERKSsS5 0x79b 0x7fa4c820635b bt 1 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4nnvm2Op9add aliasERKSs 0x1f 0x7fa4c820493f bt 2 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xb3cea9 0x7fa4c7073ea9 bt 3 home mypath anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0x1cff226 0x7fa4c8236226 Aborted Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 I have redownloaded the package and rebuilt it But the problem did not get solved 2 I met with this error just after I finish the python setup py install as noted by the installation guide 3,,"FCInter,sxjscience,sxjscience",2017-03-05 08:42:23,2017-03-06 10:11:44
IS,Any features like tensorflow minimize loss,Dear all I wonder is there are any features in mxnet which work like tensorflow is minimize loss functions as in tensorflow it is only needed to write a loss function in python and do not need to define the backward and forward operations That might be very useful for developers and for mxnet to become more popular Many thanks,,"sxjscience,sxjscience",2017-03-06 10:51:03,2017-03-06 11:36:06
IS,Adding random noise to optimizer,I am trying to implement the idea of adding random gradient noise during backpropagation I have implemented it by modifying the old python code and it works but I would like to implement it into the current c optimizers src operator optimizer op inl h I found that I need a random variable generator to do that and found some relevant code in src operator dropout inl h What I do not know is how to add a resource requester into optimizers Can I simply use an old fashioned way Or can I add a resource requester in NNVM REGISTER OP Any help would be appreciated,,sxjscience,2017-03-06 09:03:26,2017-03-06 12:58:17
IS,in the mxnet0 9 4 has a bug in python35 win10 but in python27 is normal,1 2,,yajiedesign,2017-03-04 15:59:57,2017-03-06 15:12:20
PR,fix str full width characters,,,yajiedesign,2017-03-06 08:06:32,2017-03-06 17:21:58
PR,fix typo,,,Godricly,2017-03-06 07:03:10,2017-03-06 17:28:39
IS,Visualization broken on LSTM,Failed to visualize a LSTM network on Jupyter Notebook MXNet version 3b9d82dd909f9a0bdae3a6177db515ede5c53796 Code,,"eric-haibin-lin,Piyush3dB,eric-haibin-lin",2017-02-17 22:48:33,2017-03-06 18:18:24
PR,doc refactored api python module py,a preview is available at added a summary list slightly changed docstring for the sphinx format also removed the github bar and lang selector in navbar,,"mli,Godricly",2017-03-06 06:24:47,2017-03-06 19:36:13
IS,how to print loss value for each batch,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 how to print loss value for each batch 2 3 What have you tried to solve it 1 I have read the issue 809 and read the callback py but I do not think its what I want Does anyone have any ideas 2 3,,"luoyetx,TheTweak",2017-03-03 15:19:14,2017-03-06 22:39:02
IS,how to use mx sym Reshape a row matrix Copy four times according to the direction of the row,I have the mx sym Reshape introduction and using method But I do not know the other using method for example B mx sym Reshape data A shape 1 the shape 2 3 4 and so on In the mxnet official website about symbol Reshape it does not have any using code example This following code which I konw,,"kevinthesun,sxjscience",2017-01-07 10:48:52,2017-03-07 02:42:14
IS,specific initialize for the variable inside mx sym,in prelu the variable inside is gamma which share the same name with bn and the default init of gamma in bn is 1 0 but in prelu it need be initialized less than 1 0 for convergence always 0 25 how to use the new initialization api mx init InitDesc just tryed two wrong usage,,"tornadomeet,piiswrong,tornadomeet",2017-03-07 06:13:49,2017-03-07 06:25:51
PR,Fixed docs,symbol md referred to mxnet symbol var does not exist a mx sym var 'a' Fixed in all places to Variable e g a mx sym Variable 'a' Also improved documentation on README md,,zackchase,2017-03-07 07:25:55,2017-03-07 17:20:29
IS,Could we make CI tests available locally,Could we make as many tests as possible available locally so that a developer can invoke make test to test through locally,,"jli05,eric-haibin-lin",2017-03-07 14:33:51,2017-03-07 18:31:07
IS,error loading the Inception or CaffeNet model,I downloaded the Inception and CaffeNet models from the model zoo but it has the following errors loading them code import os urllib import mxnet as mx model load mx model FeedForward load ' full caffenet' 0 ctx mx gpu int 0 numpy batch size 1 mx viz plot network model load symbol Errors 21 50 21 mxnet dmlc core include dmlc logging h 235 21 50 21 mxnet dmlc core include dmlc json h 635 Check failed ch ' ' Error at Line 0 around DOCTYPE HTML PUBLIC IETF DTD HTML 2 0 EN Expect ' ' but get ' ' Traceback most recent call last File test py line 6 in module model load mx model FeedForward load ' full caffenet' 0 ctx mx gpu int 0 numpy batch size 1 File mxnet python mxnet model py line 837 in load symbol arg params aux params load checkpoint prefix epoch File mxnet python mxnet model py line 362 in load checkpoint symbol sym load ' s symbol json' prefix File mxnet python mxnet symbol py line 966 in load check call LIB MXSymbolCreateFromFile c str fname ctypes byref handle File mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 21 50 21 mxnet dmlc core include dmlc json h 635 Check failed ch ' ' Error at Line 0 around DOCTYPE HTML PUBLIC IETF DTD HTML 2 0 EN Expect ' ' but get ' ',,,2017-03-07 21:53:12,2017-03-07 22:18:45
PR,Generalize SoftmaxActivation,For mode instance SoftmaxActivation currently correctly computes the forward pass on any rank 1 tensor on CPU but throws an error on GPU with cuDNN Its highly useful to support mode instance for any rank tensor eg using different image conventions like w x h x c This PR adds proper support for this fixes the broken grad for CPU calculation for mode instance and rank of input 2 makes cuDNN support mode instance and rank of input 2 properly document this behaviour add tests for this behaviour and mode channel for which no tests currently exist,,"sbodenstein,sbodenstein",2017-02-17 20:47:50,2017-03-08 15:41:57
IS,CI test,not a issue,,mli,2017-03-08 21:56:18,2017-03-08 22:00:41
PR,Visual Studio nvcc crash workaround,We observed nvcc crashing on CUB RadixSort compiling in Visual Studio with cuda compilers other than version 8 0 44 This fix switches the sorting from CUB to Thrust when mxnet is being compiled in Windows with any other CUDA version than 8 0 44,,ap-hynninen,2017-03-08 21:17:14,2017-03-08 22:24:35
PR,Added GRU cell from examples to new RNN API,,,"tdomhan,sxjscience,tdomhan,piiswrong,sxjscience,piiswrong,tdomhan,piiswrong,piiswrong,tdomhan,tdomhan,piiswrong,piiswrong",2017-03-02 20:57:58,2017-03-09 00:31:06
IS,I can not find the python file of the definition model only the official training of good models such as imagenet vgg16 vgg19,,,,2017-03-05 13:06:46,2017-03-09 01:09:03
IS,Request on supporting Pooling symbol with NHWC or NCHW feature,Just as convolution operator,,,2017-03-09 01:26:02,2017-03-09 02:23:52
IS,SqueezeNet converted from caffemodel mismatch shape results mod set params ERROR,I recently downloaded caffemodel from DeepScale SqueezeNet and converted caffemodel deploy prototxt squeezenet v1 0 caffemodel in SqueezeNet v1 0 directory to MXNet model using MXNet caffeconvert tool After writing run inference py according to official guide how to use pretrained model to predict I ran python run infercen py It outputed below error messages see Error Message Environment info Operating System TegraX1 Ubuntu 16 04 2 LTS Linux tegra ubuntu 3 10 96 tegra Compiler gcc version 5 4 0 20160609 Ubuntu Linaro 5 4 0 6ubuntu1 16 04 4 Package used Python R Scala Julia python MXNet version 0 9 4 Or if installed from source Yes Python version and distribution Python 2 7 12 Error Message It always logged as above error message Sigh,,"ysh329,tornadomeet,ysh329",2017-03-04 10:59:28,2017-03-09 02:45:38
IS,graph viz broken returns Error in symbol as json operator is invalid for atomic vectors,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 7 RHEL 7 Compiler g Package used Python R Scala Julia R MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo R version 3 3 1 2016 06 21 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 7 x64 build 7601 Service Pack 1 locale 1 LC COLLATE English United States 1252 LC CTYPE English United States 1252 LC MONETARY English United States 1252 4 LC NUMERIC C LC TIME English United States 1252 attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 mxnet 0 9 4 mlbench 2 1 1 loaded via a namespace and not attached 1 htmlwidgets 0 8 magrittr 1 5 rsconnect 0 7 htmltools 0 3 5 visNetwork 1 0 4 tools 3 3 1 Rcpp 0 12 9 8 codetools 0 2 15 stringi 1 1 2 digest 0 6 12 stringr 1 2 0 jsonlite 1 3 Error Message Please paste the full error message including stack trace Error in symbol as json operator is invalid for atomic vectors Minimum reproducible example I try require mlbench Loading required package mlbench require mxnet Loading required package mxnet Loading required package methods data Sonar package mlbench Sonar 61 as numeric Sonar 61 1 train ind c 1 50 100 150 train x data matrix Sonar train ind 1 60 train y Sonar train ind 61 test x data matrix Sonar train ind 1 60 test y Sonar train ind 61 mx set seed 0 model mx mlp train x train y hidden node 10 out node 2 out activation softmax num round 20 array batch size 15 learning rate 0 07 momentum 0 9 eval metric mx metric accuracy graph viz model symbol as json Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error graph viz model symbol as json What have you tried to solve it 1 different parameters and saving the variable out but no luck,,"jeremiedb,jeremiedb",2017-03-08 05:03:21,2017-03-09 03:21:40
IS,any one has 0 9 3 source code,after upgrade to 0 9 4 slower and many stranger bug how can i get the 0 9 3 when i run the example warpctc lstm ocr py get Segmentation fault core dumped,,tornadomeet,2017-03-07 11:40:46,2017-03-09 04:58:23
PR,Cleanup compilations tests,Changed amalgamation Makefile to be flexible with different flags so we can compile the following 1 make USE BLAS atlas 2 make USE BLAS atlas MIN 1 3 make USE BLAS atlas MSHADOW USE MKL 1 4 make USE BLAS atlas MSHADOW USE CUDA 1 5 make USE BLAS atlas DISABLE OPENMP 0 Cleaned up Dockerfile and Jenkins script,,"lxn2,piiswrong,lxn2,lxn2",2017-03-08 21:27:59,2017-03-09 18:05:13
PR,Added Pipeline transformer capabilities with MXNet,This allows the use of spark ml which is the preferred route for the future of spark Note I have not been able to test this because after following the directions on the mxnet page I am still unable to run scalapkg It is based off of something similar that I did for dl4j So if someone could test it that would be awesome If I get it building I will let someone know Example same as before mxnet lives under transformers for pipeline More importantly it opens up the ability to use spark is classification evaluators and to join pipeline stages,,"dmmiller612,yzhliu,yzhliu,yzhliu,yzhliu,dmmiller612,piiswrong,dmmiller612,yzhliu,dmmiller612,dmmiller612,dmmiller612,piiswrong,dmmiller612,yzhliu",2017-03-06 22:53:39,2017-03-09 18:06:15
PR,WIP Proposal Extend imdecode to uint16,I am working with image data that does not fit well into uint8 Given that I am working with 100k 1M images I need a data iterator that is fast such as the ImageIter I have been looking into extending the ImageIter to use other types besides uint8 for reading data It appears that after the initial read there is no problem with container type I have tried writing a new iter in python using python reads and they are still slow So I have turned my attention to extending mxnet is imdecode function to work with uint16 in addition to uint8 This PR is a sketch if there is interest I will continue There are a number of issues remaining just as mshadow has no uint16 type L264 L270 to replace mshadow kUint8 I am open to comments on my approach An alternative would be to allow I O from CV mat files which can store any format,,"jmerkow,piiswrong,jmerkow,piiswrong,jmerkow,piiswrong,jmerkow,piiswrong,piiswrong,jmerkow",2017-03-05 18:05:18,2017-03-09 18:17:12
IS,cannot import name 'rmspropalex update' can be solved by commenting the method,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Amazon Linux Compiler Package used Python R Scala Julia Python3 MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Python 3 5 1 default Sep 13 2016 18 48 37 GCC 4 8 3 20140911 Red Hat 4 8 3 9 on linux Type help copyright credits or license for more information import mxnet Traceback most recent call last File stdin line 1 in module File home ec2 user mxnet python mxnet init py line 23 in module from import optimizer File home ec2 user mxnet python mxnet optimizer py line 6 in module from ndarray import sgd update sgd mom update adam update rmsprop update rmspropalex update ImportError cannot import name 'rmspropalex update' Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 comment the rmspropalex update and it works 2 3,,piiswrong,2017-03-09 19:42:55,2017-03-09 19:54:42
PR,Remove static keyword from ilog2,In file included from src operator tensor ordering op cc 7 In file included from src operator tensor ordering op inl h 17 src operator tensor indexing op h 173 12 warning unused function 'ilog2' Wunused function static int ilog2 unsigned int a 1 warning generated,,"dleen,dleen,piiswrong,dleen",2017-03-09 23:37:38,2017-03-10 00:05:00
PR,symbol not serializable,javelinjs Temporary fix for this issue,,Roshrini,2017-03-09 19:09:40,2017-03-10 01:23:01
PR,Improve Caffe Converter,Improved the documents Refactored the codes without changing semantic meaning Added convert caffe modelzoo py which downloads and converts several models from caffe model zoo Added test converter py The future roadmap support multiple inputs it seems that there are bugs when caffe is not installed namely only installed protobuf pair test with caffe that is comparing the output between mxnet and caffe currently we only test the accuracy of the converted model is reasonable test more caffe models,,mli,2017-03-05 01:24:05,2017-03-10 04:36:39
PR,Add valgrind checks to cpu tests,,,"lxn2,lxn2",2017-03-04 01:41:57,2017-03-10 04:37:23
PR,float16 training for resnet,Adds float16 training for cudnn batch norm Introduces example resnet fp16 py script for float16 training of the resnet model Tested for convergence on resnet 50 by comparing results with float32 training,,"ap-hynninen,sxjscience,ap-hynninen,sxjscience,piiswrong,piiswrong,ap-hynninen,ap-hynninen,piiswrong",2017-03-08 22:52:55,2017-03-10 06:12:19
PR,CI add script to build and run docker,also added pylintrc,,"mli,mli",2017-03-10 05:44:56,2017-03-10 07:25:02
IS,How to estimate the MXNet gpu memory consumption,I have implement a encoder decoder model for machine translation task The lstm implementation is like that in mxnet example rnn old lstm py And the basic setup is as follows But the memory consumption is puzzling I try two experiments with different settings 1 embed 100 hidden 200 memory 1222mb 2 embed 400 hidden 800 memory 2895mb There are two puzzling things 1 The memory consumption is too large compared to the theoretically computed memory for parameters 2 It seems to consume more memory than pytorch with same network architecture maybe due to my poor coding way but how does coding way influence the memory consumption So I have three questions 1 Is it normal for a network with the settings above to consume so much memory 2 How does the coding way influence the memory consumption 3 How to estimate the MXNet gpu memory consumption,,,2017-03-02 08:47:58,2017-03-10 12:13:18
PR,Add str method to EvalMetric to display the metric values,,,"larroy,piiswrong,larroy,larroy",2017-03-09 13:06:25,2017-03-10 17:43:50
PR,Scala module save load checkpoint,,,"yzhliu,Ldpe2G,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,CodingCat,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,yzhliu,piiswrong,CodingCat,yzhliu,yzhliu,CodingCat,piiswrong,yzhliu",2017-02-26 07:40:15,2017-03-10 17:52:14
IS,Installing MXNet on Python 3 6,I'm trying to install MXNet on Python 3 6 I got the libmxnet so compiled but setup py by default installs MXNet on Python 2 7 Is there an easy way to modify setup py such that it will install the MXNet package on Python 3 6 e g usr local lib python3 6 dist packages instead I'm using Ubuntu Server 16 04 LTS Thank you,,,2017-03-04 19:12:08,2017-03-10 18:15:43
IS,NVCC crash when compiling indexing op cu,Environment info Operating System Windows 10 0 14393 Compiler MSVC 19 0 24210 0 VS14 CUDA 8 0 61 Package used Python R Scala Julia libmxnet MXNet commit hash 3b9d82d Error Message 5 Building NVCC Device object CMakeFiles cuda compile dir src operator tensor Release cuda compile generated indexing op cu obj 5 indexing op cu nvcc error cicc died with status 0xC0000005 ACCESS VIOLATION Steps to reproduce 1 check out mxnet 2 turn off openCV support write MKL path info in makefile and generate VS14 sln file 3 open sln set Release x64 and compile I tried to compile the project 4 times It always crash at indexing op cu What have you tried to solve it 1 switch to an older commit b11d3a2 tag v0 9 3 will solve the problem,,"piiswrong,piiswrong,ap-hynninen,piiswrong,piiswrong,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,ap-hynninen,sxjscience,ap-hynninen,piiswrong,ap-hynninen,ap-hynninen,piiswrong,ap-hynninen,ap-hynninen,ap-hynninen,ap-hynninen,piiswrong,ap-hynninen,ap-hynninen,sxjscience",2017-02-16 15:41:23,2017-03-10 18:41:50
PR,Fix spell checker,Fix spell checker sudo issue,,"kevinthesun,lxn2,lxn2,lxn2,lxn2,piiswrong,kevinthesun,piiswrong,kevinthesun,mli",2017-03-08 02:29:42,2017-03-10 19:12:43
PR,Remove static keyword from ilog2,In file included from src operator tensor ordering op cc 7 In file included from src operator tensor ordering op inl h 17 src operator tensor indexing op h 173 12 warning unused function 'ilog2' Wunused function static int ilog2 unsigned int a 1 warning generated Also fix filename in documentation,,dleen,2017-03-10 23:24:59,2017-03-11 01:49:13
PR,Fix spell checker,Move dependency installation to ci build,,kevinthesun,2017-03-10 21:40:43,2017-03-11 05:40:53
PR,Replace concatenate with concat when merging from ctxs,,,"sxjscience,mli",2017-03-08 15:56:52,2017-03-11 18:12:00
PR,mkl spring update 1,1 solve resnet inception v3 do not coverage for memory reuse 2 bn support global stats Signed off by lingyan lingyan guo intel com,,"glingyan,glingyan",2017-03-11 09:03:50,2017-03-11 22:59:43
PR,Scala add SequentialModule,follows training log,,Ldpe2G,2017-03-11 03:41:11,2017-03-12 05:31:26
PR,Fix build on VS,I find that the build fails when using VS2015 Seems that the VS has some bugs in nested macro expansion This commit should fix the build on VS have you met this problem,,"sxjscience,yajiedesign",2017-03-12 05:16:23,2017-03-12 06:37:08
IS,Bucketing module checkpoint handling,It would have been nice if there were high level checkpoint functions that were able to save restore all symbols used by the bucketing module Current default functionality save rnn checkpoint saves a random symbol that happens to be current at the epoch end,,"sergeykolychev,Godricly,sergeykolychev",2017-03-07 08:58:28,2017-03-12 08:49:45
IS,Problem with 'test' after saving checkpoint with stack rnn True for cudnn lstm bucketing py example,,,"sergeykolychev,sergeykolychev",2017-03-10 10:29:33,2017-03-12 08:50:45
IS,Error when change num hidden 256,Environment info Operating System Deep Learning AMI for Amazon Linux This is beta version of the Deep Learning AMI for Amazon Linux Error Message 22 45 14 home ec2 user src mxnet dmlc core include dmlc logging h 235 22 45 14 src ndarray ndarray cc 231 Check failed from shape to shape operands shape mismatch Traceback most recent call last File line 10 in File home ec2 user mxnet notebooks python tutorials1 rnn model py line 34 in init arg params key copyto self executor arg dict key File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 533 in copyto return internal copyto self out other File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 1225 in unary ndarray function c array ctypes c char p c str str i for i in kwargs values File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 22 45 14 src ndarray ndarray cc 231 Check failed from shape to shape operands shape mismatch Steps to reproduce When following I changed num hidden from 512 to 256 and the model training finished without errors But I got the following error message when I tried to make the inference model at the following 3rd step 1 num hidden 256 2 model fitting etc 3 model rnn model LSTMInferenceModel num lstm layer len vocab 1 num hidden num hidden num embed num embed num label len vocab 1 arg params arg params ctx mx gpu dropout 0 2 I tried different values for num hidden and they all give the above error,,,2017-03-11 21:01:50,2017-03-12 19:13:19
PR,BidirectionalCell and SequentialRNNCell Unroll,piiswrong haibin lin Changes include 1 BidirectionalCell support 2 Unroll changes for SequentialRNNCell to avoid depending on stepped unroll 3 cudnn example changes,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,szha,piiswrong,szha,szha,piiswrong,szha,piiswrong,szha,szha,piiswrong,piiswrong,piiswrong,fhieber,szha,fhieber,piiswrong",2017-03-11 05:49:49,2017-03-12 22:37:22
PR,Fix docstring,fix doc string,,Godricly,2017-03-13 02:17:52,2017-03-13 05:18:28
PR,Fix plus icon position,,,kevinthesun,2017-03-13 00:43:39,2017-03-13 05:42:51
IS,Why is the training speed too slow and is the performance so weird on 4 Titan X GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 5 Compiler gcc 4 8 4 Package used Python R Scala Julia python MXNet version Or if installed from source installed by git clone mxnet recursive MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 13 anaconda 4 3 0 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3 Build environment nvidia 375 26 cuda 8 0 gcc 4 8 4 ubundu 14 04 5 cudnn 5 1 When I use 4 pascal GPUs Titan X to retrain ResNet 50 from model load epoch 90 on imagenet'12 dataset The speed is very slow Furthermore the accuracy is also wrong The following picture shows two procedures And there are two weird accuracy MXNET is the latest version I do not know why Before change to pascal GPU I use 4 M40 GPU have no issue Old MXNET version please see the second picture image Obviously this is an abnormal phenomenon Normally the results should be shown following as image,,"piiswrong,ap-hynninen,ap-hynninen",2017-02-20 15:02:00,2017-03-13 08:08:13
IS,Uploading to shinyapps,Hi guys just found this amazing package I was trying to upload to shinyapps io but stumble upon this error knowing that mxnet is not in CRAN repository and different installation than just devtools install github is it possible to make an app in shinyapps with this package thanks,,"jeremiedb,statist-bhfz,thirdwing,thirdwing",2017-03-04 15:33:22,2017-03-13 14:33:45
PR,fixed a typo,,,sergeykolychev,2017-03-13 07:12:53,2017-03-13 16:17:59
PR,Add unfuse to FusedRNNCell,piiswrong This pr adds unfuse to FusedRNNCell which generates an equivalent SequentialRNNCell of the FusedRNNCell of the same spec which can be used for step unroll and can run on CPU This helps solve the problem that users have to understand the naming convention of the FusedRNN parameters by taking care of the naming conventions inside the function,,"szha,sergeykolychev,szha,sergeykolychev,loofahcus,szha",2017-03-13 07:04:51,2017-03-13 16:34:21
IS,Import Error update after latest pull,After pulling the latest version of mxnet on Monday March 13 2017 I get the following import error when I try import mxnet as mx ImportError cannot import name rmspropalex update I was able to solve it by removing the import rmspropalex update from file mxnet python mxnet optimizer py but I am not sure if that is going cause any problems later on Below is the detailed information Environment info Operating System mac OS X El Capiton version 10 11 5 Compiler clang 800 0 38 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 8861ccc032fa64f4a13b84fb82f6edb4febef2be Python version and distribution Python 2 7 12 Anaconda 4 2 0 Error Message Traceback most recent call last File prover py line 23 in module import mxnet as mx File Users Forough mxnet python mxnet init py line 23 in module from import optimizer File Users Forough mxnet python mxnet optimizer py line 6 in module from ndarray import sgd update sgd mom update adam update rmsprop update rmspropalex update ImportError cannot import name rmspropalex update Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx What have you tried to solve it 1 I was able to solve it by removing the import rmspropalex update from file mxnet python mxnet optimizer py,,piiswrong,2017-03-13 17:35:39,2017-03-13 17:39:32
PR,WIP refactor loss layers,sbodenstein refactored handling of loss layers Output layers will be deprecated going forward Will add documentations next I will make a log softmax layer that can do reduce on any axis,,"piiswrong,sbodenstein,piiswrong,sbodenstein,sbodenstein,piiswrong",2017-03-09 00:23:48,2017-03-13 17:43:53
PR,CI Jenkinsfile with CPU GPU build and unittest,This PR adds a Jenkinsfile which defines the CI workflow It improves our previous jenkins setup by a single jenkins task contains all jobs which is more maintainable and readable the so a files are re used between different tasks all PRs and branches are tested a demo is available at This PR address some aspects in issue 5321,,"mli,piiswrong,mli,mli",2017-03-13 07:22:36,2017-03-13 19:14:04
PR,fix the link of operation engine documentation,,,jingpengw,2017-03-13 13:21:57,2017-03-13 19:34:28
IS,MXNet v0 9 3a bug,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Docker Ubuntu 14 04 Package used Python R Scala Julia Python MXNet version 0 9 3a Python version and distribution 2 7 6 Error Message This bug is caught by Yao is notebook automated testing script with Karishma is linear regression ipynb notebook MXNet score mean square error metric sometimes returns 'nan' Minimum reproducible example In mxnet notebooks python tutorials linear regression ipynb cell No 9 and No 10 It might be related to NDArrayIte reset,,"kevinthesun,kevinthesun",2017-02-23 18:59:31,2017-03-13 19:47:23
IS,Illegal Instruction core dumped,I am learning mxnet but while using mod fit the program always throws Illegal Instruction core dumped The code I am using is completely taken from the mxnet io website the problem remains same even if I change the data iterator to different function please help thank you The code is import mxnet as mx import numpy as np class SimpleBatch object def init self data label pad None self data data self label label self pad pad class SimpleIter def init self mu sigma batch size num batches self mu mu self sigma sigma self batch size batch size self num batches num batches self data shape batch size mu shape 1 self label shape batch size self cur batch 0 def iter self return self def reset self self cur batch 0 def next self return self next def provide data self return wouldata' self data shape def provide label self return isoftmax label' self label shape def next self if self cur batch self num batches self cur batch 1 num classes self mu shape 0 label np random randint 0 num classes self label shape data np zeros self data shape for i in range num classes data label i np random normal self mu i self sigma i sum label i self data shape 1 return SimpleBatch data mx nd array data label mx nd array label pad 0 else raise StopIteration class SyntheticData Genrate synthetic data def init self num classes num features self num classes num classes self num features num features self mu np random rand num classes num features self sigma np ones num classes num features 0 1 def get iter self batch size num batches 10 return SimpleIter self mu self sigma batch size num batches mlp net mx sym Variable wouldata' net mx sym FullyConnected net name 'fc1' num hidden 64 net mx sym Activation net name arelu1' act type relu net mx sym FullyConnected net name 'fc2' num hidden 10 net mx sym SoftmaxOutput net name isoftmax' synthetic 10 classes dataset with 128 dimension data SyntheticData 10 128 mx viz plot network net mod mx mod Module symbol net context mx cpu data names wouldata' label names isoftmax label' import logging logging basicConfig level logging INFO batch size 32 mod fit data get iter batch size eval data data get iter batch size optimizer isgd' optimizer params 'learning rate' 0 1 eval metric 'acc' num epoch 5 y mod predict data get iter batch size ishape of predict s' y shape,,piiswrong,2017-03-13 06:33:33,2017-03-13 19:48:36
PR,CI test triggle,,,mli,2017-03-13 21:12:31,2017-03-13 21:41:31
IS,Keras MXNet Interface Keras MXNet,In keras example imdb lstm py random binomial NotImplementedError In keras example convolution lstm py mxnet base MXNetError value 0 for Parameter num outputs should be greater equal to 1 in operator SliceChannel name num outputs 0 squeeze axis 1 In keras example mnist acgan py Error SoftmaxCrossEntropy only accept 1D label In keras example antirectifier py AssertionError Symbol mulscalar0 None In keras example variational autoencoder deconv py mxnet base MXNetError Error in operator uniform15 18 19 01 src operator deconvolution inl h 75 Check failed pad y target shape 0 16 vs 100 too big target shape When training keras example variational autoencoder py on multiple gpu mxnet base MXNetError Error in operator broadcast mul1 18 19 05 src operator tensor elemwise binary broadcast op h 48 Check failed l 1 r 1 operands could not be broadcast together with shapes 50 2 100 2 In keras examples imdb bidirectional lstm py mnist hierarchical rnn py babi rnn py and addition rnn py Training on multiple gpu returns low accuracy In keras example imdb cnn lstm py mxnet base MXNetError Error in operator pooling0 18 07 08 src operator pooling inl h 216 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 4 exceeds input 1 padded to 1 In keras example babi rnn py It takes about 30 minutes for eras mxnet backend to build babi rnn model on m4 xlarge cpu machine while tensor flow and theano only takes a few seconds,,"kevinthesun,kevinthesun",2017-03-13 20:43:12,2017-03-13 22:11:04
PR,test ci do not merge,,,mli,2017-03-13 21:16:29,2017-03-13 22:17:25
PR,test Update README md,,,mli,2017-03-13 22:01:49,2017-03-13 22:17:33
PR,ci run make clean before make,Try to do an incremental build first fallback to building from scratch if fails,,mli,2017-03-13 22:47:00,2017-03-14 00:41:38
PR,CI Amalgamation,,,mli,2017-03-14 01:22:22,2017-03-14 04:07:31
IS,How to propagate a negative gradient to maximize the result,I want to train a network to maximize the cosline similarity between two vector any idea to do so Thanks in advance,,piiswrong,2017-03-14 03:21:41,2017-03-14 09:31:17
IS,bin im2rec speed is very slow,the performance bin im2rec is well at beginning but after a while the speed is dropped down 10x 100x slower so what is wrong i encounter this many times no other task is running in the linux server 200w small image with less than 30 30 size,,"tornadomeet,piiswrong,tornadomeet,mli,tornadomeet",2017-03-10 02:24:47,2017-03-14 09:42:18
IS,neural style example error with gpu 0 ok with gpu 1,Environment info Operating System Ubuntu 16 04 Python version and distribution 2 7 CUDA V8 0 61 cuDNN 5 1 NVIDIA GTX1060 MXNET commit parse head abf4c76804a133c4e4adba1b817cbf50cafaf243 I can run R examples on gpu with no error Steps to reproduce Run the neural style example If i run with gpu 1 i e CPU it works,,,2017-03-13 22:58:25,2017-03-14 10:55:32
PR,CI Add amalgamation build,in addition add retry to git submodule update,,mli,2017-03-14 17:03:08,2017-03-14 17:03:45
PR,Signed Unsigned warnings fixed build system adjusted for scalable container builds,,,"cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,yzhliu,yzhliu,piiswrong,piiswrong,piiswrong,cjolivier01",2017-03-08 22:26:10,2017-03-14 19:26:06
PR,Fix bug in instance norm op,InstanceNorm currently fails for input tensors with less than 3 elements as there is an incorrect check for the backward mode This is fixed In addition there is no reason to limit input tensor ranks to 2 as it currently does It works perfectly fine with rank 2 tensors so have generalized this op,,"sbodenstein,piiswrong,sbodenstein",2017-03-14 12:25:25,2017-03-14 21:06:59
PR,Scala add optimizer DCASGD,follows 5140 training logs of mnist dataset,,Ldpe2G,2017-03-14 08:47:12,2017-03-14 21:10:00
PR,module api and mxnet scala through ide doc added,javelinjs Module API doc and how to run MXNet Scala through IDE doc added,,Roshrini,2017-03-13 23:24:53,2017-03-14 21:35:01
PR,Improvements to PTB bucketed LSTM,dropout support needed for many stacked LSTM models from language modeling to NMT including the repro of the TensorFlow PTB language model support of non SGD momentum optimizers need to renove momentum option from optimizer param list,,"mkolod,mkolod",2017-03-14 20:50:40,2017-03-14 23:03:56
PR,add 'export' to benchmark variable settings,,,szha,2017-03-14 19:54:15,2017-03-15 00:12:07
PR,added dockerfile for crosstool chain,Dockerfile to setup cross tool and build just build mxnet for ARM devices via Jenkins test infrastructure,,"arank,mli",2017-03-15 00:55:16,2017-03-15 03:24:06
PR,add timeout to each build,,,mli,2017-03-15 05:12:40,2017-03-15 05:12:59
IS,Documentation of monitor,Hello I would like to log the outputs of certain layers of my NN in a forward sweep Perhaps a monitor could be a solution to this but I have found no documentation whatsoever beside the install monitor function for a module Could you give me hitn where to find more information Thanks,,,2017-03-14 15:38:40,2017-03-15 08:16:09
IS,Added weights have no impact on results,Hello I am trying to add weights to my NN is outputs before forwarding them into the SoftmaxOutput layer The weights are fixed parameter that I pass to the module during binding Strangely enough these weights have no effect whatsoever on the softmax ouput weights mx sym Variable 'weights' various layers fc2 mx sym FullyConnected weighted fc2 fc2 weights This layer has no effect loss mx sym SoftmaxOutput weighted fc2 name isoftmax' I have tried this with and without various weights and always obtained exactly the same outputs for all training steps with given random seed The module is created like this weights mx nd array self module mx mod Module self model data names 'weights' fixed param names 'weights' self module bind label shapes data shapes wouldata' 'weights' weights shape self module init params arg params 'weights' weights allow missing True I am using Python 3 6 on Windows MXNet last updated on 09 03 17 Could you give me some advice on how to achieve what I am trying to do namely adding a fixed quantity to the output before sending it into the SoftmaxOutput Many thanks in advance for any clarifications,,,2017-03-14 15:57:32,2017-03-15 08:46:16
IS,A3C example needs improvements on readability,The current version of A3C is out dated and have very few comments Some of the test codes are no longer compatible with other script Will this example be updated some day in the future Any kind of refinement is highly appreciated Thanks,,"loofahcus,loofahcus,loofahcus",2017-03-01 09:49:56,2017-03-15 11:59:37
PR,Improvements to PTB bucketed LSTM,dropout support needed for many stacked LSTM models from language modeling to NMT including the repro of the TensorFlow PTB language model support of non SGD momentum optimizers need to renove momentum option from optimizer param list,,"mkolod,piiswrong,mkolod,piiswrong,mkolod,piiswrong",2017-03-14 23:12:17,2017-03-15 17:06:05
PR,List string metrics on the documentation of BaseModule fit,,,larroy,2017-03-15 10:57:56,2017-03-15 17:07:42
PR,Padding GPU allocations,Padding GPU allocations a first step to allow scattering arrays across GPUs for NCCL support,,"ptrendx,piiswrong,piiswrong,ptrendx,ptrendx,piiswrong,piiswrong",2017-03-06 19:44:42,2017-03-15 17:12:21
PR,fix caffe importer,Fixing undefined variable fname being passed to open Affects the importer when only protobuf is installed not Caffe,,sbodenstein,2017-03-15 15:48:42,2017-03-15 18:08:00
PR,Enable MSHADOW USE SSE use SSE3 outside MSVC,This brings the CMake build closer to parity with the Makefile Enable MSHADOW USE SSE on MSVC This is safe because arch SSE2 is the default for x86 platforms on the lowest supported MSVC 2013 v vs 120 aspx Enable MSHADOW USE SSE when msse3 is valid on gcc This protects SSE from being problematically enabled on non x86 platforms Switch msse2 to msse3 on gcc clang There is no direct SSE3 support on MSVC only SSE2 by default and AVX 2 Note This has a side effect of assuming that we are never targeting ARM on MSVC if we wish to leave the door open for this target we need to trap SSE behind an option flag or perform a test with the compiler as the Visual Studio generators do not offer a simple way to bind preprocessor defines to architectures Please let me know if this is the case,,"alextnewman,piiswrong",2016-12-07 14:30:53,2017-03-15 18:35:14
PR,construct symbol for converting ZF net,constructed symbol and converted ZFnet caffemodel for fine tuning with faster rcnn,,"liangfu,piiswrong,liangfu,precedenceguo,liangfu,precedenceguo,liangfu,piiswrong,mli,liangfu,liangfu",2016-12-13 10:57:23,2017-03-15 18:37:01
PR,Torch plugin update for chages of MXNet and Torch,Hi Torch plugin is unable to compile with latest MXNet and Torch because there are some changes MXNet side TShape was moved from mshadow to mxnet diff 3182059d3b0e272b01119635baa09eb3 Torch side THCState does not have a cudaStream t currentStream field anymore Instead of that I used cudaStream t THCState getCurrentStream THCState state remaining issues I have tested running torch function py and torch module py in examples torch with ctx mx cpu 0 mx gpu 0 mx gpu 1 but sometimes not always only torch module py with ctx mx gpu raised errors as follows,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,mli",2016-12-22 02:20:00,2017-03-15 18:42:31
PR,WIP scalar support for unary ndarray ie nd sqrt 2 nd log 3,ref BTW it looks like MXNet NDArray only accept array like data so nd sqrt 2 will return array 1 41421354 instead of array 1 41421354,,"ZihengJiang,piiswrong,piiswrong,piiswrong,ZihengJiang,piiswrong,mli",2017-02-04 00:53:43,2017-03-15 18:46:29
PR,debugging mxnet c operator,simple example on how to debug c operators of mxnet,,piiswrong,2017-02-07 17:36:51,2017-03-15 18:49:04
PR,CI improve pylint,added a pylintrc file with disabled messages copied from tensorflow fixed pylint errors in python mxnet TODOs I disabled invalid name in image py and rnn cell py temporarily to pass the lint I also added too many good names to pass the lint We also should check the folders tools example tests in the future,,mli,2017-03-08 20:28:19,2017-03-15 18:58:45
PR,Fix tests target in Makefile and add automatic running of tests,Add I GTEST INC when generating d files otherwise make test fails on Line 8 of tests cpp unittest mk on macOS Sierra,,jli05,2017-03-04 15:52:18,2017-03-15 19:08:51
IS,Out of bounds error in NDArray iterator,Environment info Operating System OSX Compiler default Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 9414d51ebe9f7fdb505f736fe407fe68b743da4d Python version and distribution Python 2 7 12 Error Message 09 44 11 mxnet dmlc core include dmlc logging h 300 09 44 11 mxnet include mxnet ndarray h 276 Check failed shape 0 idx 3 vs 3 index out of range Stack trace returned 6 entries bt 0 0 libmxnet so 0x000000010bb7f48e ZN4dmlc15LogMessageFatalD2Ev 46 bt 1 1 libmxnet so 0x000000010bb6dbc5 ZN4dmlc15LogMessageFatalD1Ev 21 bt 2 2 libmxnet so 0x000000010bb71a64 ZNK5mxnet7NDArray2AtEj 644 bt 3 3 libmxnet so 0x000000010bb716b5 MXNDArrayAt 101 Minimum reproducible example import mxnet as mx x mx nd array 1 2 3 for a in x print a What have you tried to solve it Add a bounds check in ndarray py,,dleen,2017-03-14 23:40:00,2017-03-15 20:35:50
PR,Fix out of bounds error in iterator,Fixes 5401 import mxnet as mx x mx nd array 1 2 3 for a in x print a NDArray 1 0 NDArray 1 0 NDArray 1 0 09 44 11 mxnet dmlc core include dmlc logging h 300 09 44 11 mxnet include mxnet ndarray h 276 Check failed shape 0 idx 3 vs 3 index out of range Stack trace returned 6 entries bt 0 0 libmxnet so 0x000000010bb7f48e ZN4dmlc15LogMessageFatalD2Ev 46 bt 1 1 libmxnet so 0x000000010bb6dbc5 ZN4dmlc15LogMessageFatalD1Ev 21 bt 2 2 libmxnet so 0x000000010bb71a64 ZNK5mxnet7NDArray2AtEj 644 bt 3 3 libmxnet so 0x000000010bb716b5 MXNDArrayAt 101,,dleen,2017-03-14 23:46:52,2017-03-15 20:35:50
IS,using float16 check fail,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Win7 Compiler VS2015 Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 3 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace d program files x86 jenkins workspace mxnet mxnet dmlc core include dmlc logging h 300 21 26 49 D Program Files x86 Jenkins workspace mxnet mxnet include mxnet operator h 267 Check failed in type at i mshadow default type flag in type at i 1 Unsupported data type 2 Traceback most recent call last File C Program Files x86 JetBrains PyCharm 2016 3 2 helpers pydev pydevd py line 1596 in module Minimum reproducible example if you are using your own code please provide a short script that reproduces the error example code example image classification alexnet fp16 py Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 run train cifar10 py parameter is network alexnet fp16 batch size 64 gpus 0 2 3 What have you tried to solve it 1 2 3,,"piiswrong,piiswrong",2017-03-15 13:36:27,2017-03-16 07:35:21
IS,I wanna train ResNet 50 based on my data,As title I have two questions below 1 Which repo the model ResNet 50 comes from From tornadomeet ResNet or KaimingHe deep residual networks converted by tool 2 What is the detailed preprocessing of these image data on pretrained stage,,"ysh329,ysh329,thatindiandude,mli,ysh329",2017-03-15 02:37:41,2017-03-16 09:09:39
IS,Distributed training Speed extremely slow,Environment info Operating System Centos 7 0 Package used Python R Scala Julia Python MXNet version 0 93 Python version and distribution 2 7 We are testing the distributed train over multi machine mxnet We compile Mxnet with parameter USE DIST KVSTORE 1 and successful run train a MLP on mnist But we found that the speed of distributed training was unusually slow compared to the speed of stand alone training Stand alone commands and speed python train mnist py INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Epoch 0 Batch 100 Speed 21875 54 samples sec Train accuracy 0 771040 INFO root Epoch 0 Batch 200 Speed 21260 40 samples sec Train accuracy 0 913906 INFO root Epoch 0 Batch 300 Speed 21302 58 samples sec Train accuracy 0 933750 INFO root Epoch 0 Batch 400 Speed 21216 02 samples sec Train accuracy 0 936875 INFO root Epoch 0 Batch 500 Speed 22835 21 samples sec Train accuracy 0 933594 INFO root Epoch 0 Batch 600 Speed 21612 71 samples sec Train accuracy 0 947500 INFO root Epoch 0 Batch 700 Speed 23362 35 samples sec Train accuracy 0 954375 INFO root Epoch 0 Batch 800 Speed 21683 75 samples sec Train accuracy 0 954219 INFO root Epoch 0 Batch 900 Speed 21656 14 samples sec Train accuracy 0 955781 INFO root Epoch 0 Train accuracy 0 958615 INFO root Epoch 0 Time cost 2 804 INFO root Epoch 0 Validation accuracy 0 957803 Command and speed of distributed training on local machine by using two workers launch py n 2 launcher local python train mnist py kv store dist sync INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Epoch 0 Batch 100 Speed 2580 90 samples sec Train accuracy 0 104425 INFO root Epoch 0 Batch 100 Speed 2574 24 samples sec Train accuracy 0 096689 INFO root Epoch 0 Batch 200 Speed 4408 62 samples sec Train accuracy 0 093594 INFO root Epoch 0 Batch 200 Speed 4399 50 samples sec Train accuracy 0 099531 INFO root Epoch 0 Batch 300 Speed 3343 81 samples sec Train accuracy 0 095469 INFO root Epoch 0 Batch 300 Speed 3342 38 samples sec Train accuracy 0 097344 INFO root Epoch 0 Batch 400 Speed 3247 69 samples sec Train accuracy 0 096250 INFO root Epoch 0 Batch 400 Speed 3245 97 samples sec Train accuracy 0 104531 INFO root Epoch 0 Batch 500 Speed 3364 46 samples sec Train accuracy 0 102188 INFO root Epoch 0 Batch 500 Speed 3364 97 samples sec Train accuracy 0 098437 INFO root Epoch 0 Batch 600 Speed 3729 89 samples sec Train accuracy 0 097500 INFO root Epoch 0 Batch 600 Speed 3732 76 samples sec Train accuracy 0 097812 INFO root Epoch 0 Batch 700 Speed 5105 08 samples sec Train accuracy 0 087969 INFO root Epoch 0 Batch 700 Speed 5097 00 samples sec Train accuracy 0 104375 INFO root Epoch 0 Batch 800 Speed 3931 05 samples sec Train accuracy 0 099062 INFO root Epoch 0 Batch 800 Speed 3930 63 samples sec Train accuracy 0 101406 INFO root Epoch 0 Batch 900 Speed 3763 65 samples sec Train accuracy 0 098906 INFO root Epoch 0 Batch 900 Speed 3758 21 samples sec Train accuracy 0 099531 INFO root Epoch 0 Train accuracy 0 104307 INFO root Epoch 0 Time cost 16 560 INFO root Epoch 0 Train accuracy 0 099662 INFO root Epoch 0 Time cost 16 584 INFO root Epoch 0 Validation accuracy 0 098029 INFO root Epoch 0 Validation accuracy 0 098029 Command and speed of distributed training on two computers with same hardware configration launch py n 2 launcher ssh H hosts python train mnist py kv store dist sync INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Epoch 0 Batch 100 Speed 614 03 samples sec Train accuracy 0 097463 INFO root Epoch 0 Batch 100 Speed 613 66 samples sec Train accuracy 0 096225 INFO root Epoch 0 Batch 200 Speed 577 24 samples sec Train accuracy 0 097812 INFO root Epoch 0 Batch 200 Speed 576 24 samples sec Train accuracy 0 096250 INFO root Epoch 0 Batch 300 Speed 609 78 samples sec Train accuracy 0 094531 INFO root Epoch 0 Batch 300 Speed 610 73 samples sec Train accuracy 0 098125 INFO root Epoch 0 Batch 400 Speed 624 24 samples sec Train accuracy 0 100000 INFO root Epoch 0 Batch 400 Speed 623 74 samples sec Train accuracy 0 100625 INFO root Epoch 0 Batch 500 Speed 642 81 samples sec Train accuracy 0 096719 INFO root Epoch 0 Batch 500 Speed 643 28 samples sec Train accuracy 0 099531 INFO root Epoch 0 Batch 600 Speed 613 65 samples sec Train accuracy 0 097187 INFO root Epoch 0 Batch 600 Speed 613 63 samples sec Train accuracy 0 096406 INFO root Epoch 0 Batch 700 Speed 626 13 samples sec Train accuracy 0 106875 INFO root Epoch 0 Batch 700 Speed 626 15 samples sec Train accuracy 0 096562 INFO root Epoch 0 Batch 800 Speed 621 21 samples sec Train accuracy 0 098437 INFO root Epoch 0 Batch 800 Speed 620 83 samples sec Train accuracy 0 099687 INFO root Epoch 0 Batch 900 Speed 589 75 samples sec Train accuracy 0 098281 INFO root Epoch 0 Batch 900 Speed 589 72 samples sec Train accuracy 0 104844 INFO root Epoch 0 Train accuracy 0 101351 INFO root Epoch 0 Time cost 98 129 INFO root Epoch 0 Train accuracy 0 097551 INFO root Epoch 0 Time cost 98 144 INFO root Epoch 0 Validation accuracy 0 098029 INFO root Epoch 0 Validation accuracy 0 098029 INFO root Epoch 1 Batch 100 Speed 639 28 samples sec Train accuracy 0 097308 INFO root Epoch 1 Batch 100 Speed 640 32 samples sec Train accuracy 0 096844 We do not know why the speed so slow in distributed training someone can help us Thanks,,"mli,piiswrong,mli",2017-02-27 03:19:23,2017-03-16 09:21:15
IS,Is nd file nessessary in predict cpp using pretrained model,I want to write prediction using predict cpp and I read the image classification predict cc I found at least 4 params defined by user 1 json file from model zoo 2 param file from model zoo 3 synset file from example code prediction using pretrained model 4 nd file mean file I do not know how can I get this file but I found a intro about this in train process the mean file path is not exists so will automatically caculate the file and save the mean in test process the mean file path is exists so will load and use the mean from How to get or generate the mean image Issue 604 dmlc mxnet Thus if I wirte prediction using cpp how can I get nd file mean file or Is this file a nessessary option,,"ysh329,ysh329",2017-03-16 08:14:57,2017-03-16 13:12:32
PR,fix typo,,,,2017-03-16 11:54:22,2017-03-16 13:31:20
IS,Batch epoch number ambiguity in GPU CPU cluster,I have such a code below This would be okay if self batchEnd in 3 GPU clusters was called by 1 3 of self batchEnd in 1 GPU However even if GPU count is 1 2 3 or more the number of self batchEnd calls is the same I have confirmed this situation by watching when self epochEnd is called After the last self batchEnd call self epochEnd is called Another option would be to share the same batch by GPU cluster However in that case GPU cluster does not have any advantage over single GPU since single batch in 1 GPU takes 4 seconds but the single batch in 3 GPU takes 5 6 seconds So could you clarify this point If I use n GPU s each epoch in n GPU s contains n epochs in single GPU Is it true Off topic Let is assume I have a PC with 2 CPUs Is there any difference between context mx cpu and context mx cpu 0 mx cpu 1 in terms of time efficiency To my calculations and understandings from your help documentation they are the same,,,2017-03-11 16:53:02,2017-03-16 15:50:59
PR,docs Update invalid notebook links,Due to dmlc mxnet notebooks original MXNet notebooks were removed Thus some notebook links were invalid,,ysh329,2017-03-16 14:49:22,2017-03-16 17:41:16
PR,Scalable build and warning fixes,,,"cjolivier01,mli,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong",2017-03-14 19:49:07,2017-03-16 17:49:48
IS,Very Slow MxNet distributed training,I am working on the example training mnist py and I notice that it is very slow when I run the distributed training Also cross entropy value gave me Nan When I ran it on my single machine everything works fine but I believe it has slow speed training per sample Here is the result for training on single machine python train mnist py network mlp INFO root Epoch 0 Batch 100 Speed 40726 16 samples sec Train cross entropy 1 054846 INFO root Epoch 0 Batch 200 Speed 41420 49 samples sec Train cross entropy 0 544122 INFO root Epoch 0 Batch 300 Speed 41170 83 samples sec Train cross entropy 0 471626 INFO root Epoch 0 Batch 400 Speed 39867 68 samples sec Train cross entropy 0 437326 INFO root Epoch 0 Batch 500 Speed 40247 01 samples sec Train cross entropy 0 414339 INFO root Epoch 0 Batch 600 Speed 44374 19 samples sec Train cross entropy 0 396647 INFO root Epoch 0 Batch 700 Speed 40458 16 samples sec Train cross entropy 0 377359 INFO root Epoch 0 Batch 800 Speed 39988 00 samples sec Train cross entropy 0 360503 INFO root Epoch 0 Batch 900 Speed 40203 55 samples sec Train cross entropy 0 358936 INFO root Epoch 0 Train cross entropy 0 350466 INFO root Epoch 0 Time cost 1 476 I changed map py file to def get symbol num classes 10 kwargs data mx symbol Variable wouldata' data mx sym Flatten data data fc3 mx symbol FullyConnected data data name 'fc3' num hidden num classes mlp mx symbol SoftmaxOutput data fc3 name isoftmax' return mlp Here is the result for distributed training tools launch py n 3 H hosts sync dst dir tmp mxnet python train mnist py network mlp kv store dist sync INFO root Epoch 0 Batch 100 Speed 149 58 samples sec Train cross entropy nan INFO root Epoch 0 Batch 100 Speed 149 42 samples sec Train cross entropy nan INFO root Epoch 0 Batch 100 Speed 149 78 samples sec Train cross entropy nan Finally Does TensorFlow faster than Mxnet Sincerely,,,2017-03-16 05:27:05,2017-03-16 18:00:46
PR,refactor ImperativeInvoke,split ImperativeInvoke into small functions,,"ZihengJiang,ZihengJiang",2017-03-16 07:14:21,2017-03-16 21:53:44
PR,CI Add MKLML build and test,1 added mklml build and test 2 removed tests python gpu test forward py which will be covered in integration test 3 used a new version pip instead of pip 1 5 4 shipped with ubuntu,,"mli,piiswrong,piiswrong,mli,mli,piiswrong,mli,piiswrong,mli,mli",2017-03-15 23:18:40,2017-03-16 22:01:51
IS,Concurrent threads make inference slow C,I tried doing multiple inference in parallel on EC2 C4 8xl If it takes one second to run inference on one image using one thread I would expect the time 1 sec to remain the same if I run inference using multiple threads in parallel given C4 8xl has 36 virtual CPUs But here is what I observe Running one thread Each inference takes around 1 5 sec Running 2 concurrent threads Each inference takes around 2 5 sec Running 4 concurrent threads Each inference takes around 5 sec Here is the code I used to test I used the Inception BN model from Here is output from the above code on C4 8XL Environment info Operating System Amazon Linux Compiler gcc version gcc GCC 4 8 3 20140911 Red Hat 4 8 3 9 Copyright C 2013 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE Package used Python R Scala Julia C MXNet version Or if installed from source MXNet commit hash git rev parse HEAD git rev parse HEAD e4d5e60ec3feaff7f844ae2f35431189ead1f52e Minimum reproducible example Steps to reproduce,,"indhub,piiswrong,indhub,piiswrong,indhub",2017-03-16 18:46:18,2017-03-16 22:15:56
PR,Fix small api style issue,,,kevinthesun,2017-03-16 22:11:19,2017-03-16 22:17:38
PR,CI Add brief doc in README md,,,mli,2017-03-16 23:00:21,2017-03-16 23:00:31
IS,how to clip the weights of the params in module,I am trying to implement a Wasserstein GAN with mxnet An necessary procedure is to clamp the weights of the discriminator to a fixed range i e 0 01 0 01 i am wondering whether there is any convenient interface to do that with module api convenient means other than manually bind an executor and do the clamp of each NDArray Can any one share any experience Thanks,,sergeykolychev,2017-03-14 01:40:22,2017-03-17 00:24:57
IS,Why,A I got a poor ACC and it still betwwen 0 0 005 B when i am running i got a warning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead and Calling initializer with init str NDArray has been deprecated please use init mx init InitDesc NDArray instead these will influnse my work i do not know how to use mx mod module C I found use im2rec with recursive ture or false all lead the same label and when i use shuffle false it also can not work i got a randomize file result info 2017 03 17 09 21 13 137 Node 0 Epoch 18 Batch 17300 Speed 258 90 samples sec Train top k accuracy 10 0 001250 2017 03 17 09 21 13 137 Node 0 Epoch 18 Batch 17300 Speed 258 90 samples sec Train top k accuracy 20 0 003750 2017 03 17 09 21 16 237 Node 0 Epoch 18 Batch 17350 Speed 258 04 samples sec Train accuracy 0 000000 2017 03 17 09 21 16 237 Node 0 Epoch 18 Batch 17350 Speed 258 04 samples sec Train top k accuracy 5 0 001250 2017 03 17 09 21 16 238 Node 0 Epoch 18 Batch 17350 Speed 258 04 samples sec Train top k accuracy 10 0 001250 2017 03 17 09 21 16 238 Node 0 Epoch 18 Batch 17350 Speed 258 04 samples sec Train top k accuracy 20 0 002500 2017 03 17 09 21 19 317 Node 0 Epoch 18 Batch 17400 Speed 259 79 samples sec Train accuracy 0 000000 2017 03 17 09 21 19 317 Node 0 Epoch 18 Batch 17400 Speed 259 79 samples sec Train top k accuracy 5 0 000000 2017 03 17 09 21 19 317 Node 0 Epoch 18 Batch 17400 Speed 259 79 samples sec Train top k accuracy 10 0 000000 2017 03 17 09 21 19 317 Node 0 Epoch 18 Batch 17400 Speed 259 79 samples sec Train top k accuracy 20 0 001250 2017 03 17 09 21 22 382 Node 0 Epoch 18 Batch 17450 Speed 261 07 samples sec Train accuracy 0 000000 2017 03 17 09 21 22 382 Node 0 Epoch 18 Batch 17450 Speed 261 07 samples sec Train top k accuracy 5 0 000000 2017 03 17 09 21 22 382 Node 0 Epoch 18 Batch 17450 Speed 261 07 samples sec Train top k accuracy 10 0 000000 this is my list file 198956 2502 000000 2077 00000535 jpg 26282 393 000000 10350 028 jpg 203015 2571 000000 2139 168 jpg 246952 3195 000000 2700 066 jpg 674824 9463 000000 8342 027 jpg 181413 2256 000000 1856 004 jpg 512786 7096 000000 6211 00000848 jpg 145517 1730 000000 1382 00000525 jpg 738795 10317 000000 9110 008 jpg 162113 1966 000000 1595 046 jpg mycode from common import find mxnet import mxnet as mx import argparse import os sys from common import train model def googlenet def ConvFactory data num filter kernel stride 1 1 pad 0 0 name None suffix '' conv mx symbol Convolution data data num filter num filter kernel kernel stride stride pad pad name 'conv s s' name suffix act mx symbol Activation data conv act type arelu' name arelu s s' name suffix return act def InceptionFactory data num 1x1 num 3x3red num 3x3 num d5x5red num d5x5 pool proj name 1x1 c1x1 ConvFactory data data num filter num 1x1 kernel 1 1 name ' s 1x1' name 3x3 reduce 3x3 c3x3r ConvFactory data data num filter num 3x3red kernel 1 1 name ' s 3x3' name suffix ' reduce' c3x3 ConvFactory data c3x3r num filter num 3x3 kernel 3 3 pad 1 1 name ' s 3x3' name double 3x3 reduce double 3x3 cd5x5r ConvFactory data data num filter num d5x5red kernel 1 1 name ' s 5x5' name suffix ' reduce' cd5x5 ConvFactory data cd5x5r num filter num d5x5 kernel 5 5 pad 2 2 name ' s 5x5' name pool proj pooling mx symbol Pooling data data kernel 3 3 stride 1 1 pad 1 1 pool type pool name ' s pool s pool' pool name cproj ConvFactory data pooling num filter proj kernel 1 1 name ' s proj' name concat concat mx symbol Concat c1x1 c3x3 cd5x5 cproj name 'ch concat s chconcat' name return concat def InceptionFactory2 data num 3x3red num 3x3 num d5x5red num d5x5 pool proj name 3x3 reduce 3x3 c3x3r ConvFactory data data num filter num 3x3red kernel 1 1 name ' s 3x3' name suffix ' reduce' c3x3 ConvFactory data c3x3r num filter num 3x3 kernel 3 3 stride 2 2 pad 1 1 name ' s 3x3' name double 3x3 reduce double 3x3 cd5x5r ConvFactory data data num filter num d5x5red kernel 1 1 name ' s 5x5' name suffix ' reduce' cd5x5 ConvFactory data cd5x5r num filter num d5x5 kernel 5 5 stride 2 2 pad 2 2 name ' s 5x5' name pool proj pooling mx symbol Pooling data data kernel 3 3 stride 2 2 pad 1 1 pool type pool name ' s pool s pool' pool name concat concat mx symbol Concat c3x3 cd5x5 pooling name 'ch concat s chconcat' name return concat data mx sym Variable data conv1 ConvFactory data 64 kernel 7 7 stride 2 2 pad 3 3 name conv1 pool1 mx sym Pooling conv1 kernel 3 3 stride 2 2 pool type max conv2 ConvFactory pool1 64 kernel 1 1 stride 1 1 name conv2 conv3 ConvFactory conv2 192 kernel 3 3 stride 1 1 pad 1 1 name conv3 pool3 mx sym Pooling conv3 kernel 3 3 stride 2 2 pool type max in3a InceptionFactory pool3 64 96 128 16 32 max 32 name in3a in3b InceptionFactory in3a 64 96 128 32 64 max 64 name in3b in3c InceptionFactory2 in3b 128 256 32 64 max 64 name in3c pool4 mx sym Pooling in3c kernel 3 3 stride 2 2 pad 1 1 pool type max in4a InceptionFactory in3c 256 96 192 32 64 max 64 name in4a in4b InceptionFactory in4a 224 112 224 32 64 max 64 name in4b in4c InceptionFactory in4b 192 128 256 32 64 max 64 name in4c in4d InceptionFactory in4c 160 144 288 32 64 max 64 name in4d in4e InceptionFactory2 in4d 160 256 64 128 max 128 name in4e pool5 mx sym Pooling in4e kernel 3 3 stride 2 2 pool type max in5a InceptionFactory in4e 384 192 384 48 128 max 128 name in5a in5b InceptionFactory in5a 384 192 384 48 128 max 128 name in5b pool6 mx sym Pooling in5b kernel 7 7 stride 1 1 pool type avg flatten mx sym Flatten data pool6 fc1 mx sym FullyConnected data flatten num hidden 1000 softmax mx symbol SoftmaxOutput data fc1 name isoftmax' return softmax def get iterator args kv data shape 3 224 224 train mx io ImageRecordIter path imgrec media pixel ssd512G tran rec mean img media pixel ssd512G mean2 bin data name wouldata' shuffle True data shape data shape batch size args batch size rand crop True rand mirror True num parts kv num workers part index kv rank val mx io ImageRecordIter path imgrec media pixel ssd512G tran0 rec mean img media pixel ssd512G mean21 bin data name wouldata' shuffle False rand crop False rand mirror False data shape data shape batch size args batch size num parts kv num workers part index kv rank return train val def parse args parser argparse ArgumentParser description 'train an image classifer on mnist' parser add argument ' network' type str default 'googlenet' help 'the cnn to use' parser add argument ' data dir' type str default ' media pixel ssd512G ' help 'the input data directory' parser add argument ' gpus' type str default '0 1' help 'the gpus will be used e g 0 1 2 3 ' parser add argument ' num examples' type int default 5359 help 'the number of training examples' parser add argument ' batch size' type int default 16 help 'the batch size' parser add argument ' lr' type float default 01 help 'the initial learning rate' parser add argument ' model prefix' type str help 'the prefix of the model to load save' parser add argument ' save model prefix' type str help 'the prefix of the model to save' parser add argument ' num epochs' type int default 2000 help 'the number of training epochs' parser add argument ' load epoch' type int help load the model on an epoch using the model prefix parser add argument ' kv store' type str default 'local' help 'the kvstore type' parser add argument ' lr factor' type float default 0 5 help 'times the lr with a factor for every lr factor epoch epoch' parser add argument ' lr factor epoch' type float default 1 help 'the number of epoch to factor the lr could be 5' return parser parse args if name ' main ' args parse args net googlenet train train model fit args net get iterator Environment info Operating System Ubuntu Package used Python R Scala Julia Python MXNet version 0 9 3,,ysh329,2017-03-17 01:10:03,2017-03-17 02:04:18
PR,Fix api style,Fix highlight issue,,kevinthesun,2017-03-17 01:26:40,2017-03-17 04:12:07
PR,CI Add integration tests,includes caffe converters test pre trained imagenet models,,mli,2017-03-17 04:29:53,2017-03-17 05:02:32
PR,CI tar libs to reduce the stash unstash time,,,"mli,mli",2017-03-17 04:51:26,2017-03-17 05:04:00
IS,bin ld usr local lib libcblas a cblas sgemm o relocation R X86 64 32 against rodata str1 1' can not be used when making a shared object recompile with fPIC,I install mxnet on linux when I ran make j nproc I got the error below and I can not find any solution on the internet g DMSHADOW FORCE STREAM Wall Wsign compare O3 I data lipengfei software mxnet mshadow I data lipengfei software mxnet dmlc core include fPIC I data lipengfei software mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local include opencv I usr local include fopenmp DMXNET USE NVRTC 0 shared o lib libmxnet so build src operator mkl mkl cppwrapper o build src operator mkl mkl memory o build src operator tensor elemwise binary broadcast op extended o build src operator tensor elemwise binary op extended o build src operator tensor init op o build src operator tensor broadcast reduce op index o build src operator tensor elemwise binary scalar op logic o build src operator tensor elemwise binary broadcast op basic o build src operator tensor broadcast ra build src c api c api executor o educe op value o build src operator tensor control flow op o build src operator tensor elemwise binary op basic o build src operator tensor elemwise unary op o build src operator tensor elemwise sum o build src operator tensor elemwise binary scalar op extended o build src operator tensor indexing op o build src operator tensor ordering op o build src operator tensor elemwise binary broadcast op logic o build src operator tensor sample op o build src operator tensor elemwise binary op logic o build src operator tensor elemwise binary scalar op basic o build src operator tensor matrix op o build src operator nnpack nnpack util o build src io io o build src io image aug default o build src io iter csv o build src io image io o build src io iter image recordio o build src io iter mnist o build src common mxrtc o build src nnvm legacy op util o build src nnvm legacy json util o build src ndarray ndarray function o build src ndarray ndarray o build src operator instance norm o build src operator svm output o build src operator native op o build src operator pooling o build src operator crop o build src operator spatial transformer o build src operator deconvolution o build src operator swapaxis o build src operator batch norm o build src operator softmax output o build src operator operator util o build src operator sequence reverse o build src operator pad o build src operator sequence last o build src operator make loss o build src operator operator o build src operator optimizer op o build src operator lrn o build src operator correlation o build src operator fully connected o build src operator sequence mask o build src operator grid generator o build src operator convolution o build src operator identity attach KL sparse reg o build src operator bilinear sampler o build src operator dropout o build src operator cudnn batch norm o build src operator loss binary op o build src operator activation o build src operator regression output o build src operator l2 normalization o build src operator upsampling o build src operator concat o build src operator leaky relu o build src operator roi pooling o build src operator ndarray op o build src operator custom o build src operator cudnn convolution o build src operator slice channel o build src operator cross device copy o build src operator softmax activation o build src operator rnn o build src engine profiler o build src engine naive engine o build src engine threaded engine pooled o build src engine threaded engine o build src engine engine o build src engine threaded engine perdevice o build src storage storage o build src c api c api executor o build src c api c api symbolic o build src c api c api ndarray o build src c api c predict api o build src c api c api o build src c api c api error o build src executor inplace addto detect pass o build src executor graph executor o build src executor attach op execs pass o build src executor attach op resource pass o build src kvstore kvstore o build src resource o build src initialize o data lipengfei software mxnet dmlc corea build src c api c api symbolic o libdmlc a pthread lm lcblas fopenmp lrt L usr local lib lopencv ml lopencv objdetect lopencv shape lopencv stitching lopencv superres lopencv videostab lopencv calib3d lopencv features2d lopencv highgui lopencv videoio lopencv imgcodecs lopencv video lopencv photo lopencv imgproc lopencv flann lopencv core L usr include atlas Wl whole archive data lipengfei software mxnet nnvm lib libnnvm a Wl no whole archive a build src c api c api ndarray o a build src c api c predict api o a build src c api c api o a build src c api c api error o a build src executor inplace addto detect pass o a build src executor graph executor o a build src executor attach op execs pass o a build src executor attach op resource pass o a build src kvstore kvstore o a build src resource o a build src initialize o g DMSHADOW FORCE STREAM Wall Wsign compare O3 I data lipengfei software mxnet mshadow I data lipengfei software mxnet dmlc core include fPIC I data lipengfei software mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local include opencv I usr local include fopenmp DMXNET USE NVRTC 0 std c 11 o bin im2rec tools im2rec cc build src operator mkl mkl cppwrapper o build src operator mkl mkl memory o build src operator tensor elemwise binary broadcast op extended o build src operator tensor elemwise binary op extended o build src operator tensor init op o build src operator tensor broadcast reduce op index o build src operator tensor elemwise binary scalar op logic o build src operator tensor elemwise binary broadcast op basic o build src operator tensor broadcast reduce op value o build src operator tensor control flow op o build src operator tensor elemwise binary op basic o build src operator tensor elemwise unary op o build src operator tensor elemwise sum o build src operator tensor elemwise binary scalar op extended o build src operator tensor indexing op o build src operator tensor ordering op o build src operator tensor elemwise binary broadcast op logic o build src operator tensor sample op o build src operator tensor elemwise binary op logic o build src operator tensor elemwise binary scalar op basic o build src operator tensor matrix op o build src operator nnpack nnpack util o build src io io o build src io image aug default o build src io iter csv o build src io image io o build src io iter image recordio o build src io iter mnist o build src common mxrtc o build src nnvm legacy op util o build src nnvm legacy json util o build src ndarray ndarray function o build src ndarray ndarray o build src operator instance norm o build src operator svm output o build src operator native op o build src operator pooling o build src operator crop o build src operator spatial transformer o build src operator deconvolution o build src operator swapaxis o build src operator batch norm o build src operator softmax output o build src operator operator util o build src operator sequence reverse o build src operator pad o build src operator sequence last o build src operator make loss o build src operator operator o build src operator optimizer op o build src operator lrn o build src operator correlation o build src operator fully connected o build src operator sequence mask o build src operator grid generator o build src operator convolution o build src operator identity attach KL sparse reg o build src operator bilinear sampler o build src operator dropout o build src operator cudnn batch norm o build src operator loss binary op o build src operator activation o build src operator regression output o build src operator l2 normalization o build src operator upsampling o build src operator concat o build src operator leaky relu o build src operator roi pooling o build src operator ndarray op o build src operator custom o build src operator cudnn convolution o build src operator slice channel o build src operator cross device copy o build src operator softmax activation o build src operator rnn o build src engine profiler o build src engine naive engine o build src engine threaded engine pooled o build src engine threaded engine o build src engine engine o build src engine threaded engine perdevice o build src storage storage o build src c api c api executor o build src c api c api symbolic o build src c api c api ndarray o build src c api c predict api o build src c api c api o build src c api c api error o build src executor inplace addto detect pass o build src executor graph executor o build src executor attach op execs pass o build src executor attach op resource pass o build src kvstore kvstore o build src resource o build src initialize o data lipengfei software mxnet dmlc core libdmlc a data lipengfei software mxnet nnvm lib libnnvm a pthread lm lcblas fopenmp lrt L usr local lib lopencv ml lopencv objdetect lopencv shape lopencv stitching lopencv superres lopencv videostab lopencv calib3d lopencv features2d lopencv highgui lopencv videoio lopencv imgcodecs lopencv video lopencv photo lopencv imgproc lopencv flann lopencv core L usr include atlas bin ld usr local lib libcblas a cblas sgemm o relocation R X86 64 32 against rodata str1 1' can not be used when making a shared object recompile with fPIC usr local lib libcblas a error adding symbols Bad value collect2 error ld returned 1 exit status make lib libmxnet so Error 1 make Waiting for unfinished jobs usr local lib libcblas a sdotsub o In function sdotsub ' sdotsub f text 0x7 undefined reference to sdot ' usr local lib libcblas a ddotsub o In function ddotsub ' ddotsub f text 0x7 undefined reference to ddot ' usr local lib libcblas a cblas sgemm o In function cblas sgemm' cblas sgemm c text 0x138 undefined reference to sgemm ' cblas sgemm c text 0x1fe undefined reference to sgemm ' usr local lib libcblas a cblas dgemm o In function cblas dgemm' cblas dgemm c text 0x140 undefined reference to dgemm ' cblas dgemm c text 0x20a undefined reference to dgemm ' collect2 error ld returned 1 exit status make bin im2rec Error 1,,,2017-03-17 10:24:12,2017-03-17 10:30:24
IS,Ask Caffe converter,I try to convert this model for age and gender prediction I got really different result than caffe model Does I failed or is it possible that caffe and mxnet differences generate this,,,2017-03-17 14:41:03,2017-03-17 14:44:15
PR,Bulk execution,Implement bulk execution x Bulk exec for inference x Bulk exec for training forward x Bulk exec for training backward x Prefetch data and bind If MXNET EXEC BULK EXEC INFERENCE is set to 1 bulk the entire graph for inference Benchmark result for inference Version Seconds per epoch 1 GPU Baseline 56 Bulk Inference 35 1 1 6x speedup python lstm bucketing py gpus 0 test num hidden 200 num embed 200 using train data If MXNET EXEC BULK EXEC TRAIN is set to 1 bulk during training If MXNET EXEC BULK EXEC MAX NODE TRAIN is set to 10 by default which means at most 10 nodes are executed in bulk Version Seconds per epoch 1 GPU 4 GPU Baseline 120 745 39 588 Bulk Forward 97 221 35 391 Bulk Forward Backward 91 475 1 32x speedup 34 9 Bulk Forward Backward Prefetch 88 624 32 774 python lstm bucketing py num hidden 100 num embed 100 num layers 6,,"eric-haibin-lin,tqchen,piiswrong,piiswrong,piiswrong,piiswrong,tqchen,piiswrong,piiswrong,piiswrong,mli,eric-haibin-lin,eric-haibin-lin",2017-03-03 06:31:52,2017-03-17 17:46:40
PR,Improve convolution,This implementation ported im2col and col2im algorithms from Caffe to MXNet and rewrote convolution operator to support 1 2 3 D convolutions on both CPU and GPU Benchmark environment Amazon EC2 p2 xlarge instance Benchmark results of forward and backward passes Convolution This implementation Convolution v1 Currently existing implementation 1D data shape 10 16 512 kernel 8 CPU GPU Convolution v1 N A N A Convolution 33ms 2ms CuDNN N A N A 2D data shape 10 16 64 64 kernel 8 8 CPU GPU Convolution v1 1 102s 24ms Convolution 280ms 16ms CuDNN N A 1 4ms 3D data shape 4 8 10 30 20 kernel 4 8 8 CPU GPU Convolution v1 N A N A Convolution 1 575s 16ms CuDNN N A 4ms Benchmark python script,,"reminisce,piiswrong,piiswrong,piiswrong,reminisce,piiswrong,reminisce,piiswrong,reminisce,reminisce,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,reminisce,reminisce,piiswrong,reminisce,piiswrong,piiswrong,sxjscience,piiswrong,piiswrong,piiswrong,piiswrong",2017-03-07 05:32:51,2017-03-17 19:42:12
PR,add dropout parity for unfuse in FusedRNNCell,piiswrong this pr adds dropout parity for FusedRNNCell It would be good if nvidia people could help verify whether function wise this is aligned i e dropout on output Also for testing since the dropout control is a bit hard I set the dropout rate to be small to make sure at least the logical branch is tested,,"szha,loofahcus,szha,szha,loofahcus",2017-03-16 07:28:44,2017-03-17 21:37:25
PR,fix dropout MKL issue,mli pls review and approve,,"zhenlinluo,piiswrong,zhenlinluo,piiswrong,piiswrong,piiswrong,piiswrong,zhenlinluo,zhenlinluo,piiswrong,zhenlinluo,mli,zhenlinluo",2016-12-05 19:54:53,2017-03-17 23:19:41
PR,doc make api document more compact,new image old image new image old image,,mli,2017-03-17 22:32:15,2017-03-17 23:29:50
PR,doc add virtual padding before methods,new image old image,,mli,2017-03-18 00:33:15,2017-03-18 00:33:27
PR,Changing according to new checkpointing method changes for Module API,,,"Roshrini,piiswrong",2017-03-16 19:12:16,2017-03-18 03:51:26
PR,fix dropout MKL issue,mli this is new pr for dropout mkl fix w o seeds Pls review,,zhenlinluo,2017-03-17 23:18:47,2017-03-18 06:40:43
PR,Fix typo,,,fukatani,2017-03-18 16:39:03,2017-03-18 18:31:04
PR,Added eval to Symbol class,Added an eval method to the symbol class Previously one had to run a S normal shape 2 2 d a bind e d forward Now you could just run a S normal shape 2 2 a eval Note eval takes args self args None ctx cpu The order of args and ctx is flipped to allow for ctx to be given optionally Also args is given as a named argument so that it can be omitted in the rare case that the symbol requires no placeholders to be filled Resolves Issue 5280,,"zackchase,piiswrong,piiswrong,zackchase,zackchase,piiswrong,zackchase,piiswrong,piiswrong,piiswrong,zackchase,zackchase,zackchase,piiswrong,zackchase",2017-03-08 00:22:28,2017-03-18 20:52:08
PR,Unify conding style about None spacing and indent,use is for comparing with None adjust spacing according to PEP8 example speech demo io func kaldi parser py was indented by tab character change to 4 spaces,,fukatani,2017-03-19 01:59:38,2017-03-19 06:16:07
PR,Autograd for NDArray,Brief Introduction Bring the idea from MinPy Autograd for NDArray will allow users compute gradient for imperative programs easily Progress x Support for stateless ndarray operators x Support for ndarray operators with state Future Partial Gradient Support for hybrid of symbol and ndarray,,"ZihengJiang,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,ZihengJiang,piiswrong,piiswrong,piiswrong,piiswrong,ZihengJiang,ZihengJiang,ZihengJiang,ZihengJiang",2017-02-24 00:13:38,2017-03-19 06:26:26
PR,doc new api document,preview at,,mli,2017-03-19 05:51:53,2017-03-19 06:45:00
IS,Can Mxnet import data to the memory in batches for my data is much larger than the node device is memory,,,,2017-03-20 01:22:37,2017-03-20 01:36:15
PR,Fix bug in pooling inl h pool type pool enum kValid,Typo In file src operator pooling inl h line 248 it should be pooling convention pool enum kValid,,,2017-03-19 12:22:33,2017-03-20 01:56:02
PR,Use print function for python3 compatibility,,,fukatani,2017-03-19 10:30:04,2017-03-20 01:56:43
PR,softmax and log softmax,,,piiswrong,2017-03-20 17:09:59,2017-03-20 19:02:19
PR,doc update io api,preview at TODOs remove the global pylint disable on the top of image py add more document to image py and recordio py do any of you have time to check the two TODOs,,mli,2017-03-20 06:39:33,2017-03-20 19:49:14
PR,redirect cfn to,jspisak can you suggest if this needs change,,nswamy,2017-03-17 19:02:56,2017-03-20 21:15:29
PR,fix np concatenate bug,,,"piiswrong,zhreshold",2017-03-21 02:54:07,2017-03-21 04:51:37
PR,Add Prefix NameManager use case in python tutorial,I added a short use case of Prefix NameManager in the symbol tutorial,,"jbcdnr,jbcdnr",2017-03-21 10:07:49,2017-03-21 17:11:17
PR,add contrib package,precedenceguo Moved extra operators to contrib Had to fix lint please check if I made any error BTW do you have unittests for these ops,,"piiswrong,mli,piiswrong,zhreshold,piiswrong,precedenceguo,piiswrong,fullfanta",2017-03-20 19:03:54,2017-03-21 18:37:53
PR,Add pad param to DataBatch returned by BucketSentenceIter,Pad value is not provided when BucketSentenceIter next returns DataBatch and this may broke module predict Since BucketSentenceIter will discard a few data insufficient for a mini batch at the end of each bucket so set pad 0 seems okay,,Cloudyrie,2017-03-21 18:01:53,2017-03-21 19:20:33
PR,modify example reinforcement learning a3c a3c py,i try to modify a3c py according to my understanding the rate of convergence and score become better,,"piiswrong,piiswrong",2017-03-18 14:34:15,2017-03-21 19:21:18
IS,Add documentation for Prefix NameManager,I cannot find any documentation for the Prefix NameManager for python It is a very nice feature but there is no way for the end user to find it without looking into the code It let is you prefix all the symbols created in a block with a string a bit like tensorflow scoping Adding a use case in the doc would be a good first step,,"jbcdnr,jbcdnr",2017-03-15 15:42:35,2017-03-21 21:54:07
PR,Fix Keras Multiple Device Issue,In keras mxnet we set 'allow missing when initialize module parameters This is fine for single device However in multiple device scenario kvstore creation requires parameters on CPU so we need to synchronize executor parameters to CPU before creating kvstore or kvstore will get all zero parameters making no progress in training,,kevinthesun,2017-03-21 21:10:18,2017-03-21 22:03:28
PR,add forget bias option to LSTM cell,,,"geoalgo,piiswrong,geoalgo,piiswrong,piiswrong,geoalgo,geoalgo,piiswrong,geoalgo,piiswrong,piiswrong,piiswrong,szha,szha,piiswrong,piiswrong,piiswrong,szha,szha,geoalgo,piiswrong,geoalgo,piiswrong,geoalgo,geoalgo,piiswrong,piiswrong,geoalgo,geoalgo,geoalgo,geoalgo,piiswrong,geoalgo,piiswrong,piiswrong,geoalgo",2017-03-14 17:28:20,2017-03-21 23:52:37
IS,problem about a3c py,i just learn the a3c example of RL and i fell confused in the code of a3c py as follows pi module get outputs 1 h args beta mx nd log pi 1e 6 1 module backward mx nd array adv h i think according to the previous code the loss backward should be as follows pi module get outputs 0 h args beta module get outputs 1 module backward mx nd array mx nd log pi 1e 6 adv h do i make wrong concept of a3c or make a mistake about the use of python interface,,piiswrong,2017-03-05 09:57:15,2017-03-22 00:54:37
IS,NNPACK BUG MXPredGetOutput occasionally stucked when getting the 2nd output node,The bug only shows up when using NNPACK as backend When I am trying to MXPredGetOutput from multiple node the result from 1st node would always be got successfully however the 2nd node would have a low chance to stuck forever It shows up occasionally but it is critical for high frequency tasks such as face detection I also confirm that it happens on android device and in older version like v0 8 GDB shows this function is waiting for some threads Environment info Operating System Ubuntu 14 04 android Compiler GCC Package used NNPACK MXNet version v0 93 v0 8 GDB debug message when paused 0 pthread cond wait GLIBC 2 3 2 185 0x7ffff7bc8404 1 std condition variable wait std unique lock std mutex usr lib x86 64 linux gnu libstdc so 6 0x7ffff3c622bc 2 mxnet engine ThreadedEngine WaitForVar mxnet engine Var home pallas mxnet lib libmxnet so 0x7ffff40af409 3 mxnet NDArray SyncCopyToCPU void unsigned long const home pallas mxnet lib libmxnet so 0x7ffff412ba58 4 MXPredGetOutput home pallas mxnet lib libmxnet so 0x7ffff40a42ae 5 GetOutput mxnet cpp helper cpp 186 0x4ed828 C source code 3 Do Predict Forward MXPredForward pred hnd 4 Get Output Result int output size 0 bbox pred cls prob vector mx uint output index 0 1 vector mx uint bbox shape std vector float bbox data GetOutput pred hnd output index 0 bbox shape bbox data output size vector mx uint cls shape std vector float cls data GetOutput pred hnd output index 1 cls shape cls data output size void GetOutput PredictorHandle pred hnd const int output index vector mx uint shapes std vector float data int output size mx uint shape 0 mx uint shape len shapes clear Get Output Result MXPredGetOutputShape pred hnd output index shape shape len size t size 1 for mx uint i 0 i shape len i size shape i shapes push back shape i std cout shape for mx uint i 0 i shape len i std cout shape i std cout std endl prepare the space for output data resize size output size size MXPredGetOutput pred hnd output index data 0 size,,,2017-03-21 08:50:37,2017-03-22 03:22:50
PR,Integrate cpp package,As mentioned in dmlc mxnet cpp 71 we propose to merge mxnet cpp into mxnet project It will attract more attention to mxnet cpp and it will also enable integrated test with mxnet so that future checkins to mxnet will not break mxnet cpp Meanwhile it is important for industrial environment e g data center where python and other dependencies are normally not installed,,"lx75249,piiswrong,lx75249,piiswrong,hjk41,piiswrong,piiswrong,lx75249,lx75249,piiswrong,lx75249,piiswrong,piiswrong,lx75249,piiswrong,lx75249",2017-03-05 04:58:22,2017-03-22 03:55:51
PR,Improve pooling,This pull request re implemented pooling operator to support 1 2 3 D pooling operations without using expression template to increase performance Pooling This implementation PoolingV1 Currently existing operator in MXNet CuDNNPooling cuDNN pooling operator Table 1 max pooling benchmark results New implementation s 1D max pooling is compared with the equivalent 2D case 1D data 10 3 100000 kernel 64 br 2D data 10 3 100000 1 kernel 64 1 CPU ms GPU ms Pooling 1D 251 7 Pooling 2D 680 14 CuDNNPooling 2D N A 12 PoolingV1 2D 4428 36 br Table 2 max pooling 2D benchmark results 2D data 10 3 100 100 kernel 8 8 CPU ms GPU ms Pooling 144 4 CuDNNPooling N A 5 PoolingV1 639 9 br Table 3 max pooling 3D benchmark results 3D data 10 3 100 100 100 kernel 8 8 8 CPU ms GPU ms Pooling 2509 53 CuDNNPooling N A 42 br Table 4 avg pooling benchmark results New implementation s 1D avg pooling is compared with the equivalent 2D case 1D data 10 3 100000 kernel 64 br 2D data 10 3 100000 1 kernel 64 1 CPU ms GPU ms Pooling 1D 227 13 Pooling 2D 938 17 CuDNNPooling 2D N A 10 PoolingV1 2D 2998 26 br Table 5 avg pooling 2D benchmark results 2D data 10 3 100 100 kernel 8 8 CPU ms GPU ms Pooling 123 6 CuDNNPooling N A 7 PoolingV1 412 8 br Table 6 avg pooling 3D benchmark results 3D data 10 3 100 100 100 kernel 8 8 8 CPU ms GPU ms Pooling 2301 85 CuDNNPooling N A 60,,"reminisce,piiswrong,piiswrong,piiswrong,reminisce,mli,mli,reminisce,piiswrong,piiswrong,reminisce,piiswrong,reminisce,piiswrong,glingyan,piiswrong,glingyan,piiswrong,reminisce,sxjscience,glingyan,glingyan,sxjscience,reminisce,sxjscience,reminisce,piiswrong,reminisce,sxjscience,glingyan,sxjscience,reminisce,piiswrong,glingyan,reminisce,piiswrong,leezu",2017-03-21 18:14:51,2017-03-22 06:01:58
IS,BatchNorm problem with fp16 on Windows,Environment info Operating System Windows 10 Package used Python MXNet version Windows pre built GPU Python version and distribution Anaconda python2 7 cuDNN5 1 CUDA 8 0 Error Message Minimum reproducible example Using mxnet examples symbols resnet fp16 py to train on cifar 10 It is a straight forward code using Module APIs I can use fp16 for CNN models without BatchNorm like VGG But when adding BatchNorm I got the above error log,,,2017-03-21 12:59:57,2017-03-22 06:34:20
PR,refactor normalize sequence,szha dropout update,,"piiswrong,szha,loofahcus,piiswrong,szha",2017-03-21 23:10:36,2017-03-22 07:11:16
IS,mxnet model FeedForward has been deprecated Please use mxnet mod Module instead,How to write by using mx mod module,,qiyuangong,2017-03-14 17:14:33,2017-03-22 10:09:00
IS,Question about distributed training,The official tutorials says we can train one net with multi machine by the commond tools launch py n 2 H hosts sync dst dir tmp mxnet python train mnist py network lenet kv store dist sync Does that means we do not need compile the code on every machine I do all the steps as tutorials but mxnet will report errors unless the version of library strictly match on every machine So I recompile mxnet on every machine it do not report errors neither work just stuck the shell I check the process by ps aux and there is indeed process whose command is python train mnist py network lenet kv store dist sync but its status is Sl Could anyone give me more detailed information about training distributed,,"piiswrong,mli",2016-11-25 07:18:18,2017-03-22 11:27:10
PR,scala package fix the too wide accessibilities of scala classes mxnet core,When going through the code of mxnet core I found that the accessibility of some classes are too wide i e these classes shall not be exposed to the external usage I cleaned these in this PR the other changes include 1 remove some author tags as they are not allowed in ASF projects 2 change the package name of examples from mxnet examples to mxnetexamples this change will make the development of examples in a full perspective of an external user and it also helps to ensure that we are not mistakenly narrow the accessibilities would you mind helping to review,,"CodingCat,Ldpe2G,CodingCat,piiswrong,Ldpe2G,CodingCat",2017-03-19 15:36:43,2017-03-22 13:58:05
PR,MKLML build in scalable build warnings rnn and ssd operators,,,"cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01",2017-03-21 15:15:15,2017-03-22 17:09:45
PR,Fix markdown syntax error,Fix markdown syntax error about titles,,ysh329,2017-03-22 14:43:27,2017-03-22 17:39:38
PR,Add str method to DataBatch,,,"larroy,piiswrong,larroy",2017-03-21 11:11:08,2017-03-22 17:44:56
PR,A better thread safe profiler constructor and dump profile method,A better thread safe singleton Get function Move DumpProfile out of destructor of engine Fix operator name bug in bulk execution at graph executor,,"ZihengJiang,piiswrong,piiswrong,ZihengJiang,piiswrong,piiswrong,ZihengJiang,yajiedesign,piiswrong,ZihengJiang,piiswrong,ZihengJiang",2017-03-20 02:46:50,2017-03-22 19:00:06
PR,Heavily revised symbol py ndarray py docstrings,Symbol py docstrings had many problems Fixed numerous spelling grammar issues Made many descriptions clearer Added some code examples where it is not obvious how to call the code Maintainers please check code examples for style Here I just used Markdown syntax,,"zackchase,piiswrong,piiswrong,zackchase,piiswrong",2017-03-21 08:40:17,2017-03-22 21:12:20
IS,get a no learned accuracy in image classification,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I prepared my network just as showed in 5019 mxnet example image classification symbols and train my own data like 4681 mxnet example image classification train cifar10 py 2 but I got a train accuracy about 1 num classes What have you tried to solve it 1 I have tried to change the lr and batch size but no help 2 3,,,2017-03-03 08:21:44,2017-03-23 02:43:09
PR,MKLML build in scalable build warnings rnn and ssd operators,Rebuilt PR since Appveyor could not handle the merge for some reason,,"cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-03-22 17:12:08,2017-03-23 03:13:06
PR,Fix doctstring for symbols example and ndarray cpu version example,Description 1 Fix symbols py docstrubg 2 Fix some ndarray docstring example I did not get my gpu version mxnet set up so only fixed docstring for cpu version example I will work on the rest of the docstring after I have GPU enabled mxnet Testing mxnet built successfully html built successfully and verified locally,,yuruofeifei,2017-03-22 05:16:10,2017-03-23 03:16:56
PR,Fix the bug about resume training in optimizer py issue 5428,See issue 5428 for details,,,2017-03-20 07:44:04,2017-03-23 04:26:48
PR,Add test with MKL option,Allows us to add a job on CPU that tests with MKL,,"lxn2,lxn2,piiswrong,yzhliu,Ldpe2G,piiswrong,lxn2,yzhliu,lxn2,yzhliu,szha,glingyan,glingyan,glingyan,yzhliu,szha,szha,piiswrong,glingyan,glingyan,glingyan,yzhliu,mli",2017-03-03 23:27:06,2017-03-23 06:05:08
PR,Fix in order to allow specifying more than one input shape for predict api,Needed for lstm examples that have Reshape op that depends on defined shape of the softmax label The comments in the h file for the MXPredCreate are pretty confusing and certainly need to be rewritten in the light of this change I however a bit perplexed on what exactly these comments should say can you suggest anything I submitted a pull against Go interface that uses this code as well,,"sergeykolychev,sergeykolychev",2017-03-23 08:15:27,2017-03-23 14:21:13
PR,Add snapcraft packaging,Adds snapcraft build configuration and command to run an in snap Python 2 runtime This allows MXNet to be used either by calling snap bin mxnet python or by adding the following to your environment and using your system is python LD LIBRARY PATH snap mxnet current lib snap mxnet current usr lib LD LIBRARY PATH PYTHONPATH snap mxnet current lib python2 7 site packages snap mxnet current usr lib python2 7 dist packages PYTHONPATH The snap can be build with snapcraft snap on Ubuntu 16 04 or later Use snapcraft cleanbuild to build it in an LXC container instead,,"mhall119,piiswrong,piiswrong,mhall119,piiswrong,piiswrong,mhall119,mhall119,piiswrong,piiswrong,piiswrong,mhall119,piiswrong,mhall119",2017-02-01 20:56:25,2017-03-23 19:15:19
PR,fix model to fit the new contrib structure fix links,Fix contrib names in model fix links to models,,zhreshold,2017-03-23 16:04:49,2017-03-23 19:17:16
PR,Fix docstring test for symbol py and ndarray py examples,Fix all the doc tests for symbol py and ndarray py examples Also add the doc test in Travis CI test passed on Mac and Ubuntu Testing 191 tests in 230 items 191 passed and 0 failed Test passed,,yuruofeifei,2017-03-24 00:51:11,2017-03-24 03:46:19
PR,fix initializer,,,piiswrong,2017-03-23 20:08:29,2017-03-24 03:46:34
IS,ubuntu 16 04 how to use Intel integrating graphic card to rendering desktop NV card to do calculate,I have a machine which have two graph card one integrate in the intel cpu and another plug in pci slot NV The OS is ubuntu 16 04 2 I want to use the integrating one for desktop rendering and the NV for MXNet calculate By default the OS choose the NV car to use after I switch to use the integrating one in NVIDIA X Server settings the nvidia smi dose not working so any one have experience on this issue,,,2017-03-14 08:11:37,2017-03-24 04:07:15
IS,Mxnet Module API predict from a model already trained,Hello I try to predict a picture received the output of a model with the new Module API Where am I wrong,,"sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev",2017-03-18 14:33:38,2017-03-24 11:44:06
PR,Pick,bikestra,,piiswrong,2017-03-24 06:10:18,2017-03-24 19:04:58
PR,Skip the line in lst if the format is incorrect,Currently im2rec py will block not exit not continue if lst file contains some incorrect format while using multiple preprocessing Do the validation and add try catch to avoid this problem,,jiayue666,2017-03-24 18:15:56,2017-03-24 19:08:43
IS,Training and validation accuracy very low in Binary Classification,I'm trying to do binary classification to predict if a person is likely to vote 1 or not 0 based on 50 numeric features I have 1261382 training examples and I'm unsure about the batch size and optimal number of epochs I should have For some reason my training and validation accuracy is very low around 0 00063 Here is my code Any idea what I'm doing wrong or how I can improve the network Thanks,,,2017-03-04 19:28:52,2017-03-24 20:29:35
PR,Add tensorboard support in Speedometer,piiswrong Add tensorboard logging support in Speedometer by optional import Any other suggestion in callback functions Should I change the Python setup py and make dmlc tensorboard a requirement,,"zihaolucky,zihaolucky,zihaolucky,piiswrong,zihaolucky,piiswrong,zihaolucky,zihaolucky,piiswrong,zihaolucky,piiswrong,zihaolucky,piiswrong,jmerkow,zihaolucky,jmerkow,ysh329",2017-03-11 16:42:59,2017-03-24 20:40:45
PR,final ARM crosstool docker setup,Updating docker file from to reflect requirements for functional build in Jenkins This is the correct Dockerfile to run ARM test with so please merge ASAP,,"arank,piiswrong,piiswrong,arank,piiswrong,arank",2017-03-22 18:32:02,2017-03-24 21:29:54
PR,Move convolution and pooling to nn,,,"reminisce,piiswrong",2017-03-24 18:11:20,2017-03-24 23:26:07
PR,Fix slicechannel type check,This PR fixes the problem of SliceChannel operator only accepting data type float32 It should support any data types supported in MXNet Test script,,"reminisce,piiswrong",2017-03-24 16:58:27,2017-03-24 23:26:17
PR,Upgrade DMLC Core and OSX Travis build,see,,"tqchen,tqchen,piiswrong,tqchen,tqchen,pluskid,tqchen,szha,pluskid,mli,szha,tqchen,pluskid",2017-03-17 16:44:06,2017-03-24 23:31:46
PR,upgrade dmlc core,,,tqchen,2017-03-24 23:35:31,2017-03-24 23:35:49
PR,added crosstool changes,Last PR was FUBAR so nuked it from orbit and created this,,arank,2017-03-24 21:29:30,2017-03-25 00:04:09
PR,update fc with nnpck,due to nnpack has fixed bug of fc layer in so also update the MXNet part,,tornadomeet,2017-03-25 01:08:14,2017-03-25 04:51:34
PR,Tensorsketch,Add fft ifft and count sketch operators Only available on GPU for now Also changed submodule mshadow mshadow extension complex h will submit another pr,,"piiswrong,piiswrong,piiswrong",2017-03-14 22:52:31,2017-03-25 05:33:21
IS,OSError anaconda3 lib python3 6 site packages mxnet 0 9 4 py3 6 egg mxnet libmxnet so undefined symbol ZNK6google8protobuf11MessageLite25InitializationErrorStringEv,Environment info Operating System Ubuntu 16 04 Compiler gcc 4 8 5 Package used Python R Scala Julia python 3 6 MXNet version 0 9 4 Or if installed from source Yes MXNet commit hash git rev parse HEAD e4f73f1f4e76397992c4b0a33c139d52b4b7af0e Python version and distribution 3 6 Error Message Please paste the full error message including stack trace,,wangg12,2017-03-24 02:37:24,2017-03-25 06:38:57
IS,Can not find label in softmax logisticregression output,I met this error in both SoftmaxOutput and LogisticRegressionOutput the message is quite confusing,,zihaolucky,2017-03-25 10:37:59,2017-03-25 15:01:18
IS,Infer Shape bug,I found this bug while working on 5567 The symptom is that symbol infer shape partial successfully return all arg and out shapes whereas symbol infer shape fails and returns None The error message includes an empty list of failed symbols Operating System OSX Compiler clang8 0 Package used Python R Scala Julia python MXNet commit hash git rev parse HEAD Python version and distribution 2 7 13 from homebrew Error Message Please paste the full error message including stack trace Not exactly an error as it shows up as a warning What have you tried to solve it 1 tried to symmetrically check shapes in WhereOpShape in operator tensor control flow op h but no luck,,szha,2017-03-25 00:19:33,2017-03-25 18:15:44
PR,add dockerfiles and hosted docker images for python julia r scala,See docker README md for details We separated the dockerfiles into multiple parts so that they can be reused on other places such as ci build and documents It increases the difficulty to build automatically on hub docker com So we need to host a nightly docker build job on our server,,mli,2017-03-26 04:38:06,2017-03-26 16:40:07
PR,fix some installation script problem add a dependency lib in ubuntu,add a missing cp step in installation scripts add a dependency lib in install mxnet ubuntu r tested on ubuntu 16 04,,jiajiechen,2017-03-24 18:20:03,2017-03-26 16:41:57
PR,ci test,,,yajiedesign,2017-03-26 02:32:18,2017-03-27 00:40:09
PR,Crosstool tests,Fixed the amalgamation Makefile by adding needed reference to DMLC to make amalgamation compile and added dockerfile for arm cross toolchain build tests,,"arank,mli,mli,mli,mli,arank,arank,arank",2017-03-16 22:13:00,2017-03-27 18:07:32
PR,Made test pass deterministically in the presence of cuDNN algo find,Given near zero values need absolute tolerance to get deterministic passing Problem seen for seed 1234 for example,,DickJC123,2017-03-27 18:23:00,2017-03-27 21:10:41
PR,Fixing normalize sequence call in RNNCell unroll to use valid l,This fix passes in the available argument length to normalize sequence for outputs in BaseRNNCell and FusedRNNCell unroll methods in place of length None In normalize sequence checks exists to check length len inputs if inputs is a list Thus any mismatches will be caught anyway Making this fix solves this following error In running cell unroll a new error that now comes up is,,rishita,2017-03-27 19:35:28,2017-03-27 21:11:02
PR,Adding MXNet Keras integration tests for functionality and performance benchmarking,Created a base for MXNet Keras integration tests for testing functionality of MXNet as Keras backend and also keep track of benchmark numbers on run time memory consumption and accuracy These tests is tested and will run nightly on our nightly Jenkins build server These tests helps us to make sure changes we add in MXNet is not breaking keras functionality Currently scripts install Keras from dmlc keras This will be pip install keras once we push MXNet keras backend to fchollet keras These tests will also helps us to keep an eye on MXNet benchmark numbers and nightly tests will fail whenever a new code that is bringing down the performance train time memory consumption accuracy is pushed,,sandeep-krishnamurthy,2017-03-27 06:03:13,2017-03-27 22:54:17
PR,doc updated get started page,preview at,,"mli,mli",2017-03-22 04:49:43,2017-03-27 23:41:42
PR,ci win,,,"yajiedesign,mli,yajiedesign,mli,yajiedesign,mli,mli",2017-03-27 16:03:09,2017-03-28 05:03:39
PR,fix pad constant grad omp error,,,yajiedesign,2017-03-28 01:58:32,2017-03-28 06:06:00
PR,Making Perl API visible on mxnet io,mli Hello guys I think that the Perl API is finally ready to join the happy family of other languages on the Here is the overview of what is in the pull 1 Multiple bugfixes including the fix for recent travis test failures 2 Full set of unit tests for all subsystems 3 Support for RNN 4 Support for Images and RecordIO 5 Multiple examples 6 Reworked documentation 7 Full set of Perl api md docs to put onto the Thanks,,"sergeykolychev,mli,sergeykolychev,sergeykolychev,mli,sergeykolychev,sergeykolychev",2017-03-28 02:37:28,2017-03-28 07:12:10
IS,Check failed e cudaSuccess CUDA no CUDA capable device is detected,Environment info Operating System Ubuntu 16 04 Compiler What have you tried to solve it 1 I searched a bit and could not find a solution The Ubuntu installation script from pollutes the global environment so I do not want it I can normally run keras tensorflow theano stuff so I do not think there is an issue with my system,,,2017-03-28 01:55:46,2017-03-28 14:13:56
PR,Added perl docs to the navbar and disabled slow linux travis tests,,,"sergeykolychev,sergeykolychev",2017-03-28 13:54:37,2017-03-28 17:52:47
PR,Bugfix remove get engine in initialization,Thanks to,,ZihengJiang,2017-03-28 07:37:36,2017-03-28 17:57:40
IS,the name method of outputs of slicechannel should be changed,Hi now name method is the following it use the ascii table to name the outputs of slicechannel and start from char '0' so it can only support 256 30 226 outputs maximally I suggest to use number to replace it,,piiswrong,2017-03-28 07:58:29,2017-03-28 18:25:45
PR,doc add track numbers,,,mli,2017-03-28 22:26:18,2017-03-28 22:26:29
PR,add infer type to custom op,,,"piiswrong,piiswrong,piiswrong,eric-haibin-lin,Ldpe2G",2017-03-27 23:48:20,2017-03-29 00:27:58
PR,zoneout,piiswrong zoneout implementation using sym where,,"szha,szha,szha",2017-03-25 00:13:12,2017-03-29 00:30:00
PR,cpp package Fix multiple definition issue,Currently if we include MxNetCpp h in several cpp files linker will complains that there are multiple definition of mxnet cpp functions because of one definition rule This commit do several modifications to obey the one definition rule 1 Add inline keyword to functions 2 Get rid of global variable use static class variable instead 3 In optimizer hpp I delayed the registration of sgd optimizer to the first call of OptimizerRegistry Find 4 In kvstore hpp two callback functions updater and controller are now KVStore is static member functions Also extern C is removed because I think it is not necessary when the function is callbacked by function pointer However I'm not 100 sure on this point please correct me if I'm wrong cc thank you,,"sifmelcara,lx75249,sifmelcara,sifmelcara,lx75249,sifmelcara,lx75249,sifmelcara,lx75249,sifmelcara,lx75249,sifmelcara,lx75249,sifmelcara,lx75249,piiswrong,yajiedesign,piiswrong,yajiedesign,yajiedesign",2017-03-23 16:45:09,2017-03-29 04:34:47
PR,Inference in ndarray Reshape,Makes shape the default parameter in nd reshape rather than the current deprecated target shape so that the function behaves as expected e g mx nd reshape X 1 1 works as expected Added support for shape inference 1 on NDarray objects i e for an array X X reshape 1 works,,"JeanKossaifi,piiswrong,JeanKossaifi,piiswrong,piiswrong,JeanKossaifi",2017-03-28 22:05:49,2017-03-29 04:35:42
PR,Fix pslite,rely on,,"yajiedesign,mli",2017-03-28 08:35:46,2017-03-29 04:36:04
PR,OpWrapperGenerator not Py3 compatible,The change introduced in 21b2da07f6ff10669b7649860b913e17e6184708 breaks the build if Python 3 is in the path This forces Python 2 to be used until OpWrapperGenerator is fixed to support Python 3,,doctaweeks,2017-03-23 16:42:13,2017-03-29 04:58:01
IS,AttributeError 'generator' object has no attribute 'next',Environment info Operating System ubuntu16 04 Package used Python R Scala Julia python3 5 MXNet version 0 9 5 Error Message Traceback most recent call last File home rincy PycharmProjects ClassifyEvent mxmodel embedding train py line 88 in module train model args mx cpu 0 File home rincy PycharmProjects ClassifyEvent mxmodel embedding train py line 81 in train model num epoch args num epoch File usr local lib python3 5 dist packages mxnet 0 9 5 py3 5 egg mxnet module base module py line 466 in fit next data batch data iter next AttributeError 'generator' object has no attribute 'next' Minimum reproducible example Here is the code of data iterator When I run this code under the environment of mxnet 0 9 4 everything went fine and I can get the evaluate score model checkpoint But when I update mxnet to 0 9 5 it returns this error What have you tried to solve it I find these pages which related to this issue I know little about py2 Is the difference between py3 and py2 causes this issue How to solve it Thanks for your attention,,piiswrong,2017-03-29 03:11:12,2017-03-29 06:07:36
IS,import mxnet as mx error,image,,,2017-03-26 05:28:10,2017-03-29 06:18:49
IS,compile mxnet dmlccore lib error on VS2015,dmlccore lib local filesys obj error LNK2038 MSC VER 1900 1800 image classification predict obj 4 LIBCMTD lib stdexcpt obj error LNK2005 public cdecl std bad cast bad cast class std bad cast const 0bad cast std QEAA AEBV01 Z dmlccore lib local filesys obj 4 LIBCMTD lib stdexcpt obj error LNK2005 public cdecl std exception exception class std exception const 0exception std QEAA AEBV01 Z dmlccore lib local filesys obj 4 LIBCMTD lib stdexcpt obj error LNK2005 public virtual cdecl std bad cast bad cast void 1bad cast std UEAA XZ dmlccore lib local filesys obj 4 LIBCMTD lib stdexcpt obj error LNK2005 public virtual cdecl std exception exception void 1exception std UEAA XZ dmlccore lib local filesys obj 4 LIBCMTD lib stdexcpt obj error LNK2005 public virtual char const cdecl std exception what void const what exception std UEBAPEBDXZ dmlccore lib local filesys obj 4 LIBCMTD lib sprintf obj error LNK2005 sprintf s dmlccore lib local filesys obj 4 LIBCMTD lib vsnprnc obj error LNK2005 vsprintf s l dmlccore lib local filesys obj 4 dmlccore lib local filesys obj error LNK2019 void cdecl operator delete void unsigned int64 3 YAXPEAX K Z int public virtual void cdecl dmlc io LocalFileSystem ListDirectory struct io LocalFileSystem URI const class std vector struct dmlc io FileInfo class std allocator struct dmlc io FileInfo ' 1' dtor 4 dtor 4 0 ListDirectory LocalFileSystem io dmlc UEAAXAEBUURI 23 PEAV vector UFileInfo io dmlc V allocator UFileInfo io dmlc std std Z 4HA 4 dmlccore lib local filesys obj error LNK2019 Init thread header class std Iostream error category cdecl std Immortalize class std Iostream error category void Immortalize V Iostream error category std std YAAEAV Iostream error category 0 XZ 4 dmlccore lib local filesys obj error LNK2019 Init thread abort int class dtor 0 Iostream error category cdecl std Immortalize class std Iostream error category void ' 1' dtor 0 dtor 0 0 Immortalize V Iostream error category std std YAAEAV Iostream error category 0 XZ 4HA 4 dmlccore lib local filesys obj error LNK2019 Init thread footer class std Iostream error category cdecl std Immortalize class std Iostream error category void Immortalize V Iostream error category std std YAAEAV Iostream error category 0 XZ 4 dmlccore lib local filesys obj error LNK2019 acrt iob func public virtual class dmlc SeekStream cdecl dmlc io LocalFileSystem Open struct dmlc io URI const char const const bool Open LocalFileSystem io dmlc UEAAPEAVSeekStream 3 AEBUURI 23 QEBD N Z 4 dmlccore lib local filesys obj error LNK2019 stdio common vsprintf s vsprintf s l 4 dmlccore lib local filesys obj error LNK2019 std exception copy public cdecl std exception exception class std exception const 0exception std QEAA AEBV01 Z 4 dmlccore lib local filesys obj error LNK2019 std exception destroy public virtual cdecl std exception exception void 1exception std UEAA XZ 4 dmlccore lib local filesys obj error LNK2001 Init thread epoch 4 D Python MXNet mxnet1 mxnet build example image classification predict cpp Debug image classification predict exe fatal error LNK1120 9,,"yajiedesign,yajiedesign",2017-03-26 05:30:59,2017-03-29 06:18:57
IS,scala package Raise the error 'terminate called without an active exception',javelinjs I run the mxnet on the spark with yarn cluster the important config of submission is below From the log of worker I find it raise the error '' when the first epoch is finished image Do you know why,,yzhliu,2017-02-20 12:35:59,2017-03-29 10:34:08
PR,DOC ADD python before launch py to correct the bash command for multi devices,When I tested the command from the documents and README md image classification example there were missing python command before launch py So I add python before launch py to correct the bash command for multi devices,,Soonhwan-Kwon,2017-03-29 07:22:52,2017-03-29 16:40:17
PR,Scala add inferType to CustomOp,follows 5589,,Ldpe2G,2017-03-29 15:40:25,2017-03-29 17:38:38
IS,Training an MLP using my own data,I am learning MXNet and to see if I really understand what is going on I decided to try and learn a multilayer perceptron on some toy data a generated I create some synthetic data of size n X d and corresponding labels of size n X 1 I also have test data of size m X d and corresponding m X 1 labels I create the iterables as batch size 100 train iter mx io NDArrayIter Xtr ytr batch size shuffle True val iter mx io NDArrayIter Xtt ytt batch size and an MLP as input data net mx sym Variable wouldata' input layer net mx sym FullyConnected data net name 'fc1' num hidden 100 net mx sym Activation data net name 'act1' act type arelu' second layer net mx sym FullyConnected data net name 'fc2' num hidden 50 net mx sym Activation data net name 'act2' act type 'tanh' output layer net mx sym FullyConnected data net name 'fc3' num hidden 1 net mx sym Activation data net name 'output' act type isigmoid' I then set up the model and train model fit X train iter eval data val iter batch end callback mx callback Speedometer batch size 200 I get the following error infer shape error Arguments data 100 500L softmax label 100 1L The sizes seem fine to me Any idea what is happening,,"ysh329,piiswrong,piiswrong",2017-03-29 04:57:09,2017-03-29 17:44:52
IS,compile failed with latest mxnet,win10 vs2015 mxnet0 9 4 python2 7 Error LNK2019 unresolved external symbol public cdecl ps Customer Customer int class std function void cdecl struct ps Message const const 0Customer ps QEAA HAEBV function A6AXAEBUMessage ps Z std Z referenced in function public cdecl ps KVServer float KVServer float int 0 KVServer M ps QEAA H Z mxnet D Library Mxnet build kvstore obj 1 Severity Code Description Project File Line Suppression State Error LNK2019 unresolved external symbol public cdecl ps Customer Customer void 1Customer ps QEAA XZ referenced in function public virtual cdecl ps KVServer float KVServer float void 1 KVServer M ps UEAA XZ mxnet D Library Mxnet build kvstore obj 1 cmake Found OpenBLAS libraries D Library OpenBLAS lib libopenblas dll a Found OpenBLAS include D Library OpenBLAS include CUDA detected 8 0 Found cuDNN include D Library CUDNN include library D Library CUDNN lib x64 cudnn lib Added CUDA NVCC flags for sm 52 OpenCV LIBS opencv core opencv highgui opencv imgproc opencv imgcodecs OpenCV found D Library OpenCV32 build x64 vc14 lib Try OpenMP C flag openmp Performing Test OpenMP FLAG DETECTED Performing Test OpenMP FLAG DETECTED Success Try OpenMP CXX flag openmp Performing Test OpenMP FLAG DETECTED Performing Test OpenMP FLAG DETECTED Success Found OpenMP openmp Found cuDNN include D Library CUDNN include library D Library CUDNN lib x64 cudnn lib Found PROTOBUF Compiler D Library Protobuf builds Release protoc exe Could NOT find GTest missing GTEST LIBRARY GTEST INCLUDE DIR GTEST MAIN LIBRARY Configuring done Generating done,,,2017-03-18 09:09:09,2017-03-30 02:51:28
IS,how do I extract feature from trained model with c,,,,2017-03-27 14:19:21,2017-03-30 03:08:50
PR,Fix the behaviour of ndarray T,For an array X X T has no reason to work only if X is a matrix,,JeanKossaifi,2017-03-30 02:18:59,2017-03-30 06:15:31
PR,Added ndim property to ndarray,Making mx nd array more consistent with NumPy by adding an ndim property Also useful to avoid having to write len array shape every time,,"JeanKossaifi,piiswrong",2017-03-30 00:19:04,2017-03-30 06:15:44
PR,Extensive revision of Python API docs,Revised docs in Python API extensively,,"zackchase,piiswrong,zackchase",2017-03-29 01:33:15,2017-03-30 06:20:18
PR,contrib fft ifft countsketch,Move fft ifft countsketch to contrib Add test file to test operator gpu py,,"piiswrong,piiswrong",2017-03-24 19:37:16,2017-03-30 06:47:44
IS,Have pretrained incepiton v2 model,I'm making some benchmarks about inference on some embeded system devices I only found pretrained inception v3 batchnorm model on MXNet model zoo Thus I wanna ask does anyone have pretrained incepiton v2 batchnorm model Thanks a lot smile,,"ysh329,tornadomeet,ysh329",2017-03-30 04:43:49,2017-03-30 08:34:01
IS,Tutorial nlp cnn error in training block,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Mac OS 10 11 5 Compiler GCC 4 2 1 Compatible Apple LLVM 7 0 2 clang 700 1 81 on darwin Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 11 If you are using R package please provide R sessionInfo Error Message Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error All of the code from up to the training block What have you tried to solve it 1 should line args grad name mx nd zeros shape ctx be changed to initializer mx init InitDesc name arg dict name,,,2017-03-30 17:37:29,2017-03-30 18:51:23
PR,Revert contrib fft ifft countsketch,Reverts dmlc mxnet 5563 I merged too fast It is causing linking errors on test server Please debug and resubmit,,piiswrong,2017-03-30 16:35:34,2017-03-30 21:12:57
PR,add attention cells,dot attention,,"piiswrong,piiswrong,piiswrong,eric-haibin-lin,szha,piiswrong",2017-03-22 20:30:21,2017-03-30 21:49:29
IS,How to create a custom GPU operation with custom CUDA kernels for computation,Although mxnet documentation did mention that it is possible to Use C mshadow and CUDA to create custom operations it has no real examples of how to used a custom CUDA kernel taking gpu pointers as part of a custom operation Can anyone provide a simple example of this flow,,"luoyetx,Ldpe2G",2017-03-04 01:32:07,2017-03-30 21:50:02
IS,Windows error 126,Traceback most recent call last File ipython input 1 30eb4f951ea5 line 1 in module import mxnet File D Program Files Anaconda2 lib site packages mxnet 0 9 3 py2 7 egg mxnet init py line 7 in module from base import MXNetError File D Program Files Anaconda2 lib site packages mxnet 0 9 3 py2 7 egg mxnet base py line 43 in module LIB load lib File D Program Files Anaconda2 lib site packages mxnet 0 9 3 py2 7 egg mxnet base py line 35 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File D Program Files Anaconda2 lib ctypes init py line 362 in init self handle dlopen self name mode,,yajiedesign,2017-03-30 01:28:45,2017-03-31 01:25:04
PR,Caffe without the patch cpp package fixed also with Caffe plugin,,,"cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,lx75249,lx75249,lx75249,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,lx75249,lx75249,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01",2017-03-26 02:53:31,2017-03-31 03:13:36
PR,add env path repare for win pip,,,yajiedesign,2017-03-31 01:20:51,2017-03-31 04:47:07
IS,Wrong predictions in R on array and array iterator data structure,,,,2017-03-31 11:55:05,2017-03-31 15:40:51
PR,doc update build from source md,preview at,,"mli,mli,sergeykolychev,sergeykolychev",2017-03-30 21:25:36,2017-03-31 17:41:35
PR,Debug kill mxnet py command can not kill remote processes,Fixed host parameter bug when the remote host has no phrase And also fixed un controlable SSL verification message using oStrincHostKeyChecking no It prevents the message Are you sure you want to continue connecting yes no and in this case you can not type yes,,kindruth,2017-03-31 06:59:48,2017-03-31 17:56:14
PR,doc updated get started index page,preview at TODO scala julia r perl examples are not completed,,"mli,piiswrong,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,mli,mli,mli,piiswrong,mli,domdivakaruni",2017-03-30 19:54:07,2017-03-31 18:01:43
PR,fix NDArrayGetData to return to use the right datatype,This failed if one tried to get the underlying data pointer for a NDArray with a dtype other then Float32 I do not know if any of the other packages need to be adapted I discovered this while working on the Julia package cc,,"vchuravy,piiswrong,piiswrong,pluskid,vchuravy",2017-03-30 05:42:00,2017-03-31 22:13:44
PR,docs win build docs,,,yajiedesign,2017-04-01 00:11:10,2017-04-01 01:49:10
IS,FCN loading checkpoint for training without pretrained model,In the FCN example the code is trying to load a check point when training from scratch without pretrained model I think this is pretty illogical By looking at the init fcnxs py I think we can remove this dependence An infer shape is called within initialization function we can use the inferred shape to initial network,,Godricly,2017-03-10 10:53:32,2017-04-01 09:04:44
IS,scala Data iterator for large image data set which does't fit in memory,Not really an issue more a question I work with an image data set that does not fit in memory It seems like a quite common use case but I could not find in the scala API a data iterator for data set that does not fit in memory Did I miss something Should I just implement my own iterator that extend DataIter Is there an example somewhere,,"benqua,benqua",2017-03-13 20:41:31,2017-04-01 11:37:17
IS,scala mklml Failed creation convert to int,Description On master I was able to compile mxnet make and mxnet scala make scalapkg successfully with the below configuration and a LD LIBRARY PATH set to home benq code mxnet mkl lib It compiles successfully Error Message When running a scala program with the generated jar I get the following error message src operator mkl mkl memory cc 47 Check failed status E SUCCESS 1 vs 0 Failed creation convert to int with status 1 for buffer fwd top data MKLPoolingOpthe full error message including stack trace which is the same error message as described in However I am pretty sure that I use the mklml version downloaded by the prepare mkl sh script because if LD LIBRARY PATH is not set I get another error message saying it cannot find the relevant so file The downloaded file is mklml lnx 2017 0 2 20161122 tgz and it has been successfully extracted in the home benq code mxnet mkl folder it would not compile otherwise,,"benqua,benqua",2017-02-09 12:18:06,2017-04-01 11:38:20
PR,Change mkl fully connected operator to stateless,Move LayerSetUp to constructor Remove unused codes Put resource members to local variables This allows Forward and Backward to be called standalone So it is not necessary to call Forward before Backward,,"jermainewang,jermainewang,glingyan,jermainewang,glingyan,jermainewang,jermainewang,jermainewang,jermainewang,cjolivier01",2017-03-27 22:23:02,2017-04-01 14:21:54
IS,Does mxnet support layer Like EuclideanLossLayer in caffe now,Does mxnet support layer Like EuclideanLossLayer in caffe now,,pluskid,2016-09-01 13:48:52,2017-04-01 15:09:00
PR,Adds mx nd moveaxis,Adds a moveaxis function for ndarrays,,"JeanKossaifi,piiswrong,JeanKossaifi,piiswrong,JeanKossaifi,piiswrong,JeanKossaifi",2017-03-30 21:57:39,2017-04-01 17:34:47
PR,modify mx ndarray reshape documentation,zackchase please take a look,,"nswamy,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,mli,nswamy,madjam",2017-03-29 00:20:53,2017-04-02 21:37:13
PR,Example ssd major update,New image detection iterator in src io iter image det recordio cc similar to iter image recordio cc This iterator coupled with image det aug default can handle various augmentation options for image detection As a result training is significantly faster Better performance on mAP basically 5 mAP improvement Can monitor validation mAP during training at the end of each epoch Caffe converter for SSD models is available,,"zhreshold,piiswrong,zhreshold,piiswrong,zhreshold,zhreshold",2017-03-31 04:25:27,2017-04-03 00:41:37
PR,Implement 3D deconvolution cuDNN,This pull request adds support for 3D deconvolution when compiled with cudnn and run on GPU It does not break the current 2D Deconvolution non cudnn code and simply throws an error if the non cudnn code is called with a 3D kernel As the non gpu version does not support 3D deconvolution this pull request does not add test cases to compare that their output is similar Instead I manually compared the output to the PyTorch 3D Deconvolution implementation I have seen that the cudnn convolution code uses DMLC DECLARE FIELD workspace set default 1024 set range 0 8192 Shall the cudnn deconvolution code be adapted to use a default of 1024 for as well Currently the default is 512 The following code can be used to compare with PyTorch,,"leezu,sxjscience,leezu,piiswrong,leezu,sxjscience,leezu,piiswrong,leezu",2017-03-29 06:07:59,2017-04-03 14:38:19
PR,simple changes to mxnet image ImageIter to make it more newbie friendly,This pull request makes it a bit easier to build off of mx io ImageIter to make your own image iters by pulling out some code out into their own functions Example,,"jmerkow,piiswrong,piiswrong,jmerkow,piiswrong,piiswrong,jmerkow,jmerkow,jmerkow,jmerkow",2017-03-06 23:59:44,2017-04-03 16:11:39
PR,Temporarily Revert to fix formatting issues modify mx nd reshape doc,This reverts commit 138344683e65c87af20250e3f4cdcc5a72ac3cc5,,nswamy,2017-04-03 18:06:50,2017-04-03 18:11:00
PR,compile with macOS support,,,"liangfu,piiswrong,liangfu,piiswrong,liangfu,piiswrong,szha,szha,liangfu,szha,szha,liangfu,piiswrong,howard0su,liangfu,mli",2016-12-16 15:20:09,2017-04-03 18:35:37
PR,fixed messing up of progressbar tidy up the logging print,Printout after change,,mli,2016-11-16 08:06:55,2017-04-03 18:35:53
PR,add cmake option to support NNPACK,add cmake option to support NNPACK,,"liangfu,piiswrong,piiswrong,howard0su,liangfu,liangfu,tornadomeet,piiswrong,tornadomeet,tornadomeet",2016-12-14 08:02:01,2017-04-03 18:36:15
PR,New CUDA kernels for binary broadcast for add sub mul div eq operations,,,"ap-hynninen,piiswrong,piiswrong,ap-hynninen,piiswrong",2017-03-31 18:41:37,2017-04-03 21:55:57
PR,add link to 512 model,add 512 model link fix a pad issue,,zhreshold,2017-04-03 19:44:58,2017-04-03 22:01:04
PR,Update the cloud md,The previous link of Caffe has a good tutorial is wrong,,xiandong79,2017-04-03 06:17:03,2017-04-03 22:16:07
PR,Revert Revert contrib fft ifft countsketch,Reverts dmlc mxnet 5634 Looks like this still gives cufft error on windows,,"piiswrong,sxjscience,sxjscience,cjolivier01",2017-03-31 03:19:24,2017-04-03 22:18:42
PR,add zero propagation,,,"piiswrong,piiswrong,piiswrong,Ldpe2G,Ldpe2G",2017-03-30 18:31:34,2017-04-03 23:48:01
PR,WIP Rewriting image IO pipeline to improve performance,The current image IO pipeline using ImageRecordIter is slow due to unnecessary memory copies between stages and using only a single thread for most of the pipeline This PR aims to improve the performance by merging those stages into 1 step,,"ptrendx,piiswrong,piiswrong,ptrendx,ptrendx,piiswrong,piiswrong,winstywang,winstywang,piiswrong,piiswrong,ptrendx",2017-02-08 19:46:20,2017-04-04 05:08:57
PR,Adds GetDType signature to mxnet cpp is ndarray h and fixes typo in implementation,Minor typo fix for mxnet cpp,,Hebali,2017-04-04 03:44:29,2017-04-04 17:51:58
PR,Fix data type issues of one hot take and pick and minor fix for conv pool,one hot issue reported at pool NNPACK compile issue,,"reminisce,piiswrong,piiswrong,reminisce,reminisce,piiswrong,piiswrong,reminisce,piiswrong,piiswrong,reminisce",2017-03-29 17:07:10,2017-04-04 18:02:27
IS,mx mod Module predict error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia python MXNet version 0 93 0 94 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message matrix op inl h 712 Check failed param begin i shp i param end i shp i param begin i param end i Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error test module iris txt Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 mod mx mod Module symbol net context mx cpu data names wouldata' label names isoftmax label' logging basicConfig level logging INFO epoch num 15 mod fit train iter eval data val iter optimizer 'adam' optimizer params 'learning rate' 1 eval metric 'acc' num epoch epoch num print 'Finished training' y mod predict val iter num batch 1 2 new samples np array 6 4 3 2 4 5 1 5 6 0 2 2 5 0 1 5 6 9 3 2 5 7 2 3 5 6 2 8 4 9 2 0 7 7 2 8 6 7 2 0 6 7 3 3 5 7 2 1 7 2 3 2 6 0 1 8 6 2 2 8 4 8 1 8 6 4 2 8 5 6 2 1 6 1 3 0 4 9 1 8 dtype float result mod predict mx io NDArrayIter data new samples label None batch size 1 3 What have you tried to solve it 1 I try to use the mx model FeedForward predict it worked But it is a deprecated api 2 3,,,2017-04-01 10:36:15,2017-04-05 00:02:24
PR,add zero propagation,add zero prop add test fix fix fix fix,,"piiswrong,ZihengJiang",2017-04-03 23:48:20,2017-04-05 00:32:40
IS,Build error cv does not name a type with USE OPENCV 0,While building from source an error is thrown This is weired because USE OPENCV 0 in config mk Environment info Operating System Ubuntu 16 04 Or if installed from source MXNet commit hash git rev parse HEAD 911f7474deafc3cc95a2589b89432f94f22cbbe1 Error Message src io image det aug default cc 18 14 error cv does not name a type using Rect cv Rect float Makefile 207 recipe for target 'build src io image det aug default o' failed make build src io image det aug default o Error 1 Minimum reproducible example Easily reproducible by following the step by step below on a newly installed ubuntu 16 04 without opencv Steps to reproduce 1 git clone recursive 2 cd mxnet 3 emacs make config mk change USE OPENCV 1 to USE OPENCV 0 4 make j8 It compiled fine on the same system with the same configuration with mxnet version 2735414c9f55e3e33d7eeb0c0293e0175e2644d6,,"benqua,freddycct,freddycct",2017-04-04 19:33:00,2017-04-05 17:00:07
PR,fix compilation error for USE OPENCV 0,fix 5685,,"mli,freddycct,mli,freddycct",2017-04-04 23:16:34,2017-04-05 17:00:07
PR,rename ndarray or symbol to NDArray or Symbol,attempt to fix 5281 and 5577,,"mli,piiswrong,mli",2017-03-30 20:46:52,2017-04-05 17:05:02
PR,Typos in the comment examples,,,,2017-04-05 17:46:10,2017-04-05 17:48:15
PR,fix argument parsing,,,"piiswrong,mli",2017-04-05 05:25:08,2017-04-05 17:51:07
PR,Scala fix bug of DataParallelExecutorGroup,fix the problem described in issue 5695,,Ldpe2G,2017-04-05 12:44:08,2017-04-05 17:53:14
PR,Update index md,Fixed hyphenation was an underscore,,zackchase,2017-04-05 19:27:02,2017-04-05 20:34:27
PR,Move autograd into master branch,jermainewang,,"ZihengJiang,piiswrong,piiswrong,piiswrong,ZihengJiang,piiswrong,jermainewang,piiswrong,jermainewang,piiswrong,piiswrong,tqchen",2017-03-31 19:06:14,2017-04-05 23:00:04
PR,save load for module loss metric,,,"piiswrong,piiswrong",2017-03-15 23:00:47,2017-04-05 23:04:18
PR,reverse previous change on the install mxnet ubuntu script,According to user feedback the previous added cp make config mk in ubuntu install python r script will overrides the config mk that user has edited by following I should not edit the script itself instead I should edit the install mxnet for r by adding cp make config mk in the first line of To install MXNet for R section,,jiajiechen,2017-04-06 06:37:45,2017-04-06 06:42:05
IS,scala Module API NullPointerException while calling bind with forTraining false,While doing feature extraction from a pre trained vgg16 network a NullPointerException is thrown when bind is called with forTraining false See code below Error Message the NullPointerException is thrown when bind is called,,"benqua,Ldpe2G,benqua",2017-04-05 10:17:33,2017-04-06 07:43:17
PR,reverse previous change on script edit markdown,According to recent user feedback the previous added cp make config mk in ubuntu install python r scripts will overrides the config mk that user has edited to enable GPU by following install mxnet for python I should not edit the script itself instead I should edit the install mxnet for r by adding in the To install MXNet for R section so that R user would not skip this part I think the get start instruction assumes people install install mxnet ubuntu python sh before they run install mxnet ubuntu r sh but R use might jump to To install MXNet for R section and ignores the content in Install MXNet for Python,,jiajiechen,2017-04-06 06:56:55,2017-04-06 16:45:03
PR,fix formatting issues with Reshape documentation,fix formatting issues with Reshape preview can be seen in the screenshot Reshape pdf,,"nswamy,madjam,madjam,madjam,zackchase,mli,zackchase,mli,nswamy,nswamy,nswamy,nswamy,mli,nswamy,eric-haibin-lin,piiswrong,nswamy,mli,nswamy,nswamy",2017-04-04 20:15:35,2017-04-06 18:06:22
PR,Scala add AutoGrad,follows 5652,,"Ldpe2G,piiswrong,yzhliu,piiswrong,piiswrong,yzhliu,Ldpe2G,Ldpe2G,yzhliu,Ldpe2G",2017-04-06 11:26:50,2017-04-06 21:27:17
PR,doc build so with default config mk for doc,,,mli,2017-04-04 23:08:59,2017-04-06 23:08:00
PR,added mxnet js test dockerfile,added dockerfile to test build of mxnet js on ubuntu machines with latest emcc version,,arank,2017-04-06 23:40:31,2017-04-06 23:41:38
IS,Take for ever to run forward,Environment info Operating System Deep Learning AMI for Amazon Linux Package used Python MXNet version 0 7 0 Python version and distribution 2 7 I am trying to use the Customized Symbol define at but it seems to take for ever to run the forward method Debug shows that the it gets stuck at step self assign out data 0 req 0 mx nd array y Here is the code import mxnet as mx import numpy as np class Softmax mx operator CustomOp def forward self is train req in data out data aux x in data 0 asnumpy y np exp x x max axis 1 reshape x shape 0 1 y y sum axis 1 reshape x shape 0 1 self assign out data 0 req 0 mx nd array y print Softmax forward def backward self req out grad in data out data in grad aux print Softmax backward l in data 1 asnumpy ravel astype np int y out data 0 asnumpy y np arange l shape 0 l 1 0 self assign in grad 0 req 0 mx nd array y register this operator into MXNet by name softmax operator register softmax class SoftmaxProp mx operator CustomOpProp def init self softmax is a loss layer so we don t need gradient input from layers above super SoftmaxProp self init need top grad False def list arguments self return wouldata' 'label' def list outputs self return 'output' def infer shape self in shape data shape in shape 0 label shape in shape 0 0 output shape in shape 0 return data shape label shape output shape def create operator self ctx shapes dtypes return Softmax class SimpleBatch object def init self data label pad 0 self data data self label label self pad pad class SimpleIter def init self data names data shapes data gen label names label shapes label gen num batches 10 self provide data zip data names data shapes self provide label zip label names label shapes self num batches num batches self data gen data gen self label gen label gen self cur batch 0 def iter self return self def reset self self cur batch 0 def next self return self next def provide data self return self provide data def provide label self return self provide label def next self if self cur batch self num batches self cur batch 1 data mx nd array g d 1 for d g in zip self provide data self data gen assert len data 0 Empty batch data label mx nd array g d 1 for d g in zip self provide label self label gen assert len label 0 Empty batch label return SimpleBatch data label else raise StopIteration num classes 10 net mx sym Variable wouldata' net mx sym FullyConnected data net name 'fc1' num hidden 64 net mx sym Activation data net name arelu1' act type relu net mx sym FullyConnected data net name 'fc2' num hidden num classes net mx sym SoftmaxOutput data net name isoftmax' net mx symbol Custom data net op type isoftmax' print net list arguments print net list outputs n 32 data SimpleIter wouldata' n 100 lambda s np random uniform 1 1 s 'custom0 label' n lambda s np random randint 0 num classes s mod mx mod Module symbol net label names 'custom0 label' mod fit data num epoch 5 Can anyone give a hand Thanks,,"piiswrong,piiswrong",2017-04-07 00:31:04,2017-04-07 01:53:50
PR,Adding quick intro and install section in home page,Added quick intro What is MXNet and Quick Install steps in MXNet home page Tested on local machine for content and CSS minimize maximize scroll text size etc Credits Quick intro content and wireframe is contributed by Snapshot is attached img width 1280 alt screen shot 2017 04 05 at 3 46 31 pm src,,"sandeep-krishnamurthy,madjam,sandeep-krishnamurthy,piiswrong,sandeep-krishnamurthy,zackchase,mli,zackchase,mli",2017-04-05 22:53:04,2017-04-07 03:10:28
IS,Does it make sense when using distributed training with different type GPUs,Sorry for my pool English I have two machines one has 4 Titan X Pascal the other one has 4 1080Ti Both have nearly Single Precision Performance I am wondering if I can deploy distributed training with these two machines with different type GPUs Thanks,,eric-haibin-lin,2017-04-05 03:19:34,2017-04-07 03:56:23
IS,How do I get the internal layers value when training,How do I get the internal layers value when training I need to debug,,,2017-04-07 02:30:43,2017-04-07 05:54:55
PR,mxnet spring update 2,1 Bypass pooling for kFull 2 Fix upstream Full connection coverage problem 3 Enable relu in place 4 Fix multi node coverage problem 5 add test conv for Jenkins,,"glingyan,piiswrong,piiswrong,glingyan,glingyan,piiswrong,glingyan,glingyan,glingyan,glingyan",2017-04-07 00:14:04,2017-04-07 07:09:55
PR,fix build error for USE DIST KVSTORE,Signed off by Lingyan Guo lingyan guo intel com,,"glingyan,glingyan",2017-04-07 14:21:39,2017-04-07 16:53:33
PR,Improving SSD example,Adding 3 notebooks 1 ssd prepare for downloading data 2 ssd predict for demo 3 ssd train for training on VOC dataset Moreover the ssd iterator has been adapted to python3,,"piiswrong,zhreshold,piiswrong,piiswrong,piiswrong,mli",2017-03-21 07:09:23,2017-04-07 18:43:38
PR,Markdown to notebook,Convert markdown to notebook,,"kevinthesun,mli,mli,mli,mli,mli,kevinthesun,kevinthesun,mli,mli,kevinthesun,kevinthesun,mli,kevinthesun,kevinthesun",2017-04-05 21:36:01,2017-04-07 21:12:10
PR,fix mkl mem for faster rcnn,fix mkl experimental bug when using faster rcnn and custom op,,"piiswrong,zhenlinluo,glingyan,glingyan",2017-04-07 20:36:26,2017-04-08 17:03:45
PR,Update image py,Enable the insertion of non default mean and std as np array This is required if the NN expects different normalization than the default one,,"mli,piiswrong,piiswrong",2017-04-07 17:54:36,2017-04-08 17:04:23
PR,enable layout for rnn state enable additional attribute via kwargs,enable layout for rnn state enable additional attributes via kwargs in mx sym var,,"sxjscience,piiswrong,piiswrong,piiswrong,sxjscience",2017-04-07 16:14:59,2017-04-08 17:07:05
PR,added mxnet js tests,Added docker build for emcc from src to compile mxnet js on ubuntu,,"arank,mli,mli,arank,arank",2017-04-06 23:46:17,2017-04-08 17:07:26
PR,fix MSHADOW USE CUDNN check,,,"loofahcus,piiswrong,loofahcus,loofahcus",2017-04-06 04:01:51,2017-04-08 17:08:04
PR,Loss mx,,,,2017-04-08 17:11:07,2017-04-08 17:21:05
PR,Exposing iscontiguous property,Added a iscontiguous property to mx nd array,,"JeanKossaifi,JeanKossaifi,piiswrong,JeanKossaifi,piiswrong,piiswrong",2017-04-04 19:29:13,2017-04-08 17:21:48
PR,Fix test autograd py,,,ZihengJiang,2017-04-08 18:23:40,2017-04-08 20:26:27
PR,perl ZoneoutCell nd inferred reshape and moveaxis cosmetic changes to Image Iter reworked docs,ZoneoutCell nd inferred reshape and moveaxis cosmetic changes to Image Iter few bug fixes Reworked POD in order to be pretty at,,sergeykolychev,2017-04-08 20:38:22,2017-04-09 05:16:17
PR,add pretrained resnext 101 64x4d model,,,Jerryzcn,2017-04-08 21:02:45,2017-04-09 05:16:32
IS,example rcnn python cv2 error,Am trying to run the rcnn example as per the instructinos contained in the README I have a recent mxnet did make clean make Looks like a problem with opencv but i do not know how to fix it without breaking something else Ubuntu 16 04 mxnet git clone from a few days ago Python 2 7 12,,,2017-04-07 18:17:26,2017-04-09 10:35:27
IS,Cannot find symbol MultiBoxPrior and other contribs,'Module' object has no attribute 'MultiBoxPrior' Environment info Operating System Ubuntu 16 04 MXNet version installed from source MXNet commit hash git rev parse HEAD f0a450d6d5605f4d5d59f974460673be4251b70c Python version and distribution python 2 7 12 Error Message Traceback most recent call last File ssd demo py line 168 in module ctx args nms thresh args force nms File ssd demo py line 106 in get detector get symbol len CLASSES nms thresh force nms File ssd symbol symbol vgg16 reduced py line 163 in get symbol net get symbol train num classes File ssd symbol symbol vgg16 reduced py line 119 in get symbol train num channels num channels clip True interm layer 0 File ssd symbol common py line 164 in multibox layer anchors mx contrib symbol MultiBoxPrior from layer sizes size str ratios ratio str AttributeError 'module' object has no attribute 'MultiBoxPrior' Minimum reproducible example Run the demo code from example ssd demo py Steps to reproduce Run the demo code from example ssd demo py What have you tried to solve it Build the mxnet project from sources with different options DEBUG 1 DEV 1 USE PROFILE 1 and then python setup py install,,"piiswrong,zhreshold",2017-03-29 01:05:26,2017-04-09 12:41:33
PR,improve parallel actor critic,Quit an episode when all envs finish instead of any env finishes to make the rewards increase gradually,,loofahcus,2017-04-09 06:13:26,2017-04-09 17:34:27
IS,Fix for NDArrayIter documentation,In mxnet io NDArrayIter mxnet io NDArrayIter for last batch handle it is written roll over is intended for training and can cause problems if used for prediction However I have examined the code and module predict handles this issue I think the corresponding sentence should be removed detailed for Module API,,,2017-04-05 00:37:29,2017-04-09 19:25:51
PR,Add error for NDArray bool casting,,,"ZihengJiang,piiswrong,ZihengJiang",2017-04-09 07:41:12,2017-04-10 00:36:39
IS,failed in amalgamation to TX1,Environment info Operating System linut4t Compiler gcc version 5 4 0 MXNet version 0 9 3 Error Message Please paste the full error message including stack trace D MIN 0 dmlc minimum0 cc dmlc d g std c 11 Wno unknown pragmas Wall DMSHADOW USE CBLAS 1 DDISABLE OPENMP 1 DMSHADOW USE CUDA 0 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DDMLC LOG STACK TRACE 0 DMSHADOW FORCE STREAM DMXNET USE OPENCV 0 DMXNET PREDICT ONLY 1 I usr local opt openblas I usr local opt openblas include M MT mxnet predict0 o I pwd I pwd mshadow I pwd dmlc core include I pwd nnvm include I pwd include D MIN 0 mxnet predict0 cc mxnet predict0 d In file included from home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow extension packet inl h 203 0 from home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow extension implicit gemm h 11 from home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow dot engine inl h 11 from home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow expr engine inl h 431 from home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow extension h 10 from home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow tensor h 1047 from home ubuntu work mxnet ssd cpp mxnet amalgamation src ndarray ndarray function h 10 from home ubuntu work mxnet ssd cpp mxnet amalgamation src ndarray ndarray function cc 8 from mxnet predict0 cc 29 home ubuntu work mxnet ssd cpp mxnet amalgamation mshadow mshadow extension packet sse inl h 10 23 fatal error emmintrin h No such file or directory compilation terminated Makefile 63 recipe for target 'mxnet predict0 d' failed Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 make clean make What have you tried to solve it 1 add define MSHADOW USE SSE 0 to prep nnvm sh 2 add define MSHADOW USE SSE 0 to nnvm c 3 add define MSHADOW USE SSE 0 to mxnet predict0 cc 4 add define MSHADOW USE SSE 0 to mxnet predict all cc Then I got the following error mxnet predict all cc 17401 42 warning comparison between signed and unsigned integer expressions Wsign compare if static cast int ignore label k g internal compiler error Killed program cc1plus Please submit a full bug report,,,2017-04-09 08:56:30,2017-04-10 03:20:12
PR,Change argument name from src to data,Change argument name from src to data for ops that do not have FListInputNames properly set 5736,,lx75249,2017-04-10 08:40:57,2017-04-10 15:52:37
PR,Module Forward reshape,Add support for different batch size data for forward One major use case is predicting single input or any data of which batch size is different from training data,,kevinthesun,2017-04-10 19:07:53,2017-04-10 19:08:10
PR,Use Pre installed EC2 GPU Instance,I add the section Use Pre installed EC2 GPU Instance so that we can avoid the complicated installation steps Also I noticed that AMI e g ami 12fd8178 with dependencies installed is no more available and some issues on GitHub e g 1790 also prove my idea So maybe the things about ami 12fd8178 should be deleted,,"xiandong79,mli,xiandong79,piiswrong",2017-04-06 04:27:15,2017-04-10 19:14:17
PR,Resnext101 64x4d,,,"Jerryzcn,mli,Jerryzcn",2017-04-10 20:15:32,2017-04-10 20:23:17
IS,Training Data Form,Kindly I have a dataset of 6 variables 5 features 4 categorical 1 numeric class label 1 loaded the data of 184 row 2 Split it 2 3 so I have 122 record for training 62 record for Testing 3 So in order to to create the CNN I need to define the dims of the training so I added the following train data matrix Datatrain train x train 6 train y train 6 train array train x dim train array c 122 5 1 ncol train x but actually I got the following error Error in dim train array c 122 5 1 ncol train x dims product 3660 do not match the length of object 610,,,2017-04-08 04:00:19,2017-04-10 20:35:39
IS,MKL2017 numerical instability,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu Compiler gcc 5 4 Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error cd example rnn sed i ' args mom d' lstm bucketing py get ptb data sh python lstm bucketing py optimizer adam When using a MXNet version that is compiled with USE MKL2017 1 this will get to a Train Perplexity of around 400 after one Epoch When USE MKL2017 0 this goes down to 200 in the same number of steps I can reproduce the problem also on my Ubuntu machine If I enable USE MKL2017 1 but comment out the support for just MKLFullyConnectedOP I see results that are similar to MKL2017 turned off i e at least part of the problem lies there What have you tried to solve it 2017 03 31 08 35 53 727 Epoch 0 Batch 1300 Speed 192 69 samples sec Train Perplexity 599 170847 2017 03 31 08 35 54 749 Epoch 0 Train Perplexity 614 747938 2017 03 31 08 35 54 749 Epoch 0 Time cost 289 619,,"cjolivier01,piiswrong,zhenlinluo",2017-03-31 15:35:08,2017-04-10 21:08:21
PR,Disable MKL is fully connected layer for the convergence issue,This change does not affect the performance So we can use atlas or openblas instead We will add MKL is sgemms instead of MKL is DNN primitive This will resolve 5650,,"piiswrong,piiswrong",2017-04-10 19:01:31,2017-04-10 21:08:21
PR,Python API revisions,Second pass over all py docstrings covered module subdirectory this time and moved all docstrings into closer compliance with NumPy doc standard,,"zackchase,sergeykolychev,zackchase,madjam,zackchase,mli,pluskid,zackchase,piiswrong,zackchase,madjam,zackchase,piiswrong,zackchase,zackchase,eric-haibin-lin,zackchase,piiswrong",2017-04-04 21:14:14,2017-04-10 21:27:00
PR,Fix minor issue in mxnet keras nightly test,1 Fix path issue in mxnet keras nightly test script 2 Added execute permission to mxnet keras nightly test scripts can you please review this change,,"sandeep-krishnamurthy,lxn2,lxn2,sandeep-krishnamurthy",2017-04-10 17:41:06,2017-04-10 21:31:33
PR,Copy code button,Copy code button copycode1 copycode2 It will automatically remove all texts which are not executable except code comments Can you review this change Thanks,,"kevinthesun,zackchase,piiswrong,mli,madjam,kevinthesun,kevinthesun,kevinthesun,mli",2017-04-10 18:33:53,2017-04-10 22:22:53
PR,Fix executor monitor bug,fix missing monitor callbacks 5644 5691,,"eric-haibin-lin,piiswrong,piiswrong,piiswrong,eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin",2017-04-10 05:09:59,2017-04-10 22:30:14
PR,Support different batch size for forward input,Add support for different batch size data for forward One major use case is predicting single input or any data of which batch size is different from training data,,"kevinthesun,piiswrong,kevinthesun",2017-04-03 21:38:17,2017-04-10 23:36:58
IS,why mxnet ndarray not support multi dimension indexing,a mx nd ones 2 3 a 0 multi dimension indexing is not supported,,"piiswrong,voidrank",2017-04-10 08:03:08,2017-04-11 01:27:10
IS,the official docker images build failed,Here is the link of official docker images It seems that the build process is failed in recent months,,"pineking,pineking",2017-04-11 02:00:32,2017-04-11 02:16:41
IS,Install mxnet on amazon linux,I am trying to follow the standard installation guide but get the following error ec2 user ip xx x xx xxx build sudo make PREFIX usr local install 89 Built target opencv cudaoptflow pch dephelp 89 Generating precomp hpp gch opencv cudaoptflow RELEASE gch 89 Built target pch Generate opencv cudaoptflow 89 Building NVCC Device object modules cudaoptflow CMakeFiles cuda compile dir src cuda cuda compile generated pyrlk cu o nvcc error 'cicc' died due to signal 11 Invalid memory reference CMake Error at cuda compile generated pyrlk cu o cmake 266 message Error generating file home ec2 user opencv build modules cudaoptflow CMakeFiles cuda compile dir src cuda cuda compile generated pyrlk cu o make 2 modules cudaoptflow CMakeFiles cuda compile dir src cuda cuda compile generated pyrlk cu o Error 1 make 1 modules cudaoptflow CMakeFiles opencv cudaoptflow dir all Error 2 make all Error 2,,,2017-04-11 04:40:29,2017-04-11 06:03:49
IS,How to understand terms kNCHW NCW NCHW NCDHW kNHWC,How to understand terms kNCHW NCW NCHW NCDHW kNHWC kNCHW kNHWC Totally lost,,"ZihengJiang,sxjscience,ZihengJiang,ZihengJiang",2017-04-11 08:42:31,2017-04-11 09:21:24
PR,Scala make func start with ' contrib ' public,5712,,"Ldpe2G,yzhliu,Ldpe2G,piiswrong,Ldpe2G,Ldpe2G,Ldpe2G,yzhliu",2017-04-09 23:58:37,2017-04-11 14:50:12
IS,mxnet R Package install using devtools install github dmlc mxnet R package,I'm getting the following error when trying to install using devtools Downloading GitHub repo dmlc mxnet master from URL Installing mxnet ' usr lib R bin R' no site file no environ no save no restore quiet CMD INSTALL ' tmp RtmpCUvylF devtools1ca26ea451c2 dmlc mxnet 723966c R package' library ' home neten R x86 64 pc linux gnu library 3 3' install tests installing source package mxnet ERROR a 'NAMESPACE' file is required removing home neten R x86 64 pc linux gnu library 3 3 mxnet restoring previous home neten R x86 64 pc linux gnu library 3 3 mxnet Error Command failed 1 In addition Warning message GitHub repo contains submodules may not function as expected,,jeremiedb,2017-04-09 06:49:27,2017-04-11 15:48:36
PR,example Debug error caused by missplaced import find mxnet in googlenet,When test the googlenet symbol stuck into the error caused by missplaced import find mxnet which is already called in file train mnist py from common import find mxnet So I deleted the import line which can not be called,,Soonhwan-Kwon,2017-04-11 15:59:48,2017-04-11 16:35:55
PR,Fix initialization problem,,,"ZihengJiang,piiswrong,ZihengJiang,ZihengJiang,Ldpe2G,piiswrong",2017-04-10 22:09:13,2017-04-11 16:38:23
PR,Quick fix installation errors,Quick fix critical issues in installation instructions This is only a quick fix for damage control I am working on other PR with more detailed thorough changes to build from sources installation instructions Can we get this reviewed and merged ASAP,,"sandeep-krishnamurthy,nswamy,nswamy,nswamy,nswamy,madjam,sandeep-krishnamurthy",2017-04-11 16:34:13,2017-04-11 22:24:43
PR,refactor autograd,ZihengJiang,,"piiswrong,ZihengJiang,ZihengJiang,ZihengJiang,ZihengJiang",2017-04-10 16:06:08,2017-04-11 23:43:52
PR,Minpy,,,"hotpxl,hotpxl",2017-04-12 03:26:40,2017-04-12 03:26:50
PR,Changed from numpy to ndarray for Perplexity,no result change but potentially improves the performance,,mli,2017-04-12 00:19:54,2017-04-12 03:37:26
PR,Perf md update for AWS instances,mli Update the perf data on latest mxnet pls review and merge,,"zhenlinluo,mli",2017-04-12 00:59:29,2017-04-12 03:38:01
IS,ndarray split function not working as instruction,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux Compiler G Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 22482aecf99ed6e1bc82803845aa53dbddb835dc If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Traceback most recent call last File test py line 9 in module splits mx nd split a axis 1 num outputs 4 File mxnet path python mxnet ctypes ndarray py line 164 in generic ndarray function c array ctypes c char p c str val for val in vals File mxnet path python mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Invalid Parameter format for num outputs expect int but value ' NDArray 5x4x2x2 0 ' in operator SliceChannel name axis 1 num outputs NDArray 5x4x2x2 0 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error others 1 In the mx nd Concat function the C in upper case While the mx nd split the s is in lower case The naming should be unified,,"Godricly,Godricly,Godricly",2017-04-12 03:27:59,2017-04-12 05:15:47
PR,fix ndarray split,Fix,,"sxjscience,Godricly,sxjscience,Godricly,sxjscience",2017-04-12 04:13:35,2017-04-12 05:46:45
PR,Add 'argnum' for autograd,,,ZihengJiang,2017-04-12 00:40:47,2017-04-12 06:28:24
PR,Ndarray api docs,Changes to broadcast to broadcast axis linearregressionoutput logisticregressionoutput maeregressionoutput function documentation,,"Roshrini,nswamy,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,nswamy,nswamy,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,madjam,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,piiswrong,Roshrini,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,Roshrini,zackchase,zackchase,mli,mli,zackchase,Roshrini,zackchase,madjam,piiswrong,Roshrini,piiswrong",2017-04-05 22:14:52,2017-04-12 14:23:20
PR,Wait for ndarray to be readable in dist kvstore push,This fixes a crash when using kvstore imperatively in dist mode,,madjam,2017-04-12 07:07:27,2017-04-12 16:56:26
PR,Griddim,This is a follow up on PR 232 on mshadow and a prerequisite to PR 5726 on MxNet With this change the newest version of mshadow is brought into MxNet Though the changed value for kMaxGridNum would crash several operators as they had been using this value as the size of the grid in a single dimension Now there is kMaxGridDim in mshadow that specifies the max size in one dimension and all affected operators are updated to use this value instead of kMaxGridNum PR 5726 sampling operators depends on this change here as it has to bring in the latest mshadow,,asmushetzel,2017-04-12 09:43:22,2017-04-12 17:24:28
PR,Minor wording edits to the new op md documentation,,,KellenSunderland,2017-04-12 08:08:01,2017-04-12 17:26:37
PR,fix warning for test UserWarning Data provided by label shapes don,'t match names specified by label names vs 'cls prob label',,"ZiyueHuang,piiswrong,piiswrong,ZiyueHuang,ZiyueHuang,precedenceguo",2017-04-12 04:53:26,2017-04-12 17:27:07
PR,Fix mxnet ndarray arange documentation,mli Please review and merge,,"indhub,indhub",2017-04-12 20:31:56,2017-04-12 20:41:51
PR,New docs,Edited tutorials and how tos landing pages Removed the nonsense text and made the titles of pages more descriptive We should also edit the CSS to put a little more space 5px between bullets so that this is more readable beautiful,,"zackchase,piiswrong,kevinthesun",2017-04-12 20:41:48,2017-04-13 00:22:44
PR,Add R mirror,sandeep krishnamurthy I'm adding the R mirror into this MNIST script which I'm running as the basic testing for the R lang Dockerfile Without this change it does not know where to install from,,"lxn2,lxn2,thirdwing",2017-04-11 05:27:06,2017-04-13 00:27:02
PR,Copy edits on landing pages,Improved the style and fixed a few small mistakes on the landing pages for tutorials and how tos,,"zackchase,zackchase",2017-04-13 01:12:27,2017-04-13 01:36:23
IS,SSD example GPU memory leak,Environment info Operating System Ubuntu 14 04 Compiler gcc version 4 8 4 Package used Python R Scala Julia Python MXNet version 0 9 5 When running the SSD examples the GPU memory increases from the beginning to the end which looks like a GPU memory leak problem Did anybody get the same issue,,zhreshold,2017-04-11 12:47:24,2017-04-13 01:51:41
IS,some problem about implementing ftrl,ftrl steps are as followed in picture ftrl when update weight there exist sign judgement and i do not know how to assign weight through judging the sign of z small help will welcome,,"piiswrong,piiswrong,piiswrong",2017-02-04 12:10:44,2017-04-13 04:08:01
PR,Update footer html,Fixing copyright notice Was out of date 2015 2016,,zackchase,2017-04-13 04:36:43,2017-04-13 06:54:30
IS,How to use a constant Matrix as Input Data,Hi I'm trying to add a constant Matrix into my neual network This is part of my network design xipart net mx sym Variable name wouldataxi' xipart net mx sym FullyConnected data xipart net weight fc1 weight bias fc1 bias num hidden 64 name fc1 xipart net mx sym Activation xipart net name arelu1' act type relu xipart net mx sym Dropout data xipart net p 0 2 xipart net mx sym FullyConnected data xipart net weight fc2 weight bias fc2 bias num hidden k name fc2 dimension batch size k m mx sym Variable name 'M' shape 10 10 M k k xipart net mx symbol dot lhs xipart net rhs m dimension batchsize k But I cannot Input the M with the shape 10 10 I set the shape of M 10 10 but it told me infer shape error Arguments M 64 10 dataxi 64 784 dataxj 64 784 isinM 64 64 is my batchsize The Next is myDataIter and i hope how to use a constant Matrix as input Data Thank u class MyDataIter mx io DataIter def init self z label batch size super MyDataIter self init self data z self label label self batch size batch size self provide data wouldataxi' batch size 784 wouldataxj' batch size 784 'isinM' batch size 'M' 10 10 self provide label,,,2017-04-10 06:23:30,2017-04-13 07:23:45
IS,Dying BilinearSampler,Hi All I am trying to insert the BilinearSampler layer in my network However the network parameters stop updating after a few iterations The reason is that the output of the BilinearSampler become all zeros according to the document out boundary points will be padded as zeros after the sampler Is there any way to fix this Many thanks,,piiswrong,2017-04-11 12:06:07,2017-04-13 09:59:04
IS,callbacks destroy 'value' in BatchEndParam for subsequent callbacks of that type,It appears that calling one callback seem destroy the value in BatchEndParam Using callbacks like Results in the following log INFO root Epoch 0 Batch 10 Speed 60 70 samples sec Train accuracy 0 122869 INFO root Epoch 0 Batch 20 Speed 60 85 samples sec Train accuracy 0 157031 INFO root Epoch 0 Batch 20 Speed 61 00 samples sec Train accuracy nan INFO root Epoch 0 Batch 30 Speed 61 01 samples sec Train accuracy 0 164844 INFO root Epoch 0 Batch 40 Speed 61 20 samples sec Train accuracy 0 191406 INFO root Epoch 0 Batch 40 Speed 61 39 samples sec Train accuracy nan INFO root Epoch 0 Batch 50 Speed 60 78 samples sec Train accuracy 0 194531 INFO root Epoch 0 Batch 60 Speed 60 84 samples sec Train accuracy 0 221875 INFO root Epoch 0 Batch 60 Speed 60 90 samples sec Train accuracy nan,,"jmerkow,zihaolucky,jmerkow,zihaolucky",2017-03-28 00:39:53,2017-04-13 14:16:27
IS,How to get symbol by symbol clocking in a module including data iter,I am trying to obtain clocking information for each symbol especially data on a Module I built a simpleiter exactly like in the tutorial except for the following modification With my real network I would like to be able to see how much time my custom dataiter is costing me as compared to the other symbols layers in order to determine if I should spend time trying to speed up and then if I was actually successful in speeding it up,,"jmerkow,tornadomeet,jmerkow,tornadomeet,jmerkow,jmerkow",2017-03-05 00:19:15,2017-04-13 14:19:13
PR,Add brew osx search path for openblas,This allows cmake to find openblas for me on OS X El Capitan with the latest homebrew,,KellenSunderland,2017-04-13 08:13:02,2017-04-13 16:39:00
PR,added new distributions to sampling operators added operators for sa,mpling multiple distributions that are parametrized by input arguments tensors This pull request depends on PR 228 for mshadow I'm insecure how we proceed as this PR will fail as long as PR 228 has not yet been pulled This branch adds methods to sample from more often used distributions Exponential Gamma Poisson Negative Binomial All these sampling methods are currently only available for CPU They are available for ndarray and symbolic APIs Moreover a set of multi sample operators is available where distribution parameters are tensor inputs With this MxNet can be used to estimate parameters of a probability distribution by a neural network and then sample from this distribution and feed the sampling then into other parts of a neural network That is a critical functionality used for a lot of applications and similar to what TensorFlow offers Tests have been added to test random py for all these changes In the future we may think about extending and unifying the random samplers further Goal should be to have additional operators in there for example for likelihood And provide GPU versions for all distribution types,,"asmushetzel,piiswrong,asmushetzel,piiswrong,piiswrong,asmushetzel,asmushetzel,piiswrong,asmushetzel,sxjscience,asmushetzel,piiswrong,piiswrong,sxjscience,asmushetzel,asmushetzel,loofahcus,asmushetzel,loofahcus",2017-04-07 13:12:50,2017-04-13 17:03:18
PR,Fix documentation for mxnet ndarray arange,mli Could you please review and merge,,"indhub,sxjscience",2017-04-12 21:13:47,2017-04-13 17:44:49
IS,Inconsistent versions of dmlc core are used by mxnet and nnvm separately,Both mxnet and nnvm require dmlc core as a dependency However two different versions dmlc core 3dfbc62 for mxnet and dmlc core 3a51614 for nnvm are introduced introduce mxnet is soruce tree via submodules According to commit comparing differences between them are non trival Could you check it,,"weijianwen,piiswrong,weijianwen",2017-04-13 13:04:19,2017-04-14 02:25:06
PR,index clipping for embedding testing,,,"ap-hynninen,piiswrong,ap-hynninen",2017-04-13 19:19:36,2017-04-14 05:54:28
IS,How to get the outputs of hidden layers when using Module for training,I need to get the output of a hidden layer during inference I use Module for training After the training is done I can use to get the output of the entire network But I also want to inspect the output of a hidden layer Is there any way to access that using Module is methods,,,2017-04-13 01:27:52,2017-04-14 05:58:31
IS,How to set int8 or float16 to predict,I downloaded pretrained model from model zoo However MXNet will load model as float32 format defaultly I want to make inference as float16 or int8 format I think there are two ways to accomplish 1 transform float32 model into float16 int8 2 compute as float16 int8 format on the whole prediction Does MXNet have some example codes which have accomplished one or two way s above Thanks a lot,,"ysh329,Godricly,ysh329",2017-04-13 08:49:04,2017-04-14 06:59:38
PR,Head color and list padding,Change color to slategray untitled111,,"kevinthesun,zackchase,zackchase",2017-04-13 18:31:37,2017-04-14 18:07:38
PR,Fix docs for some NDArray functions,Modified docs for functions sort argsort argmin argmax argmax channel sequenceLast SequenceMask SequenceReverse true divide take batch take Also fixed minor formatting issue of broadcast operators from previous commit 5703 Please take a look,,"Roshrini,zackchase,Roshrini",2017-04-12 23:47:08,2017-04-14 23:16:11
IS,rcnn multi gpu problem,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu16 04 Compiler gcc5 4 Package used Python R Scala Julia python MXNet version v0 9 1 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 6 If you are using R package please provide R sessionInfo Error Message When I use one 1080 gpu to train the speed is 9 sanples s when I use two 1080 the speed has improved a little but three 1080 has the same speed with two 1080 when I use four 1080 the speed even decrease I want to know how to solve it thank you below is the information printed one 1080 Called with argument Namespace begin epoch 0 dataset 'PascalVOC' dataset path wouldata VOCdevkit' end epoch 10 frequent 20 gpus '0' image set '2007 trainval' kvstore wouldevice' lr 0 001 lr step '7' network 'vgg' no flip False no shuffle False prefix 'model e2e' pretrained 'model vgg16' pretrained epoch 0 resume False root path wouldata' work load list None 'ANCHOR RATIOS' 0 5 1 2 'ANCHOR SCALES' 8 16 32 'FIXED PARAMS' 'conv1' 'conv2' 'FIXED PARAMS SHARED' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' 'IMAGE STRIDE' 0 'NUM ANCHORS' 9 'NUM CLASSES' 37 'PIXEL MEANS' array 103 939 116 779 123 68 'RCNN FEAT STRIDE' 16 'RPN FEAT STRIDE' 16 'SCALES' 300 2000 'TEST' 'BATCH IMAGES' 1 'CXX PROPOSAL' True 'HAS RPN' False 'NMS' 0 3 'PROPOSAL MIN SIZE' 16 'PROPOSAL NMS THRESH' 0 7 'PROPOSAL POST NMS TOP N' 2000 'PROPOSAL PRE NMS TOP N' 20000 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'ASPECT GROUPING' True 'BATCH IMAGES' 1 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' True 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'CXX PROPOSAL' True 'END2END' True 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 7 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 2000 'RPN PRE NMS TOP N' 12000 image sets '2007 trainval' root path data dataset path data VOCdevkit args dataset PascalVOC num images 213 voc 2007 trainval gt roidb loaded from data cache voc 2007 trainval gt roidb pkl append flipped images to roidb filtered 0 roidb entries 426 426 providing maximum shape wouldata' 1 3 300 2000 'gt boxes' 1 100 5 'label' 1 20250 'bbox target' 1 36 18 125 'bbox weight' 1 36 18 125 output shape 'bbox loss reshape output' 1L 128L 148L 'blockgrad0 output' 1L 128L 'cls prob reshape output' 1L 128L 37L 'rpn bbox loss output' 1L 36L 18L 35L 'rpn cls prob output' 1L 2L 162L 35L lr 0 001 lr epoch diff 7 lr iters 2982 INFO root Epoch 0 Batch 20 Speed 10 00 samples sec Train RPNAcc 0 890625 RPNLogLoss 0 287063 RPNL1Loss 0 526779 RCNNAcc 0 706845 RCNNLogLoss 1 481539 RCNNL1Loss 2 049723 INFO root Epoch 0 Batch 40 Speed 9 97 samples sec Train RPNAcc 0 935785 RPNLogLoss 0 186718 RPNL1Loss 0 513707 RCNNAcc 0 728849 RCNNLogLoss 1 222760 RCNNL1Loss 1 999619 INFO root Epoch 0 Batch 60 Speed 9 90 samples sec Train RPNAcc 0 954982 RPNLogLoss 0 137305 RPNL1Loss 0 442200 RCNNAcc 0 743212 RCNNLogLoss 1 071653 RCNNL1Loss 1 980510 INFO root Epoch 0 Batch 80 Speed 9 94 samples sec Train RPNAcc 0 964651 RPNLogLoss 0 109225 RPNL1Loss 0 389600 RCNNAcc 0 765529 RCNNLogLoss 0 937045 RCNNL1Loss 1 937965 INFO root Epoch 0 Batch 100 Speed 9 93 samples sec Train RPNAcc 0 970838 RPNLogLoss 0 091041 RPNL1Loss 0 360943 RCNNAcc 0 782178 RCNNLogLoss 0 841284 RCNNL1Loss 1 902046 INFO root Epoch 0 Batch 120 Speed 9 94 samples sec Train RPNAcc 0 974851 RPNLogLoss 0 079258 RPNL1Loss 0 334735 RCNNAcc 0 796746 RCNNLogLoss 0 762674 RCNNL1Loss 1 857773 INFO root Epoch 0 Batch 140 Speed 9 97 samples sec Train RPNAcc 0 977920 RPNLogLoss 0 070084 RPNL1Loss 0 320325 RCNNAcc 0 807070 RCNNLogLoss 0 706777 RCNNL1Loss 1 820371 INFO root Epoch 0 Batch 160 Speed 10 01 samples sec Train RPNAcc 0 980396 RPNLogLoss 0 062868 RPNL1Loss 0 308339 RCNNAcc 0 817158 RCNNLogLoss 0 660157 RCNNL1Loss 1 780312 INFO root Epoch 0 Batch 180 Speed 10 03 samples sec Train RPNAcc 0 982217 RPNLogLoss 0 057189 RPNL1Loss 0 294265 RCNNAcc 0 828039 RCNNLogLoss 0 615535 RCNNL1Loss 1 732486 INFO root Epoch 0 Batch 200 Speed 9 99 samples sec Train RPNAcc 0 983675 RPNLogLoss 0 052811 RPNL1Loss 0 278643 RCNNAcc 0 836715 RCNNLogLoss 0 579340 RCNNL1Loss 1 681014 INFO root Epoch 0 Batch 220 Speed 9 94 samples sec Train RPNAcc 0 984640 RPNLogLoss 0 050201 RPNL1Loss 0 264532 RCNNAcc 0 843785 RCNNLogLoss 0 548730 RCNNL1Loss 1 633998 INFO root Epoch 0 Batch 240 Speed 9 93 samples sec Train RPNAcc 0 985672 RPNLogLoss 0 047016 RPNL1Loss 0 251615 RCNNAcc 0 849585 RCNNLogLoss 0 523885 RCNNL1Loss 1 594296 INFO root Epoch 0 Batch 260 Speed 9 94 samples sec Train RPNAcc 0 986515 RPNLogLoss 0 044285 RPNL1Loss 0 240738 RCNNAcc 0 854795 RCNNLogLoss 0 501578 RCNNL1Loss 1 551764 INFO root Epoch 0 Batch 280 Speed 10 09 samples sec Train RPNAcc 0 987364 RPNLogLoss 0 041663 RPNL1Loss 0 231286 RCNNAcc 0 859625 RCNNLogLoss 0 481967 RCNNL1Loss 1 516398 INFO root Epoch 0 Batch 300 Speed 9 93 samples sec Train RPNAcc 0 988035 RPNLogLoss 0 039375 RPNL1Loss 0 223666 RCNNAcc 0 863450 RCNNLogLoss 0 465354 RCNNL1Loss 1 482711 INFO root Epoch 0 Batch 320 Speed 9 82 samples sec Train RPNAcc 0 988683 RPNLogLoss 0 037289 RPNL1Loss 0 217470 RCNNAcc 0 867090 RCNNLogLoss 0 449521 RCNNL1Loss 1 447850 INFO root Epoch 0 Batch 340 Speed 10 02 samples sec Train RPNAcc 0 989209 RPNLogLoss 0 035550 RPNL1Loss 0 211590 RCNNAcc 0 869341 RCNNLogLoss 0 437771 RCNNL1Loss 1 420329 INFO root Epoch 0 Batch 360 Speed 9 99 samples sec Train RPNAcc 0 989363 RPNLogLoss 0 034730 RPNL1Loss 0 206660 RCNNAcc 0 871992 RCNNLogLoss 0 425827 RCNNL1Loss 1 396775 INFO root Epoch 0 Batch 380 Speed 10 06 samples sec Train RPNAcc 0 989870 RPNLogLoss 0 033272 RPNL1Loss 0 202089 RCNNAcc 0 874323 RCNNLogLoss 0 415736 RCNNL1Loss 1 373278 INFO root Epoch 0 Batch 400 Speed 9 97 samples sec Train RPNAcc 0 990268 RPNLogLoss 0 031959 RPNL1Loss 0 197227 RCNNAcc 0 876734 RCNNLogLoss 0 404630 RCNNL1Loss 1 349587 INFO root Epoch 0 Batch 420 Speed 9 81 samples sec Train RPNAcc 0 990554 RPNLogLoss 0 030994 RPNL1Loss 0 192591 RCNNAcc 0 879045 RCNNLogLoss 0 394721 RCNNL1Loss 1 328278 INFO root Epoch 0 Train RPNAcc 0 990665 INFO root Epoch 0 Train RPNLogLoss 0 030673 INFO root Epoch 0 Train RPNL1Loss 0 191121 INFO root Epoch 0 Train RCNNAcc 0 879511 INFO root Epoch 0 Train RCNNLogLoss 0 392670 INFO root Epoch 0 Train RCNNL1Loss 1 321912 INFO root Epoch 0 Time cost 43 541 INFO root Saved checkpoint to model e2e 0001 params four 1080 Called with argument Namespace begin epoch 0 dataset 'PascalVOC' dataset path wouldata VOCdevkit' end epoch 10 frequent 20 gpus '0 1 2 3' image set '2007 trainval' kvstore wouldevice' lr 0 001 lr step '7' network 'vgg' no flip False no shuffle False prefix 'model e2e' pretrained 'model vgg16' pretrained epoch 0 resume False root path wouldata' work load list None 'ANCHOR RATIOS' 0 5 1 2 'ANCHOR SCALES' 8 16 32 'FIXED PARAMS' 'conv1' 'conv2' 'FIXED PARAMS SHARED' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' 'IMAGE STRIDE' 0 'NUM ANCHORS' 9 'NUM CLASSES' 37 'PIXEL MEANS' array 103 939 116 779 123 68 'RCNN FEAT STRIDE' 16 'RPN FEAT STRIDE' 16 'SCALES' 300 2000 'TEST' 'BATCH IMAGES' 1 'CXX PROPOSAL' True 'HAS RPN' False 'NMS' 0 3 'PROPOSAL MIN SIZE' 16 'PROPOSAL NMS THRESH' 0 7 'PROPOSAL POST NMS TOP N' 2000 'PROPOSAL PRE NMS TOP N' 20000 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'ASPECT GROUPING' True 'BATCH IMAGES' 1 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' True 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'CXX PROPOSAL' True 'END2END' True 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 7 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 2000 'RPN PRE NMS TOP N' 12000 flag 1 image sets '2007 trainval' root path data dataset path data VOCdevkit args dataset PascalVOC num images 213 voc 2007 trainval gt roidb loaded from data cache voc 2007 trainval gt roidb pkl append flipped images to roidb filtered 0 roidb entries 426 426 loading training data flag 2 providing maximum shape wouldata' 4 3 300 2000 'gt boxes' 4 100 5 'label' 4 20250 'bbox target' 4 36 18 125 'bbox weight' 4 36 18 125 output shape 'bbox loss reshape output' 1L 128L 148L 'blockgrad0 output' 1L 128L 'cls prob reshape output' 1L 128L 37L 'rpn bbox loss output' 4L 36L 18L 36L 'rpn cls prob output' 4L 2L 162L 36L lr 0 001 lr epoch diff 7 lr iters 745 15 36 44 src kvstore comm h 304 only 4 out of 12 GPU pairs are enabled direct access It may affect the performance You can set MXNET ENABLE GPU P2P 0 to turn it off 15 36 44 src kvstore comm h 313 v 15 36 44 src kvstore comm h 313 v 15 36 44 src kvstore comm h 313 v 15 36 44 src kvstore comm h 313 v INFO root Epoch 0 Batch 20 Speed 7 02 samples sec Train RPNAcc 0 889044 RPNLogLoss 0 313767 RPNL1Loss 0 445389 RCNNAcc 0 705171 RCNNLogLoss 1 501360 RCNNL1Loss 1 997102 INFO root Epoch 0 Batch 40 Speed 7 06 samples sec Train RPNAcc 0 930474 RPNLogLoss 0 205314 RPNL1Loss 0 347862 RCNNAcc 0 728706 RCNNLogLoss 1 226386 RCNNL1Loss 1 921626 INFO root Epoch 0 Batch 60 Speed 6 89 samples sec Train RPNAcc 0 950980 RPNLogLoss 0 150189 RPNL1Loss 0 322292 RCNNAcc 0 747182 RCNNLogLoss 1 053173 RCNNL1Loss 1 912972 INFO root Epoch 0 Batch 80 Speed 7 05 samples sec Train RPNAcc 0 961938 RPNLogLoss 0 117977 RPNL1Loss 0 309947 RCNNAcc 0 770882 RCNNLogLoss 0 912873 RCNNL1Loss 1 879763 INFO root Epoch 0 Batch 100 Speed 6 78 samples sec Train RPNAcc 0 968634 RPNLogLoss 0 097867 RPNL1Loss 0 294203 RCNNAcc 0 791480 RCNNLogLoss 0 807440 RCNNL1Loss 1 824472 INFO root Epoch 0 Train RPNAcc 0 970031 INFO root Epoch 0 Train RPNLogLoss 0 093694 INFO root Epoch 0 Train RPNL1Loss 0 291345 INFO root Epoch 0 Train RCNNAcc 0 796414 INFO root Epoch 0 Train RCNNLogLoss 0 784517 INFO root Epoch 0 Train RCNNL1Loss 1 812663 INFO root Epoch 0 Time cost 63 529 INFO root Saved checkpoint to model e2e 0001 params Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"eric-haibin-lin,eric-haibin-lin",2017-04-06 13:57:21,2017-04-15 05:57:29
PR,Add AWS Seoul Summit Demos to example folder,hunkim and I will give a brief introduction of MXNet in AWS Seoul Summit 2017 April 20th The link points to the demos we will use in the presentation,,sxjscience,2017-04-15 06:20:42,2017-04-15 09:55:01
IS,test symbol py batch take fails on master branch,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Deep Learning AMI Ubuntu Linux 1 1 ami 53b23433 MXNet version 42102b5f2e13f19bd650eea0cf492ca9d601612b If you are using python package please provide Python version and distribution Python 2 7 6 Error Message Please paste the full error message including stack trace,,"eric-haibin-lin,eric-haibin-lin",2017-04-15 19:33:08,2017-04-15 20:08:30
PR,R fix the message and unload error on jupyter notebook,,,thirdwing,2017-04-15 20:03:00,2017-04-15 20:53:33
PR,R fix image classification examples close 5080,,,thirdwing,2017-04-15 20:57:08,2017-04-15 22:43:05
IS,R image classification examples are broken,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,thirdwing,2017-02-20 22:32:36,2017-04-15 22:43:12
IS,Linker error when building mlp cpp with VS2015,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 64 bit Compiler VS2015 C Package used Python R Scala Julia MXNet version Latest nightly Windows build 20170221 CUDA 8 0 61 cudnn 8 0v5 1 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace mlp obj error LNK2001 unresolved external symbol imp MXOptimizerFree mlp obj error LNK2001 unresolved external symbol imp MXOptimizerFindCreator mlp obj error LNK2001 unresolved external symbol imp MXOptimizerUpdate error LNK2001 unresolved external symbol imp MXOptimizerCreateOptimizer Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Just compiled the mlp cpp sample in a C VS2015 solution This worked in an earlier install from MXNet but based on a version from MxNet cpp with cudnnv3 1 CUDA 7 5 and VS2013 entire MXNet version in a separate folder structure 2 3 What have you tried to solve it 1 Tried to find where the symbols that cannot be linked are defined Ca not find those in the source code on Github 2 Googled for a solution 3,,,2017-02-21 17:25:45,2017-04-16 02:22:39
IS,Save both checkpoints and log in R,Hi Im trying to store both a checkpoint for each iteration as well as the log of the performance on the validation set I have built a simple R function but it doesnt seem to work Any hints on how to accomplish this EpochEnd function period log logger log prefix model period model mx callback save checkpoint prefix model period model mx callback log train metric period log logger log library mxnet data BostonHousing package mlbench train ind seq 1 506 3 train x data matrix BostonHousing train ind 14 train y BostonHousing train ind 14 test x data matrix BostonHousing train ind 14 test y BostonHousing train ind 14 data mx symbol Variable data fc1 mx symbol FullyConnected data num hidden 1 lro mx symbol LinearRegressionOutput fc1 logger mx metric logger new model mx model FeedForward create lro X train x y train y eval data list data test x label test y array layout rowmajor ctx mx cpu num round 5 array batch size 20 learning rate 2e 6 momentum 0 9 eval metric mx metric rmse epoch end callback EpochEnd 1 logger boston 1 Somehow the log is saved but not the checkpoints,,thirdwing,2017-04-03 18:22:09,2017-04-16 02:32:48
PR,R fix docs close 5683,,,thirdwing,2017-04-16 02:06:19,2017-04-16 02:57:28
IS,fail to draw a DAG when I follow the official R doc,when I follow the official doc I fail to draw the DAG using these code,,"harryprince,jeremiedb,harryprince",2017-04-04 15:58:00,2017-04-16 02:57:35
IS,how to get node id in model py,Can we get the node id in model py through any module of python Uploading QQ 20170416155655 png,,,2017-04-16 08:00:17,2017-04-16 08:58:42
PR,fix typo in image py,,,abuccts,2017-04-16 10:27:25,2017-04-17 00:24:05
PR,Fix type warning when dev 1,,,ZihengJiang,2017-04-16 08:20:04,2017-04-17 00:24:56
PR,Scala refactor EvalMetric add Perplexity and CompositeEvalMetric,,,"Ldpe2G,Ldpe2G,yzhliu,Ldpe2G",2017-04-13 09:39:18,2017-04-17 02:04:29
PR,Debug data shape related error of toy ctc example,Solved related issue 5435 which happened because data shape of symbol seq len 80 and real data shape seq len 800 does not met I modified data iter is provide data and gen rand to support lstm unroll is symbol seq len 80 and confirmed it works well Below is the training log 2017 04 15 16 27 06 025 Start training with gpu 0 2017 04 15 16 28 11 823 Epoch 0 Batch 50 Speed 1257 73 samples sec Train Accuracy 0 000000 2017 04 15 16 28 13 122 Epoch 0 Batch 100 Speed 1231 47 samples sec Train Accuracy 0 000000 2017 04 15 16 28 14 348 Epoch 0 Batch 150 Speed 1305 72 samples sec Train Accuracy 0 000000 2017 04 15 16 28 15 595 Epoch 0 Batch 200 Speed 1283 31 samples sec Train Accuracy 0 000000 2017 04 15 16 28 16 912 Epoch 0 Batch 250 Speed 1215 02 samples sec Train Accuracy 0 000000 2017 04 15 16 28 18 191 Epoch 0 Batch 300 Speed 1251 47 samples sec Train Accuracy 0 000000 2017 04 15 16 28 19 430 Epoch 0 Batch 350 Speed 1291 18 samples sec Train Accuracy 0 000000 2017 04 15 16 28 20 736 Epoch 0 Batch 400 Speed 1225 21 samples sec Train Accuracy 0 000000 2017 04 15 16 28 22 045 Epoch 0 Batch 450 Speed 1222 62 samples sec Train Accuracy 0 000000 2017 04 15 16 28 23 342 Epoch 0 Batch 500 Speed 1233 66 samples sec Train Accuracy 0 000000 2017 04 15 16 28 24 615 Epoch 0 Batch 550 Speed 1257 35 samples sec Train Accuracy 0 000000 2017 04 15 16 28 25 893 Epoch 0 Batch 600 Speed 1252 21 samples sec Train Accuracy 0 000000 2017 04 15 16 28 27 153 Epoch 0 Batch 650 Speed 1269 36 samples sec Train Accuracy 0 000000 2017 04 15 16 28 28 392 Epoch 0 Batch 700 Speed 1291 39 samples sec Train Accuracy 0 000000 2017 04 15 16 28 29 692 Epoch 0 Batch 750 Speed 1231 25 samples sec Train Accuracy 0 000000 2017 04 15 16 28 30 935 Epoch 0 Batch 800 Speed 1287 14 samples sec Train Accuracy 0 000000 2017 04 15 16 28 32 168 Epoch 0 Batch 850 Speed 1297 67 samples sec Train Accuracy 0 000000 2017 04 15 16 28 33 460 Epoch 0 Batch 900 Speed 1239 32 samples sec Train Accuracy 0 000000 2017 04 15 16 28 34 744 Epoch 0 Batch 950 Speed 1246 30 samples sec Train Accuracy 0 000000 2017 04 15 16 28 36 027 Epoch 0 Batch 1000 Speed 1246 97 samples sec Train Accuracy 0 000000 2017 04 15 16 28 37 325 Epoch 0 Batch 1050 Speed 1233 15 samples sec Train Accuracy 0 000000 2017 04 15 16 28 38 567 Epoch 0 Batch 1100 Speed 1287 80 samples sec Train Accuracy 0 000000 2017 04 15 16 28 39 782 Epoch 0 Batch 1150 Speed 1317 17 samples sec Train Accuracy 0 000000 2017 04 15 16 28 40 994 Epoch 0 Batch 1200 Speed 1320 13 samples sec Train Accuracy 0 000000 2017 04 15 16 28 42 213 Epoch 0 Batch 1250 Speed 1313 24 samples sec Train Accuracy 0 000000 2017 04 15 16 28 43 451 Epoch 0 Batch 1300 Speed 1292 53 samples sec Train Accuracy 0 000000 2017 04 15 16 28 44 686 Epoch 0 Batch 1350 Speed 1295 47 samples sec Train Accuracy 0 000000 2017 04 15 16 28 46 028 Epoch 0 Batch 1400 Speed 1192 42 samples sec Train Accuracy 0 000000 2017 04 15 16 28 47 366 Epoch 0 Batch 1450 Speed 1196 41 samples sec Train Accuracy 0 000000 2017 04 15 16 28 48 595 Epoch 0 Batch 1500 Speed 1301 52 samples sec Train Accuracy 0 000000 2017 04 15 16 28 49 823 Epoch 0 Batch 1550 Speed 1303 15 samples sec Train Accuracy 0 000000 2017 04 15 16 28 51 119 Epoch 0 Batch 1600 Speed 1234 43 samples sec Train Accuracy 0 000000 2017 04 15 16 28 52 353 Epoch 0 Batch 1650 Speed 1297 16 samples sec Train Accuracy 0 000000 2017 04 15 16 28 53 619 Epoch 0 Batch 1700 Speed 1264 41 samples sec Train Accuracy 0 000000 2017 04 15 16 28 54 920 Epoch 0 Batch 1750 Speed 1229 41 samples sec Train Accuracy 0 000000 2017 04 15 16 28 56 172 Epoch 0 Batch 1800 Speed 1278 26 samples sec Train Accuracy 0 000000 2017 04 15 16 28 57 424 Epoch 0 Batch 1850 Speed 1278 35 samples sec Train Accuracy 0 000000 2017 04 15 16 28 58 635 Epoch 0 Batch 1900 Speed 1321 37 samples sec Train Accuracy 0 000000 2017 04 15 16 28 59 862 Epoch 0 Batch 1950 Speed 1304 36 samples sec Train Accuracy 0 000000 2017 04 15 16 29 01 146 Epoch 0 Batch 2000 Speed 1245 59 samples sec Train Accuracy 0 000000 2017 04 15 16 29 02 449 Epoch 0 Batch 2050 Speed 1228 02 samples sec Train Accuracy 0 000000 2017 04 15 16 29 03 660 Epoch 0 Batch 2100 Speed 1322 37 samples sec Train Accuracy 0 000000 2017 04 15 16 29 04 930 Epoch 0 Batch 2150 Speed 1259 39 samples sec Train Accuracy 0 000000 2017 04 15 16 29 06 205 Epoch 0 Batch 2200 Speed 1255 53 samples sec Train Accuracy 0 000000 2017 04 15 16 29 07 429 Epoch 0 Batch 2250 Speed 1306 73 samples sec Train Accuracy 0 000000 2017 04 15 16 29 08 753 Epoch 0 Batch 2300 Speed 1208 87 samples sec Train Accuracy 0 000000 2017 04 15 16 29 09 988 Epoch 0 Batch 2350 Speed 1295 36 samples sec Train Accuracy 0 000000 2017 04 15 16 29 11 220 Epoch 0 Batch 2400 Speed 1299 24 samples sec Train Accuracy 0 017500 2017 04 15 16 29 12 540 Epoch 0 Batch 2450 Speed 1212 11 samples sec Train Accuracy 0 103750 2017 04 15 16 29 13 785 Epoch 0 Batch 2500 Speed 1286 16 samples sec Train Accuracy 0 468750 2017 04 15 16 29 15 019 Epoch 0 Batch 2550 Speed 1296 61 samples sec Train Accuracy 0 635625 2017 04 15 16 29 16 245 Epoch 0 Batch 2600 Speed 1304 37 samples sec Train Accuracy 0 824375 2017 04 15 16 29 17 482 Epoch 0 Batch 2650 Speed 1294 61 samples sec Train Accuracy 0 968125 2017 04 15 16 29 18 708 Epoch 0 Batch 2700 Speed 1305 09 samples sec Train Accuracy 0 998750 2017 04 15 16 29 19 963 Epoch 0 Batch 2750 Speed 1274 84 samples sec Train Accuracy 0 999375 2017 04 15 16 29 21 220 Epoch 0 Batch 2800 Speed 1272 97 samples sec Train Accuracy 0 998125 2017 04 15 16 29 22 493 Epoch 0 Batch 2850 Speed 1257 39 samples sec Train Accuracy 1 000000,,Soonhwan-Kwon,2017-04-15 16:39:02,2017-04-17 04:37:24
PR,refactor executor dedup logics,replace custom var dedup logics with a common one defined in engine h,,"eric-haibin-lin,piiswrong,piiswrong,piiswrong,eric-haibin-lin,piiswrong",2017-04-16 02:17:17,2017-04-17 05:24:30
PR,BatchNorm Document CUDNN BN MIN EPSILON,When using an eps value CUDNN BN MIN EPSILON cudnn will fail with CUDNN STATUS BAD PARAM,,"leezu,sxjscience,sxjscience,leezu,leezu,leezu,piiswrong,sxjscience,leezu",2017-04-13 10:09:17,2017-04-17 07:56:56
PR,Solved toy ctc example error by debugging input data shape,Solved related issue 5435 which happened because data shape of symbol seq len 80 and real data shape seq len 800 does not met I modified data iter is provide data and gen rand to support lstm unroll is symbol seq len 80 and confirmed it works well Below is the training log 2017 04 15 16 27 06 025 Start training with gpu 0 2017 04 15 16 28 11 823 Epoch 0 Batch 50 Speed 1257 73 samples sec Train Accuracy 0 000000 2017 04 15 16 28 13 122 Epoch 0 Batch 100 Speed 1231 47 samples sec Train Accuracy 0 000000 2017 04 15 16 28 14 348 Epoch 0 Batch 150 Speed 1305 72 samples sec Train Accuracy 0 000000 2017 04 15 16 28 15 595 Epoch 0 Batch 200 Speed 1283 31 samples sec Train Accuracy 0 000000 2017 04 15 16 28 16 912 Epoch 0 Batch 250 Speed 1215 02 samples sec Train Accuracy 0 000000 2017 04 15 16 28 18 191 Epoch 0 Batch 300 Speed 1251 47 samples sec Train Accuracy 0 000000 2017 04 15 16 28 19 430 Epoch 0 Batch 350 Speed 1291 18 samples sec Train Accuracy 0 000000 2017 04 15 16 28 20 736 Epoch 0 Batch 400 Speed 1225 21 samples sec Train Accuracy 0 000000 2017 04 15 16 28 22 045 Epoch 0 Batch 450 Speed 1222 62 samples sec Train Accuracy 0 000000 2017 04 15 16 28 23 342 Epoch 0 Batch 500 Speed 1233 66 samples sec Train Accuracy 0 000000 2017 04 15 16 28 24 615 Epoch 0 Batch 550 Speed 1257 35 samples sec Train Accuracy 0 000000 2017 04 15 16 28 25 893 Epoch 0 Batch 600 Speed 1252 21 samples sec Train Accuracy 0 000000 2017 04 15 16 28 27 153 Epoch 0 Batch 650 Speed 1269 36 samples sec Train Accuracy 0 000000 2017 04 15 16 28 28 392 Epoch 0 Batch 700 Speed 1291 39 samples sec Train Accuracy 0 000000 2017 04 15 16 28 29 692 Epoch 0 Batch 750 Speed 1231 25 samples sec Train Accuracy 0 000000 2017 04 15 16 28 30 935 Epoch 0 Batch 800 Speed 1287 14 samples sec Train Accuracy 0 000000 2017 04 15 16 28 32 168 Epoch 0 Batch 850 Speed 1297 67 samples sec Train Accuracy 0 000000 2017 04 15 16 28 33 460 Epoch 0 Batch 900 Speed 1239 32 samples sec Train Accuracy 0 000000 2017 04 15 16 28 34 744 Epoch 0 Batch 950 Speed 1246 30 samples sec Train Accuracy 0 000000 2017 04 15 16 28 36 027 Epoch 0 Batch 1000 Speed 1246 97 samples sec Train Accuracy 0 000000 2017 04 15 16 28 37 325 Epoch 0 Batch 1050 Speed 1233 15 samples sec Train Accuracy 0 000000 2017 04 15 16 28 38 567 Epoch 0 Batch 1100 Speed 1287 80 samples sec Train Accuracy 0 000000 2017 04 15 16 28 39 782 Epoch 0 Batch 1150 Speed 1317 17 samples sec Train Accuracy 0 000000 2017 04 15 16 28 40 994 Epoch 0 Batch 1200 Speed 1320 13 samples sec Train Accuracy 0 000000 2017 04 15 16 28 42 213 Epoch 0 Batch 1250 Speed 1313 24 samples sec Train Accuracy 0 000000 2017 04 15 16 28 43 451 Epoch 0 Batch 1300 Speed 1292 53 samples sec Train Accuracy 0 000000 2017 04 15 16 28 44 686 Epoch 0 Batch 1350 Speed 1295 47 samples sec Train Accuracy 0 000000 2017 04 15 16 28 46 028 Epoch 0 Batch 1400 Speed 1192 42 samples sec Train Accuracy 0 000000 2017 04 15 16 28 47 366 Epoch 0 Batch 1450 Speed 1196 41 samples sec Train Accuracy 0 000000 2017 04 15 16 28 48 595 Epoch 0 Batch 1500 Speed 1301 52 samples sec Train Accuracy 0 000000 2017 04 15 16 28 49 823 Epoch 0 Batch 1550 Speed 1303 15 samples sec Train Accuracy 0 000000 2017 04 15 16 28 51 119 Epoch 0 Batch 1600 Speed 1234 43 samples sec Train Accuracy 0 000000 2017 04 15 16 28 52 353 Epoch 0 Batch 1650 Speed 1297 16 samples sec Train Accuracy 0 000000 2017 04 15 16 28 53 619 Epoch 0 Batch 1700 Speed 1264 41 samples sec Train Accuracy 0 000000 2017 04 15 16 28 54 920 Epoch 0 Batch 1750 Speed 1229 41 samples sec Train Accuracy 0 000000 2017 04 15 16 28 56 172 Epoch 0 Batch 1800 Speed 1278 26 samples sec Train Accuracy 0 000000 2017 04 15 16 28 57 424 Epoch 0 Batch 1850 Speed 1278 35 samples sec Train Accuracy 0 000000 2017 04 15 16 28 58 635 Epoch 0 Batch 1900 Speed 1321 37 samples sec Train Accuracy 0 000000 2017 04 15 16 28 59 862 Epoch 0 Batch 1950 Speed 1304 36 samples sec Train Accuracy 0 000000 2017 04 15 16 29 01 146 Epoch 0 Batch 2000 Speed 1245 59 samples sec Train Accuracy 0 000000 2017 04 15 16 29 02 449 Epoch 0 Batch 2050 Speed 1228 02 samples sec Train Accuracy 0 000000 2017 04 15 16 29 03 660 Epoch 0 Batch 2100 Speed 1322 37 samples sec Train Accuracy 0 000000 2017 04 15 16 29 04 930 Epoch 0 Batch 2150 Speed 1259 39 samples sec Train Accuracy 0 000000 2017 04 15 16 29 06 205 Epoch 0 Batch 2200 Speed 1255 53 samples sec Train Accuracy 0 000000 2017 04 15 16 29 07 429 Epoch 0 Batch 2250 Speed 1306 73 samples sec Train Accuracy 0 000000 2017 04 15 16 29 08 753 Epoch 0 Batch 2300 Speed 1208 87 samples sec Train Accuracy 0 000000 2017 04 15 16 29 09 988 Epoch 0 Batch 2350 Speed 1295 36 samples sec Train Accuracy 0 000000 2017 04 15 16 29 11 220 Epoch 0 Batch 2400 Speed 1299 24 samples sec Train Accuracy 0 017500 2017 04 15 16 29 12 540 Epoch 0 Batch 2450 Speed 1212 11 samples sec Train Accuracy 0 103750 2017 04 15 16 29 13 785 Epoch 0 Batch 2500 Speed 1286 16 samples sec Train Accuracy 0 468750 2017 04 15 16 29 15 019 Epoch 0 Batch 2550 Speed 1296 61 samples sec Train Accuracy 0 635625 2017 04 15 16 29 16 245 Epoch 0 Batch 2600 Speed 1304 37 samples sec Train Accuracy 0 824375 2017 04 15 16 29 17 482 Epoch 0 Batch 2650 Speed 1294 61 samples sec Train Accuracy 0 968125 2017 04 15 16 29 18 708 Epoch 0 Batch 2700 Speed 1305 09 samples sec Train Accuracy 0 998750 2017 04 15 16 29 19 963 Epoch 0 Batch 2750 Speed 1274 84 samples sec Train Accuracy 0 999375 2017 04 15 16 29 21 220 Epoch 0 Batch 2800 Speed 1272 97 samples sec Train Accuracy 0 998125 2017 04 15 16 29 22 493 Epoch 0 Batch 2850 Speed 1257 39 samples sec Train Accuracy 1 000000,,Soonhwan-Kwon,2017-04-17 05:01:22,2017-04-17 16:33:30
IS,MNIST training example not exiting on CPU instances after training completion,Hi Am running MNIST training on 2 CPU instances using MXNET with 10 epochs Default script provided However even after the training is all done I could see that the execution reaches the end of train mnist py script execution is not exiting Can anyone help me with this issue Environment info Operating System Amazon Linux Package used Python R Scala Julia Python MXNet Installed from sources MXNet commit hash git rev parse HEAD 266e439d366f8d3022f52c700a77229811d4cd77 Python version and distribution Python 2 7 12 Steps to reproduce 1 cd HOME mxnet examples image classification 2 tools launch py n 1 H hosts python train mnist py hosts file consists of the worker hostnames 2 workers in my case as follows deeplearning worker1 deeplearning worker2,,Soonhwan-Kwon,2017-02-20 00:18:06,2017-04-17 18:22:17
PR,Small fix,Fix for untitled,,"kevinthesun,zackchase,zackchase",2017-04-17 18:19:58,2017-04-17 20:28:36
PR,Add cpp package build option in CI,Fix cpp package build and add it to CI build Also revert doc changes for reshape,,"eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin",2017-04-14 23:43:00,2017-04-18 03:56:08
IS,Can I split the train data in the network,for example data mx sym Variable data data1 data k data2 data k Or can I define two data iterations maybe I will put the second one into the networks at its hidden layer,,,2017-04-18 03:26:22,2017-04-18 07:14:23
IS,Problem with installing mxnet on R v 3 3 3,I have a Problem installing mxnet under the current R Version 3 3 3 The issue is probably similiar to this Can you help me on this topic Environment info Operating System Windows 10 R Studio Minimum reproducible example install packages drat repos drat addRepo dmlc install packages mxnet Error Message Warning in install packages cannot open URL '' HTTP status was '502 Proxy error The IP of the web site cannot be found ' Warning in install packages cannot open URL '' HTTP status was '502 Proxy error The IP of the web site cannot be found ' Warning in install packages unable to access index for repository cannot open URL '' Installing package into C Users xxx Documents R win library 3 3 as lib is unspecified Warning in install packages cannot open URL '' HTTP status was '502 Proxy error The IP of the web site cannot be found ' Warning in install packages cannot open URL '' HTTP status was '502 Proxy error The IP of the web site cannot be found ' Warning in install packages unable to access index for repository cannot open URL '' Warning in install packages package mxnet is not available for R version 3 3 3 Warning in install packages cannot open URL '' HTTP status was '502 Proxy error The IP of the web site cannot be found ' Warning in install packages cannot open URL '' HTTP status was '502 Proxy error The IP of the web site cannot be found ' Warning in install packages unable to access index for repository cannot open URL '',,"jeremiedb,thirdwing,jeremiedb",2017-04-12 09:33:10,2017-04-18 08:01:55
IS,Symbol asks for memory all the time when bind,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 15 10 Compiler gcc 4 9 x Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD 2ad9a29640cf8677676be67e75dc82cc2f5c1bec If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace NO error message Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 move SequentialRNNCell into pairwise model to avoid to ask for memory repeately 2 put all code into a method however it does not work 3 check my dataIter works well The sym is a toy model for metric learning about image and text img sym is used to extract image feature so as info sym does extract text feature,,,2017-04-18 08:24:46,2017-04-18 09:26:58
PR,rm a duplicate line in doc,,,abuccts,2017-04-18 07:03:56,2017-04-18 17:14:52
PR,Removed SliceChannel implementations from mxnet cpp op suppl h to prevent redefinition conflict with mxnet cpp op h,Removed two implementations of the op SliceChannel from cpp package include mxnet cpp op suppl h These implementations are equivalent to and redundant with implementations found in cpp package include mxnet cpp op h which is generated by cpp package src OpWrapperGenerator OpWrapperGenerator py The redundant implementations will cause a re definition error Since the OpWrapperGenerator py automates generation of cpp package include mxnet cpp op h it seems removing the implementations op suppl h would be a more straightforward fix,,"Hebali,piiswrong",2017-04-17 23:49:35,2017-04-18 17:15:42
PR,Dockerfile fix for scala gpu,Help to review Tested on ec2 p2 instance,,yzhliu,2017-04-17 16:50:41,2017-04-18 17:16:23
PR,Update doc for some NDArray functions array arange and full,mli Please review and merge,,"indhub,piiswrong,indhub,nswamy,nswamy,nswamy,nswamy,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,mli,indhub,indhub,indhub,indhub,indhub,indhub,indhub,indhub,indhub,zackchase,piiswrong,indhub,piiswrong,zackchase,zackchase",2017-04-13 21:08:36,2017-04-18 19:35:36
PR,Fix concat cc documentation,The dimension sizes of the input arrays on the given axis should be the same However z does not have the same size as x y on dimension 1,,leezu,2017-04-18 11:43:56,2017-04-18 19:41:06
PR,Update doc for log softmax function,,,indhub,2017-04-19 01:12:38,2017-04-19 03:34:45
PR,kvstore py doc strings clean up,,,"madjam,piiswrong,zackchase,zackchase,zackchase,zackchase,piiswrong,madjam",2017-04-12 20:08:43,2017-04-19 04:43:48
PR,Compiles against cuDNNv6 with new support for dilated convolution,,,"DickJC123,piiswrong,DickJC123,piiswrong,DickJC123,piiswrong,piiswrong",2017-03-28 21:03:11,2017-04-19 05:00:04
PR,fix op name according to pr 5726,,,loofahcus,2017-04-19 07:59:28,2017-04-19 08:43:01
PR,Fix spelling errors in recordio module,,,jcbsv,2017-04-19 11:21:08,2017-04-19 16:50:33
PR,Fix spelling error in recordio module description,,,jcbsv,2017-04-19 11:17:52,2017-04-19 16:51:03
PR,Simple support cudnnv6,,,"yajiedesign,piiswrong,yajiedesign",2017-04-13 03:24:13,2017-04-19 20:15:11
PR,update doc for math concat slice crop functions,The doc for the below math functions are modified expm1 cos tan arcsin arccos arctan degrees radians sinh cosh tanh arcsinh arccosh arctanh gamma gammaln Please take a look and merge,,"nswamy,piiswrong,madjam,madjam,madjam,madjam,mli,mli,mli,mli,piiswrong,nswamy,piiswrong,nswamy,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,piiswrong,madjam,nswamy,piiswrong,nswamy,piiswrong,piiswrong,nswamy",2017-04-06 21:43:03,2017-04-19 22:25:27
IS,test,,,domdivakaruni,2017-04-19 22:50:13,2017-04-19 22:51:07
PR,refactor ndarray function parsing add feed dict to bind for autograd,,,"piiswrong,piiswrong,piiswrong,asmushetzel,ZihengJiang,ZihengJiang",2017-04-18 17:53:48,2017-04-19 23:16:53
PR,improve infer type error msg,,,piiswrong,2017-04-14 20:01:13,2017-04-19 23:17:08
PR,Perl Updated tests examples synched perl layer with recent changes in python,,,sergeykolychev,2017-04-19 20:49:05,2017-04-20 01:36:06
IS,A query about the example text conv,,,,2017-04-20 03:14:20,2017-04-20 03:18:50
PR,resnet v1 in fp16,Implements the original resnet network in fp16 precision,,"ap-hynninen,piiswrong,ap-hynninen,piiswrong,ap-hynninen,ap-hynninen,ptrendx,ptrendx,ptrendx,DickJC123,ptrendx",2017-04-19 20:10:46,2017-04-20 04:54:56
PR,Fix NDArray operator docs,Modified pick and Embedding operator docs Please take a look,,"Roshrini,mli,mli,Roshrini,nswamy,nswamy,nswamy,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,Roshrini,Roshrini,Roshrini",2017-04-18 18:29:38,2017-04-20 07:12:32
PR,implement ftrl,FTRL is online learning algorithm Its reference is Ad Click Prediction a View from the Trenches,,"sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,piiswrong,sxjscience,sxjscience,sxjscience",2017-04-13 14:26:48,2017-04-20 17:13:15
PR,SGD optimization added test,Optimizes SGD by fusing multiple kernel calls into a single Kernel Launch kernel,,"ap-hynninen,piiswrong,piiswrong,piiswrong,ap-hynninen",2017-04-19 23:59:36,2017-04-20 17:15:35
PR,fix UnicodeDecodeError,,,yajiedesign,2017-04-20 07:50:10,2017-04-20 17:15:55
PR,Perl bugfixes tests examples sync with python,new random functions fixes and tests for Monitor bugfixes for callbacks and LR Scheduler classes miscelaneous bugfixes updated char lstm example with inferred sampling,,sergeykolychev,2017-04-20 03:49:53,2017-04-20 17:18:06
IS,MXNet binary 404 Error,Tried to do a windows RStudio install following the instructions install packages drat repos drat addRepo dmlc install packages mxnet I got the below error Error Message Installing package into rpackages lsjohn1 as lib is unspecified downloaded 0 bytes Warning in install packages URL '' status was '404 Not Found' Error in download file url destfile method mode wb cannot download all files Warning in install packages download of package mxnet failed R sessionInfo sessionInfo R version 3 3 1 2016 06 21 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 14 04 5 LTS locale 1 LC CTYPE en US UTF 8 LC NUMERIC C LC TIME en US UTF 8 4 LC COLLATE en US UTF 8 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 7 LC PAPER en US UTF 8 LC NAME C LC ADDRESS C 10 LC TELEPHONE C LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 BiocInstaller 1 24 0 loaded via a namespace and not attached 1 drat 0 1 2 tools 3 3 1 Minimum reproducible example install packages drat repos drat addRepo dmlc install packages mxnet,,thirdwing,2017-04-19 17:51:07,2017-04-20 17:37:46
PR,Improve search,Improve search function 1 Add search result preview 10 results will be displayed 2 Modify search result ranking to make it more reasonable 3 Limit search result number to 100 Use pagination to show 10 results per page search1 search2 search3,,kevinthesun,2017-04-20 22:58:00,2017-04-20 23:10:52
PR,Fix search result url,Fix result url Remove ' md',,kevinthesun,2017-04-20 23:51:38,2017-04-21 00:12:14
IS,what is CSR in nnvm graph h,what is CSR,,yuruofeifei,2017-04-20 06:23:22,2017-04-21 00:44:05
PR,fix UnicodeDecodeError,Remove unexpected character,,"piiswrong,yajiedesign",2017-04-20 23:05:45,2017-04-21 01:18:11
PR,Update base py,,,piiswrong,2017-04-21 04:34:23,2017-04-21 06:39:19
PR,Compute perplexity by averaging over the document,solve the problem that the perplexity is related to the batch size,,mli,2017-04-20 03:42:30,2017-04-21 06:39:59
PR,scala change proj version,piiswrong Could you verify if this version works,,"yzhliu,piiswrong,yzhliu,piiswrong,sergeykolychev,piiswrong,yzhliu",2017-04-05 06:58:50,2017-04-21 06:44:33
IS,training resnet 164 in cifar10 train accuracy,I want to train resnet 164 in cifar10 If I use one machine to train the batch size is 128 the train accuracy is 48 at the end of the first epoch but if I use two machine to train kvstore dist sync the batch size is 64 on each machine the train accuracy is only 43 not 48 at the end of the first epoch why because of the loss of gradients precision anyone can explain it thx,,"tornadomeet,tornadomeet,tornadomeet",2017-04-20 11:59:03,2017-04-21 10:50:41
PR,Fixed argparse usage for exts,Currently the type for exts is specified as type list If one tries to change the extension to png for example with python tools im2rec py exts png Then internally args exts ' ' 'p' 'n' 'g' and this is not what is needed This is because specifying type list stores the variable as a list of lists Instead we specify that the number of arguments exts receives is one or more So type list becomes nargs ' ',,,2017-04-21 15:15:46,2017-04-21 16:43:47
PR,Improve Search UI,Improve search UI especially on small screen Fix search result description issue search1 search2,,kevinthesun,2017-04-21 20:45:08,2017-04-21 21:06:27
PR,Avoids running unneeded backprop kernels,Within CuDNNConvolutionOp Backward this adds checks to avoid launching backprop kernels unnecessarily i e when req kNullOp This would result typically in avoiding the L1 dgrad kernel launch in CNN is,,"DickJC123,piiswrong,DickJC123,piiswrong",2017-04-19 23:22:02,2017-04-22 04:15:01
IS,Multi GPU training speed LOW,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 0 4 Compiler g Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source MXNet commit hash git rev parse HEAD 0 9 3 release If you are using python package please provide python 3 5 Python version and distribution python 3 5 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace mx mxnet 0 9 3 example image classification python train mnist py network mlp gpus 0 1 INFO root Epoch 0 Batch 100 Speed 189 20 samples sec Train accuracy 0 100402 mx mxnet 0 9 3 example image classification python train mnist py network mlp gpus 0 INFO root Epoch 0 Batch 300 Speed 81716 01 samples sec Train accuracy 0 928594 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python train mnist py network mlp 2 python train mnist py network mlp gpus 0 3 python train mnist py network mlp gpus 0 1 What have you tried to solve it 1 2 3,,,2017-04-22 08:15:07,2017-04-23 02:07:57
PR,small revision of CMakeList revise the docstring of pick,batch take is added as an alias of pick,,"sxjscience,piiswrong",2017-04-11 13:18:55,2017-04-23 05:28:45
PR,ADD speech recognition example,This example based on DeepSpeech2 of Baidu helps you to build Speech To Text STT models at scale using CNNs fully connected networks Bi RNNs Bi LSTMs and Bi GRUs for network layers batch normalization for training efficiency and a Baidu is WarpCTC for loss calculations In order to make your own STT models besides all you need is to just edit a configuration file not actual codes,,"Soonhwan-Kwon,piiswrong,Soonhwan-Kwon,Soonhwan-Kwon,piiswrong,piiswrong,sbodenstein,yzhang87,Soonhwan-Kwon,Soonhwan-Kwon,piiswrong,Soonhwan-Kwon",2017-04-21 07:40:46,2017-04-24 08:36:25
PR,Use dict items instead of dict iteritems for Python 3 compatibility,,,jcbsv,2017-04-24 15:20:16,2017-04-24 16:41:30
PR,ADD speech recognition example,Removed the implicit LICENSE file in example 5923 and add more details and guides for the example This example based on DeepSpeech2 of Baidu helps you to build Speech To Text STT models at scale using CNNs fully connected networks Bi RNNs Bi LSTMs and Bi GRUs for network layers batch normalization for training efficiency and a Baidu is WarpCTC for loss calculations In order to make your own STT models besides all you need is to just edit a configuration file not actual codes,,"Soonhwan-Kwon,piiswrong,sbodenstein,Soonhwan-Kwon",2017-04-24 08:35:38,2017-04-24 17:08:15
PR,Python kvstore doc fixes,,,madjam,2017-04-21 17:57:05,2017-04-24 17:13:26
PR,Add test for pip installations,This commit adds Dockerfiles and test script to test all variants of MXNet pip packages I have tested this on a copy of the Jenkins slave host with an updated NVIDIA driver 375 26 with the following command tests jenkins run test pip installations sh WORKSPACE where WORKSPACE was set to home ec2 user mxnet the Jenkins workspace Currently the test will fail after testing mxnet with Python 3 6 as the accuracy is lower than expected We can either put that case at the very end or change the script to not fail until all cases have been tested krishnamurthy,,"lxn2,sandeep-krishnamurthy,sandeep-krishnamurthy,lxn2,lxn2,lxn2,mli,szha,piiswrong,lxn2",2017-04-20 19:16:47,2017-04-24 17:16:45
PR,option to not reset eval metric with Speedometer,Speedometer reset metrics which invalidates callbacks coming after them For instance mod fit train data train iter eval data test iter eval metric mx metric CrossEntropy 'acc' batch end callback mx callback Speedometer batch size 1 mx callback log train metric 1 gives the following log INFO root Epoch 0 Batch 1 Speed 311 33 samples sec Train cross entropy 5 322994 INFO root Epoch 0 Batch 1 Speed 311 33 samples sec Train accuracy 0 015625 INFO root Iter 0 Batch 1 Train cross entropy nan INFO root Iter 0 Batch 1 Train accuracy nan I have set the default reset option of Speedometer to False to have the same default behavior as the other callback log train metric also because it is unexpected for the user to have a default that has a side effect on the metric,,"geoalgo,geoalgo,piiswrong,geoalgo,piiswrong,piiswrong,geoalgo",2017-04-13 12:02:23,2017-04-24 18:08:44
PR,Update documentation for mxnet initializer Mixed,mli Please review and merge,,indhub,2017-04-21 23:58:56,2017-04-24 22:30:16
PR,Update documentation for SVMOutput,mli Please review and merge,,"indhub,madjam,indhub",2017-04-21 19:07:27,2017-04-24 22:31:17
PR,Symbol docs fixes,Modified some of symbol docs Please take a look,,"Roshrini,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam",2017-04-21 18:20:00,2017-04-24 22:33:56
PR,Adding install instruction for Ubuntu CPU Python,1 Adding new install page for MXNet One page for all Programming language and Installation types with menus for users to choose 2 Adding instructions to Ubuntu Python CPU pip virtualenv docker building from source 3 This page is not directly linked from any mxnet io page Hence will be still hidden We will turn it on after completing instructions for Ubuntu GPU Mac OS CPU and GPU Tested all the instructions on Ubuntu 14 04 and 16 04 Few screen shots attached Can you please review and merge this Can you please provide your feedback Can you please review this changes and suggest if any changes are required to make it easy for your testing strategy img width 1230 alt screen shot 2017 04 18 at 3 32 56 pm src img width 1259 alt screen shot 2017 04 18 at 3 33 29 pm src,,"sandeep-krishnamurthy,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy",2017-04-18 22:39:47,2017-04-24 22:34:55
PR,edit ndarray API docs,Edited some math function reduce function docs nansum nanprod edited description sum prod mean max edited parameter description Minor Edits ones minor wording edits Edited some Array manipulation routines repeat edit and add example flip reverse edit and add example Edited some optimizer update function More edits in SGD update momentum update Adam update edited description and parameters For RMSprop and AlexRMSprop just edited parameters,,"jiajiechen,nswamy,nswamy,nswamy,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,jiajiechen,jiajiechen,nswamy,nswamy,jiajiechen,jiajiechen,piiswrong,Roshrini,jiajiechen,jiajiechen,jiajiechen,jiajiechen,jiajiechen,jiajiechen,piiswrong,zackchase,zackchase,piiswrong",2017-04-12 21:45:30,2017-04-24 23:04:59
IS,Is there unit test for OPs,I am writing OP I want do some unit test but I have not find test examples for OPs I want know if there is a easy way to test it,,Godricly,2017-04-24 01:50:28,2017-04-25 01:34:14
PR,More API Doc Edits,l2 normalization Activation BlockGrad same as stop gradient MakeLoss,,"jiajiechen,mli,mli,mli,mli,mli,zackchase,zackchase,zackchase,zackchase,jiajiechen,jiajiechen,jiajiechen,jiajiechen,jiajiechen,jiajiechen,madjam,madjam,jiajiechen,jiajiechen,zackchase,mli,yajiedesign",2017-04-19 00:56:21,2017-04-25 02:23:53
PR,Update documentation for mxnet image imdecode,mli Please review and merge,,"indhub,piiswrong",2017-04-24 21:25:33,2017-04-25 02:25:19
PR,Fix script by adding path to Dockerfile,Previously I pushed changes to test Pip packages but in the test script I did not add the path to where I actually added the Dockerfiles This commit fixes that See a Jenkins run against my branch krishnamurthy,,"lxn2,sandeep-krishnamurthy",2017-04-24 22:04:12,2017-04-25 02:25:50
PR,Improve the doc of pick Update dmlc core,Add PickParam to improve the doc of pick and revise the default value of axis to be 1 We could add the mode attribute similar to nd take in the future,,"sxjscience,piiswrong",2017-04-23 10:50:18,2017-04-25 02:28:30
PR,Update doc for Custom operator,Update documentation for Custom operator Please review and merge,,"indhub,madjam,mli,mli,mli,indhub,indhub,indhub,nswamy,nswamy,zackchase,madjam,indhub,indhub,zackchase,zackchase,piiswrong",2017-04-18 00:06:10,2017-04-25 03:43:12
PR,ADD missing Libri sample json FIX minor bugs in speech recognition example,ADD missing Libri sample json Enabled show log on every n th update for log readability FIX checkpoint undesirably saved on previous epoch 1 when mode is load Fix Load data scheme from sortagrad to shuffled dataset,,Soonhwan-Kwon,2017-04-25 05:39:50,2017-04-25 06:12:59
IS,Release for R is not building on Mac from sources,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System uname a Darwin 192 168 1 6 tpgi com au 16 5 0 Darwin Kernel Version 16 5 0 Fri Mar 3 16 52 33 PST 2017 root xnu 3789 51 2 3 RELEASE X86 64 x86 64 Compiler Mac OS clang the Clang C C and Objective C compiler Package used Python R Scala Julia R MXNet version 0 9 3a distribution from GitHub tar gz Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 apple darwin15 6 0 64 bit Running under macOS Sierra 10 12 4 locale 1 en AU UTF 8 en AU UTF 8 en AU UTF 8 C en AU UTF 8 en AU UTF 8 attached base packages 1 stats graphics grDevices utils datasets methods 7 base loaded via a namespace and not attached 1 Rcpp 0 12 10 visNetwork 1 0 4 digest 0 6 12 4 withr 1 0 2 R6 2 2 0 jsonlite 1 1 7 git2r 0 15 0 magrittr 1 5 httr 1 2 1 10 stringi 1 1 5 curl 2 2 drat 0 1 2 13 devtools 1 12 0 RevoUtils 10 0 2 tools 3 3 2 16 stringr 1 2 0 htmlwidgets 0 8 htmltools 0 3 5 19 memoise 1 0 0 knitr 1 15 1 Error Message Please paste the full error message including stack trace 192 168 1 6 mxnet 0 9 3a pb make rpkg Makefile 27 mshadow make mshadow mk No such file or directory Makefile 28 Users pb Downloads mxnet 0 9 3a dmlc core make dmlc mk No such file or directory Makefile 126 Users pb Downloads mxnet 0 9 3a ps lite make ps mk No such file or directory make No rule to make target Users pb Downloads mxnet 0 9 3a ps lite make ps mk' Stop Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error What have you tried to solve it 1 Install binary broken for my env 2 Build the package from sources,,,2017-04-25 04:20:02,2017-04-25 07:19:20
IS,deprecation,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-04-25 11:21:56,2017-04-25 11:22:21
PR,cpp package Add C basic tutorial and build instruction,,,"lx75249,piiswrong,lx75249,piiswrong",2017-04-25 13:12:26,2017-04-25 16:57:02
PR,Image docs modified,Modified some of image docs Please take a look,,"Roshrini,madjam,madjam",2017-04-25 17:04:44,2017-04-25 19:39:22
IS,C the require input Shape of Convolution change while bind,Environment info Operating System Win10 Compiler MSVC 15 CUDAv8 MSVC14 Package used Python R Scala Julia C MXNet commit hash commit b5a71d5 But I get same error message for the nightly build 20170423 mxnet x64 vc14 gpu 7z Error Message Console output 23 02 47 D mxnet dmlc core include dmlc logging h 300 23 02 47 D mxnet src ndarray ndarray cc 239 Check failed from shape to shape operands shape mismatchfrom shape 64 to shape 64 1 41 41 And there is a c exception will thrown with error message Error in operator conv conv01 Shape inconsistent Provided 64 1 3 3 inferred shape 64 64 3 3 I got an exception I believe it is the same thing as the original code BTW I have to build the mxnet with debug symbol in order to trace the code to get more detail error message Here is my debug result This function call the C interface and throw the exception which is not helpful L181 Here is the line throw the exception Error in operator conv conv01 L445 After I traced into this function I gave up L76,,"lx75249,lx75249",2017-04-24 03:56:32,2017-04-25 23:39:06
PR,KVStore Add support for other data types,mli,,"ZihengJiang,piiswrong,piiswrong,Godricly,ZihengJiang,ZihengJiang,piiswrong,Godricly,mli,mli,ZihengJiang",2017-04-13 05:27:07,2017-04-25 23:49:13
IS,where dose the nnvm Node inputs be setup,when InferShape in,,,2017-04-25 09:33:56,2017-04-26 06:53:10
PR,Convert caffe AbsVal to mx symbol abs in caffe converter,,,matteosal,2017-04-26 10:28:34,2017-04-26 16:43:33
PR,Module fix input grads order,The order of the input grad arrays is inconsistent with the order of data shapes For example we have set the data names with the order 'b' 'c' 'a' but the input gradient will be 'a' 'b' 'c' This PR fixed the problem Example code The input grads returns grad of 'a' grad of 'b' grad of 'c' However we are providing the data shapes in the order of 'b' 'c' 'a' It should thus be grad of 'b' grad of 'c' grad of 'a',,sxjscience,2017-04-26 02:49:08,2017-04-26 16:45:34
PR,Update documentation for mxnet metric np,mli Please review and merge,,indhub,2017-04-25 22:47:04,2017-04-26 16:46:11
PR,Removing unnecessary copies from backward pass of binary elementwise ops,,,"ptrendx,piiswrong",2017-04-21 22:23:38,2017-04-26 16:53:38
PR,Activation ops,,,"ZihengJiang,piiswrong,tqchen,ZihengJiang,piiswrong,tqchen,piiswrong",2017-04-22 06:17:52,2017-04-26 16:55:41
PR,update env var doc,piiswrong,,"eric-haibin-lin,piiswrong",2017-04-25 06:31:28,2017-04-26 16:56:00
PR,Correction to LSTMCell docstring,,,jcbsv,2017-04-26 16:19:38,2017-04-26 17:35:37
PR,Quantization ops 'quantize' wouldequantize',Make it public for other guys to contribute and use,,"ZihengJiang,piiswrong,piiswrong",2017-04-14 21:51:53,2017-04-26 21:21:52
IS,closed,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,domdivakaruni,2017-04-26 22:21:14,2017-04-26 22:21:46
IS,Thread local storage implementation is buggy,Hey there so I was randomly browsing the codebase because of jetlag in France and stumbled upon the thread local storage code here L34 L74 I think what happens is that the code does not properly releases the underlying objects when a thread starts and only releases them when the program exits Note that static MX TREAD LOCAL T ptr nullptr only declares an POD type pointer and when the thread exits only ptr itself is freed not the object ptr points to The deletion happens at ThreadLocalStore and since ThreadLocalStore is a process wide singleton this destructor only gets called when the program exits It usually is not a problem if you do not spawn too many threads but is a hidden bomb if you use it in a place where threads are actively created and deleted This seems to be a typical mistake when writing a thread local storage usually the keyword thread local is preferred it is C 11 I guess the reason of not using it to allow Mac OS I do not know what Apple is thinking about honestly but if that is the purpose it might be safer to use boost TLS or folly TLS or something like that,,"tqchen,tqchen,antinucleon",2017-04-27 00:19:40,2017-04-27 01:02:04
PR,Fix NDArray delay allocation inconsistency,Here L41 the TBlob is used before allocation This bug happens if one ndarray is binded to executor before allocation,,"lx75249,piiswrong,tqchen",2017-04-26 19:39:37,2017-04-27 02:10:42
IS,An elegent solution for WGAN gp gradient penalty,Is there any elegant way to compute gradient of WGAN gp gradient loss term in MXNet It seems that I have 2 handcraft it the one in tf looks very convenient,,"Godricly,sxjscience,Godricly",2017-04-26 09:03:29,2017-04-27 02:46:32
PR,Fix for scala test,,,ZihengJiang,2017-04-27 02:32:09,2017-04-27 03:40:18
IS,Mxnet Compile Problom,I compiled a libopenblas x64 linux a How can I set the makefile to use the libopenblas x64 linux a when compiling the mxnet in CentOS env,,,2017-03-16 07:38:25,2017-04-27 07:38:24
IS,params file load question,NDArray LoadToMap can load params by file name Is there any interface that can load params by raw bytes,,,2017-04-22 06:39:03,2017-04-27 07:38:32
PR,Update doc for mxnet ndarray elemwise add,mli Please review and merge,,"indhub,piiswrong,mli,indhub,nswamy,nswamy,indhub,nswamy,piiswrong,indhub,indhub,piiswrong,piiswrong",2017-04-19 20:20:06,2017-04-27 16:55:15
PR,Add attrs for backward ops,,,ZihengJiang,2017-04-27 08:33:46,2017-04-27 16:56:34
PR,Fix take op kNullOp issue,While current 'take' operator is require type is 'kNullOp' it should be skipped,,"kevinthesun,piiswrong,ZihengJiang,piiswrong",2017-04-26 23:12:17,2017-04-27 16:57:01
PR,dmlc core submodule updated,,,sbodenstein,2017-04-27 08:07:53,2017-04-27 16:57:28
PR,Adding installation instruction for Ubuntu GPU Python,1 Adding installation instructions for Ubuntu GPU Python 2 Pip Virtualenv Docker Source build are different types of installation guide supported 3 Tested all the instructions In few sections text and description may look very similar to CPU instruction set However this is not optimized to reuse text content to enable readability testability and be consistent Can you please review this change Thanks Sandeep,,sandeep-krishnamurthy,2017-04-26 18:31:33,2017-04-27 17:02:22
PR,fix float16 random,ptrendx,,"piiswrong,piiswrong",2017-04-26 21:08:07,2017-04-27 17:08:35
PR,Doc updates to ensure method descriptions are in 3rd person declarative,,,madjam,2017-04-26 19:08:50,2017-04-27 17:14:14
PR,Documentation Ndarray instancenorm doc modified,mli Please take a look,,Roshrini,2017-04-27 16:37:11,2017-04-27 17:58:02
PR,fix float16 random,,,piiswrong,2017-04-27 17:10:36,2017-04-27 19:14:38
PR,Fix indentation in LSTMBias docstring,The missing indentation caused the now indented line to appear as a parameter mxnet initializer LSTMBias in the documentation,,jcbsv,2017-04-27 21:17:29,2017-04-27 21:35:01
PR,update docs for lrn,mli Please review,,"nswamy,zackchase,zackchase,zackchase,zackchase,zackchase,nswamy,nswamy,zackchase,nswamy,zackchase",2017-04-20 11:36:19,2017-04-27 21:41:30
PR,Fix take op kNullOp issue,,,kevinthesun,2017-04-27 17:47:55,2017-04-27 21:46:09
IS,the default optimizer params arescale grad' 1 0 batch size is not right when kvstore None,each device will update the weight with,,,2017-04-27 13:45:17,2017-04-28 02:38:00
PR,doc remove Chinese docs,Chinese docs are out of date we should have use machine translation to get various language docs instead of translating by hands,,mli,2017-04-27 23:32:10,2017-04-28 03:37:07
PR,Clarify order of data and labels in DataBatch,,,leezu,2017-04-28 15:24:48,2017-04-28 17:29:12
PR,doc remove docs tutorials computer vision,docs tutorials computer vision md are mainly copied from the examples folders They are out of date I aware that zack is cleaning these tutorials But I hope I can remove earlier because these files fail my new doc builder,,"mli,piiswrong,mli",2017-04-28 05:34:29,2017-04-28 18:04:29
PR,doc remove docs packages and docs system,these two folders contain redirect links it should be safe to remove them now,,mli,2017-04-28 18:58:48,2017-04-28 18:59:18
PR,doc update doc for pooling convention,Missing parentheses in pooling convention expressions,,abuccts,2017-04-29 12:30:26,2017-04-30 01:18:41
PR,Update documentation for 'mxnet initializer register',mli Please review and merge,,"indhub,madjam,zackchase",2017-04-26 22:53:35,2017-04-30 03:50:35
PR,Update documentation for Normal Uniform One and Zero initializers,mli Please review and merge,,"indhub,piiswrong,indhub,indhub",2017-04-27 21:42:01,2017-04-30 03:51:40
PR,documentation update update doc for pad,screen shot 2017 04 21 at 2 14 30 pm,,"nswamy,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,zackchase,zackchase,zackchase,zackchase,zackchase,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,zackchase",2017-04-21 21:20:02,2017-04-30 03:52:45
IS,How to install and use tensorboard in MXNet,I found there is a tutorial about Understanding the vanishing gradient problem through visualization mxnet 0 9 3 documentation which used tensorboard to visualize result change However how can I enable this magic tensorboard Install TensorFlow first and then Thanks a lot TB,,"ysh329,piiswrong,ysh329,zihaolucky,ysh329,zihaolucky,ysh329",2017-04-29 14:55:36,2017-04-30 07:39:52
IS,skip exec node seems wo not work for forward backward nodes in computational graph,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler gcc 4 8 4 Package used Python R Scala Julia Python and C MXNet version Or if installed from source build from source with GPU enabled MXNet commit hash git rev parse HEAD 974d7dad55c6a07ccae8333af1c6f064920fe9c9 If you are using python package please provide Python version and distribution Python 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I'm doing a synchronous distributed machine learning project and is trying to kill the stragglers in the distributed cluster for a certain iteration basically it means when k out of n workers complete their gradient computing we just use gradients from k workers to update the model and kill the remaining n k workers If there is an easier way to do something like this in MXNet please do not hesitate to let me know What I'm trying to do is to skip some execution nodes in the computational graph for the straggler workers i e slow workers when k n workers complete their job it is like to short circuit a certain iteration on a slow worker and force it to go into next iteration manually I tried to do this by setting skip exec node in OpNode Then when I test this something weird happened the code I'm using is in the following section So what I'm trying is an extreme condition that I just skip nearly all the nodes in a computational graph Before I implement skipping several nodes the time cost is like 'Time cost for forward backward 0 0002989768981933594' 'Time cost for update metric 0 25156497955322266' The update metric operation is somehow computing the train accuracy here I'm not sure why it cost so much time like it was stated in After I implement the skip execution thing the time cost is quite weird 'Time cost for forward backward 0 0003230571746826172' 'Time cost for update metric 0 11911201477050781' So it seems skip exec node will skip some execution for update metric operation rather than forward backward execution because the time cost is not reducing Why this is the case And is anyone have any idea about how can I implement the short circuit idea I mentioned before in other way Minimum reproducible example if you are using your own code please provide a short script that reproduces the error The code I use to skip executions in computational graph the original code is here L780 bool SHORT CIRCUIT TEST true void GraphExecutor RunOps bool is train size t topo start size t topo end Update context const auto idx graph indexed graph for some short circuit test here if SHORT CIRCUIT TEST true for size t nid topo start nid topo end nid op nodes nid skip exec node true The code I use in Python to test the time cost the original code is here L368 while not end of batch data batch next data batch if monitor is not None monitor tic start tmp1 time time self forward backward data batch duration1 time time start tmp1 print Time cost for forward backward f duration1 self update try pre fetch next batch next data batch next data iter self prepare next data batch except StopIteration end of batch True start tmp2 time time self update metric eval metric data batch label duration2 time time start tmp2 print Time cost for update metric f duration2 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-04-30 17:03:56,2017-04-30 19:27:11
PR,Change to CUDNN CALL,,,ZihengJiang,2017-04-30 20:26:31,2017-05-01 00:34:09
PR,fix utf8,,,yajiedesign,2017-04-30 15:33:38,2017-05-01 00:38:00
PR,Perl Docker files sync with latest python changes examples tutorials expanded fixed docs,,,"sergeykolychev,sergeykolychev",2017-04-30 05:14:05,2017-05-01 00:38:40
PR,update viz for py to support 3d net,Update print summary and plot network to support conv and pooling layer visualization for both 2D and 3D network,,abuccts,2017-04-29 12:19:40,2017-05-01 00:43:40
PR,Update osx installation script to move mxnet if present,Update osx installation script to move mxnet directory if already present in the home directory,,bhavinthaker,2017-04-25 19:28:32,2017-05-01 00:45:13
IS,Error in one hot in MXNet with cuDNNv6,DickJC123 I was working on building the binary distribution with statically linked cuDNNv6 and discovered that mxnet python api does not pass some tests I'm guessing that some changes have been missed from 5606 Environment info Operating System ubuntu14 04 Compiler gcc g 4 8 4 Package used Python R Scala Julia python Installed from source Python version and distribution Python 2 7 6 Error Message Please paste the full error message including stack trace is built with CUDA7 5 and cuDNNv6 See L42 for config Error happens when running tests python gpu test operator gpu py Steps to reproduce 1 build libmxnet so by setting the flags in the minimum reproducible example 2 cd python 3 nosetests verbose tests python gpu test operator gpu py,,"szha,szha",2017-04-29 01:08:37,2017-05-01 01:58:20
PR,Cures SEGV seen in test symbol py test zero prop2 when DEBUG 1,test symbol py test zero prop2 triggers and checks for an exception The current behavior of having LOG FATAL call abort when DEBUG 1 prevents the regression tests from completing in this case,,DickJC123,2017-05-01 02:27:28,2017-05-01 03:16:57
PR,Scala update Visualization scala,,,Ldpe2G,2017-05-01 10:10:37,2017-05-01 14:09:19
IS,mxnet r how to initialize new training session with results from previous session,Dumb noob question but after reading through tutorials and documentation and trying for several hours I can not figure out how to initialize weights to the results of a previous training session I would like to be able to check every 50 rounds or so how the model is fitting my test set and update learning rate momentum etc accordingly I have tried setting mx model FeedForward create arg params old model arg params but it does not seem to work Thanks in advance,,,2017-05-01 18:04:30,2017-05-01 19:42:59
PR,update doc for Leaky ReLU,update documentation for split and LeakyReLU Please review,,"nswamy,zackchase,zackchase,zackchase,nswamy,zackchase,nswamy,nswamy,nswamy,madjam,nswamy,nswamy,zackchase,zackchase",2017-04-17 21:22:31,2017-05-01 21:23:59
PR,Toc Restructure,Restructure table of contents Left toc represents file tree toc and allows user to quickly navigate to other pages Right toc represents in page section contents toc1 Two exceptions 1 Index page does not need right doc since it is the same as left one toc3 2 API page concatenates right doc into left one toc2 Also add highlighting for navigation entry and toc entry to show users their current position,,"kevinthesun,mli,kevinthesun",2017-05-01 23:27:01,2017-05-02 00:26:02
PR,revised English on these documents,,,zackchase,2017-05-01 23:21:28,2017-05-02 00:29:27
PR,update mshadow,See This commit also includes changes in mshadow by by by,,"szha,szha",2017-05-01 23:33:24,2017-05-02 03:52:06
PR,Fix search function for new toc,,,kevinthesun,2017-05-02 05:42:55,2017-05-02 06:35:43
IS,Profiler all operators are collapsed as a single unit in the profiler,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Distributor ID Ubuntu Description Ubuntu 14 04 5 LTS Release 14 04 Codename trusty Compiler gcc Ubuntu 4 8 4 2ubuntu1 14 04 3 4 8 4 Copyright C 2013 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE Package used Python R Scala Julia Python 3 4 3 MXNet version Or if installed from source MXNet commit hash git rev parse HEAD e4338628641ea8d7db0b6dd249428262aafe98b2 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Problem In the json file produced from profiler all operators are collapsed as a single unit as below making it impossible for me to break down the computation Also the name of the entry seems truncated as it starts with ' ' but not closed with ' ' This did not happen in commit 8861ccc032fa64f4a13b84fb82f6edb4febef2be which I had been using Is this grouping of operators intended If so is there a way for me to control the level of grouping,,"bikestra,eric-haibin-lin,eric-haibin-lin,ZihengJiang,bikestra",2017-04-24 23:59:50,2017-05-02 23:24:10
PR,Update API doc for scale down and resize short in python mxnet image py,mli please see if it is OK,,zackchase,2017-04-28 22:11:30,2017-05-03 02:03:56
IS,How to save the parameters in executor and continue training in module,I find the api about save parameters is Module save checkpoint However in the example RL ddpg algrithm the parameters are all in executor I try this construct a module for save parameter and copy the parameter in executor to the module executor but it does not work is this a correct way to do it any advice,,,2017-05-02 12:37:57,2017-05-03 07:01:12
PR,More How Tos Revised,revised model parallelism and nnpack how to documents,,"zackchase,madjam,madjam,madjam,madjam,madjam",2017-05-02 22:19:09,2017-05-03 07:35:01
PR,CTC Loss Operator,This PR merges WarpCTC into MXNet so that MXNet has a native CTC loss layer Some questions issues This currently depends on ModernGPU which is a header only GPU library This dependency should eventually be removed in favor of CUB Lint I have not conformed ModernGPU and WarpCTC to MXNet code style Should this be done or is there a way to set the linter to ignore these folders I do not have any license info in the WarpCTC files What should I put I have already removed dependency on some custom reduction routines and am using MShadow instead So the GPU timings might be a bit different from WarpCTC Some design issues This layer uses the following sizes for inputs and outputs data has dims sequence length batch size alphabet size 1 labels has dims batch size label sequence length and output batch size The output is a loss for every element in the batch The implementation supports variable length label sequences via 0 padding A separate PR will implement variable length sequence support for the data input,,"sbodenstein,piiswrong,sbodenstein,piiswrong,piiswrong,ap-hynninen,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein",2017-04-13 21:50:14,2017-05-03 07:40:18
PR,Fix memory error in threaded engine,This bug make mxnet frequently get segfault on my machine after some debugging and code tracing I realized it happens in the following condition 1 Executor calls DeleteOperator L241 L256 2 DeleteOperator pushes ThreadedOpr Delete threaded opr into the threaded engine 3 But the write dependency of ThreadedOpr Delete threaded opr is not satisfied because operation A is using a dependent variable of ThreadedOpr Delete threaded opr so ThreadedOpr Delete threaded opr waits A 4 Operation A finishes and calls ThreadedEngine OnComplete A L351 L394 5 We mark write dependencies of A as complete these code L358 L378 6 After step 5 this line L378 the function pushed in step 2 may immediately execute thus the object which threaded opr points to is destroyed and the memory address may be occupied by other objects 6 We read threaded opr temporary in this line L391 and get a non zero garbage value then executes ThreadedOpr Delete threaded opr L392 and destroy an innocent object threaded opr points to Probably also a threaded opr because object pool being used This patch fixes the issue cc original author,,"sifmelcara,tqchen,tqchen,hotpxl,sifmelcara",2017-05-03 14:39:11,2017-05-03 16:05:53
IS,ThreadedOpr Delete threaded opr being called multiple times,Environment info Operating System Linux Compiler gcc 5 4 0 Package used Python R Scala Julia C MXNet version 703e8eeb117109ea345639610780e30775056412 current master Error Message segmentation fault or free invalid pointer Steps to reproduce 1 set cuda cudnn openblas opencv flags and USE CPP PACKAGE 1 in config mk 2 make j4 3 run LD LIBRARY PATH LD LIBRARY PATH lib valgrind build cpp package example lenet 4 a memory error will be detected after training start Without valgrind the segmentation fault appears randomly so I use valgrind to help immediately catch the code causes the segmentation fault when it happened After running Valgrind I found that the segmentation fault is caused by ThreadedOpr Delete threaded opr L392 being called multiple times makes the destructor execute twice and possibly mess up the object pool Valgrind log file log L127 I also observed that placing a mutex in ThreadedEngine OnComplete ThreadedOpr threaded opr prevents the segmentation fault makes Valgrind happy and MXNet never crashes again although I believe this is not the correct solution I traced the code and still have no idea why the OnComplete callback will be called more than once using same threaded opr so I decided to open the issue and hope someone can help resolve the problem Thank you very much,,"sifmelcara,lx75249,sifmelcara,sifmelcara",2017-04-29 14:59:47,2017-05-03 16:12:26
IS,fjetiekdulifc,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,thatindiandude,2017-05-03 21:25:22,2017-05-03 21:40:46
PR,Fixing typos,,,"zackchase,zackchase",2017-05-03 18:19:15,2017-05-04 00:33:15
PR,Typo dose does,,,twitwi,2017-05-03 10:43:53,2017-05-04 00:35:41
PR,Test get started installation guide,This extracts installation instruction sets from krishnamurthy is installation guide and runs it Currently looks at instruction sets for pip docker virtualenv build from source for Linux Python CPU and Linux Python GPU The output of a successful Jenkins run can be viewed here,,lxn2,2017-05-02 21:58:29,2017-05-04 00:36:11
PR,Context CSVIter doc modified,mli Please take a look,,"Roshrini,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,madjam,madjam,madjam,madjam,piiswrong,piiswrong,madjam,eric-haibin-lin,Roshrini,Roshrini,zackchase",2017-04-28 18:54:40,2017-05-04 00:40:26
PR,Update documentation for mxnet metric create,mli Please review and merge,,"indhub,madjam,indhub,zackchase,zackchase,indhub",2017-04-26 20:39:19,2017-05-04 00:55:41
PR,Fix tutorial links,zackchase,,"kevinthesun,madjam,zackchase",2017-05-03 20:14:55,2017-05-04 00:58:12
IS,Issue about the arch file in speech recognition example,Hi all I am trying to run the example of DeepSpeech2 using the example of speech recognition with librispeech but there is an error occurred in load model File main py line 212 in module model loaded model num epoch load model args contexts data train File main py line 144 in load model model loaded symbol template arch args File home nd mxnet example speech recognition arch deepspeech py line 89 in arch net batchnorm net File home nd mxnet example speech recognition stt layer batchnorm py line 29 in batchnorm output mean var output mean var File usr local lib python2 7 dist packages mxnet ctypes symbol py line 181 in creator ctypes byref sym handle File usr local lib python2 7 dist packages mxnet base py line 75 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Cannot find argument 'output mean var' Possible Arguments eps float optional default 0 001 Epsilon to prevent div 0 momentum float optional default 0 9 Momentum for moving average fix gamma boolean optional default True Fix gamma while training use global stats boolean optional default False Whether use global moving statistics instead of local batch norm This will force change batch norm into a scale shift operator in operator BatchNorm name momentum 0 9 fix gamma True output mean var False use global stats False eps 0 001 Many thanks,,,2017-05-03 11:14:00,2017-05-04 03:44:22
IS,tensor Segmentation fault with GPU context,I am not sure I use the tensor in the right way in op is forward and backward can run correctly in CPU context but error in GPU context,,piiswrong,2017-05-04 02:12:52,2017-05-04 05:37:06
IS,How to save the final model in training mnist data using train cifar10 py or train mnist py,I used train cifar10 py and mx model save checkpoint to save final model Problem But I met the problem below Related Issues How to save the final model in training mnist data using train mnist py Issue 5323 dmlc mxnet,,"ysh329,ysh329,ysh329,ysh329",2017-04-20 08:05:55,2017-05-04 09:12:03
PR,Documentation improvements to sampling operators,,,"madjam,piiswrong,madjam,madjam,zackchase,zackchase,zackchase,zackchase,zackchase,madjam",2017-04-25 19:56:28,2017-05-04 15:17:37
PR,Some more symbol docs modified,Modified some of symbol docs like iter call compose bind simple bind eval var save neg attr list attr attr dict Please take a look,,"Roshrini,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,piiswrong,Roshrini,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,piiswrong,piiswrong,Roshrini,Roshrini,Roshrini",2017-04-26 16:43:34,2017-05-04 19:40:50
PR,Doc sample change,Revised documentation including providing examples in response to issue 6000 for following operators random uniform random normal random gamma random exponential random poisson random negative binomial random generalized negative binomial sample uniform sample normal sample gamma sample exponential sample poisson sample negative binomial sample generalized negative binomial Screenshots attached img width 1280 alt screen shot 2017 05 03 at 10 46 21 pm src img width 1280 alt screen shot 2017 05 03 at 10 46 30 pm src img width 1280 alt screen shot 2017 05 03 at 10 46 40 pm src img width 1280 alt screen shot 2017 05 03 at 10 46 48 pm src img width 1280 alt screen shot 2017 05 03 at 10 46 54 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 01 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 07 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 28 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 34 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 38 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 43 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 48 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 54 pm src img width 1280 alt screen shot 2017 05 03 at 10 47 59 pm src img width 1280 alt screen shot 2017 05 03 at 10 48 24 pm src,,"asmushetzel,madjam,madjam,madjam,madjam,madjam,asmushetzel,nswamy,domdivakaruni",2017-05-03 22:27:51,2017-05-04 23:33:02
PR,Update graph executor cc,,,piiswrong,2017-05-04 10:31:14,2017-05-04 23:36:31
IS,Should parameter server have GPU installed,Hi I am planning distributed training by using MXNet Do I need to allocate a GPU machine for parameter server Is MXNet parameter server CPU based or GPU based Thanks Juilin,,"eric-haibin-lin,eric-haibin-lin",2017-04-24 05:08:57,2017-05-05 16:04:31
PR,Update doc for mx random seed,mli Please review and merge,,"indhub,piiswrong",2017-05-05 07:59:52,2017-05-05 17:14:59
IS,mxnet binary installation error Mac 10 12 4 R 3 4 0,Hello I have just updated to R 3 4 0 in my Mac macSierra machine and when trying to install mxnet following what appears here install the mxnet package for r get an error It seems like the page is not reachable Thanks Carlos,,"thirdwing,thirdwing",2017-04-27 21:34:43,2017-05-06 03:36:40
IS,mxnet Installation error in R v3 4 0 win10 64bit,While installing mxnet package for R v3 4 0 win10 64bit I'm getting following error Please help I was using this steps install packages drat repos drat addRepo dmlc install packages mxnet Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Installing package into C Users Mrugank Documents R win library 3 4 as lib is unspecified Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Warning in install packages unable to access index for repository cannot open URL '' Package which is only available in source form and may need compilation of C C Fortran mxnet These will not be installed,,"thirdwing,jeremiedb,thirdwing",2017-04-23 13:43:50,2017-05-06 03:37:51
PR,small fix,shape order fixed in input shape data,,liueryun,2017-05-06 01:47:30,2017-05-06 11:33:13
PR,Fixed imdecode crash bug when flag 0,Fixed by checking for flag before cvtColor issue Compiled and checked on my local machine Do I need to run any tests,,gurumurthys,2017-05-06 00:11:01,2017-05-06 11:36:43
PR,Fix for osx cuda build,Fix for,,"ZihengJiang,piiswrong",2017-05-05 19:44:52,2017-05-06 11:38:34
PR,Fix NDArray bool checking,Since 5745 autoencoder solver and fcn xs solver will fail due to bool casting ValueError The truth value of an NDArray with more than one element is ambiguous,,seanpmorgan,2017-05-05 18:24:26,2017-05-06 11:39:26
PR,Docs for MXRecordIO MXIndexedRecordIO modified,mli Please take a look,,"Roshrini,madjam,madjam,madjam,piiswrong,madjam,Roshrini,zackchase,piiswrong,Roshrini,Roshrini",2017-04-27 20:44:19,2017-05-06 11:49:09
PR,Update documentation for mx callback Speedometer,mli Please review and merge,,"indhub,zackchase,piiswrong,piiswrong",2017-05-01 18:39:51,2017-05-06 11:56:30
PR,update doc for Load,addressed comments,,"nswamy,madjam,madjam,madjam,madjam,piiswrong,madjam,madjam,piiswrong",2017-05-03 21:06:42,2017-05-06 12:12:58
PR,Installation instructions for MacOS and Cloud,1 Added installation instructions for MacOS CPU Python 2 Added links to AWS AMI and CFN MXNet on Cloud 3 Tested all the instructions Note I am not fully happy with the size structure of this file getting bigger and it becomes more complex to manage when we add other Programming language installation guide I will create an issue and assign it to myself to simplify this post v0 10 doc release Can you please review this changes,,"sandeep-krishnamurthy,mli,mli,mli,mli,mli,mli,mli,mli,lxn2,lxn2,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,lxn2,sandeep-krishnamurthy,lxn2,sandeep-krishnamurthy,piiswrong,piiswrong,sandeep-krishnamurthy",2017-04-27 18:19:17,2017-05-06 12:14:41
IS,can not build with CUDA on macOS 10 12 4,I'm having trouble building mxnet shared library from source with Cuda and Cudnn I can compile and run the CUDA samples so nvcc seems to be working ok building with attached config mk and make j Environment info Operating System macOS 10 12 4 Compiler Apple LLVM version 8 0 0 clang 800 0 42 1 Target x86 64 apple darwin16 5 0 Thread model posix InstalledDir Applications Xcode 8 2 1 app Contents Developer Toolchains XcodeDefault xctoolchain usr bin nvcc NVIDIA R Cuda compiler driver Copyright c 2005 2016 NVIDIA Corporation Built on Tue Jan 10 13 22 46 CST 2017 Cuda compilation tools release 8 0 V8 0 61 MXNet commit hash git rev parse HEAD 703e8eeb117109ea345639610780e30775056412 Error Message cd Users robin mxnet dmlc core Applications Xcode 8 2 1 app Contents Developer usr bin make libdmlc a USE SSE 1 config Users robin mxnet config mk cd Users robin mxnet cd Users robin mxnet nnvm Applications Xcode 8 2 1 app Contents Developer usr bin make lib libnnvm a DMLC CORE PATH Users robin mxnet dmlc core cd Users robin mxnet make 1 libdmlc a' is up to date make 1 lib libnnvm a' is up to date usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES g O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 50 code compute 50 Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 I Users robin mxnet mshadow I Users robin mxnet dmlc core include fPIC I Users robin mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include DMSHADOW USE CUDNN 1 I Users robin mxnet cub DMXNET USE NVRTC 1 M MT build src operator contrib quantize gpu o src operator contrib quantize cu build src operator contrib quantize gpu d usr local cuda bin nvcc c o build src operator contrib quantize gpu o std c 11 Xcompiler D FORCE INLINES g O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 50 code compute 50 Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 I Users robin mxnet mshadow I Users robin mxnet dmlc core include fPIC I Users robin mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 I System Library Frameworks Accelerate framework Versions Current Frameworks vecLib framework Versions Current Headers DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include DMSHADOW USE CUDNN 1 I Users robin mxnet cub DMXNET USE NVRTC 1 src operator contrib quantize cu In file included from tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 1 var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 21 31 error typedef redefinition with different types 'mshadow half half t' vs ' ZN7mshadow4half6half tE' typedef mshadow half half t ZN7mshadow4half6half tE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note previous definition is here typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 117 1150 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaSetupArgument void char par6 sizeof par6 size t 48UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par7 sizeof par7 size t 5 var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 117 1260 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaAddressOf par7 sizeof par7 size t 56UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par8 sizeof par8 size t 58UL cudaSuc var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 122 1150 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaSetupArgument void char par6 sizeof par6 size t 48UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par7 sizeof par7 size t 5 var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 122 1260 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaAddressOf par7 sizeof par7 size t 56UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par8 sizeof par8 size t 58UL cudaSuc var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 127 1169 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaSetupArgument void char par6 sizeof par6 size t 48UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par7 sizeof par7 size t 5 var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 127 1279 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaAddressOf par7 sizeof par7 size t 56UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par8 sizeof par8 size t 58UL cudaSuc var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 132 1151 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaSetupArgument void char par6 sizeof par6 size t 48UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par7 sizeof par7 size t 5 var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 132 1261 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaAddressOf par7 sizeof par7 size t 56UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par8 sizeof par8 size t 58UL cudaSuc var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 137 1151 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaSetupArgument void char par6 sizeof par6 size t 48UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par7 sizeof par7 size t 5 var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 137 1261 error invalid application of isizeof' to an incomplete type ' ZNSt3 114numeric limitsIN7mshadow4half6half tEE4typeE' aka ' ZN7mshadow4half6half tE' cudaAddressOf par7 sizeof par7 size t 56UL cudaSuccess return if cudaSetupArgument void cudaAddressOf par8 sizeof par8 size t 58UL cudaSuc var folders g8 n9zkdg4d6xvb2jwc 0mv88wr0000gn T tmpxft 00001a5c 00000000 4 quantize compute 50 cudafe1 stub c 19 16 note forward declaration of ' ZN7mshadow4half6half tE' typedef struct ZN7mshadow4half6half tE ZNSt3 123 libcpp numeric limitsIN7mshadow4half6half tELb0EE4typeE usr local cuda include thrust system cuda detail bulk malloc hpp 110 7 warning private field 'm program break' is not used Wunused private field void m program break usr local cuda include thrust system cuda detail bulk malloc hpp 113 8 warning private field 'm max data segment size' is not used Wunused private field size t m max data segment size usr local cuda include thrust system cuda detail bulk malloc hpp 242 15 warning private field 'm is free' is not used Wunused private field private bool m is free 1 usr local cuda include thrust system cuda detail bulk malloc hpp 243 8 warning private field 'm size' is not used Wunused private field size t m size 8 sizeof size t 1 usr local cuda include thrust system cuda detail bulk malloc hpp 244 8 warning private field 'm prev' is not used Wunused private field block m prev usr local cuda include thrust system cuda detail bulk malloc hpp 445 19 warning private field 'm in use' is not used Wunused private field private unsigned m in use 6 warnings and 11 errors generated make build src operator contrib quantize gpu o Error 1 config mk txt,,"piiswrong,ZihengJiang",2017-04-29 06:50:35,2017-05-06 14:01:56
PR,TOC click unfold,Unfold toc entry on clicking if it is opened,,"kevinthesun,mli",2017-05-05 22:16:20,2017-05-07 08:37:56
PR,doc new sphnix plugin,Updated our doc build pipeline 1 moved functions in sphnix utils into a sphnix plugin located in mxdoc py 2 be able to convert markdown files into jupyter notebooks if placed INSERT SOURCE DOWNLOAD BUTTONS on some where a demo is at 3 removed the dependency to the mxnet notebooks repo instead i converted several notebooks into markdowns and placed at docs 4 mac users can use tests ci build ci build sh doc make C docs html now if docker is installed,,"mli,piiswrong,mli,zackchase,mli",2017-05-04 20:59:36,2017-05-08 05:19:26
PR,doc use debug mode to build,,,mli,2017-05-08 06:01:04,2017-05-08 06:01:12
IS,Question about simple matrix operations using cpp api,Inspired by the example in python when i run this piece of code the output is a random number nearly zero like 2 19945e 38 1 16171e 38 My question is how to write a symbol to perform simple matrix plus properly using mxnet c api It seems to be a stupid question Thanks in advance,,"lx75249,lx75249,lx75249,lx75249",2017-05-08 07:13:20,2017-05-08 09:05:30
PR,Move ctc loss to contrib,piiswrong,,sbodenstein,2017-05-08 07:39:06,2017-05-08 13:07:35
PR,Fix toc link,Modify toc hyperlink to be absolute url,,"kevinthesun,mli",2017-05-08 03:02:39,2017-05-08 13:09:04
PR,Fix for invalid numpy float indexing,Being able to index a numpy array using a float value has long since been deprecated and as of numpy 1 12 it is has been removed It throws Unfortunately due to dynamic typing this will be a difficult bug to find all uses of,,seanpmorgan,2017-05-07 15:58:08,2017-05-08 13:09:23
PR,Fix python3 compatibilities,Two fixes for python3 compatibility In python3 dict values yields a dict values object which does not support indexing It will throw Which is a rather confusing error See,,seanpmorgan,2017-05-07 15:44:57,2017-05-08 13:10:09
PR,example ADD practical functions and options for speech recognition example,Add practical functions and options for speech recognition example add bi graphemes scheme for labeling to make equivalent implement for deep speech 2 add constant learning rate annealing scheduler to make equivalent implement for deep speech 2 add logging options for practical use to enable disable or show every n th logging,,Soonhwan-Kwon,2017-05-07 12:58:09,2017-05-08 13:11:46
PR,doc small changes to tutorials,,,mli,2017-05-08 18:30:04,2017-05-08 18:30:10
PR,Fix left toc link,,,kevinthesun,2017-05-08 18:04:58,2017-05-08 18:52:25
IS,How can I change the code from 'model' to 'module',Operating System Ubuntu 14 04 5 LTS Compiler pycharm Package used Python R Scala Julia Python Code MXNet version 0 9 3,,"alues,qiyuangong,alues,qiyuangong",2017-03-19 11:00:52,2017-05-08 20:28:05
IS,incomplete write when trying to save load module,I am training a model iteratively by saving the model after each minibatch and loading it to continue training When I reach around 1575 iterations I get an error saying there is an incomplete write Any idea what might be wrong The size of the data that i'm loading does not seem to affect this much EDIT it seems this is a memory issue and my machine is out of memory This is because im saving so many files Is there a way I can use epoch end callback so as to overwrite the file saved in the previous iteration the part of the code that is relevant checkpoint mx callback do checkpoint prefix if batch 0 model fit train data tr iter optimizer 'AdaGrad' optimizer params 'learning rate' rate arescale grad' 1 0 batch size num epoch 1 initializer mx init Normal batch end callback mx callback Speedometer batch size 1 epoch end callback checkpoint current ep 1 else beg ep current ep sym arg params aux params mx model load checkpoint prefix current ep model mx mod Module symbol sym model fit train data tr iter num epoch beg ep 1 arg params arg params aux params aux params begin epoch beg ep batch end callback mx callback Speedometer batch size 1 epoch end callback checkpoint current ep 1 Environment info Operating System AWS EC2 Deep Learning AMI Ubuntu Python version and distribution Python 2 7 Error Message INFO root Saved checkpoint to default prefix 0169 params INFO root Epoch 169 Train accuracy 0 998757 INFO root Epoch 169 Time cost 0 665 INFO root Saved checkpoint to default prefix 0170 params INFO root Epoch 170 Train accuracy 0 998660 INFO root Epoch 170 Time cost 0 671 INFO root Saved checkpoint to default prefix 0171 params INFO root Epoch 171 Train accuracy 0 998704 INFO root Epoch 171 Time cost 0 713 INFO root Saved checkpoint to default prefix 0172 params INFO root Epoch 172 Train accuracy 0 998771 INFO root Epoch 172 Time cost 0 673 INFO root Saved checkpoint to default prefix 0173 params INFO root Epoch 173 Train accuracy 0 998686 INFO root Epoch 173 Time cost 0 684 INFO root Saved checkpoint to default prefix 0174 params INFO root Epoch 174 Train accuracy 0 998658 INFO root Epoch 174 Time cost 0 630 23 28 37 include dmlc logging h 300 23 28 37 src io local filesys cc 39 Check failed std fwrite ptr 1 size fp size FileStream Write incomplete Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io10FileStream5WriteEPKvm 0x305 0x7f9360567805 bt 1 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZNK5mxnet7NDArray4SaveEPN4dmlc6StreamE 0x78a 0x7f93602a21fa bt 2 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet7NDArray4SaveEPN4dmlc6StreamERKSt6vectorIS0 SaIS0 EERKS4 ISsSaISsEE 0x9e 0x7f93602a261e bt 3 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so MXNDArraySave 0x50a 0x7f936019c52a bt 4 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f93b982dadc bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call 0x1fc 0x7f93b982d40c bt 6 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48e 0x7f93b9a445fe bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x15f9e 0x7f93b9a45f9e bt 8 python PyEval EvalFrameEx 0x98d 0x5244dd bt 9 python PyEval EvalCodeEx 0x2b1 0x555551 Traceback most recent call last File recommender v2 py line 170 in module epoch end callback checkpoint File usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet module base module py line 436 in fit callback epoch self symbol arg params aux params File usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet callback py line 58 in callback save checkpoint prefix iter no 1 sym arg aux File usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet model py line 344 in save checkpoint nd save param name save dict File usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet ndarray py line 1297 in save keys File usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet base py line 75 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 23 28 37 src io local filesys cc 39 Check failed std fwrite ptr 1 size fp size FileStream Write incomplete Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io10FileStream5WriteEPKvm 0x305 0x7f9360567805 bt 1 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZNK5mxnet7NDArray4SaveEPN4dmlc6StreamE 0x78a 0x7f93602a21fa bt 2 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet7NDArray4SaveEPN4dmlc6StreamERKSt6vectorIS0 SaIS0 EERKS4 ISsSaISsEE 0x9e 0x7f93602a261e bt 3 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so MXNDArraySave 0x50a 0x7f936019c52a bt 4 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f93b982dadc bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call 0x1fc 0x7f93b982d40c bt 6 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48e 0x7f93b9a445fe bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x15f9e 0x7f93b9a45f9e bt 8 python PyEval EvalFrameEx 0x98d 0x5244dd bt 9 python PyEval EvalCodeEx 0x2b1 0x555551,,,2017-04-26 23:36:57,2017-05-08 23:10:41
IS,ERROR segmention fault,Cannot the CSVIter import a file too large my csv is about 1 14G with 7 million lines,,,2017-04-21 07:47:56,2017-05-09 00:11:35
PR,Added reflection padding,Implemented a reflection padding method adapted from Torch Currently has two limitations compared to edge and constant padding 1 Only supports 4D input edge and constant support 5D too 2 Cannot pad by an amount bigger than the input size Behaviour when padding exceeds input size is well defined in numpy bounce back and forth between input edged until padding area is filled and can be added Also improved sanity checks and moved them from forward backward to InferShape,,"matteosal,sbodenstein,sbodenstein,matteosal,sbodenstein,matteosal,sbodenstein,matteosal,sbodenstein,sbodenstein,matteosal,piiswrong,sbodenstein",2017-05-05 12:30:59,2017-05-09 02:36:20
IS,ubuntu source compile error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 1 Compiler g 4 8 4 2ubuntu1 14 04 3 cuda 8 0 44 Package used Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 104162b68853801481338c796078d63899a09913 If you are using python package please provide Python version and distribution python2 7 Error Message compile error src operator tensor multisample op h In instantiation of void mxnet op MultiSampleOpForward const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu generator mxnet op GeneralizedNegativeBinomialSampler src operator tensor multisample op cc 359 1 required from here src operator tensor multisample op h 145 74 error class mshadow Random mshadow cpu float has no member named GetRandInt Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 nothing 2 3,,Soonhwan-Kwon,2017-05-09 03:33:06,2017-05-09 04:35:26
PR,Scala Change version to 0 9 5 SNAPSHOT,,,yzhliu,2017-05-09 08:32:33,2017-05-09 09:04:18
IS,Piecewise constant functions,I have input data that take the form X i j 0 1 or 2 and a continuous output Y i Is there some way to model piecewise constant functions in mxnet,,,2017-04-21 14:19:53,2017-05-09 09:30:48
PR,Fixing LICENSE file and adding NOTICE,The current LICENSE file is a source header I have replaced that with a full Apache 2 0 text file and added a NOTICE file The NOTICE file will need changing post migration when the code is fixed to match Apache Software Foundation formats policies,,hyandell,2017-05-09 07:25:08,2017-05-09 21:26:17
PR,doc improvement softmax metrics and initializer,SoftmaxOutput softmax cross entropy Initializer constant minor fixes in some descriptions that have render problems because of incorrect indentation,,"jiajiechen,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,jiajiechen,jiajiechen,madjam,madjam,madjam,madjam,madjam,madjam,madjam,jiajiechen,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,jiajiechen,jiajiechen,jiajiechen,nswamy",2017-04-23 04:41:52,2017-05-09 23:58:37
PR,DataBatch and NDArrayIter doc modified,mli Please take a look,,"Roshrini,madjam,madjam,madjam,madjam,madjam,zackchase,zackchase,zackchase,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,nswamy,nswamy,Roshrini",2017-05-03 20:55:09,2017-05-10 00:00:12
PR,Explicitly specify quiet in R install version,Solves an installation issue when building the R package from source same symptoms as,,nerdcha,2017-05-09 06:03:49,2017-05-10 00:10:17
PR,API doc improvement Dropout and SoftmaxActivation,Add Dropout and SoftmaxActivation Thanks for reviewing my PR Note Confirmed with Eric SoftmaxActivation will be deprecated,,"jiajiechen,nswamy,nswamy,nswamy,madjam,jiajiechen,jiajiechen,zackchase,zackchase,zackchase",2017-05-03 18:06:45,2017-05-10 00:14:58
PR,Update documentation for mx callback do checkpoint,mli Please review and merge,,"indhub,madjam,zackchase,piiswrong",2017-05-01 23:00:50,2017-05-10 00:29:33
PR,fix cudnn support check,Should be 6020,,sxjscience,2017-05-08 11:50:20,2017-05-10 02:22:05
PR,Update documentation for plot graph,mli Please review and merge,,"indhub,piiswrong,piiswrong,piiswrong,indhub,zackchase,indhub",2017-05-04 07:53:49,2017-05-10 02:43:25
PR,Restruct get started,Restructure get started page to 'why mxnet' and 'installation' getstarted3 Move images to web data Resize images to 60 Make installation as index page krishnamurthy,,"kevinthesun,piiswrong,zackchase,zackchase,zackchase,piiswrong,zackchase,piiswrong,piiswrong,zackchase,piiswrong,zackchase,piiswrong,zackchase,sandeep-krishnamurthy",2017-05-09 00:01:31,2017-05-10 02:50:58
PR,Update documentation of Initializer dumps,Add documentation for return and example of Initializer dumps,,"EvanzzzZ,zackchase,zackchase,EvanzzzZ",2017-05-05 17:56:35,2017-05-10 03:15:20
PR,Doc Improvement RMSProp and RMSPropAlex,Add description for RMSProp update and RMSPropAlex update in optimizer op cc,,"jiajiechen,madjam,madjam,madjam,madjam,piiswrong,zackchase,zackchase,zackchase,zackchase,jiajiechen,jiajiechen,jiajiechen,jiajiechen",2017-05-04 22:02:55,2017-05-10 03:29:24
PR,Docforcs fft ifft,Add documents for countsketch fft and ifft,,,2017-05-07 19:29:50,2017-05-10 04:07:15
IS,how to write the predict python code in BucketingModule with variable length sequence,in my project i try my project with cnn lstm ctc net i start my module with mod mx mod BucketingModule sym gen default bucket key data train default bucket key context contexts and the model is train succeed but in prediction stage i try mx model load checkpoint modelPrefix 50 Cannot find Operator WarpCTC in registry mxnet predict Predictor 'Predictor' object has no attribute 'handle' in bound method Predictor del of mxnet predict Predictor object at 0x7f749cf97a10 ignored but all not succeed how do i wirte the bucketing prediction python code thanks,,"Godricly,Godricly,Godricly,Godricly",2017-05-05 01:48:40,2017-05-10 09:25:22
PR,Improve executor API documentation,Executor class Executor backward Executor copy params from Executor reshape Executor set monitor callback Executor debug str,,"kevinthesun,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,zackchase",2017-05-04 23:02:43,2017-05-10 10:12:07
IS,Problem with implementing tree LSTM is in mxnet,I am implementing tree LSTM is for predicting the negativity positivity of sentences in mxnet and I am using the BucketingModule to do this The tree LSTM is structured similar to the structure of the parse tree of each sentence Therefore each training sample will need a different structure So I am using one bucket per input sample Also not all the LSTM blocks in a single structure have shared weights For example in a tree consisting of n nodes k of the nodes can share the same weights and the other n k can share their weights The partition is not limited to two In fact it often is the case that most of the blocks do not share weights with the others My first problem is with training If I train using isgd' and 'momentum' 0 0 the training works fine However if I change momentum or try to use other optimizers such as 'adam' the training returns an inconsistency shape error due to the self states dictionary in the Updater class in optimizer py It initializes the self states dictionary for the first sentence When the second sentence is fed the index of the states is not necessarily the same as the first sentence But the updater re uses the already created states for the first sentence and fails at look up time for the second equation My second question is how can I do mini batch training under this setting My code is available on github The main file to run is neuralTrig py NOTE In order to run the code the 'allow extra params' flag needs to be sat to True in exec copy params from arg params aux params allow extra params True in function set params self arg params aux params in file executor group py line 332 Environment info Operating System MAC OS X El Capitan Compiler clang 800 0 38 Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 58e334639c4d5a875bb5b8b33036c3fab8ed7115 If you are using python package please provide Python version and distribution Python 2 7 13 Anaconda 4 3 1 x86 64 If you are using R package please provide R sessionInfo Error Message 08 11 32 Users Forough mxnet dmlc core include dmlc logging h 300 08 11 32 src operator nn tensor elemwise op common h 31 Check failed assign dattr vec i Incompatible attr in node at 2 th input expected 1 120 got 120 240 Stack trace returned 9 entries bt 0 0 libmxnet so 0x00000001074d5f35 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x00000001074d3539 ZN4dmlc15LogMessageFatalD1Ev 9 bt 2 2 libmxnet so 0x000000010754de72 ZZN5mxnet2op12ElemwiseAttrIN4nnvm6TShapeEXadL ZNS0 13shape is noneERKS3 EEXadL ZNS0 12shape assignEPS3 S5 EELb1EEEbRKNS2 9NodeAttrsEPNSt3 16vectorIT NSA 9allocatorISC EEEESG RKSC ENKUlPNSB IS3 NSD IS3 EEEEPKcE clESL SN 498 bt 3 3 libmxnet so 0x000000010754db26 ZN5mxnet2op12ElemwiseAttrIN4nnvm6TShapeEXadL ZNS0 13shape is noneERKS3 EEXadL ZNS0 12shape assignEPS3 S5 EELb1EEEbRKNS2 9NodeAttrsEPNSt3 16vectorIT NSA 9allocatorISC EEEESG RKSC 198 bt 4 4 libmxnet so 0x0000000107bf7e92 ZN5mxnet2op13ElemwiseShapeILi3ELi1EEEbRKN4nnvm9NodeAttrsEPNSt3 16vectorINS2 6TShapeENS6 9allocatorIS8 EEEESC 226 bt 5 5 libmxnet so 0x0000000107a11b7e Z12SetShapeTypePKN4nnvm2OpERKNS 9NodeAttrsERKN5mxnet7ContextERKNSt3 16vectorINS6 7NDArrayENSA 9allocatorISC EEEERKiPSF 1326 bt 6 6 libmxnet so 0x0000000107a156b5 MXImperativeInvoke 1093 bt 7 7 ctypes so 0x0000000101e59f57 ffi call unix64 79 bt 8 8 0x00007fff5e40bc90 0x0 140734774688912 Traceback most recent call last File neuralTrig py line 214 in module main File neuralTrig py line 184 in main epoch end callback mx rnn do rnn checkpoint cell 'trainedModel' 1 File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet module base module py line 475 in fit self update File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet module bucketing module py line 401 in update self curr module update File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet module module py line 570 in update kvstore self kvstore File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet model py line 123 in update params updater index num device k g w File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet optimizer py line 690 in call self optimizer update index weight grad self states index File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet optimizer py line 324 in update lr lr wd wd self kwargs File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet ctypes ndarray py line 133 in generic ndarray function c array ctypes c char p c str str i for i in kwargs values File Users Forough anaconda2 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 08 11 32 src operator nn tensor elemwise op common h 31 Check failed assign dattr vec i Incompatible attr in node at 2 th input expected 1 120 got 120 240 Stack trace returned 9 entries bt 0 0 libmxnet so 0x00000001074d5f35 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x00000001074d3539 ZN4dmlc15LogMessageFatalD1Ev 9 bt 2 2 libmxnet so 0x000000010754de72 ZZN5mxnet2op12ElemwiseAttrIN4nnvm6TShapeEXadL ZNS0 13shape is noneERKS3 EEXadL ZNS0 12shape assignEPS3 S5 EELb1EEEbRKNS2 9NodeAttrsEPNSt3 16vectorIT NSA 9allocatorISC EEEESG RKSC ENKUlPNSB IS3 NSD IS3 EEEEPKcE clESL SN 498 bt 3 3 libmxnet so 0x000000010754db26 ZN5mxnet2op12ElemwiseAttrIN4nnvm6TShapeEXadL ZNS0 13shape is noneERKS3 EEXadL ZNS0 12shape assignEPS3 S5 EELb1EEEbRKNS2 9NodeAttrsEPNSt3 16vectorIT NSA 9allocatorISC EEEESG RKSC 198 bt 4 4 libmxnet so 0x0000000107bf7e92 ZN5mxnet2op13ElemwiseShapeILi3ELi1EEEbRKN4nnvm9NodeAttrsEPNSt3 16vectorINS2 6TShapeENS6 9allocatorIS8 EEEESC 226 bt 5 5 libmxnet so 0x0000000107a11b7e Z12SetShapeTypePKN4nnvm2OpERKNS 9NodeAttrsERKN5mxnet7ContextERKNSt3 16vectorINS6 7NDArrayENSA 9allocatorISC EEEERKiPSF 1326 bt 6 6 libmxnet so 0x0000000107a156b5 MXImperativeInvoke 1093 bt 7 7 ctypes so 0x0000000101e59f57 ffi call unix64 79 bt 8 8 0x00007fff5e40bc90 0x0 140734774688912 Minimum reproducible example Here is my code on git The main file to run is neuralTrig py Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error Here is my code on git The main file to run is neuralTrig py If we set the momentum to something other than zero it fails If we use another optimzier it also fails NOTE In order to run the code the 'allow extra params' flag needs to be sat to True in exec copy params from arg params aux params allow extra params True in function set params self arg params aux params in file executor group py line 332,,szha,2017-05-09 17:46:11,2017-05-10 14:21:59
IS,MXNet initialization with Constant One or Zero returning errors,I am using mxnet initializer Mixed to provide a mixed initialization and get an error whenever I try to use Constant One or Zero Examples below net some network that has been created init mx initializer Mixed 'fcout bias' ' ' mx init Constant 10 0 mx init Normal 0 01 Traceback most recent call last File stdin line 1 in module AttributeError 'module' object has no attribute 'Constant' ' Same error with mx init One or Zero But if I try init mx initializer Mixed 'fcout bias' ' ' mx init Uniform 10 0 mx init Normal 0 01 no error Have the other initializers not been implemented in the MXNet version present in the AMI How do I make sure I have all the latest features Environment info Operating System Ubuntu Compiler AWS EC2 Deep Learning AMI Package used Python R Scala Julia Python Python version and distribution Python 2 7 6 Error Message AttributeError 'module' object has no attribute 'Constant' Minimum reproducible example all layers net get internals all layers list outputs wouldata' 'fc1 weight' 'fc1 bias' 'fc1 output' 'ac1 output' 'kl1 moving avg' 'kl1 output' wouldl1 output' 'fcout weight' 'fcout bias' 'fcout output' 'acout output' isoftmax label' 'ce preds' init mx initializer Mixed 'fcout bias' ' ' mx init Constant 10 0 mx init Normal 0 01 Traceback most recent call last File stdin line 1 in module AttributeError 'module' object has no attribute 'Constant',,"pluskid,pluskid,pluskid,pluskid,pluskid,pluskid",2017-05-08 22:29:33,2017-05-10 14:30:30
IS,Using CrossEntropyLoss in the output layer,I am trying to use the CrossEntropyLoss layer as an output and get an error saying it needs 2 inputs data and label But when I instead use SoftmaxOutput I only need to provide the data and the label is provided via the training iterator Is there a way I can use the cross entropy loss with only providing data Minimum reproducible example depth 3 data mx sym Variable wouldata' layer 1 is as defined net mx sym FullyConnected data data name 'fc1' num hidden 64 net mx sym Activation data net name 'ac1' act type relu net mx sym Dropout data net name wouldl1' p 0 1 remaining hidden layers for layer in xrange 2 depth 1 net mx sym FullyConnected data net name 'fc' str layer num hidden 64 net mx sym Activation data net name 'ac' str layer act type relu net mx sym Dropout data net name wouldl' str layer p 0 1 output layer net mx sym FullyConnected data net name 'fcout' num hidden 10 net mx sym SoftmaxOutput data net name isoftmax' net rt CrossEntropyLoss data net return net uncommenting the second last line above and commenting the line above it gives me the error,,,2017-04-19 21:06:13,2017-05-10 16:30:16
PR,Adding back all other OS install guide,Critical changes We need to get this merged ASAP Adding back all install guides that were accidentally deleted In each of this page for Python installation I added a Note and hyperlink to new install guide we have written and tested Next steps Link these pages appropriately from main install page Test and clean up the install guide There is no new content I locally built and tested the rendering Can we please merge this soon We have broken links,,sandeep-krishnamurthy,2017-05-10 16:30:31,2017-05-10 18:18:13
PR,Improve howto modeling,Fix issues in howto modeling section,,kevinthesun,2017-05-09 22:25:04,2017-05-11 01:21:33
PR,Docbash,Architecture docs index md and program model md rewrite,,"zackchase,sergeykolychev,piiswrong,zackchase,piiswrong,piiswrong,zackchase",2017-05-06 18:10:39,2017-05-11 01:26:01
PR,Docs changes to BucketSentenceIter Optimizer ImageIter,,,"lxn2,piiswrong,piiswrong,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,piiswrong,zackchase,lxn2,zackchase",2017-05-05 00:02:13,2017-05-11 01:36:46
PR,update cudnn version,update cudnn version,,alues,2017-05-10 22:05:29,2017-05-11 01:37:23
PR,DOCS Add docs for several APIs,zackchase,,"reminisce,piiswrong,reminisce,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,reminisce",2017-05-04 23:42:55,2017-05-11 05:06:02
PR,Update documentation for scale down and resize short in python mxnet image py,mli please see if it is OK There is something wrong with my original branch so I made some changes,,"madjam,madjam,madjam,madjam,zackchase",2017-05-03 02:18:41,2017-05-11 05:07:38
PR,fix batchnorm has no effect on fully connected layer in speech recognition example,Fixed the bug that batchnorm has no effect on one last layer fully connected of deep speech 2,,Soonhwan-Kwon,2017-05-11 06:30:50,2017-05-11 08:17:39
IS,A suggestion for future direction,If you are an experienced neural network programmer you should know what is missing in the current state of the art deep learning DL toolkits such as Theano Caffein Tensorflow etc So I am writing this post to give you guys some suggestion for the MXNET development And I hope to see some light in MXNET which is a new and growing neural network NN toolkit Core Issues 0 Coding efficiency Einstein has said if two theories can both explain the same phenomenon we typically adopt the simpler one Analogously if two NN programming languages can implement the same neural network we prefer the one with a simpler code This remind me one of the remarkable difference between Theano and Tensorflow where you can use X 2 4 0 2 extract a slice from a rank 4 Tensor but you have to call a complicated slicing function in Tensorflow For MXNET I realize that many of your API functions requires a very long prefix e g I just want to ask you a simple question how complex can it be just to write down y tanh W x b Do you really need to make things that complicated just to achieve the purpose mentioned in the next point 1 Auto batching Whenever we want to implement a NN from a research paper most of the effort we spend is in fact in doing the batching The original mathematical equations are in fact quite straight forward to implement it is the batching which is troublesome you may refer to the paper titled deep learning with dynamic computation graphs which shows some on going effort in the Google DL team Up to today no state of the art NN toolkit can automatically handle batching This might sound good for you 2 Dynamic computation graph The ability to change computation graph dynamically during runtime is crucial for future artificial intelligence This is because even for human and animals the structure of the brain is neural network changes as the body grow up and learns We can see that nature does not evolve species with fixed structure neural networks this implies that all static computation graph NN toolkits must somehow have some fundamental limitations which is still not very clear at state of the art level of research Up to today only MXNET and DyNet support dynamic computation graph also known as imperative programming which define by run most NN toolkits only support static computation graph also known as declarative programming which define and run There are two issues here 1 performance efficiency we all know that declarative programming can compile the graph to make it run faster however nowadays we have the JIT just in time compiler concept if NN toolkit can be designed to dynamically compile the computation graph while running the graph then it will be great for imperative programming to catch up with declarative programming in terms of execution efficiency 2 modelling capability can we say that imperative programming is a functional superset of declarative programming Or in another word whatever tasks that can be achieved in declarative programming can also be achieved in imperative programming but not vice versa At least I believe so a dynamically varying structure can achieve something that a fixed structure network cannot achieve because given the data you do not know what network structure is optimal for this task You need to try all possible structures for a fixed structure network which there are too many but the dynamic network will learn the structure instead For MXNET is it really necessary to achieve both declarative programming and imperative programming If one is a superset of the other why not just focus on the superset In conclusion I do not believe MXNET is a very promising DL toolkit given its current API structure unless there is some big leap breaking changes,,,2017-02-03 03:59:52,2017-05-11 09:32:03
IS,Lack of control flow operators functionally inferior to Tensorflow and Theano,After going through so much inconveniences in Tensorflow and Theano now I am trying MXNET I have read through the MXNET API there is no control flow operation i e functionally equivalent to tf select tf cond or theano switch theano ifelse This missing functionality will limit the toolkit is capability in handling variable length sequences Take note that without control flow operators you can still perform LSTM based variable sequence length prediction using masking However there is still something that you cannot do For example in training a RNN language model I would like to reset the state vector to some specific trainable parameter whenever I encounter a sentence begin i e s So I hereby hope the developer can implement this important function Thanks,,"shivarajugowda,piiswrong,piiswrong",2016-12-31 13:43:57,2017-05-11 10:29:44
PR,update kvstore docs,6000,,"eric-haibin-lin,madjam,madjam,piiswrong,piiswrong,piiswrong,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,eric-haibin-lin",2017-05-04 23:00:41,2017-05-11 12:21:24
PR,updated docstring for set lr mult and set wd mult,screen shot 2017 05 04 at 11 45 55 pm screen shot 2017 05 04 at 11 45 45 pm,,"bikestra,piiswrong,piiswrong,zackchase,zackchase,zackchase,zackchase,zackchase,bikestra,piiswrong,piiswrong,nswamy",2017-05-05 06:47:38,2017-05-11 12:59:31
PR,Enabling other language options on install page,Enabled other Language R Scala Julia options on the install page Fixed broken link on home page Can you please review and merge this changes,,sandeep-krishnamurthy,2017-05-11 17:33:15,2017-05-11 23:59:04
PR,Added documentation for random crop,Added parameter documentation and examples,,"gurumurthys,piiswrong,gurumurthys,zackchase,zackchase,zackchase,zackchase,gurumurthys,pracheer,gurumurthys,gurumurthys,nswamy,piiswrong,gurumurthys",2017-05-05 05:20:21,2017-05-12 04:34:01
IS,Cannot even perform matrix multiplication using CPU,On Machine 1 Ubuntu 16 04 2 LTS 64 bit I have installed a fresh copy of mxnet using the standard command line pip install mxnet cu80 Just trying the simplest maths function matrix multiplication fails as follows So I cannot even get started on my Desktop that is quite disappointing However I have done exactly the same thing on Machine 2 with identical OS version miniconda2 etc surprisingly it works without crashing My suspicion is the following Machine 1 has CPU Intel R Core TM i5 3470 CPU 3 20GHz which only support AVX instruction set but not AVX2 instruction set Machine 2 has CPU Intel R Xeon R CPU E5 2643 v4 3 40GHz which support both AVX and AVX2 instruction set MXNET is CPU matrix multiplication code is compiled with AVX optimization which assumes both AVX and AVX2 support However it does not check for AVX2 instruction support How can the developers be so inexperienced on instruction level optimization Numpy Scipy Tensorflow Theano etc does not have these kind of problems though,,"piiswrong,szha,szha,szha",2017-05-11 08:00:37,2017-05-12 04:35:02
PR,Half2 implementation,,,"ap-hynninen,piiswrong,eric-haibin-lin,ap-hynninen,piiswrong,piiswrong,ap-hynninen,piiswrong",2017-05-08 21:50:44,2017-05-12 04:36:17
IS,Mask for RNN symbol,Hi I noticed that mxnet has supported RNN symbol for a while How to mask the padded zeros And I also found this what is element mask Doc says mxnet symbol element mask args kwargs rhs elmentwise mask lhs with broadcast What does this elmentwise mask mean Multiply by zero,,piiswrong,2016-09-28 06:05:08,2017-05-12 05:30:46
IS,different foward results on two machines,Hi all We managed to train a segmentation network and use it to get predicted results in forward step The results are the same for several machines However on one old machine with cpu i5 new gpu nvidia1060 the results are different The data before the foward step is exactly the same and the returned data after network foward is different No batch norm or dropout is used in the network Any suggestions,,"piiswrong,piiswrong",2017-05-11 17:10:38,2017-05-12 06:23:29
IS,Docs 404 Not Found,docs 404 Not Found,,"ysh329,ysh329",2017-05-12 07:55:12,2017-05-12 07:56:20
IS,Using imperative programming returns incompatible attr in node error,I am attempting to reproduce the network I have with imperative programming in R While using the symbolic programming the network is a simple mxnet with one hidden layer and 20 nodes NHIDDEN 20 array batch size 200 act mx symbol Variable data fc1 mx symbol FullyConnected act num hidden NHIDDEN name 'fc1' act mx symbol Activation data fc1 act type tanh name act fc2 mx symbol FullyConnected act num hidden 1 name 'fc2' My imperative program is W mx symbol random normal loc 0 scale 1 shape c 1 NHIDDEN b mx symbol random normal loc 0 scale 1 shape c 1 NHIDDEN W out mx symbol random normal loc 0 scale 1 shape c 1 NHIDDEN b out mx symbol random normal loc 0 scale 1 shape c 1 1 hidden layer mx symbol tanh mx symbol dot act W b y out mx symbol dot hidden layer W out b out This fails with the error mxnet dmlc core include dmlc logging h 304 15 10 36 src operator nn tensor elemwise op common h 33 Check failed assign dattr vec i Incompatible attr in node plus4 at 1 th input expected 200 1 got 20 1 How should I define this simple network using the imperative style of programming,,,2017-05-11 20:16:16,2017-05-12 13:00:20
PR,Fixed typos,,,"zackchase,zackchase",2017-05-11 13:53:44,2017-05-12 18:28:50
PR,set params documentation changes,Corrected a typo in Module set params documentation,,"anirudh2290,anirudh2290",2017-05-12 23:39:06,2017-05-13 03:54:12
PR,mxnet io issue fix,1 Add callback API 2 Change website title from MXNet Documents to MXNet A Scalable Deep Learning Framework 3 redirect get started and get started index html to get started why mxnet html so that search engine will point to why mxnet page,,kevinthesun,2017-05-12 22:26:14,2017-05-13 03:56:30
PR,Pre nn patch,,,"piiswrong,piiswrong,lx75249,piiswrong",2017-05-11 05:32:23,2017-05-13 13:14:43
PR,Fix RMSProp update rule,When using Alex is version of RMSProp to train a network I found it sometimes update some network weights to NaN After that I found the RMSProp implementation is a little bit different from Alex is paper In equation 40 the epsilon should be under the square root I guess I got NaN because state n state g state g can sometimes be something like 1e 20 because of floating point arithmetic error Indeed after I applied this patch it stops update network weights to NaN,,"sifmelcara,piiswrong,sifmelcara,sergeykolychev",2017-05-13 13:37:26,2017-05-13 21:53:25
PR,Fixing typos,,,madjam,2017-05-14 06:29:46,2017-05-14 06:30:34
PR,Fix minor typos and grammar issues in tutorial docs,There are some minor grammar issues and typos in the documentation I am correcting ones I see as I continue reading the tutorials,,"mqtlam,mqtlam,mqtlam",2017-05-10 15:24:39,2017-05-14 15:55:59
PR,fixing typos,,,madjam,2017-05-14 06:32:00,2017-05-14 17:46:01
PR,Fix minor typos and grammar issues in tutorial docs,There are some minor grammar issues and typos in the tutorial documentation I have gone through the Python Basics docs and fixed some typos grammar issues I did my best to stay true to the issue and fix only obvious typos and grammar issues There is still more work to do in terms of fixing sentences and paragraphs but that can be done in a separate pull request Originally from 6192 I messed up that one,,mqtlam,2017-05-14 15:55:02,2017-05-14 19:14:30
PR,Perl Autograd bugfixes for the visualization of convolution,Python is autograd code has not changed in a while so it looks like a semi permanent version and I decided to port it in preparation of future porting of NN interface,,"sergeykolychev,sergeykolychev",2017-05-12 00:39:57,2017-05-14 19:55:45
PR,add tensorflow style RMSProp,add tensorflow style RMSProp where the epsilon parameter is inside the sqrt operation For google is paper this version of RMSprop is used,,piiswrong,2017-05-12 04:20:02,2017-05-14 20:26:44
PR,convert float to int and use matplotlib,I have updated 2 items 1 using np zeros with floats has been deprecated so I am casting the floats to int is 2 using matplotlib instead of cv2 to plot the image This is because I was running this on a Jupyter notebook on AWS using deep learning AMI and cv2 does not plot when running Jupyter over ssh but matplotlib is able to plot both locally and over ssh on AWS EC2 I am also making a notebook example for dc gan,,yash1,2017-05-14 21:00:41,2017-05-14 23:50:02
IS,Compile lastest MXNet failed with cude 8 0,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 Package used Python R Scala Julia MXNet version 0 9 5 Or if installed from source yes MXNet commit hash git rev parse HEAD 38f7c5584016e92ba1e0ee1b00ea6632740f67ce If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace What have you tried to solve it 1 set USE CUDA PATH to use cude 7 5 problem solved,,"Ldpe2G,Ldpe2G",2017-05-14 14:08:03,2017-05-15 02:48:06
IS,I use PythonNet to import mxnet in the C and window10 but have a bug other import is normal,,,,2017-02-24 09:44:11,2017-05-15 03:27:40
PR,Rearranged and clean up of Contribute md doc,zackchase Please take a look,,"Roshrini,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,nswamy,nswamy,nswamy,zackchase,Roshrini",2017-05-09 23:08:41,2017-05-15 05:01:50
PR,Improvements to ndarray tutorial,,,"madjam,nswamy,madjam,nswamy,madjam",2017-05-14 01:04:33,2017-05-15 05:53:56
PR,Symbol tutorial improvements,,,"madjam,nswamy",2017-05-14 02:38:58,2017-05-15 05:54:24
IS,Predict the class of a text with mxnet text classfier,I use the mxnet text classifier for train a model and now I have the params files But now I want to predict the class of a given text and I do not find anything in the codes what should I do Thanks,,,2017-05-09 04:58:25,2017-05-15 06:40:57
IS,Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function,I compile the libmxnet dll on my compiling PC and successfully run it But when I copy the dll to my testing PC Errors come out The cuda versions are the same 8 0 44 How can I solve it Compiling PC Environment info Operating System WIN10 64 Compiler VS 2015 V140 MXNet version 0 9 4 cuda version 8 0 44 Hardware GTX 1060 6G Testing PC Environment info Operating System WIN10 64 MXNet version 0 9 4 libmxnet dll copyed from the Compiling PC cuda version 8 0 44 Hardware GTX TITAN X Error Message 12 05 06 F OpenSource mxnet dmlc core include dmlc logging h 300 12 05 06 f opensource mxnet mshadow mshadow cuda tensor gpu inl cuh 106 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function 12 05 06 F OpenSource mxnet dmlc core include dmlc logging h 12 05 06 F Op enSource mxnet dmlc core include dmlc logging h 300 12 05 06 f opensource mx net src engine threaded engine h 336 12 05 06 f opensource mxnet mshadow m shadow cuda tensor gpu inl cuh 106 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function An fatal error occurred in asynchronous engine operation If you do not know wha t caused this error you can try set environment variable MXNET ENGINE TYPE to N aiveEngine and run with debugger i e gdb This will force all operations to b e synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging 300 12 05 06 f opensource mxnet mshadow mshadow cuda tensor gpu inl cuh 10 6 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function 12 05 06 F OpenSource mxnet dmlc core include dmlc logging h 300 12 05 06 f opensource mxnet src engine threaded engine h 336 12 05 06 f opensource mxnet mshadow mshadow cuda tensor gpu inl cuh 106 Check failed err cudaS uccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function An fatal error occurred in asynchronous engine operation If you do not know wha t caused this error you can try set environment variable MXNET ENGINE TYPE to N aiveEngine and run with debugger i e gdb This will force all operations to b e synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,,2017-02-27 06:09:34,2017-05-15 14:01:53
PR,Fix a spelling mistake,,,Zehaos,2017-05-15 09:36:50,2017-05-15 15:30:39
PR,fix prediction bugs in speech recognition example,fix wrong prediction output when using batch norm fix LoadUnicode error when it is prediction mode,,Soonhwan-Kwon,2017-05-15 07:54:03,2017-05-15 15:31:35
PR,Improvements to predict image tutorial,,,madjam,2017-05-14 17:09:43,2017-05-15 15:32:57
PR,MNIST tutorial improvements,,,"madjam,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,madjam",2017-05-13 05:59:15,2017-05-15 15:43:01
PR,Correctly import Caffe BatchNorm,The third blob of BatchNorm in Caffe was mistakenly interpreted as the moving average is exponential smoothing factor while it is actually a scaling factor At test time accumulated mean and average are divided by this factor before running the forward as discussed here code L100 L105 This PR rescales mean and variance during conversion to reproduce the intended forward behaviour Moreover the variance regularization epsilon is extracted Turns out that CuDNN requires that to be bigger than CUDNN BN MIN EPSILON currently set to 1e 5 Is there a way to get it at conversion runtime,,"matteosal,sbodenstein,matteosal",2017-05-09 09:37:40,2017-05-15 16:43:59
PR,seems no need to use std pair,,,mli,2017-05-06 00:20:31,2017-05-15 22:33:51
PR,update resnet of faster rcnn configuration,I updated faster rcnn is resnet configuration 1 config py in config py FIXED PARAMS are defined and they are searched in module py as L43 L49 According to the results I suggest you to update,,"fullfanta,piiswrong,fullfanta,piiswrong,precedenceguo,piiswrong,fullfanta,piiswrong,fullfanta,piiswrong,piiswrong,fullfanta,fullfanta,fullfanta,precedenceguo",2017-04-27 08:48:44,2017-05-16 03:24:34
PR,Batch Norm rewrite without mshadow 1D 2D 3D float16 float32 float64 as well as operator gtest framework,Note that batch norm cu and batch norm inl h are almost entirely new code However github is not rendering the diff by default Click View so see them test op h all new code along with test util h and test perf h also is not shown for the same reason Performance bs 128 c 3 h 28 w 28 OLD CPU BatchNormV1Prop cpu 2D Timing Forward 2828 16 ms avg 5 65631 ms X 500 passes BatchNormV1Prop cpu 2D Timing Backward 20908 4 ms avg 41 8169 ms X 500 passes NEW CPU BatchNormProp cpu 2D Timing Forward 788 777 ms avg 1 57755 ms X 500 passes BatchNormProp cpu 2D Timing Backward 322 013 ms avg 0 644026 ms X 500 passes OLD GPU BatchNormV1Prop gpu 2D Timing Forward 5 365 ms avg 0 01073 ms X 500 passes BatchNormV1Prop gpu 2D Timing Backward 15 483 ms avg 0 030966 ms X 500 passes NEW GPU BatchNormProp gpu 2D Timing Forward 3 514 ms avg 0 007028 ms X 500 passes BatchNormProp gpu 2D Timing Backward 4 787 ms avg 0 009574 ms X 500 passes,,"cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,yuruofeifei,yuruofeifei,yuruofeifei,reminisce,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,sxjscience,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,yuruofeifei,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01",2017-04-21 23:08:49,2017-05-16 03:27:29
PR,fix bug in update metric,variable labels in for loop covers the input variable This makes examples rcnn fails when training on multi gpus Did the CI covers multi gpu tests I think this bug could be easily found when run multi gpu tests,,luoyetx,2017-05-16 03:22:12,2017-05-16 03:29:39
PR,Add clang format to gitignore,Used with clang format binary and a vim plugin for C style guide check,,anirudh2290,2017-05-16 00:55:07,2017-05-16 03:42:22
PR,Fixed Broken links at various Documentation files,What were proposed in this Pull Request Broken links has been fixed for following documentations pages R package README md README md docs api scala index md docs tutorials index md docs tutorials scala char lstm md docs tutorials scala mnist md docs tutorials scala mxnet scala on intellij md Can you please review this Pull Request and merge if looks good,,"chetkhatri,chetkhatri,mli,chetkhatri",2017-05-16 10:39:19,2017-05-16 15:40:26
PR,Ability to set omp threads at runtime,Discussed in 6069,,sbodenstein,2017-05-05 12:52:07,2017-05-16 15:42:29
PR,getLogger api documentation changes,log getLogger api documentation changes,,"anirudh2290,pracheer,pracheer,pracheer,pracheer,piiswrong,anirudh2290,nswamy,ZihengJiang,piiswrong,anirudh2290",2017-05-05 18:09:44,2017-05-16 15:44:41
PR,RNN residual connections,Added ResidualCell ModifierCell for vertical connections in stacked RNNs,,fhieber,2017-05-16 06:43:19,2017-05-16 16:20:58
PR,Adding mxnet slack and apache email list details,Adding How To for joining mxnet apache mailing list and slack channel details Can we review and merge this changes This is an urgent requirement from Dominic to present in ApacheCon,,"sandeep-krishnamurthy,zackchase,zackchase,zackchase,mli,mli,sandeep-krishnamurthy",2017-05-16 16:50:53,2017-05-16 17:30:06
PR,Lint for Caffe converter,Add caffe convert to pylint,,"mli,mli",2017-05-16 03:35:42,2017-05-16 18:04:55
PR,Updated Contributors md,Updated CONTRIBUTORS md CC,,"chetkhatri,chetkhatri",2017-05-16 19:17:40,2017-05-16 21:50:24
PR,Fixed Broken links at various Documentation files,What were proposed in this Pull Request Broken links has been fixed for following documentations pages R package README md README md docs api scala index md docs tutorials index md docs tutorials scala char lstm md docs tutorials scala mnist md docs tutorials scala mxnet scala on intellij md Can you please review this Pull Request and merge if looks good,,"chetkhatri,chetkhatri",2017-05-16 19:21:17,2017-05-16 22:01:47
PR,mshadow updated to include half2 inline change,,,"ap-hynninen,piiswrong,ap-hynninen,piiswrong,ap-hynninen,ap-hynninen,piiswrong",2017-05-16 17:29:18,2017-05-17 00:00:23
PR,Fix a broken link,Fix a broken link,,kemaswill,2017-05-16 23:29:56,2017-05-17 02:31:26
PR,Fix typos in cnn text classification example,Fix several typos in cnn text classification example,,kemaswill,2017-05-16 23:53:04,2017-05-17 02:31:49
PR,Added Jetson setup guide to docs,,,"arank,mli,arank,arank",2017-05-12 21:00:37,2017-05-17 03:05:09
IS,cpp package multiple GPU support,Does cpp package support multiple GPUs parallel training I am trying to implement A3C using cpp package I saw another thread How to run on multiple GPUs python It seems I need to use module instead of bind I could not find module in cpp package Do I need to use KVStore directly How How mxnet cpp SetOptimizer works,,lx75249,2017-05-16 23:03:41,2017-05-17 08:02:38
IS,Soft thresholding aka L1 regularization,The Lasso can easily be implemented using a squared error LinearRegression loss and soft thresholding on the weights i e setting weights between t and t to zero in the backward pass Is this possible to implement,,,2017-04-20 10:03:02,2017-05-17 12:35:06
PR,Added EarlyStopping strategy into callback py for python users,Added EarlyStopping class in callback py for python users to use early stopping strategy The modifications are in both callback py and base module py This is for solving 4965 and 3737 This new class is following the style of Keras which gives a patience for users to control the rounds they can wait before cancelling out the training,,,2017-05-17 12:08:42,2017-05-17 15:19:21
PR,Fixed Windows ALE and Python3 compatibility issue,Tested on Windows ALE and Python3 64 bit,,,2017-05-17 00:17:14,2017-05-17 15:19:51
PR,Fixed ALE Python3 compatibility issue,Tested the proposed fixes in Windows ALE and Phyton3 64 bit,,"piiswrong,piiswrong",2017-05-17 00:15:25,2017-05-17 15:20:03
PR,Cleaning up NNPACK installation and broken links,Added installation instructions for NNPACK because Installation guide in NNPACK repo is not compatible with MXNet Fixed broken links Few minor language corrections Can you please review this,,sandeep-krishnamurthy,2017-05-17 05:24:12,2017-05-17 15:29:42
PR,Modified device build guides,added docs for Jetson TX2 install doc and modified raspberry pi install guide,,arank,2017-05-17 03:15:38,2017-05-17 15:32:19
PR,Misc docs improvements,,,madjam,2017-05-17 00:51:36,2017-05-17 15:33:05
PR,doc enable doc build in ci,,,mli,2017-05-16 21:46:46,2017-05-17 15:34:11
PR,Fixed MXNET R Reference manual broken link,What were proposed in this Pull Request Broken link fixed for R Package Reference manual,,"chetkhatri,chetkhatri,mli,chetkhatri,piiswrong",2017-05-16 22:06:46,2017-05-17 15:35:12
PR,Fixing review comments for mxnet social channels,Fixing review comments for PR by,,"sandeep-krishnamurthy,madjam,chetkhatri",2017-05-16 20:04:55,2017-05-17 15:36:55
PR,Add install btn,1 Change 'Get Started' tab to 'Install' 2 Add 'Install' Button addinstallbtn2,,"kevinthesun,piiswrong,zackchase,kevinthesun,piiswrong",2017-05-16 18:50:30,2017-05-17 15:38:13
PR,cpp package Add monitor optimizers metrics and bug fixes,Force MSVC to use UTF 8 Recent doc updates contain UTF 8 characters MSVC regards source files as system code page by default cpp package supports MSVC and python 3 Add monitor interface Add new optimizers RMSProp AdaGrad AdaDelta Adam Add new metrics MAE MSE RMSE PSNR Fix initializer bugs Fix symbol loading bug,,"lx75249,sifmelcara,sifmelcara,sifmelcara,sifmelcara,lx75249,Hebali,lx75249,piiswrong,lx75249",2017-04-29 07:15:09,2017-05-17 16:57:38
PR,Added comments for getting data and set default data model path,Added additional comments for getting data and set the default data path and model path in order to avoid assertion error when running the example without a clue Exception in thread main java lang AssertionError assertion failed at scala Predef assert Predef scala 156 at ml dmlc mxnetexamples rnn TrainCharRnn main TrainCharRnn scala 40 at ml dmlc mxnetexamples rnn TrainCharRnn main TrainCharRnn scala at sun reflect NativeMethodAccessorImpl invoke0 Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 62 at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43 at java lang reflect Method invoke Method java 498,,"jamesliu,jamesliu,Ldpe2G,jamesliu,Ldpe2G",2017-05-15 04:02:41,2017-05-17 16:58:05
PR,add mshadow lint,add missing code group depend on and,,"yajiedesign,piiswrong,yajiedesign,yajiedesign,piiswrong,yajiedesign",2017-04-25 13:12:11,2017-05-17 16:59:00
IS,fine tuning and freezing layers,I was trying to freeze conv layers of vgg16 and only train the last few fc layers I followed and according to when we provide arg params in mx modul fit function the value here will be used to initialize the module parameters unless they are already initialized by the user via a call to init params or fit arg params has higher priority to initializer So it seems that the parameters from the pre trained model are used as initial values instead of frozen Am I correct How to freeze the parameters of layers that I do not intend to train Thanks a lot for your help,,piiswrong,2017-05-12 15:56:50,2017-05-17 18:13:51
IS,are messages in model fit logged into some file,when I ran the model fit function it printed on screen INFO root Epoch 0 Batch 8 Speed 4 61 samples sec Train accuracy 0 083333 INFO root Epoch 0 Batch 16 Speed 4 60 samples sec Train accuracy 0 109375 INFO root Epoch 0 Batch 24 Speed 4 58 samples sec Train accuracy 0 203125 INFO root Epoch 0 Batch 32 Speed 4 57 samples sec Train accuracy 0 171875 instead of printing on screen can such messages be saved to a log file Thanks,,,2017-05-12 15:17:57,2017-05-17 18:14:21
PR,Fix grad req error for binary broadcast op,issuecomment 301247606 This PR fixes the correctness issue For training speed slow down will investigate further,,"reminisce,eric-haibin-lin",2017-05-17 16:33:16,2017-05-17 18:27:50
PR,Using the right image url in tutorial,,,madjam,2017-05-17 19:47:28,2017-05-17 19:55:10
PR,Updated CONTRIBUTORS md,,,jamesliu,2017-05-17 22:56:08,2017-05-17 22:57:08
PR,Minor NDArray tutorial fixes,,,madjam,2017-05-17 22:58:10,2017-05-17 22:58:18
PR,Update index md,Removing broken link to deprecated char lstm tutorial,,zackchase,2017-05-17 22:59:26,2017-05-17 23:00:11
PR,Fix typos and correct syntax for several examples,Fix typos and correct syntax for several examples,,"kemaswill,piiswrong,piiswrong,mli,kemaswill",2017-05-17 03:45:25,2017-05-17 23:11:00
PR,fixed some APIs documentation used in Tutorial,Accuracy add formula sum add example for using axis Tutorial ndarray replace deprecated API concatenate with the current API concat,,"jiajiechen,zackchase,zackchase,mli,mli,mli,jiajiechen,piiswrong,jiajiechen,piiswrong",2017-05-12 21:08:58,2017-05-18 03:59:47
PR,Fix the errors in fcn xs when using cut off size mode,Using int instead of round because of the error TypeError slice indices must be integers or None when 'cut off size' is not None,,SCP-173-cool,2017-05-18 03:40:03,2017-05-18 04:06:45
PR,remove hack,now that there is ones like there is no need for using symbol internal thus removing reference to the internal function for the ease of maintenance,,szha,2017-05-17 22:57:03,2017-05-18 04:07:00
PR,mnist tutorial fix image rendering in docs fix toc rendering,,,"madjam,madjam",2017-05-17 20:48:20,2017-05-18 04:08:18
PR,Docs fixes,,,madjam,2017-05-17 18:55:50,2017-05-18 04:09:23
PR,Fix several typos,Fix more typos,,kemaswill,2017-05-18 04:27:04,2017-05-18 04:37:26
PR,Added documentation for imdecode,Added documentation for imdecode Removed one of the parameters out as it was not used The code crashes when flag 0 and to rgb is set to 1 default as it internally tries to convert the image into opencv format using cvtColor Hence I have added documentation that specifies to rgb to be set to 0 when flag 0,,"gurumurthys,piiswrong,zackchase,gurumurthys,gurumurthys,gurumurthys,mli,pracheer,piiswrong,gurumurthys",2017-05-05 04:48:07,2017-05-18 04:40:11
PR,Adding docstring for IRHeader,I used as a guide to write docstring for namedtuple directly beneath it I am not sure if this documentation actually adds clarity or is in fact not required Either way creating a PR for any comments Feel free to close if this is not required,,"maddyonline,piiswrong,maddyonline,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,piiswrong,maddyonline,piiswrong,nswamy,maddyonline",2017-05-05 00:12:27,2017-05-18 04:42:10
IS,How can I get several layers is features at the same time using mxnet cpp,mxnet cpp package example feature extract feature extract cpp net Symbol Load model Inception BN symbol json GetInternals global pool output This code can only get one layer is features How can I get several layers is features at the same time How can I get the final output and get several layers is features at the same time,,,2017-04-27 12:34:42,2017-05-18 05:59:31
PR,Cleaning up tuning mxnet for performance,Cleaned up issues with performance tuning How Tos Verified compiling with profiler flag setting environment variables for profiling and visualizing profile output in the browser Can you please review and merge the change,,"sandeep-krishnamurthy,piiswrong,madjam,sandeep-krishnamurthy,sandeep-krishnamurthy",2017-05-18 02:46:44,2017-05-18 16:22:28
PR,Tutorial for large scale image classification,mli Please review and merge,,"indhub,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam",2017-05-16 20:26:08,2017-05-18 21:14:33
IS,S3 integration does not work,Environment info Operating System Ubuntu Compiler GNU g Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 479538e8d33935ac3898752ca3172dd2e9884d23 If you are using python package please provide Python version and distribution Python 3 6 0 Continuum Analytics Inc If you are using R package please provide R sessionInfo Error Message Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Compile MXNet with USE S3 1 2 Run test conv py with path modified to use S3 What have you tried to solve it 1 Tried setting DMLC USE S3 directly in config mk That does not fix the issue I have attached the build log here log txt,,"indhub,szha,indhub,indhub",2017-05-18 20:15:35,2017-05-18 21:25:18
PR,Fixing broken links,Made a pass through to fix broken links,,sandeep-krishnamurthy,2017-05-18 21:31:30,2017-05-18 22:13:03
PR,Minor edits to architecture notes,,,madjam,2017-05-18 19:19:12,2017-05-18 22:16:37
PR,Improve API Style,1 Change API arguments style Use normal font weight normal font style and smaller font size 2 Add space between APIs 3 Change API name color to blue to be compatible with the whole website 4 For the api title background color which is similar to pytorch it is not easy to make this change due to our current design Maybe we can postpone this after doc release api1 api2,,kevinthesun,2017-05-18 20:24:35,2017-05-18 22:17:09
PR,Typo fix MX THREAD LOCAL,,,"ZihengJiang,tqchen,ZihengJiang,ZihengJiang,piiswrong",2017-05-04 20:59:28,2017-05-19 00:12:35
PR,Linear Regression tutorial improvements,,,"madjam,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,mli,madjam,madjam,zackchase,madjam,madjam",2017-05-14 03:46:51,2017-05-19 03:36:47
PR,Linear regression tutorial updates,This is a re submission of 6239,,madjam,2017-05-19 03:37:38,2017-05-19 03:37:58
IS,C Linking library error to build my own MxNet Project,Hi I'm trying to implement MTCNN with C and MxNet However I receive an error like ld symbol s not found for architecture x86 64 I have already built libmxnet a and libmxnet so files as written here build the cpp package Then I made a lib directory at a root of my working directory and moved those library files to it Please help out Thanks in advance Environment info Operating System OS X 10 12 4 Compiler AppleClang Package used Python R Scala Julia C MXNet commit hash git rev parse HEAD 918d48526481cf424197079ae47841e1b8afe399 Error Message I really appreciate your cooperation,,"piiswrong,lx75249",2017-05-16 08:12:42,2017-05-19 04:59:32
IS,how to input LinearRegressionOutput layer is label,,,,2017-05-19 07:59:14,2017-05-19 08:18:38
PR,LR regression tutorial fixes,This is a resubmission of 6239,,"madjam,madjam",2017-05-19 06:16:04,2017-05-19 14:30:26
PR,Delete common thread local h,,,ZihengJiang,2017-05-19 02:47:36,2017-05-19 15:46:16
PR,Changes in tutorials how to index,1 Remove contents that do not meet the quality bar for the 10 release 2 Fix a broken link 3 Minor edit to the home page,,"indhub,piiswrong",2017-05-19 01:28:07,2017-05-19 15:47:10
PR,Improvements to module tutorial,,,"madjam,mli",2017-05-14 00:21:22,2017-05-19 15:48:03
PR,Add ci test for c package and rename enum values,Generate enum values following cpp convention 6309 Add ci test for cpp package,,"lx75249,mli,mli",2017-05-19 13:38:16,2017-05-19 15:49:42
PR,Module api page cleanup,zackchase Please take a look,,"Roshrini,madjam,madjam,madjam,madjam,madjam,madjam,piiswrong,piiswrong,Roshrini,Roshrini",2017-05-15 16:19:21,2017-05-19 15:55:25
PR,Add example for ndarray dot,Added and example for ndarray do,,"anirudh2290,nswamy,piiswrong,anirudh2290,piiswrong,piiswrong,anirudh2290,anirudh2290,anirudh2290",2017-05-16 00:26:08,2017-05-19 15:56:18
PR,DataIter documentation,nswamy,,"pracheer,madjam,madjam,piiswrong,pracheer,piiswrong,pracheer,pracheer",2017-05-17 01:47:52,2017-05-19 16:30:50
PR,Improved documentation of test util functions,Improved docs for parse location and parse aux states Small fixes in docs for check symbolic forward and check symbolic backward,,apaleyes,2017-05-17 22:17:34,2017-05-19 16:31:49
IS,unknown gpu error,Can anyone help with a gpu issue Thanks in advance code import mxnet as mx a mx nd ones 2 3 mx gpu error 11 38 52 home ylin67 softwares mxnet dmlc core include dmlc logging h 303 11 38 52 home ylin67 softwares mxnet mshadow mshadow tensor gpu inl h 35 Check failed e cudaSuccess CUDA unknown error Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7fcba8a91449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow9SetDeviceINS 3gpuEEEvi 0xe0 0x7fcba9341c80 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x30 0x7fcba9349e40 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7fcbc452ac30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7fcbc8ba9184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fcbc88d6bed 11 38 52 home ylin67 softwares mxnet dmlc core include dmlc logging h 303 11 38 52 home ylin67 softwares mxnet mshadow mshadow tensor gpu inl h 35 Check failed e cudaSuccess CUDA unknown error Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7fcba8a91449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow9SetDeviceINS 3gpuEEEvi 0xe0 0x7fcba9341c80 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x30 0x7fcba9349e40 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7fcbc452ac30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7fcbc8ba9184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fcbc88d6bed terminate called after throwing an instance of wouldmlc Error' terminate called recursively what 11 38 52 home ylin67 softwares mxnet mshadow mshadow tensor gpu inl h 35 Check failed e cudaSuccess CUDA unknown error Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7fcba8a91449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow9SetDeviceINS 3gpuEEEvi 0xe0 0x7fcba9341c80 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x30 0x7fcba9349e40 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7fcbc452ac30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7fcbc8ba9184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fcbc88d6bed Aborted core dumped GPU info from nvidia smi Tue May 16 12 03 08 2017 NVIDIA SMI 375 39 Driver Version 375 39 GPU Name Persistence M Bus Id Disp A Volatile Uncorr ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M 0 Quadro K4100M Off 0000 01 00 0 On N A N A 57C P0 22W N A 809MiB 4035MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage 0 26525 G usr bin X 470MiB 0 27142 G compiz 173MiB 0 27599 G s passed by fd v8 snapshot passed by fd 163MiB,,eric-haibin-lin,2017-05-16 16:01:38,2017-05-19 18:42:16
IS,Is this a memory issue,HI Mxnet Supporters I came across the following error when I was trying to train some images I followed to fine tune a VGG16 model Can anyone help to explain why I got this error and how to fix it Thanks a lot in advance the data size NDArray 17136x3x224x224 0 train data NDArray 17136 0 train label NDArray 5712x3x224x224 0 validation data NDArray 5712 0 validation label running print out WARNING root Already bound ignoring bind usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module base module py 449 UserWarning Parameters already initialized and force init False init params call ignored allow missing allow missing force init force init INFO root Epoch 0 Batch 8 Speed 4 63 samples sec Train accuracy 0 194444 INFO root Epoch 0 Batch 16 Speed 4 63 samples sec Train accuracy 0 109375 INFO root Epoch 0 Batch 24 Speed 4 63 samples sec Train accuracy 0 218750 INFO root Epoch 0 Batch 32 Speed 4 63 samples sec Train accuracy 0 281250 INFO root Epoch 0 Batch 40 Speed 4 63 samples sec Train accuracy 0 156250 INFO root Epoch 0 Batch 48 Speed 4 63 samples sec Train accuracy 0 203125 INFO root Epoch 0 Batch 56 Speed 4 63 samples sec Train accuracy 0 296875 INFO root Epoch 0 Batch 704 Speed 4 63 samples sec Train accuracy 0 546875 INFO root Epoch 0 Batch 712 Speed 4 63 samples sec Train accuracy 0 546875 INFO root Epoch 0 Batch 720 Speed 4 63 samples sec Train accuracy 0 578125 INFO root Epoch 0 Batch 728 Speed 4 57 samples sec Train accuracy 0 640625 the error 12 55 29 home ylin67 softwares mxnet dmlc core include dmlc logging h 303 12 55 29 home ylin67 softwares mxnet mshadow mshadow stream gpu inl h 45 Check failed e cudaSuccess CUDA unspecified launch failure Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f1c0757f449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7f1c07de8768 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xdd2dc4 0x7f1c07ddedc4 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f1c07e2fdf7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f1c07e37e70 bt 5 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f1be8788c30 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f1c15597184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f1c152c4bed 12 55 29 home ylin67 softwares mxnet dmlc core include dmlc logging h 303 12 55 29 home ylin67 softwares mxnet mshadow mshadow stream gpu inl h 45 Check failed e cudaSuccess CUDA unspecified launch failure Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f1c0757f449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7f1c07de8768 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xdd2dc4 0x7f1c07ddedc4 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f1c07e2fdf7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f1c07e37e70 bt 5 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f1be8788c30 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f1c15597184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f1c152c4bed 12 55 29 home ylin67 softwares mxnet dmlc core include dmlc logging h 303 12 55 29 src engine threaded engine h 329 12 55 29 home ylin67 softwares mxnet mshadow mshadow stream gpu inl h 45 Check failed e cudaSuccess CUDA unspecified launch failure Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f1c0757f449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7f1c07de8768 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xdd2dc4 0x7f1c07ddedc4 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f1c07e2fdf7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f1c07e37e70 bt 5 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f1be8788c30 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f1c15597184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f1c152c4bed An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f1c0757f449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x338 0x7f1c07e300a8 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f1c07e37e70 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f1be8788c30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f1c15597184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f1c152c4bed terminate called after throwing an instance of wouldmlc Error' what 12 55 29 src engine threaded engine h 329 12 55 29 home ylin67 softwares mxnet mshadow mshadow stream gpu inl h 45 Check failed e cudaSuccess CUDA unspecified launch failure Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f1c0757f449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7f1c07de8768 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xdd2dc4 0x7f1c07ddedc4 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f1c07e2fdf7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f1c07e37e70 bt 5 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f1be8788c30 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f1c15597184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f1c152c4bed An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f1c0757f449 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x338 0x7f1c07e300a8 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f1c07e37e70 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f1be8788c30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f1c15597184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f1c152c4bed,,,2017-05-12 17:29:46,2017-05-19 18:43:33
PR,Added brief intro to Python API,,,madjam,2017-05-19 20:04:16,2017-05-19 20:05:13
PR,Enabling CPP doxygen API docs build,Tested locally Can you please review and merge this change This PR is targetted in immediately getting back C API docs I will work on C API landing page index html with an example in a separate PR,,"sandeep-krishnamurthy,mli,sandeep-krishnamurthy,sandeep-krishnamurthy",2017-05-19 17:52:18,2017-05-19 20:25:20
PR,Update IO Tutorial,,,"nswamy,piiswrong,piiswrong,piiswrong,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,pracheer,mli,mli,mli,mli,mli,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,madjam,nswamy,madjam,zackchase,piiswrong,nswamy,piiswrong,nswamy,nswamy,piiswrong,nswamy",2017-05-12 14:20:17,2017-05-19 23:21:39
PR,LinearRegressionOutput doc update,img width 755 alt linearregressionoutput src,,"anirudh2290,mli",2017-05-18 18:18:54,2017-05-19 23:23:53
PR,Update document for zeros like ones like softmax cross entropy c array c str,Update document for zeros like ones like softmax cross entropy c array c str and fix some docstring,,"yuruofeifei,piiswrong,nswamy,zackchase,zackchase,piiswrong,piiswrong,piiswrong,nswamy,jiajiechen,nswamy,yuruofeifei,piiswrong,yuruofeifei",2017-04-28 04:20:05,2017-05-19 23:25:04
PR,BaseRNNCell doc changes to make some method docstrings more readable,First pass at some docstring changes in BaseRNNCell Need to find a good place to hold actual examples I tried putting examples in the docstrings of methods but without context they are going to be hard to understand,,"rishita,szha,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,rishita,rishita,piiswrong,rishita,piiswrong,rishita,rishita,szha,piiswrong,rishita,rishita",2017-05-04 22:22:20,2017-05-19 23:32:11
PR,Add large scale image classification tutorial to tutorials index Also fix some formatting issues in the tutorial,mli Please review and merge,,"indhub,madjam,piiswrong,indhub,madjam,piiswrong,mli,indhub",2017-05-19 18:52:48,2017-05-19 23:32:53
PR,Fixed few typos in the NDArray tutorial,mli,,apaleyes,2017-05-19 23:28:54,2017-05-19 23:33:39
PR,Symbol tutorial improvements,zackchase Please take a look,,"Roshrini,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,mli,mli,madjam,madjam,madjam,madjam,madjam,madjam,madjam,piiswrong,mli,mli,Roshrini,Roshrini,piiswrong,zackchase,Roshrini,piiswrong,Roshrini,Roshrini",2017-05-10 21:00:54,2017-05-19 23:36:20
PR,Hotfix for 6341,,,"mli,piiswrong",2017-05-19 20:03:11,2017-05-19 23:45:52
PR,Update mshadow,,,"sxjscience,sxjscience,piiswrong,ZihengJiang",2017-05-08 08:03:30,2017-05-19 23:57:28
PR,Documentation for image center crop api,In order to figure out the possible interpolation techniques I referred to void 20resize InputArray 20src 20OutputArray 20dst 20Size 20dsize 20double 20fx 20double 20fy 20int 20interpolation,,"pracheer,piiswrong,piiswrong,piiswrong,piiswrong,zackchase,zackchase,zackchase",2017-05-05 01:48:19,2017-05-19 23:58:15
PR,Improve symbol api doc,pow maximum minimum hypot debug str,,"kevinthesun,piiswrong,mli,mli,mli,mli,mli,mli,mli,mli,mli,mli,mli,piiswrong,madjam",2017-05-12 20:36:59,2017-05-20 02:20:56
PR,Add documentation for S3 integration,mli Please review and merge,,"indhub,madjam,madjam,madjam,indhub,piiswrong,piiswrong,piiswrong",2017-05-19 22:23:44,2017-05-20 02:26:59
PR,Fix ci test,,,"lx75249,lx75249",2017-05-19 23:17:19,2017-05-20 03:06:36
PR,Code title background color,nswamy,,kevinthesun,2017-05-20 03:26:07,2017-05-20 05:22:44
PR,ci test debug,,,"lx75249,lx75249",2017-05-20 05:07:03,2017-05-20 06:41:01
IS,AttributeError module 'mxnet' has no attribute 'rnn,I install mxnet with this command pip install mxnet cu80 when I use the rnn API I get an error AttributeError module 'mxnet' has no attribute 'rnn' but I notice that in the source code mx rnn is still here,,"szha,szha,szha,szha",2017-05-02 08:54:38,2017-05-20 13:45:45
PR,Fix ci test,,,lx75249,2017-05-20 07:42:28,2017-05-20 16:33:18
PR,update doc for DataDesc,Reformat IRHeader and Update Doc for DataDesc please review and merge,,"nswamy,piiswrong,piiswrong,nswamy,piiswrong,nswamy",2017-05-20 00:38:01,2017-05-20 19:26:53
PR,Fixed few typos in the Module tutorial,Spotted a few typos as I was going through the Module tutorial,,apaleyes,2017-05-20 18:46:18,2017-05-20 19:27:31
PR,Added linear regression to tutorials list,,,madjam,2017-05-20 00:36:22,2017-05-20 19:48:42
PR,update tutorials index minor formatting fix,piiswrong,,nswamy,2017-05-20 21:09:45,2017-05-20 22:34:28
PR,Cleanup Python API pages redundant content,Cleaned up Python API pages redundant content Minor edits in how to multi devices,,"jiajiechen,mli,mli,mli,piiswrong,jiajiechen,piiswrong",2017-05-11 21:17:39,2017-05-21 01:20:44
PR,Fixing a dead link in the python Readme,,,tdomhan,2017-05-21 01:33:04,2017-05-21 02:53:19
PR,Documentation update for ImageRecordIter,Due to time constraints was not able to dive deep into the code to understand more about each of the parameters Let me know if that is necessary in order to proceed,,"pracheer,piiswrong,piiswrong,piiswrong,piiswrong,pracheer,pracheer,pracheer",2017-05-19 23:22:02,2017-05-21 04:57:29
IS,API Doc Improvements Tracking,Let is fix our docs The effort to spruce up MXNet is Python API docs is picking up steam but could use your help to add that extra oomph Many of the APIs now have proper explanations and good examples but we still have much more to go alt text Make our docs great again Here s how you can join in and help 1 Pick a few functions or at least 1 smile from the list below Ensure you only pick ones that don t have a current owner assigned and you can submit within 2 business days Post a comment to the issue with the ones you ve selected 1 Look up current documentation Find whats currently documented for the function at the mxnet io python API docs or within the C code in git 1 Write up A clear and concise description of the function and its behavior List and describe each parameter with the valid input values whether it is required or optional and default value if the parameter is optional Illustrate the function and parameter behavior using examples Refer to these examples for guidance Embedding mxnet ndarray Embedding ROIPooling mxnet ndarray ROIPooling Reshape mxnet ndarray Reshape 1 Submit your work Creating a new issue with API docs submission your function name s in the title and paste your write up markdown OR Creating a pull request for a change to the appropriate cc or py file Instructions here How to make a pull request for docs Doc Contributions 281 29 pdf API List Function Name Package Contributor Status 1 ul li IRHeader li ul recordio 2 ul li pack li ul recordio 3 ul li pack img li ul recordio 4 ul li unpack li ul recordio 5 ul li unpack img li ul recordio 6 ul li BilinearSampler li ul ndarray 7 ul li Correlation li ul ndarray 8 ul li GridGenerator li ul ndarray 9 ul li IdentityAttachKLSparseReg li ul ndarray 10 ul li SpatialTransformer li ul ndarray 11 ul li UpSampling li ul ndarray 12 ul li adam update li ul ndarray 13 ul li ones like li ul ndarray 14 ul li random exponential li ul ndarray 15 ul li random gamma li ul ndarray 16 ul li random generalized negative binomial li ul ndarray 17 ul li random negative binomial li ul ndarray 18 ul li random poisson li ul ndarray 19 ul li x rmsprop update li ul ndarray 20 ul li x rmspropalex update li ul ndarray 21 ul li sample exponential li ul ndarray 22 ul li sample gamma li ul ndarray 23 ul li sample generalized negative binomial li ul ndarray 24 ul li sample negative binomial li ul ndarray 25 ul li sample normal li ul ndarray 26 ul li sample poisson li ul ndarray 27 ul li sample uniform li ul ndarray 28 ul li sgd mom update li ul ndarray 29 ul li sgd update li ul ndarray 30 ul li smooth l1 li ul ndarray 31 ul li softmax cross entropy li ul ndarray 32 ul li waitall li ul ndarray 33 ul li zeros like li ul ndarray 34 ul li Caffe li ul metric 35 ul li Torch li ul metric 36 ul li check label shapes li ul metric 37 ul li Bilinear li ul initializer 38 ul li FusedRNN li ul initializer 39 ul li LSTMBias li ul initializer 40 ul li LogValidationMetricsCallback li ul callback 41 ul li init torch module li ul torch 42 ul li make torch function li ul torch 43 ul li CustomOp assign li ul operator 44 ul li NameManager li ul name 45 ul li Monitor li ul monitor 46 ul li CastAug li ul image 47 ul li CenterCropAug li ul image 48 ul li ColorJitterAug li ul image 49 ul li ColorNormalizeAug li ul image 50 ul li CreateAugmenter li ul image 51 ul li HorizontalFlipAug li ul image 52 ul li LightingAug li ul image 53 ul li RandomCropAug li ul image 54 ul li RandomOrderAug li ul image 55 ul li RandomSizedCropAug li ul image 56 ul li ResizeAug li ul image 57 ul li center crop li ul image 58 ul li color normalize li ul image 59 ul li fixed crop li ul image 60 ul li imdecode li ul image 61 ul li random crop li ul image 62 ul li random size crop li ul image 63 ul li resize short li ul image 64 ul li scale down li ul image 65 ul li ImageIter augmentation transform li ul image 66 ul li ImageIter imdecode li ul image 67 ul li ImageIter postprocess data li ul image 68 ul li ImageIter read image li ul image 69 ul li ImageIter check data shape li ul image 70 ul li ImageIter check valid image li ul image 71 ul li Executor get dict li ul executor 72 ul li monitor callback wrapper li ul executor 73 ul li MultiBoxDetection li ul contrib 74 ul li MultiBoxPrior li ul contrib 75 ul li MultiBoxTarget li ul contrib 76 ul li Proposal li ul contrib 77 ul li count sketch li ul contrib 78 ul li fft li ul contrib 79 ul li ifft li ul contrib 80 ul li set is training li ul contrib 81 ul li test li ul contrib 82 ul li train li ul contrib 83 ul li MXNetError li ul base 84 ul li mx float p li ul base 85 ul li py str li ul base 86 ul li print summary li ul visualization 87 ul li Symbol attr li ul symbol 88 ul li Symbol attr dict li ul symbol 89 ul li Symbol debug str li ul symbol 90 ul li Symbol grad li ul symbol 91 ul li Symbol list attr li ul symbol 92 ul li MXRecordIO close li ul recordio 93 ul li MXRecordIO open li ul recordio 94 ul li Optimizer set lr mult li ul optimizer 95 ul li Optimizer set lr scale li ul optimizer 96 ul li Optimizer set wd mult li ul optimizer 97 ul li Optimizer update li ul optimizer 98 ul li LinearRegressionOutput li ul ndarray 99 ul li x NDArray at li ul ndarray 100 ul li x NDArray slice li ul ndarray 101 ul li x NDArray sync copyfrom li ul ndarray 102 ul li gamma li ul ndarray 103 ul li gammaln li ul ndarray 104 ul li identity li ul ndarray 105 ul li load li ul ndarray 106 ul li moveaxis li ul ndarray 107 ul li DataDesc li ul io 108 ul li DataDesc get batch axis li ul io 109 ul li DataDesc get list li ul io 110 ul li DataIter getpad li ul io 111 ul li MXDataIter li ul io 112 ul li ImageDetRecordIter li ul io 113 ul li Initializer legacy init li ul initializer 114 ul li Initializer dumps li ul initializer 115 ul li ProgressBar li ul callback 116 ul li log train metric li ul callback 117 ul li module checkpoint li ul callback 118 ul li parse aux states li ul test utils 119 ul li parse location li ul test utils 120 ul li x check symbolic backward li ul test utils 121 ul li x check symbolic forward li ul test utils 122 ul li np reduce li ul test utils 123 ul li numeric grad li ul test utils 124 ul li print max err loc li ul test utils 125 ul li BucketSentenceIter li ul rnn 126 ul li encode sentences li ul rnn 127 ul li do rnn checkpoint li ul rnn 128 ul li load rnn checkpoint li ul rnn 129 ul li save rnn checkpoint li ul rnn 130 ul li BaseRNNCell begin state li ul rnn 131 ul li BaseRNNCell pack weights li ul rnn 132 ul li BaseRNNCell unpack weights li ul rnn 133 ul li BaseRNNCell unroll li ul rnn 134 ul li BidirectionalCell li ul rnn 135 ul li DropoutCell li ul rnn 136 ul li FusedRNNCell li ul rnn 137 ul li GRUCell li ul rnn 138 ul li LSTMCell li ul rnn 139 ul li ModifierCell li ul rnn 140 ul li RNNCell li ul rnn 141 ul li RNNParams li ul rnn 142 ul li RNNParams get li ul rnn 143 ul li SequentialRNNCell li ul rnn 144 ul li SequentialRNNCell add li ul rnn 145 ul li ZoneoutCell li ul rnn 146 ul li BaseRNNCell get activation li ul rnn 147 ul li FusedRNNCell slice weights li ul rnn 148 ul li dump profile li ul profiler 149 ul li profiler set config li ul profiler 150 ul li profiler set state li ul profiler 151 ul li CustomOp li ul operator 152 ul li CustomOp backward li ul operator 153 ul li CustomOp forward li ul operator 154 ul li CustomOpProp li ul operator 155 ul li CustomOpProp infer shape li ul operator 156 ul li CustomOpProp list arguments li ul operator 157 ul li CustomOpProp list auxiliary states li ul operator 158 ul li CustomOpProp list outputs li ul operator 159 ul li x NDArrayOp declare backward li ul operator 160 ul li x NumpyOp li ul operator 161 ul li x PythonOp li ul operator 162 ul li x PythonOp backward li ul operator 163 ul li x PythonOp forward li ul operator 164 ul li x PythonOp get symbol li ul operator 165 ul li x PythonOp infer shape li ul operator 166 ul li x PythonOp list arguments li ul operator 167 ul li x PythonOp list outputs li ul operator 168 ul li x PythonOp need top grad li ul operator 169 ul li register li ul operator 170 ul li CustomOpProp infer type li ul operator 171 ul li NameManager get li ul name 172 ul li Prefix li ul name 173 ul li Monitor install li ul monitor 174 ul li Monitor tic li ul monitor 175 ul li Monitor toc li ul monitor 176 ul li Monitor toc print li ul monitor 177 ul li getLogger li ul log 178 ul li x KVStore load optimizer states li ul kvstore haibin lin 179 ul li x KVStore save optimizer states li ul kvstore haibin lin 180 ul li x KVStore set optimizer li ul kvstore haibin lin 181 ul li x create li ul kvstore haibin lin 182 ul li ImageIter li ul image 183 ul li ImageIter next li ul image 184 ul li ImageIter next sample li ul image 185 ul li ImageIter reset li ul image 186 ul li x Executor li ul executor 187 ul li x Executor backward li ul executor 188 ul li x Executor copy params from li ul executor 189 ul li x Executor reshape li ul executor 190 ul li x Executor set monitor callback li ul executor 191 ul li Executor get outputs li ul executor 192 ul li x Executor debug str li ul executor 193 ul li compute gradient li ul contrib 194 ul li grad and loss li ul contrib 195 ul li mark variables li ul contrib 196 ul li set recording li ul contrib 197 ul li LogMetricsCallback li ul contrib 198 ul li TrainingStateScope li ul contrib 199 ul li build param doc li ul base 200 ul li c array li ul base 201 ul li c str li ul base 202 ul li check call li ul base 203 ul li ctypes2buffer li ul base 204 ul li ctypes2numpy shared li ul base 205 ul li load lib li ul base 206 ul li notify shutdown li ul base 207 ul li add fileline to docstring li ul base 208 ul li AttrScope li ul attribute 209 ul li AttrScope current li ul attribute 210 ul li AttrScope get li ul attribute,,"domdivakaruni,piiswrong,asmushetzel,domdivakaruni,domdivakaruni,yuruofeifei,domdivakaruni,domdivakaruni,nswamy,Lyken17,nswamy,nswamy,jiayue666,szha,domdivakaruni,asmushetzel,eric-haibin-lin,nswamy,Lyken17,nswamy,apaleyes",2017-04-27 02:29:34,2017-05-21 15:34:06
PR,Added OSX classpath support for running charrnn example,Support running training and test scripts on OSX machines by adding the specific classpath,,jamesliu,2017-05-21 06:36:46,2017-05-21 18:37:08
PR,Update ModelZoo with LocationNet and Multimedia Commons Dataset,The proposed change contains addition of Multimedia Commons Dataset and LocationNet Multimedia Commons Dataset is part of AWS Public Dataset program LocationNet is a ResNet 101 pre trained with 39 million geo tagged images from Multimedia Commons Dataset YFCC100M to recognize location README file at the link below contains model location confirmation the trained model meets published accuracy from original paper Jupyter Notebook tutorial for step by step instructions on how to use the trained model and references to any other applicable docs or arxiv papers the model is based on Contributors Jaeyoung Choi International Computer Science Institute USA Kevin Li University of California at Berkeley USA This work is sponsored by AWS Research Grant with the help of Joseph Spisak KD Singh Jed Sundwall Matt Jamieson and Mu Li from Amazon,,"sbodenstein,sbodenstein",2017-05-21 21:41:37,2017-05-21 22:25:50
PR,DOC Fix smooth l1 example display,Fix wrong display of smooth l1 example mxnet ndarray smooth l1,,"reminisce,piiswrong,reminisce",2017-05-21 21:26:48,2017-05-22 00:01:36
IS,Error installing perl bindings,OS Ubuntu 16 04 Compiler gcc 5 4 0 Package used Perl MXNet version 0 9 3 I am trying to install the perl bindings AI MXNet I downloaded this repo compiled everything then moved to the perl package folder but I am unable to install AI MXNetCAPI or AI NNVMCAPI I keep getting this error,,"sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev",2017-05-22 00:25:37,2017-05-22 03:12:45
IS,mx sym Dropout Segmentation fault,I tried to add dropout operation to train a MLP network however I encontered segmentation fault So I wrote a simple example and the segmentation fault occurred Please take a look thank you For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 04 Compiler g 5 4 0 cuda 8 0 cudnn 6 0 Package used Python R Scala Julia Python 2 7 MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD 7c39f845d32679eaa2287b2554c133d9e45923e2 If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Segmentation fault core dumped Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error if I set is train False then there is no error however when I set is train True segmentation fault occurred What have you tried to solve it 1 2 3,,piiswrong,2017-05-20 08:13:57,2017-05-22 03:23:30
IS,image classification predict can not run well,mli what you provided model can not run well And the result is not right when I use my own model I think c predict api cc has problem,,ysh329,2016-12-24 04:05:27,2017-05-22 04:01:38
PR,add num group argument in tools caffe converter convert symbol py,The original code ignores the num group parameter in convolution layers Thus this script is not able to convert caffe is alexnet model into mxnet,,nicklhy,2017-05-22 04:18:50,2017-05-22 06:49:52
IS,mxnet 0 9 5 fcn xs example error,About FCN XS example Environment info Operating System Ubuntu 16 04 Compiler g 5 4 Package used Python R Scala Julia Python MXNet version mxnet mkl cu80 0 9 5 Segmentation fault Error message from gdb Thread 90 python received signal SIGSEGV Segmentation fault Switching to Thread 0x7ffe9ebf4a00 LWP 5866 0x00007fff7a953c29 in mkl vsl sub kernel l9 sBRngMCG31M1 from usr local lib python2 7 dist packages mxnet libmklml intel so To reproduce simply run fcn xs example with mxnet package downloaded,,,2017-05-22 15:45:00,2017-05-22 15:47:34
PR,Fixed few typos in the Symbol tutorial,Spotted few typos as I was going through the Symbol tutorial,,"apaleyes,piiswrong,apaleyes,piiswrong,apaleyes",2017-05-20 18:05:24,2017-05-22 17:06:03
PR,fix a typo,Fix a typo Can you please review merge this,,nswamy,2017-05-22 20:50:53,2017-05-22 20:51:42
PR,Fix install rendering issue,Force rendering other installation pages under get started This is a short term solution In the future we should move all installation information into installation page,,kevinthesun,2017-05-22 21:34:04,2017-05-22 23:15:38
IS,cpp package Cannot open include file 'mxnet cpp op h',I synced latest code and tried to build mxnet Build fails with the following error message cpp package include mxnet cpp optimizer hpp 19 fatal error C1083 Cannot open include file 'mxnet cpp op h' No such file or directory It seems 00b2b47f70283e0706256b8585b97b63efd25c57 change set removed mxnet cpp op h but other code is still including mxnet cpp op h MxNetCpp h optimizer hpp cpp files under cpp package example these file include mxnet cpp op h Environment info Operating System Windows 10 Compiler Visual Studio 2013 Package used Python R Scala Julia Cpp package MXNet commit hash git rev parse HEAD 3f75c41fb3cd176089bfd06ab76a0e04723cefe0 Error Message 10 D nvTFS2 External Mxnet mxnet 20170517 3f75c41fb3cd176089bfd06ab76a0e04723cefe0 cpp package include mxnet cpp optimizer hpp 19 fatal error C1083 Cannot open include file 'mxnet cpp op h' No such file or directory Minimum reproducible example include MxNetCpp h,,"eric-haibin-lin,lx75249,lx75249,lx75249,lx75249",2017-05-17 21:13:02,2017-05-22 23:46:50
PR,Documentation update for tutorials index html,1 Hand Handwritten 2 Training and Inference Training and Inference 3 Some paraphrasing 4 CNNs CNN 5 Removing an unnecessary run inference on the trained model as it was not adding too much value but making the paragraph sound weird,,"pracheer,madjam,madjam",2017-05-22 23:48:12,2017-05-23 02:57:57
PR,Moving Raspberry Pi and TX2 setup to common install page,Moved Raspberry Pi and TX2 setup to common install page No new content just moved from separate page to common page Added redirection in old page to avoid broken links all across search engines Fixed URL in CPP API landing page to point to correct place Note This is important as Aran is talking to customers and mxnet io is broken for these installation instructions,,sandeep-krishnamurthy,2017-05-22 23:11:11,2017-05-23 02:58:38
PR,add get mnist in test utils py,,,mli,2017-05-22 20:55:08,2017-05-23 02:58:56
PR,fix cython build,tqchen,,piiswrong,2017-05-20 00:29:32,2017-05-23 16:04:38
PR,Fixed few typos and wordings in the Iterators tutorial,Spotted a few typos and opportunities for better wording as I was going through the Iterators tutorial,,apaleyes,2017-05-23 14:50:03,2017-05-23 16:07:45
PR,RNN API doc index page,The doc is hosted at here with the new page here,,"szha,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,piiswrong,szha,szha,piiswrong,piiswrong,szha,szha,szha,piiswrong,szha,zackchase,szha,nswamy,szha,szha",2017-05-15 08:36:15,2017-05-23 18:47:48
PR,maybe fix setup py by initialzing with cython False,,,piiswrong,2017-05-23 17:56:24,2017-05-23 19:09:43
PR,Bugfix in PandasLogger Fixing broken link in Docs,A couple small changes Also importing notebook callbacks by default,,leopd,2017-05-23 16:14:33,2017-05-23 19:10:21
PR,module api formatting fixes,Minor typos and formatting fixes for Module API Please review and merge,,"Roshrini,piiswrong,Roshrini,piiswrong,Roshrini",2017-05-23 18:55:36,2017-05-23 20:38:01
PR,Force render all pages with no index in mxnet io,,,kevinthesun,2017-05-23 21:53:52,2017-05-23 22:08:40
PR,Pull in latest dmlc mxnet,,,pracheer,2017-05-23 22:42:19,2017-05-23 22:43:21
PR,Improve Progress bar doc,mli Improved progress bar doc Progress bar now outputs to logger instead of stdout,,"saurabh3949,zackchase,zackchase,piiswrong,piiswrong,saurabh3949,Roshrini",2017-05-04 21:36:09,2017-05-23 22:51:12
IS,Potential race in Engine code,L314 In this piece of code there may be a potential race condition where the content of PushSync call finishes earlier than the finished cv wait so that the notification of PushSync is lost I think we need to guard finished cv wait lock this done return done load kill load with if done false finished cv wait lock this done return done load kill load thanks,,"piiswrong,piiswrong",2017-05-23 23:01:13,2017-05-23 23:40:22
PR,Fixing typos,,,madjam,2017-05-24 01:01:48,2017-05-24 03:46:08
IS,training speed is much slower when I install through building form source instead of using pip to install,Hello I install MXNet through building from source and pip The training speed 1 building from source 5 samples sec 2 install through pip 9 samples sec I use the same net just switching between the two mxnet python package but the speed is quite difference Why running speed through building from source is so slow ps cuda 7 5 cudnn v4 Precompiled package through pip is in 2017 05 08 source is download in 2017 05 16,,"szha,szha",2017-05-17 06:56:18,2017-05-24 05:45:39
PR,Doc Improvement SoftmaxOutput Add ignore label use example,Also mentions that ignore label is disabled when data and label have the same shapes Also can someone answer why is this feature Some users asked me but I do not have an answer for that,,"jiajiechen,zackchase,zackchase,zackchase,piiswrong,jiajiechen,madjam",2017-05-11 00:15:11,2017-05-24 06:02:10
IS,C 11 related build errors with MSVC,I'm getting some C 11 related errors trying to build the library Please see the details below Environment info Operating System Windows 7 x64 Compiler MS Visual C Build Tools 2015 MSVS 14 0 cl exe 19 00 24218 2 for x86 CUDA SDK 8 0 Package used Python R Scala Julia none yet MXNet version 0 9 3 0 9 5 0 10 0 rc0 More details in CMake output link below Error Message Steps to reproduce Here is the full log of both CMake and MSBuild including the commands This is the error message part What have you tried to solve it 1 I made a test program to make sure the compiler supports the C 11 features that seem to be causing troubles 2 I made sure the CUDA SDK supports C 11 since version 7 3 I tried 3 releases of MxNet,,"piiswrong,yajiedesign,ZihengJiang",2017-05-23 15:24:05,2017-05-24 08:05:46
IS,OSX R mxnet 0 9 5 installation error,Thanks I have just tried and I could not install it This is what I got Please let me know if you need me to provide other details Thanks Carlos,,thirdwing,2017-05-06 12:02:54,2017-05-24 08:24:42
IS,How to set MXNET ENGINE TYPE to NaiveEngine to debug,Thanks I did not find any message about MXNET ENGINE TYPE on docs,,"ysh329,thirdwing,ysh329",2017-05-20 04:15:12,2017-05-24 10:54:28
IS,CNN for text classifcation in R how to structure input data for mxnet not an issue,Hi If this is not the right forum to ask below question kindly direct me to the right one I didnot see any activity on stack overflow for mxnet R package I am really new to deep learning and trying to implement a CNN model for text classification based on the paper Convolutional Neural Networks for Sentence Classification by Yoon Kim He has provided the code on github which is in python I do not know python I know how to run a CNN in mxnet based on the following blog post I need help in how to structure the input dataset required by mxnet I know how to convert every sentence into word vectors using package text2vec glove function But since there will be many sentences how do i create the final training dataset for input into CNN model I appreciate all your help regards Amit Bothra,,thirdwing,2016-09-16 16:41:23,2017-05-24 15:29:15
PR,Removed deprecated configs,The configuration for the rcnn operator package examples rcnn operator is deprecated Now mxnet contrib symbol encapsulates the operators instead,,xioryu,2017-05-24 07:24:32,2017-05-24 16:47:31
PR,Replace ReduceToAssign with Reduce,This PR should fix this issue issuecomment 302460839 Test script haibin lin please review,,"reminisce,piiswrong,reminisce,piiswrong",2017-05-24 04:21:32,2017-05-24 16:50:43
PR,Improve progress bar doc and log the bar instead of stdout,Improved progress bar doc Progress bar now outputs to logger instead of stdout I have closed 6106 Please consider this one Thanks,,"saurabh3949,piiswrong,saurabh3949,Roshrini",2017-05-23 22:51:21,2017-05-24 17:08:32
PR,Removing unnecessary copies when copying data to device,,,"ptrendx,piiswrong,piiswrong",2017-05-22 21:12:50,2017-05-24 17:14:37
IS,question about mxnet implementation of CycleGAN,I am trying to implement CycleGAN with mxnet I need to realize two generator networks for style translation between A and B and the cycle loss will be I tried to use MakeLoss function but I am lost with how to make the cycle symbol Is there an easy way to implement this kind of cycle network in mxnet Really appreciate any suggestions and advice,,"Godricly,Ldpe2G,Ldpe2G",2017-05-23 23:01:01,2017-05-24 18:29:58
PR,Tools of caffe convert support 1X7 convolution kernel and pad w pad h params,hi all the original caffe convert code dose not support the caffe convolution layer with kernal size like 1X7 and deal with the param pad w and pad h params this pr is fix this issue,,"mli,mli,mli,mli,mli",2017-05-07 06:36:31,2017-05-24 18:57:25
PR,Fix for NNVM build,mli,,"ZihengJiang,mli",2017-05-24 05:50:27,2017-05-24 19:09:31
PR,Optimizer formatting fixes,Fixed some typos and formatting Please review and merge,,"Roshrini,piiswrong",2017-05-24 18:25:23,2017-05-24 20:24:37
PR,Fix download link,Current download buttons do not work in firefox,,kevinthesun,2017-05-24 21:27:26,2017-05-24 21:33:16
PR,Add release note,After discussing on doc standup text link without underscore is more favorable release2,,kevinthesun,2017-05-25 00:07:57,2017-05-25 00:46:08
IS,Save checkpoint error local filesys cc 39 Check failed std fwrite ptr 1 size fp size FileStream Write incomplete,,,"ysh329,ysh329",2017-05-01 09:25:54,2017-05-25 01:14:02
IS,Create shared layer as in 557,tqchen I have tried to create shared layers I tried your answer of 557 in mxnet example image classification symbols mlp py data mx symbol Variable wouldata' data mx sym Flatten data data fc1 weight mx symbol Variable 'fc1 weight' init mx init Xavier b1 weight mx symbol Variable 'b1 wweight' init mx init Xavier fc2 weight mx symbol Variable 'fc2 wweight' init mx init Xavier b2 weight mx symbol Variable 'b2 weight' init mx init Xavier fc3 weight mx symbol Variable 'fc3 weight' init mx init Xavier b3 weight mx symbol Variable 'b3 weight' init mx init Xavier fc1 mx symbol FullyConnected data data weight fc1 weight bias b1 weight name 'fc1' num hidden 128 act1 mx symbol Activation data fc1 name arelu1' act type relu fc2 mx symbol FullyConnected data act1 weight fc2 weight bias b2 weight name 'fc2' num hidden 64 act2 mx symbol Activation data fc2 name arelu2' act type relu fc3 mx symbol FullyConnected data act2 weight fc3 weight bias b3 weight name 'fc3' num hidden num classes mlp mx symbol SoftmaxOutput data fc3 name isoftmax' but I get Traceback most recent call last File mxnet example image classification train mnist py line 76 in module fit fit args sym get mnist iter File home ycliu mxnet ssd mxnet example image classification common fit py line 187 in fit monitor monitor File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module base module py line 448 in fit allow missing allow missing force init force init File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module module py line 273 in init params impl desc arr arg params File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module module py line 268 in impl initializer name arr File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet initializer py line 79 in call INITIALIZER REGISTRY klass lower kwargs init weight desc arr File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet initializer py line 371 in init weight fan in fan out shape 1 hw scale shape 0 hw scale IndexError tuple index out of range Do I need to assign the shape of weights first,,piiswrong,2017-05-24 14:03:37,2017-05-25 06:36:58
IS,cuda tensor gpu inl cuh 58 too large launch parameter MapReduceKeepDim1,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu16 04 Compiler gcc5 4 0 Package used Python R Scala Julia python3 MXNet version 0 9 5 Or if installed from source yes Error Message Could you please give me some advice to solve this problem Thanks a lot,,"piiswrong,reminisce,reminisce",2017-05-18 08:46:52,2017-05-25 07:10:36
PR,Fixing tutorials,Most of the fixes should be self evident For the tutorial on pre trained models one of the images does not exist anymore so selected a new one Long term we should put such images on web data repo but alas some other day For Handwritten digit tutorial we are missing a couple of imports in the test utils py that was recently created Note that for the pre trained model tutorial we get a softmax label warning and the probability scores are not really probabilities Will deal with that issue in another PR Testing I have tried to test all the notebooks with this change and things look fine,,pracheer,2017-05-25 00:49:49,2017-05-25 16:43:03
PR,Formatting fixes,NDArray Symbol Data loading page formatting fixes Please review and merge,,"Roshrini,madjam",2017-05-24 22:28:19,2017-05-25 16:43:38
PR,doc bash 2 5 for pack unpack pack img and unpack img,Doc Bash for 2 5 i e mx recordio pack mx recordio unpack mx recordio pack img and mx recordio unpack img,,"jiayue666,piiswrong,piiswrong,jiayue666,piiswrong,Roshrini,jiayue666,jiayue666",2017-05-07 07:40:31,2017-05-25 16:51:47
PR,fixing the early stop for maximize T,in response to issue 4587,,,2017-04-20 14:29:33,2017-05-25 18:57:02
IS,R package Bug in early stop callback function for maximize TRUE,There is a bug in the early stop callback function 3938 when eval metric needs to be maximized The mx callback early stop as soon as the evaluation increases even when the maximize is set to TRUE The line 138 of R package R callback R must be changed from,,thirdwing,2017-01-08 00:19:37,2017-05-25 18:57:11
PR,Improve style,1 Remove why mxnet page from install tab 2 Remove toc on install page 3 Change left toc title to Contents,,kevinthesun,2017-05-25 18:24:19,2017-05-25 19:41:15
PR,Correction,Correction for example in PR 6028 Can you merge this,,"Roshrini,eric-haibin-lin,Roshrini",2017-05-25 17:51:04,2017-05-25 19:41:52
PR,Update documentation for MXNetDataIter in io py 6000,Update MXNetDataIter documentation from the Doc Bash session Was not able to run doc build even on the master branch on my local Macbook Will upload the screenshot after run make on another linux box Hopefully after this first PR it will make things easier for later contributions,,"danithaca,piiswrong,piiswrong,danithaca",2017-05-05 00:04:42,2017-05-25 19:43:23
PR,fix member variable name make them end with underline,fix member variable name,,vsooda,2017-05-25 07:20:53,2017-05-25 19:45:26
PR,Fix minor issues with api index pages,1 In the notes section for ndarray references did not seem clear enough to be referring to mxnet ndarray or numpy ndarray Added the package names as prefixes to make it more obvious 2 share the same C operator source codes share the same code since we do not really need to throw in more details than required 3 Other relatively minor language changes which will be obvious from the diff Note that I'm relatively not sure about the need for 1 since it makes things more verbose but clear I think Let me know if it seems unnecessary and I will remove it,,"pracheer,mli,pracheer,pracheer",2017-05-23 22:52:27,2017-05-25 19:47:17
IS,MXnet can install but not load,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 7 Professional Compiler R for Windows 3 3 2 Package used Python R Scala Julia R for Windows 3 3 2 MXNet version package mxnet version 0 7 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 7 x64 build 7601 Service Pack 1 locale 1 LC COLLATE English United States 1252 LC CTYPE English United States 1252 3 LC MONETARY English United States 1252 LC NUMERIC C 5 LC TIME English United States 1252 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 igraph 1 0 1 Rcpp 0 12 8 rstudioapi 0 6 magrittr 1 5 munsell 0 4 3 6 colorspace 1 3 2 R6 2 2 0 brew 1 0 6 stringr 1 1 0 plyr 1 8 4 11 dplyr 0 5 0 visNetwork 1 0 3 Rook 1 1 1 tools 3 3 2 grid 3 3 2 16 gtable 0 2 0 DBI 0 5 1 influenceR 0 1 0 DiagrammeR 0 9 0 htmltools 0 3 5 21 lazyeval 0 2 0 digest 0 6 11 assertthat 0 1 tibble 1 2 gridExtra 2 2 1 26 RColorBrewer 1 1 2 ggplot2 2 2 1 htmlwidgets 0 8 viridis 0 3 4 rgexf 0 15 3 31 stringi 1 1 2 scales 0 4 1 XML 3 98 1 5 jsonlite 1 2 Error Message Please paste the full error message including stack trace I can install mxnet package but I can not make it available for use problem with version 0 9 0 DiagrammeR library mxnet lib loc R win library 3 3 Error object combine edges is not exported by 'namespace DiagrammeR' Error package or namespace load failed for mxnet Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Unistalled DiagrammeR 0 9 0 and tried to install earlier version with following commands remove packages DiagrammeR install version DiagrammeR version 0 8 1 repos cran us r project org response to install version command Warning unable to access index for repository cran us r project org src contrib scheme not supported in URL 'cran us r project org src contrib PACKAGES' Error in package find repo package repos could not find package 'DiagrammeR' 2 3,,"thirdwing,thirdwing,thirdwing",2017-01-07 05:33:38,2017-05-25 22:00:01
IS,can not require mxnet with CPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System windows 10 Compiler R studio R version 3 3 1 Package used Python R Scala Julia R MXNet version do not know I just use the command in R studio install packages drat repos drat addRepo dmlc install packages mxnet Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message mxnet Error 'namespace DiagrammeR' combine edges Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce set up install packages drat repos drat addRepo dmlc install packages mxnet require require mxnet or library mxnet What have you tried to solve it 1 install packages add a parameter dependency TRUE 2 search with google can not find answer either 3 I try to download the mxnet package and install the local file It does not work either,,"thirdwing,thirdwing",2017-01-05 14:48:12,2017-05-25 22:00:11
IS,mxnet wo not load,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows Compiler Using precompiled binaries Package used Python R Scala Julia R MXNet version R Package version 0 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 10 x64 build 14393 locale 1 LC COLLATE English United States 1252 2 LC CTYPE English United States 1252 3 LC MONETARY English United States 1252 4 LC NUMERIC C 5 LC TIME English United States 1252 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 igraph 1 0 1 Rcpp 0 12 8 rstudioapi 0 6 4 knitr 1 15 1 magrittr 1 5 munsell 0 4 3 7 colorspace 1 3 1 R6 2 2 0 brew 1 0 6 10 dplyr 0 5 0 stringr 1 1 0 plyr 1 8 4 13 visNetwork 1 0 2 Rook 1 1 1 tools 3 3 2 16 grid 3 3 2 gtable 0 2 0 DBI 0 5 1 19 influenceR 0 1 0 DiagrammeR 0 9 0 htmltools 0 3 5 22 assertthat 0 1 yaml 2 1 14 lazyeval 0 2 0 25 rprojroot 1 1 digest 0 6 11 tibble 1 2 28 gridExtra 2 2 1 RColorBrewer 1 1 2 ggplot2 2 2 0 31 htmlwidgets 0 8 viridis 0 3 4 evaluate 0 10 34 rgexf 0 15 3 rmarkdown 1 3 stringi 1 1 2 37 scales 0 4 1 backports 1 0 4 XML 3 98 1 5 40 jsonlite 1 2,,"piiswrong,piiswrong,thirdwing,thirdwing",2017-01-18 04:27:08,2017-05-25 22:02:17
PR,Update documentation for mxnet ndarray GridGenerator,Thanks for Please review and merge,,"indhub,piiswrong,piiswrong,piiswrong,piiswrong,Lyken17,indhub,Lyken17,indhub",2017-05-24 20:56:55,2017-05-25 22:28:57
PR,Update documentation for deconvolution operation,mli Please review and merge,,"indhub,piiswrong,indhub,piiswrong,indhub,piiswrong,piiswrong",2017-05-10 00:30:19,2017-05-26 00:25:12
PR,skip lines that have matplotlib,,,nswamy,2017-05-25 23:25:46,2017-05-26 00:25:50
PR,Fixing some more broken links before v0 10 release,Fixing some more broken links before we make v0 10 release,,sandeep-krishnamurthy,2017-05-25 22:42:49,2017-05-26 00:26:18
PR,R typo in RNN close 4838,,,thirdwing,2017-05-26 00:27:48,2017-05-26 00:40:39
IS,rnn model R contains possible error,Environment info MXNet version 0 8 mx model init iter rnn The following function has possible wrong variable to use in dim I think shape dim data should be shape dim X,,,2017-01-31 12:31:41,2017-05-26 00:40:46
PR,Fix linear regression,Sometimes linear regression tutorial returns 'nan' due to hardcoded training data Use uniform random data and decrease learning rate,,"kevinthesun,madjam,madjam",2017-05-24 22:12:28,2017-05-26 05:27:49
PR,Pre trained model tutorial fixes,Before the change on running the tutorial for the first time a user would get this warning UserWarning Data provided by label shapes do not match names specified by label names vs isoftmax label' It also showed probability scores of 1 due to an incorrect usage of np argsort,,pracheer,2017-05-26 01:13:17,2017-05-26 05:33:31
PR,Nightly test tutorial,Add test tutorial python script Jenkins nightly test will be added after doc release,,"kevinthesun,piiswrong,kevinthesun",2017-05-25 21:08:02,2017-05-26 05:33:54
IS,finetune in Docker Error src io local filesys cc 154 Check failed allow null LocalFileSystem fail to open '',,,"ysh329,ysh329",2017-05-26 03:34:42,2017-05-26 06:56:40
PR,R captcha recognition example,,,thirdwing,2017-05-25 16:49:50,2017-05-26 14:59:32
PR,skip lines that have matplotlib,piiswrong can you please merge this,,nswamy,2017-05-26 07:27:52,2017-05-26 16:41:48
PR,Fix cudnn deconv not checking no bias,Not checking no bias in cudnn deconv would cause undefined behavior and it results in test failure in this PR,,reminisce,2017-05-26 03:49:32,2017-05-26 16:43:04
PR,fix batchNorm cpp example,,,"vsooda,piiswrong,vsooda,vsooda,piiswrong",2017-05-26 02:15:22,2017-05-26 16:44:42
PR,Fixing up issues in install guide,Contributor Cherry picked install guide changes from this PR This allows us to go ahead with v0 10 release,,sandeep-krishnamurthy,2017-05-26 18:45:53,2017-05-26 18:52:21
PR,Fixing copy code functionality for bash command,Tested locally after making the changes,,sandeep-krishnamurthy,2017-05-26 20:57:48,2017-05-26 21:07:29
PR,Residual unroll,piiswrong,,"szha,fhieber,szha,piiswrong",2017-05-23 05:30:02,2017-05-26 22:00:18
PR,Linear regression Tutorial link,The link was initially going to mxnet test readthedocs io Changed it to mxnet io api,,pracheer,2017-05-26 22:18:08,2017-05-26 22:24:44
PR,bump up version number for release,0 10 0,,"szha,piiswrong",2017-05-26 18:42:14,2017-05-26 22:25:18
PR,R DOC update R installation guide,,,thirdwing,2017-05-26 04:24:18,2017-05-26 22:51:56
PR,RE Change Interface of NDArray TBlob for DLPack Compatible,,,ZihengJiang,2017-05-24 20:51:40,2017-05-26 22:52:52
PR,fix doc build failures,mli,,nswamy,2017-05-26 23:51:42,2017-05-27 00:11:50
PR,update news and Readme,piiswrong,,"nswamy,mli",2017-05-27 00:08:02,2017-05-27 00:25:08
PR,Empty commit,Do Not Merge This is an empty commit to test Tagged v0 10 release,,"lxn2,mli,lxn2",2017-05-26 23:32:51,2017-05-27 01:50:49
PR,Update im2rec py,Updated Line 107 of 'im2rec py' Read an image as binary,,,2017-05-27 02:41:48,2017-05-27 03:37:16
PR,New Function Add Reset into kvstore enable set params to reset weights in kvstore,6421 6005,,,2017-05-25 00:23:10,2017-05-27 06:09:45
IS,Broken link in tutorial,Both links on Next Step part are broken on Image io tutorial page I can not find this file under docs folder where is it now,,"Godricly,piiswrong",2017-01-18 15:23:24,2017-05-27 06:14:14
PR,Some minor edits to ndarray md and symbol md,As I was reading through these guides I made a few modifications There is a few cases where I have added some more context a few grammar spelling fixes and quite a few fixes to the line length I have tried to apply a uniform line length of 80 as that seems to be the closest to what was used in these files All edits here are simply suggestions I'm completely open to further edits or reverting some of the changes present in this PR,,"KellenSunderland,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,zackchase,KellenSunderland,piiswrong,KellenSunderland,piiswrong,madjam,KellenSunderland",2017-04-28 12:23:45,2017-05-27 07:46:08
IS,c Model File json Loading error,Hi I'm keep trying to implement MTCNN with C and MxNet However now I receive an error when I try to load a model file with Symbol Load If any of you ever have tried to load model files to use as classifier I would love to know how did you make it I leave what I have tried below Thanks in advance Environment info Operating System OS X 10 12 4 Compiler AppleClang Package used Python R Scala Julia C MXNet commit hash git rev parse HEAD 918d48526481cf424197079ae47841e1b8afe399 Error Message Please paste the full error message including stack trace What have you tried to solve it 1 I have tried to load with Symbol Load model file path though the error returned 2 Then I have checked if Load function is working correct and realized that MXSymbolCreateFromFile function which is called in Load function does not work well If you have any ides or noticed how I'm doing wrong please tell me Thanks for your corporation,,lx75249,2017-05-24 11:05:35,2017-05-27 10:37:01
IS,how to use part of a label vector split a label vector in network building,Hi Everyone I came across a situation Supposing that I have a single label vector as L1 L2 L3 N1 N2 N3 My network has two branches one branch uses L1 L2 L3 as labels and the other uses N1 N2 N3 I knew I may access label as label mx sym Variable 'label name' but how should I access different parts of the label vector or split the label vector when I build the network Thanks a lot,,,2017-05-28 06:44:54,2017-05-28 21:08:41
PR,Change Interface of NDArray TBlob for DLPack Compatible,tqchen,,"ZihengJiang,piiswrong,piiswrong,piiswrong,mli,ZihengJiang,tqchen,tqchen,tqchen,piiswrong,ZihengJiang,tqchen,ZihengJiang,tqchen,piiswrong,tqchen,piiswrong,tqchen,mli,ZihengJiang,ZihengJiang,piiswrong,piiswrong,piiswrong",2017-05-19 17:45:41,2017-05-30 06:37:09
IS,How to use simple bind for multiple GPUS,The following code gives this error devs mx gpu 0 mx gpu 1 exe net simple bind ctx devs data data shape it gives error F0527 05 31 35 191905 23992 base h 273 Check failed r str length 1 6 vs 7,,,2017-05-27 09:32:05,2017-05-30 06:59:20
IS,Multi Label CNN for Emotion Recognition,Hi everyone I'm trying to train a CNN to detect facial expressions in an image dataset Since I'm still new to both CNN is and R I'm starting with a very simple CNN but I do not seem to get the multi label approach working The images are 64x64 grayscale images and the labels are binary vectors with 6 elements each e g 0 1 0 0 1 1 This is the code I have so far which I have built and modified from examples Any idea of how could it be done,,"thirdwing,thirdwing",2016-11-05 22:54:38,2017-05-30 17:09:42
PR,update faster rcnn example with logging and cpu make,the contents are remove deprecated files update pycocoutils to current version use logging for output in pycocotools cocoeval py from future imports print function is not on the first line and will cause syntaxerror on python 2 7 6 support cpu make and setup py by skipping gpu nms fix proposal op note that the old code happened to be correct because base anchor is a square file changes commit 1 example rcnn py commit 2 src operator contrib proposal inl h,,precedenceguo,2017-05-30 10:01:30,2017-05-30 17:57:40
PR,Remove horizontal scroll bar,,,kevinthesun,2017-05-30 18:29:06,2017-05-30 18:48:03
PR,R DOC fix R tutorials,,,"thirdwing,mli,thirdwing",2017-05-27 00:23:26,2017-05-30 20:31:30
PR,symbolic imperative nn interface,a high level NN interface that supports symbolic imperative use With a mixed Keras and pytorch flavor,,"piiswrong,mli,mli,mli,mli,piiswrong,ZihengJiang,pluskid,pluskid,pluskid,pluskid,pluskid,piiswrong,piiswrong,piiswrong,pluskid,pluskid,piiswrong,piiswrong,piiswrong,pluskid,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,pluskid,pluskid,mli,mli,mli,sxjscience,sbodenstein,piiswrong,pluskid,yzhliu,tdomhan,pluskid",2017-04-05 23:52:08,2017-05-30 23:40:30
IS,Failed feed R array data into mxnetR for CNN model,Hi All anyone can help me to better understand how to build R array data and feed into mxnetR for CNN model training i use the following r script to convert the cifar 10 bin data into R array data and feed into mxnet for CNN training the accuracy is always around 0 1 library mxnet setwd C Users mlai3 Downloads CIFAR 10 labels read table bin batches meta txt images rgb array 0 c 32 32 3 50000 images lab c num images 10000 Set to 10000 to retrieve all images per file to memory Cycle through all 5 binary files for f in 1 5 to read file paste bin data batch f bin sep rb for i in 1 num images l readBin to read integer size 1 n 1 endian big r as integer readBin to read raw size 1 n 1024 endian big g as integer readBin to read raw size 1 n 1024 endian big b as integer readBin to read raw size 1 n 1024 endian big index num images f 1 i images rgb 1 32 1 32 1 index as array matrix r ncol 32 byrow TRUE images rgb 1 32 1 32 2 index as array matrix g ncol 32 byrow TRUE images rgb 1 32 1 32 3 index as array matrix b ncol 32 byrow TRUE images lab index l 1 close to read remove l r g b f i index to read dim images rgb c 32 32 3 50000 train mx io arrayiter data images rgb label images lab batch size 128 shuffle TRUE,,"thirdwing,thirdwing,thirdwing",2016-10-26 07:23:25,2017-05-31 01:06:08
PR,backward headgrads and detach,,,"piiswrong,sneakerkg,piiswrong,sneakerkg,piiswrong,sneakerkg",2017-05-18 23:39:49,2017-05-31 02:02:09
PR,Revert R DOC fix R tutorials 6472,This reverts commit 215ae4a0dd1a96ce75e1c451809dbfade286cfcd,,thirdwing,2017-05-31 04:18:09,2017-05-31 04:32:17
IS,BatchNorm and Dropout error,Environment info Operating System macOS Package used Python R Scala Julia Python MXNet version 0 9 5 Python version and distribution Python 3 6 Error Message When I pass in the symbol into the Batch norm and Dropout functions without explicitly saying data I get this error If I explicitly say data I get a data argument not recognized error Error Traceback most recent call last File test py line 51 in module path test1 csv path test2 csv File mxnetengine py line 65 in fit net mx symbol BatchNorm net File usr local lib python3 6 site packages mxnet ctypes symbol py line 191 in creator s compose args name name symbol kwargs File usr local lib python3 6 site packages mxnet symbol py line 298 in compose raise TypeError 'Compose expect Symbol as arguments' TypeError Compose expect Symbol as arguments Minimum reproducible example Note I have an object known as model version that has a list of hidden layer objects hidden layers of my own class which just record the number of neurons and their layer number Thank you,,,2017-05-30 15:58:45,2017-05-31 05:23:03
PR,Add parameter of number per class,We could control the number of per class images in making list via num perclass,,SCP-173-cool,2017-05-31 07:10:31,2017-05-31 07:22:09
IS,Is mxnet support distribute training on two machine one has just one gpu and another has two gpus,In the example I can pass gpus 0 for both machines Can I pass gpus 0 to one machine and gpus 0 1 to another,,,2017-05-31 11:57:48,2017-05-31 12:02:43
IS,A question about key value,How does each key value pair correspond to the weights in the network One pair corresponds to the weights of a layer or something else,,,2017-05-31 13:35:40,2017-05-31 14:59:37
IS,mxnetR parameter setting,Hi All i am current working on an image classification project by using mxnetR i faced one question about setting paramters for this package especially for the function mx model FeedForward create i read couple instructions from different sources for parameters wd and initializer in this function i want to know how to set up these two parameters what part of the network these two parameter control running results for 8 rounds for the following R script Start training with 1 devices 1 Train accuracy 0 19530612244898 2 Train accuracy 0 1938 3 Train accuracy 0 1948 4 Train accuracy 0 1916 5 Train accuracy 0 1916 6 Train accuracy 0 1916 7 Train accuracy 0 1916 8 Train accuracy 0 1916 here you are my script i use the CIFAR 10 pictures as a pilot to set up alexnet alexnet script was copied from this website library mxnet library imager Lists read table trainLabels csv header TRUE sep Cat Lists Lists label cat Dog Lists Lists label dog Bird Lists Lists label bird Deer Lists Lists label deer Horse Lists Lists label horse files list files train a array 0 c 32 32 3 5000 train y c for i in 1 1000 filename paste Cat id i png sep if length intersect filename files 1 path paste train filename sep test load image path a 1 32 1 32 1 5 i 1 1 R test a 1 32 1 32 2 5 i 1 1 G test a 1 32 1 32 3 5 i 1 1 B test train y 5 i 1 1 1 filename paste Dog id i png sep if length intersect filename files 1 path paste train filename sep test load image path a 1 32 1 32 1 5 i 1 2 R test a 1 32 1 32 2 5 i 1 2 G test a 1 32 1 32 3 5 i 1 2 B test train y 5 i 1 2 2 filename paste Bird id i png sep if length intersect filename files 1 path paste train filename sep test load image path a 1 32 1 32 1 5 i 1 3 R test a 1 32 1 32 2 5 i 1 3 G test a 1 32 1 32 3 5 i 1 3 B test train y 5 i 1 3 3 filename paste Deer id i png sep if length intersect filename files 1 path paste train filename sep test load image path a 1 32 1 32 1 5 i 1 4 R test a 1 32 1 32 2 5 i 1 4 G test a 1 32 1 32 3 5 i 1 4 B test train y 5 i 1 4 4 filename paste Horse id i png sep if length intersect filename files 1 path paste train filename sep test load image path a 1 32 1 32 1 5 i 1 5 R test a 1 32 1 32 2 5 i 1 5 G test a 1 32 1 32 3 5 i 1 5 B test train y 5 i 1 5 5 train array round a 2 rm a Define deep learning network act type can be relu sigmoid softrelu and tanh input data mx symbol Variable name data stage 1 conv1 mx symbol Convolution data input data kernel c 8 8 stride c 2 2 num filter 96 relu1 mx symbol Activation data conv1 act type relu next layer data size calculation method W pre layer data size here w 32 F kernel size here F 8 S stride step size here S 2 P zero padding parameter here P 0 after stage 1 the data size is 32 8 2 0 2 1 13 pool1 mx symbol Pooling data relu1 pool type max kernel c 3 3 next layer data size 13 3 2 0 1 1 11 lrn1 mx symbol LRN data pool1 alpha 0 0001 beta 0 75 knorm 1 nsize 5 stage 2 conv2 mx symbol Convolution data lrn1 kernel c 5 5 pad c 1 1 num filter 96 relu2 mx symbol Activation data conv2 act type relu next layer data size 11 5 2 1 1 1 9 pool2 mx symbol Pooling data relu2 kernel c 3 3 pool type max next layer data size 9 3 2 0 1 1 7 lrn2 mx symbol LRN data pool2 alpha 0 0001 beta 0 75 knorm 1 nsize 5 stage 3 conv3 mx symbol Convolution data lrn2 kernel c 3 3 pad c 1 1 num filter 384 relu3 mx symbol Activation data conv3 act type relu next layer data size 7 3 2 1 1 1 7 conv4 mx symbol Convolution data relu3 kernel c 3 3 pad c 1 1 num filter 384 relu4 mx symbol Activation data conv4 act type relu next layer data size 7 3 2 1 1 1 7 conv5 mx symbol Convolution data relu4 kernel c 3 3 pad c 0 0 num filter 256 relu5 mx symbol Activation data conv5 act type relu next layer data size 7 3 2 0 1 1 5 pool3 mx symbol Pooling data relu5 kernel c 2 2 stride c 1 1 pool type max next layer data size 5 2 2 0 1 1 4 stage 4 flatten mx symbol Flatten data pool3 fc1 mx symbol FullyConnected data flatten num hidden 1024 relu6 mx symbol Activation data fc1 act type relu dropout1 mx symbol Dropout data relu6 p 0 5 stage 5 fc2 mx symbol FullyConnected data dropout1 num hidden 256 relu7 mx symbol Activation data fc2 act type relu dropout2 mx symbol Dropout data relu7 p 0 5 stage 6 fc3 mx symbol FullyConnected data dropout2 num hidden 5 softmax mx symbol SoftmaxOutput data fc3 name isoftmax' mx set seed 0 device cpu mx cpu 1 tic proc time model mx model FeedForward create softmax X train array y train y ctx device cpu num round 10 array batch size 100 learning rate 0 05 momentum 0 9 eval metric mx metric accuracy initializer mx init uniform 0 07 wd 0 0001 epoch end callback mx callback log train metric 100,,thirdwing,2016-09-26 01:58:29,2017-05-31 16:40:59
IS,R should we use the consistent syntax parameter handling with Python side,I think we need to have some discussion on this Right now there are some difference in the syntax data handling parameter handling between R and python side which leads to Since we are using the same C API it is possible to provide consistency between R and python I want to hear some opinions on this,,"thirdwing,miguelgfierro",2016-08-26 18:06:33,2017-05-31 16:42:13
PR,nn interface,,,piiswrong,2017-05-31 06:46:09,2017-05-31 16:56:33
PR,Fixed few typos in the Python tutorials,Comparing to basic tutorials training ones had way fewer typos Nevertheless spotted some as I was going through,,apaleyes,2017-05-31 14:17:46,2017-05-31 19:41:28
PR,Remove BUILD TAG in DOCKER IMG NAME in ci build sh,To avoid generating too many images when run docker images,,mli,2017-05-31 18:27:23,2017-05-31 19:43:40
PR,Fix for build,,,ZihengJiang,2017-05-31 17:23:58,2017-05-31 19:52:35
PR,Test macos installation,This PR adds logic to test krishnamurthy is python installation guide for MacOS CPU on Travis We test the installation in a clean environment via pip virtualenv and from source,,"lxn2,piiswrong,lxn2,piiswrong,mli,lxn2,lxn2,bhavinthaker,lxn2,lxn2,mli,lxn2,lxn2,piiswrong,lxn2,piiswrong,lxn2,szha",2017-05-13 03:58:51,2017-05-31 20:54:32
PR,add making tests for GPU,I'm working on implementation of tensor methods generic unfolding of tensors CP Decomposition etc Could we add the following section to tests cpp unittest mk to make tests for GPU All the test files destined for GPU shall end by test gpu cu,,"jli05,jli05",2017-03-16 22:20:39,2017-05-31 21:53:48
PR,Revert Test macos installation,Reverts dmlc mxnet 6231 This commit broke the docs website a number of installation instructions went missing,,lxn2,2017-05-31 23:30:09,2017-06-01 00:36:09
PR,Fix Search Description,,,kevinthesun,2017-06-01 00:06:39,2017-06-01 00:36:24
PR,Remove duplicate new operator how to,,,madjam,2017-05-31 22:12:00,2017-06-01 00:37:07
IS,Build fails for Mxnet 0 10 0,Been trying to build Mxnet using Jenkins CI Here is the stack trace of the bug,,"piiswrong,piiswrong",2017-05-31 06:42:50,2017-06-01 06:22:03
PR,hide move var move mean beta gamma for visualization,Parameters bn gamma bn beta bn var bn mean occupy a lot of space while providing little information I suggest to hide them during visualization plot complex pdf plot complex resnet pdf plot clean pdf plot clean resnet pdf,,"Lyken17,tqchen",2017-06-01 05:41:54,2017-06-01 17:25:11
IS,Pull Requests Hide gamma beta var mean in graph visualization,Hi all After ResNet Batch Normalization becomes quite popular and is widely used recently However for visualization I found parameters bn gamma bn beta bn var bn mean occupy a lot of space while providing little information plot complex pdf And it also made visualization of some large networks totally unreadable plot complex resnet pdf During my experiments BN is always treated as a normal layer like a convolution Designers usually does not focus on beta gamma mean var especially on visualization I suggest to hide these parameters during visualization Examples after modification plot clean pdf plot clean resnet pdf I have submitted a pull request for this issue,,"Lyken17,piiswrong",2017-06-01 05:42:12,2017-06-01 17:28:15
IS,Install of the pre built binary R package fails,Install of the pre built binary R package using RStudio fails with 404 error file not found,,"thirdwing,thirdwing,thirdwing",2017-05-31 19:23:22,2017-06-01 17:41:57
IS,Correct array shape for mx lstm,Hello I have been following the RNN example but I'm unable to adopt it to a lay example Specifically I'm unable to set the correct parameters for mx lstm without it throwing a shape error exception similar issue Could someone please elaborate on the below for a non text example num embed integer The output dim of embedding num label integer The number of labels input size integer The input dim of one hot encoding of embedding seq len integer The length of the input sequence Environment Error Check failed from shape to shape operands shape mismatchfrom shape 10 1 to shape 10 4 Thanks,,thirdwing,2017-02-25 13:29:13,2017-06-01 17:47:08
IS,amalgamation backtrace and backtrace symbols not declared in scope,Have anyone seen the following issues Is there an easy way to fix them Thanks mxnet predict all cc 646 50 error backtrace was not declared in this scope int nframes backtrace stack MAX STACK SIZE mxnet predict all cc 648 51 error backtrace symbols was not declared in this scope char msgs backtrace symbols stack nframes,,,2017-05-31 21:26:17,2017-06-01 18:35:26
PR,Add link to style guide in contribute md,,,Roshrini,2017-06-01 16:37:23,2017-06-01 20:59:30
PR,Module tutorial improvements and metric API doc linked,madjam Please take a look,,"Roshrini,nswamy,Roshrini,nswamy,nswamy,nswamy,nswamy",2017-06-01 21:28:36,2017-06-02 04:50:53
PR,add requests graphviz used test utils and visualization to python setup,test utils py uses requests package but fails when it cannot be found similarly visualization py uses graphviz dependent packages should be installed along with mxnet installation,,"nswamy,piiswrong,szha,nswamy",2017-06-01 22:21:15,2017-06-02 05:21:38
PR,R compatibility with R 3 2 0 close 6525,,,"thirdwing,thirdwing,jiajiechen",2017-06-01 22:28:57,2017-06-02 05:23:32
IS,MXNET 0 10 0 endsWith not found,I'm using MXNET 0 10 0 on Linux Mint with GPU and get the following error when running a CNN model using R Error in mx model init params symbol input shape output shape initializer could not find function endsWith I have tried downloading and recompiling the MXNET code over the past couple of days with the same results,,thirdwing,2017-06-01 06:30:47,2017-06-02 05:23:38
IS,Problem R package install on Windows 10 NAMESPACE file is missing,Hello As I am not expert in R package installing I have been struggling a lot with the installation of mxnet R package for Windows Short version I think I have done the first steps by following the indications given in the official tutorial and in answers to other issues especially 1085 But the final command R CMD INSTALL no multiarch R package outputs an error saying a NAMESPACE file is required but there is no such file in mxnet R package and I do not know where i am suppose to find this file And if it is because I have done something wrong in the previous steps I have really no idea what to change Please I need some help or advices Environment info Operating System Windows 10 Package used R R informations R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 8 x64 build 9200 locale 1 LC COLLATE French France 1252 LC CTYPE French France 1252 LC MONETARY French France 1252 LC NUMERIC C LC TIME French France 1252 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 tools 3 3 2 How I tried to install I followed globaly the installing procedure given at but not with the versions mentioned in this tutorial they do not seem to be up to date if I believe what i found in the similar forum issues Correct me if I misunderstood I first download and install Visual studio Community 2015 with update 3 at msft vscom older downloads campaign o msft vscom older downloads cudaToolKit v8 0 associated with cudNN v5 1 Download the zip of mxnet github repository and unzip it in a mxnet folder Download the cuDNN v5 1 Jan 20 2017 for CUDA 8 0 for Windows 10 zip file found at I chose this version following the instructions of the last mxnet windows package release it condition the choice of cudaToolKit v8 0 I unzip it in a cudnn folder so in cudnn cuda I have 3 folders bin lib and include Download 20160531 win10 x64 gpu 7z and 20170505 mxnet x64 vc14 gpu 7z of the build windows update 20170505 found at Unzip the first in a folder 20160531 win10 x64 gpu Unzip the second in a folder 20170505 mxnet x64 vc14 gpu I copy the contain of previous folder cudnn cuda 3 folders bin lib and include and paste it into 20160531 win10 x64 gpu 3rdparty cudnn Now in the folder mxnet R package I create inst libs x64 and fill it with the dll files found in those folders 20160531 win10 x64 gpu 3rdparty cudart 20160531 win10 x64 gpu 3rdparty cudnn bin cudnn64 5 dll 20160531 win10 x64 gpu 3rdparty openblas bin 20160531 win10 x64 gpu 3rdparty vc 20160531 win10 x64 gpu 3rdparty lib 20170505 mxnet x64 vc14 gpu build which contain only libmxnet dll I have now 12 dll files in libs x64 I create folder include into mxnet R package inst and copy the 3 folders from 20170505 mxnet x64 vc14 gpu include Open the Windows terminal as admin I go into the folder mxnet and execute the command line R CMD INSTALL no multiarch R package Immediatly I get the following error message Error Message installing to library 'C Users Christophe Documents R win library 3 3' installing source package 'mxnet' ERROR a 'NAMESPACE' file is required removing 'C Users Christophe Documents R win library 3 3 mxnet' restoring previous 'C Users Christophe Documents R win library 3 3 mxnet',,"thirdwing,thirdwing,thirdwing,thirdwing",2017-05-08 09:59:14,2017-06-02 19:19:01
IS,Issues on installing mxnet on R with GPU support on Windows 10,I am attempting to install mxnet on R with GPU support on windows 10 but encountered numerous errors during the process After some digging I have identified the root of the issues Overall the installation file and source code for R is a total mess I followed the instruction in this link url After running R CMD INSTALL no multiarch R package on the command prompt the first issue encountered is this installing to library 'D R R 3 3 3 library' installing source package 'mxnet' ERROR a 'NAMESPACE' file is required removing 'D R R 3 3 3 library mxnet' After resolving this by referring to this link url another issue occuered due to missing nnvm file Error as follow installing to library 'D R R 3 3 3 library' installing source package 'mxnet' libs c Rtools mingw 64 bin g I D R R 3 3 3 include DNDEBUG I inst include I D R R 3 3 3 library Rcpp include I d Compiler gcc 4 9 3 local330 include O2 Wall mtune core2 c mxnet cc o mxnet o In file included from mxnet cc 9 0 ndarray h 11 24 fatal error nnvm c api h No such file or directory include nnvm c api h compilation terminated make mxnet o Error 1 Warning running command 'make f Makevars win f D R R 3 3 3 etc x64 Makeconf f D R R 3 3 3 share make winshlib mk SHLIB LDFLAGS ' SHLIB CXXLDFLAGS ' SHLIB LD ' SHLIB CXXLD ' SHLIB mxnet dll WIN 64 TCLBIN 64 OBJECTS executor o export o io o kvstore o mxnet o ndarray o symbol o ' had status 2 ERROR compilation failed for package 'mxnet' removing 'D R R 3 3 3 library mxnet' After resolving this by downloading the nnvm from some other place then there is issue with source code The source code apparently is missing some functions Error as follow installing to library 'D R R 3 3 3 library' installing source package 'mxnet' libs c Rtools mingw 64 bin g I D R R 3 3 3 include DNDEBUG I inst include I D R R 3 3 3 library Rcpp include I d Compiler gcc 4 9 3 local330 include O2 Wall mtune core2 c mxnet cc o mxnet o In file included from mxnet cc 8 0 base h In function istd string mxnet R toPyString const string const RObject ' base h 276 26 warning comparison between signed and unsigned integer expressions Wsign compare for size t i 0 i vec size i base h In function istd vector unsigned int mxnet R Dim2InternalShape const Rcpp Dimension ' base h 340 24 warning comparison between signed and unsigned integer expressions Wsign compare for size t i 0 i rshape size i mxnet cc In function 'void mxnet R ProfilerSetConfig int const string ' mxnet cc 27 53 error 'MXSetProfilerConfig' was not declared in this scope MX CALL MXSetProfilerConfig mode filename c str base h 81 14 note in definition of macro 'MX CALL' int e func mxnet cc In function 'void mxnet R ProfilerSetState int ' mxnet cc 31 35 error 'MXSetProfilerState' was not declared in this scope MX CALL MXSetProfilerState state base h 81 14 note in definition of macro 'MX CALL' int e func make mxnet o Error 1 Warning running command 'make f Makevars win f D R R 3 3 3 etc x64 Makeconf f D R R 3 3 3 share make winshlib mk SHLIB LDFLAGS ' SHLIB CXXLDFLAGS ' SHLIB LD ' SHLIB CXXLD ' SHLIB mxnet dll WIN 64 TCLBIN 64 OBJECTS executor o export o io o kvstore o mxnet o ndarray o symbol o ' had status 2 ERROR compilation failed for package 'mxnet' removing 'D R R 3 3 3 library mxnet' Overall the installation file and source code for R is pretty sloppy Would appreciate if anyone have any means on installing mxnet,,thirdwing,2017-03-26 07:13:33,2017-06-02 19:21:13
IS,window 7 install R package with GPU,Try almost two weeks still cannot solve please help me OS window 7 64bits cuda cuda7 5 cudnn cudnn v3 R version R 3 2 5 release 20160531 win10 x64 gpu I follow this R Package Installation r package installation in GPU get some error likes NAMESPACE solve copy row NAMESPACE jsonlite solve install in R Rcpp solve install in R BUT this one I have no idea My Rtools is 3 2 5 image I really need help thank you,,"thirdwing,thirdwing,thirdwing",2017-02-07 16:09:57,2017-06-02 19:23:24
IS,cannot install latest 20170206 gpu version r package on windows 10,Hi I tried to install the latest gpu version of mxnet r package on my windows machine but encounter some problems First the instruction on the website does not match with actual downloaded package For one the link to the most recent GPU enabled MXNet package from the Releases tab does not contain what is said to be there but rather provide to another location which does provide nightly windows release however the release does not follow same folder structure as indicated by the installation instruction Second there is no NAMESPACE file in the latest release package which produces an error when try to install from source I tried copy NAMESPACE from an older version 20161125 after which the installation produce the current error as listed below Another question the installation instruction mention having MS visual studio 2013 as a pre requisite But the actual installation for R section does not mention when it is used since the library is pre built dll what is MS VS 2013 required for here also does it must be 2013 can I use 2015 In general I feel the installation manual section requires some major update at least for R GPU version For example I'm now using an older version 20161125 which installs ok after some tweaking and runs a few of the examples fine But to make it work I also have to resort to some manual tweaking in particular find out the correct dll dependency and manually put the correct ones in place mostly the correct cuda libraries Please help Thank you For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System windows 10 Pro x64 Compiler Rtools 3 4 Package used Python R Scala Julia R 3 3 2 Rstudio 1 0 136 MXNet version Or if installed from source installed from source prepared as instructed in MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 8 x64 build 9200 locale 1 LC COLLATE English Singapore 1252 LC CTYPE English Singapore 1252 LC MONETARY English Singapore 1252 4 LC NUMERIC C LC TIME English Singapore 1252 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 tools 3 3 2 Error Message Please paste the full error message including stack trace installing source package 'mxnet' libs c Rtools mingw 64 bin g I C PROGRA 1 R R 33 1 2 include DNDEBUG I inst include I C Users admin Documents R win library 3 3 Rcpp include I d Compiler gcc 4 9 3 local330 include O2 Wall mtune core2 c executor cc o executor o In file included from executor h 13 0 from executor cc 10 symbol h 11 24 fatal error nnvm c api h No such file or directory include nnvm c api h compilation terminated make executor o Error 1 Warning running command 'make f Makevars win f C PROGRA 1 R R 33 1 2 etc x64 Makeconf f C PROGRA 1 R R 33 1 2 share make winshlib mk SHLIB LDFLAGS ' SHLIB CXXLDFLAGS ' SHLIB LD ' SHLIB CXXLD ' SHLIB mxnet dll WIN 64 TCLBIN 64 OBJECTS executor o export o io o kvstore o mxnet o ndarray o symbol o ' had status 2 ERROR compilation failed for package 'mxnet' removing 'C Users admin Documents R win library 3 3 mxnet' Warning in install packages running command ' C PROGRA 1 R R 33 1 2 bin x64 R CMD INSTALL no multiarch l C Users admin Documents R win library 3 3 c users admin Downloads mxnet mxnet master20170206 R package ' had status 1 Warning in install packages installation of package users admin Downloads mxnet mxnet master20170206 R package had non zero exit status Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,thirdwing,2017-02-06 06:35:18,2017-06-02 19:23:58
IS,Setting lr multiplier for convolution layer,Hi I have trouble setting lr multiplier and decay mult for convolution operation example fc1 voc12 c3 mx symbol Convolution name 'fc1 voc12 c3' data conv feat kernel 3 3 no bias True num filter num classes dilate 24 How can I set a different lr mult for this layer alone in my Deeplab implementation,,piiswrong,2017-06-01 00:36:13,2017-06-02 21:19:52
PR,Fix Nightly Tutorial Test,,,kevinthesun,2017-06-02 20:44:10,2017-06-02 21:20:47
PR,R more tweaking for R 3 2 0,jiajiechen Let is see if this fix our building on R 3 2 0 It should be the last release we support R 3 2 0,,"thirdwing,jiajiechen,thirdwing",2017-06-02 17:07:36,2017-06-02 21:21:37
PR,fix typo on BatchNorm for mirroring,After changing this line we are able to set the attribute force mirroring of a BatchNorm layer to True,,"taineleau,piiswrong,taineleau",2017-06-02 16:58:04,2017-06-02 21:22:12
IS,Where is gen data py in example bi lstm sort,As README md in mxnet example bi lstm sort mentions This is an example of using bidirection lstm to sort an array Firstly generate data by,,"ysh329,gurumurthys,ysh329",2017-03-17 02:47:37,2017-06-03 01:37:51
PR,Amalgamation test failure solved,Thanks for helping with this This should solve amalgamation test failure,,"Roshrini,piiswrong,Roshrini,piiswrong,piiswrong",2017-06-02 22:58:48,2017-06-03 02:25:50
PR,add cgan R demo scripts close 6040,thirdwing Scripts for cgan demo,,"jeremiedb,thirdwing,jeremiedb",2017-06-03 00:04:59,2017-06-03 05:21:37
IS,DCGAN demo for R package,I prepared a compact demo of conditional generative network on MNIST Feel free to add to the How To for R or let me know if some adaptations would be needed,,"jeremiedb,thirdwing",2017-04-29 19:13:54,2017-06-03 05:21:48
PR,Small fix for tutorial test,,,kevinthesun,2017-06-03 00:11:20,2017-06-03 05:32:34
PR,Scala hide move var move mean beta gamma for visualization,follows 6523,,Ldpe2G,2017-06-03 01:55:52,2017-06-03 05:34:23
PR,resnetv1,,,szha,2017-05-31 20:46:17,2017-06-03 05:35:04
PR,Improve symbol bindings,This PR 1 moves mx symbol simple bind to backend for creating ndarrays etc 2 bind ith exec calls simple bind instead of bind Benchmark Environment p2 xlarge Step 1 Create an executor group as shared group Step 2 Call DataExecutorGroup constructor 500 times and pass the executor group created in Step 1 in as shared group Timed Step 2 results New simple bind 18 3 ms per executor group creation Old simple bind 19 4 ms per executor group creation Benchmark script,,"reminisce,piiswrong,piiswrong,reminisce,reminisce,yuruofeifei,reminisce,yuruofeifei,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,reminisce,piiswrong,piiswrong,piiswrong,piiswrong,reminisce,reminisce,reminisce,eric-haibin-lin,reminisce,piiswrong,eric-haibin-lin,reminisce,eric-haibin-lin,reminisce,eric-haibin-lin,reminisce,piiswrong,eric-haibin-lin,piiswrong,reminisce",2017-04-17 18:18:56,2017-06-03 06:58:22
PR,add random multinomial sample,,,piiswrong,2017-06-03 00:29:47,2017-06-03 06:59:31
PR,R save load MXNet model with RData format close 362,Need more advice from,,"thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,hetong007",2017-05-30 23:16:46,2017-06-03 19:35:57
PR,actor critic,,,piiswrong,2017-06-03 20:12:12,2017-06-03 21:17:37
PR,Scala Optimizer support lr mult and wd mult,javelinjs would you help to review,,"Ldpe2G,yzhliu",2017-06-03 13:50:21,2017-06-04 12:44:19
PR,Add NVVM is NNSymbolListInputNames to symbolic,Add NVVM is NNSymbolListInputNames to symbolic,,"alues,piiswrong",2017-06-04 17:32:49,2017-06-04 22:48:17
PR,Some Changes to NDArray Interface,Remove NDArray raw data Change NDArray offset to byte offset,,"ZihengJiang,ZihengJiang,tqchen,piiswrong,tqchen,piiswrong,tqchen,tqchen,tqchen",2017-06-04 02:35:37,2017-06-05 00:43:47
IS,Build of R Package faild,I'm trying to build the R Package of mxnet using RStudio Build failed with the following error R CMD INSTALL no multiarch with keep source R package installing to library Users admin github mxnet R package packrat lib x86 64 apple darwin13 4 0 3 3 2 installing source package mxnet clang I Library Frameworks R framework Resources include DNDEBUG I inst include I usr local include I usr local include freetype2 I opt X11 include I Users admin github mxnet R package packrat lib x86 64 apple darwin13 4 0 3 3 2 Rcpp include fPIC Wall mtune core2 g O2 c executor cc o executor o libs In file included from executor cc 9 base h 12 10 fatal error wouldmlc base h' file not found include dmlc base h 1 error generated make executor o Error 1 ERROR compilation failed for package mxnet removing Users admin github mxnet R package packrat lib x86 64 apple darwin13 4 0 3 3 2 mxnet Exited with status 1,,"offerm,offerm",2017-06-05 13:08:07,2017-06-05 14:27:23
PR,reorg code to temporary namespace foo,,,piiswrong,2017-06-05 01:10:45,2017-06-05 17:07:13
IS,SSD example fails to run after backward headgrads and detach commit,Environment info Operating System Ubuntu 16 04 1 Compiler gcc Package used Python R Scala Julia Python MXNet version see git bisect details below Python version and distribution Python 2 7 6 from apt Error Message Please paste the full error message including stack trace 16 32 06 home ubuntu src mxnet dmlc core include dmlc logging h 304 16 32 06 src core symbolic cc 72 Symbol ComposeKeyword argument name anchors not found Candidate arguments 0 cls prob 1 loc pred 2 anchor Stack trace returned 10 entries bt 0 home ubuntu src mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f588da225bc bt 1 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm23KeywordArgumentMismatchEPKcRKSt6vectorISsSaISsEERKN4dmlc10array viewISsEE 0x22a 0x7f588f54715a bt 2 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm6Symbol7ComposeERKN4dmlc10array viewIPKS0 EERKSt13unordered mapISsS4 St4hashISsESt8equal toISsESaISt4pairIKSsS4 EEERSE 0x128f 0x7f588f543d0f bt 3 home ubuntu src mxnet python mxnet lib libmxnet so NNSymbolCompose 0x31b 0x7f588f5308db bt 4 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f593905eadc bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call 0x1fc 0x7f593905e40c bt 6 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48e 0x7f59392755fe bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x15f9e 0x7f5939276f9e bt 8 python PyEval EvalFrameEx 0x98d 0x5244dd bt 9 python 0x568b3a Traceback most recent call last File demo py line 99 in module ctx args nms thresh args force nms File demo py line 40 in get detector get symbol len CLASSES nms thresh force nms File home ubuntu src mxnet example ssd symbol symbol vgg16 ssd 300 py line 179 in get symbol net get symbol train num classes File home ubuntu src mxnet example ssd symbol symbol vgg16 ssd 300 py line 150 in get symbol train variances 0 1 0 1 0 2 0 2 nms topk nms topk File string line 47 in contrib MultiBoxDetection File home ubuntu src mxnet python mxnet ctypes symbol py line 136 in symbol creator s compose name name kwargs File home ubuntu src mxnet python mxnet symbol py line 419 in compose self handle name num args keys args File home ubuntu src mxnet python mxnet base py line 85 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 16 32 06 src core symbolic cc 72 Symbol ComposeKeyword argument name anchors not found Candidate arguments 0 cls prob 1 loc pred 2 anchor Stack trace returned 10 entries bt 0 home ubuntu src mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f588da225bc bt 1 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm23KeywordArgumentMismatchEPKcRKSt6vectorISsSaISsEERKN4dmlc10array viewISsEE 0x22a 0x7f588f54715a bt 2 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm6Symbol7ComposeERKN4dmlc10array viewIPKS0 EERKSt13unordered mapISsS4 St4hashISsESt8equal toISsESaISt4pairIKSsS4 EEERSE 0x128f 0x7f588f543d0f bt 3 home ubuntu src mxnet python mxnet lib libmxnet so NNSymbolCompose 0x31b 0x7f588f5308db bt 4 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f593905eadc bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call 0x1fc 0x7f593905e40c bt 6 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48e 0x7f59392755fe bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x15f9e 0x7f5939276f9e bt 8 python PyEval EvalFrameEx 0x98d 0x5244dd bt 9 python 0x568b3a Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 git checkout git submodule update make python setup py install export PYTHONPATH etc 2 in example ssd in demo run python script to download demo images in model download and extract pre trained model 3 python demo py What have you tried to solve it git bisect leads to 32ced389c1e52e2874c75ef75b09973835b15138 is the first bad commit commit 32ced389c1e52e2874c75ef75b09973835b15138 Author Eric Junyuan Xie piiswrong users noreply github com Date Tue May 30 19 02 08 2017 0700 backward headgrads and detach 6332 lint fix add updated grad add retain grad exclude reduce cpp cached invoke cached symbol move symbol init module symbol cython udpate updated grad fresh grad fix fix 040000 040000 adaeb00044a3e43a3385b05a6889cff069950da9 f167af6857f2e03362d6b60758ceb1f1c54a89cd M include 160000 160000 93072dc8733aa2a89459ecf16413d96ad0b998db 7796ac76ccea1fba31afc32056c83f6da38b6c57 M nnvm 040000 040000 4dd3013b83ee48214962301d6fb1fcdf0c611c5d 604df134541a3649ffce3f6ab644c00a7b4ba528 M python 040000 040000 6b9680f27430bb69f707a833a427c65fce7690bd ce3ee023e5271961230b8be9df855293b72e8e4f M src 040000 040000 3c36e17186e599db7ea9523c7dc4ca22834e48ae 082e2c610ef3883b3b9e37b8f63fddb9fbab1cdc M tests That is this commit,,piiswrong,2017-06-05 16:45:27,2017-06-05 17:55:34
PR,WIP nn api reference,The page is currently hosted here,,"szha,piiswrong",2017-06-02 21:31:53,2017-06-05 19:44:29
PR,batchnorm specify channel axis and performance optimizations for batchnorm,piiswrong 1 Batch norm takes channel axis parameter 2 Optmizations CPU batchnorm performance without SIMD approaches MKL performance BatchNormV1Prop cpu 2D Timing Forward 1972 71 ms avg 3 94541 ms X 500 passes BatchNormV1Prop cpu 2D Timing Backward 13659 1 ms avg 27 3182 ms X 500 passes MKL BatchNormProp cpu 2D Timing Forward 99 892 ms avg 0 199784 ms X 500 passes MKL BatchNormProp cpu 2D Timing Backward 121 685 ms avg 0 24337 ms X 500 passes BatchNormProp cpu 2D Timing Forward 174 854 ms avg 0 319708 ms X 500 passes BatchNormProp cpu 2D Timing Backward 166 815 ms avg 0 33363 ms X 500 passes BatchNormV1Prop gpu 2D Timing Forward 5 058 ms avg 0 010116 ms X 500 passes BatchNormV1Prop gpu 2D Timing Backward 14 812 ms avg 0 029624 ms X 500 passes BatchNormProp gpu 2D Timing Forward 1 711 ms avg 0 003422 ms X 500 passes BatchNormProp gpu 2D Timing Backward 1 752 ms avg 0 003504 ms X 500 passes 3 GPU version is faster than CUDNN,,"cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-05-23 22:56:13,2017-06-05 21:54:16
IS,Deeplab and atrous convolution,Hi I am unable to find atrous convolution layers in mxnet for deeplab Can I incorporate any caffe operations and python layers in mxnet,,Piyush3dB,2017-05-18 17:43:14,2017-06-05 22:39:55
IS,Missing parent artifacts for mxnet scala with Scala 2 11,Creating a Scala project with Scala 2 11 depending on mxnet 0 9 3a fails because of missing artifacts build sbt has It appears that the scala 2 11 artifacts require some parent pom s to be published which were by mistake not published to maven What would be desirable to have Scala 2 11 and Scala 2 12 artifacts published and usable As another wish to have sbt based scala build file so that the mxnet scala project could be built separately imported into IntelliJ with SBT etc Right now I can not seem to be able to do this,,"yzhliu,yzhliu,yzhliu",2017-06-01 20:54:37,2017-06-06 01:50:40
PR,Adding functionalities from minpy model builder,These functionalities include Binary operators for Layer An API enabling user to declare layers via syntax similar to MXNet NDArray operator API,,piiswrong,2017-05-31 03:11:56,2017-06-06 03:25:33
PR,WIP fix out option for mx nd zeros and mx nd ones,,,eric-haibin-lin,2017-06-06 03:23:56,2017-06-06 06:49:59
IS,Build fails for Mxnet 0 10 0 in warp ctc plugin,Environment info Operating System Ubuntu14 04 Compiler Package used Python R Scala Julia Python MXNet version 0 10 0 Or if installed from source git clone mxnet recursive Error Message In file included from plugin warpctc warpctc cc 8 0 plugin warpctc warpctc inl h In member function virtual void mxnet op WarpCTCOp xpu Backward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob plugin warpctc warpctc inl h 124 14 error class mxnet TBlob has no member named dev mask if data dev mask cpu kDevMask plugin warpctc warpctc inl h 127 21 error class mxnet TBlob has no member named dev mask else if data dev mask gpu kDevMask plugin warpctc warpctc inl h 133 52 error class mxnet TBlob has no member named dev mask LOG FATAL Unknown device type data dev mask plugin warpctc warpctc inl h 152 14 error class mxnet TBlob has no member named dev mask if data dev mask gpu kDevMask plugin warpctc warpctc inl h 196 14 error class mxnet TBlob has no member named dev mask if data dev mask cpu kDevMask plugin warpctc warpctc inl h 198 21 error class mxnet TBlob has no member named dev mask else if data dev mask gpu kDevMask make build plugin warpctc warpctc o Error 1 Minimum reproducible example cd git clone cd warp ctc mkdir build cd build cmake make sudo make install git clone mxnet recursive cd mxnet cp make config mk modify the config mk as following USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 WARPCTC PATH home nd warp ctc which my wrap ctc installed MXNET PLUGINS plugin warpctc warpctc mk make I git clone the latest version of MXNet make as above and error While there is no such error in the version of 0 9 5 How can I fix it Many thanks Xin q,,"piiswrong,ZihengJiang",2017-05-31 07:32:56,2017-06-06 11:12:44
IS,mx sym WarpCTC cuda memcpy or memset failed issue,Environment info Operating System Ubuntu 14 04 GPU GTX 1080 Compiler Package used Python R Scala Julia Python MXNet version 0 9 5 in python Or if installed from source git clone mxnet recursive Error Message 16 36 11 src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable INFO 2017 05 05 16 36 25 768 train terminate called after throwing an instance of istd runtime error' what Error compute ctc loss stat cuda memcpy or memset failed Minimum reproducible example cd git clone cd warp ctc mkdir build cd build cmake make sudo make install git clone mxnet recursive cd mxnet cp make config mk modify the config mk as following USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 WARPCTC PATH home nd warp ctc which my wrap ctc installed MXNET PLUGINS plugin warpctc warpctc mk CUDA ARCH gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 gencode arch compute 61 code compute 61 make cd python sudo python setup py install Steps to reproduce python main py python main py configfile default cfg the ctc layer code net mx sym WarpCTC data net label label label length num label input length seq len How can I fix it Many thanks Xin q,,"piiswrong,sbodenstein,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,sbodenstein,sbodenstein",2017-05-05 07:54:47,2017-06-06 11:13:34
IS,How can I run MXNET on GPUs of different machines,How can I run MXNET on GPUs of different machines how to config it Many thanks Xin q,,eric-haibin-lin,2017-05-10 07:42:38,2017-06-06 11:14:17
PR,Fixed caffe converter and improved test converter,While trying to use the caffe converter on bvlc googlenet I came across a bug in the conversion process which failed it I have fix the bug and used test converter to check the performance of the converted model I added support for running the test converter on CPU and also fixed a bug related to resnet conversion which also failed converting Lastly I have noticed the performance comparison done in test converter allows for an additive 30 reduction in accuracy performance in the converted model I suspect the original intent is to allow a 3 drop in performance The actual performance drop measured was ranging between 0 and 1 6 on the three models tested bvlc googlenet vgg 16 resnet 50 Following are related issues 6441 can be closed after this merge 6319 the converter still does not support networks with multiple outputs in the final deployment but it does support now network trained with multiple outputs like bvlc googlenet In short the problem encountered in this issue is resolved but its title is not accurate,,"arikpoz,arikpoz,piiswrong,arikpoz",2017-05-30 15:49:28,2017-06-06 16:41:40
PR,Add instructions to install OpenCV,,,indhub,2017-06-05 19:48:12,2017-06-06 17:02:47
PR,Fixing obsolete installation guide,Fixing issue,,"sandeep-krishnamurthy,szha,sandeep-krishnamurthy",2017-06-05 21:51:33,2017-06-06 17:27:58
PR,WIP Foo NN API reference doc,The API reference doc page for foo pending a name is hosted here,,szha,2017-06-05 19:48:24,2017-06-06 17:40:04
PR,error messages for ndarray only methods,,,"szha,piiswrong,szha",2017-06-06 04:21:55,2017-06-06 18:27:51
PR,fix rnn doc,piiswrong these were missed from api doc,,szha,2017-06-06 19:38:27,2017-06-06 23:03:13
IS,Missing broadcast add layer in MXNet Scala binding,I am trying to run a converted Caffe model a ResNet 50 model that I trained on my own set of images I have converted the model using the Caffe converter from the most recent master branch of MXNet However when trying to run with the Scala bindings I have the following problem Seems to me that broadcast add is not found in my Scala library see below Note that I could run a VGG 16 model after some edits Environment info Operating System Mac OSX El Capitan 10 11 5 Compiler SBT Package used Python R Scala Julia Scala MXNet version using ml dmlc mxnet mxnet full 2 10 osx x86 64 cpu 0 1 1 from Maven Respositories Error Message Please paste the full error message including stack trace What have you tried to solve it 1 tried to run on my own published JAR but the problem persists 2 checked the Scala library there does not seem to have a Symbol broadcast add,,,2017-06-07 00:05:00,2017-06-07 00:40:46
PR,split Layer params into Layer params and Layer all params,,,"piiswrong,szha,szha",2017-06-06 23:50:45,2017-06-07 05:19:14
PR,tools measure py add 'from functools import reduce' for python3,compatibility note needs python 2 6 one line change on quick search seems to be only such usage in tools and python,,,2017-06-06 17:59:18,2017-06-07 05:43:59
PR,Print error message if GPU is not present,,,"indhub,mli,mli,piiswrong,indhub,bhavinthaker,piiswrong,bhavinthaker,madjam,mli",2017-06-05 19:17:31,2017-06-07 06:08:07
IS,mxnet gets stuck on cudaMemGetInfo,Environment info Operating System CentOS with cuda V8 0 61 Compiler g 5 3 1 MXNet commit hash git rev parse HEAD 3d545d77d99caaca50c464a2549d12e66d9d163b Steps to reproduce 1 cd cpp package example 2 get mnist sh 3 make mlp gpu mlp gpu Part of gdb backtrace 0 0x00007fff5d718990 in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 1 0x00007fff5d718ac6 in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 2 0x00007fff5d778e8a in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 3 0x00007fff5d71fecb in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 4 0x00007fff5d99becf in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 5 0x00007fff5d99bf39 in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 6 0x00007fff5d5eed6d in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 7 0x00007fff5d5f64f8 in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 8 0x00007fff5dbf140d in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 9 0x00007fff5d5f9b94 in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 10 0x00007fff5d5fb2e9 in from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 11 0x00007fff5d5f1abc in cuda CallJitEntryPoint from usr lib64 nvidia libnvidia ptxjitcompiler so 375 26 12 0x00007fffc4bff582 in fatBinaryCtl Compile from usr lib64 nvidia libnvidia fatbinaryloader so 375 26 13 0x00007fffd3625e42 in from usr lib64 nvidia libcuda so 1 14 0x00007fffd36269c3 in from usr lib64 nvidia libcuda so 1 15 0x00007fffd357f35e in from usr lib64 nvidia libcuda so 1 16 0x00007fffd357f640 in from usr lib64 nvidia libcuda so 1 17 0x00007fffe30dfa5d in from usr local cuda 8 0 lib64 libcudart so 8 0 18 0x00007fffe30d3e60 in from usr local cuda 8 0 lib64 libcudart so 8 0 19 0x00007fffe30decc6 in from usr local cuda 8 0 lib64 libcudart so 8 0 20 0x00007fffe30e3401 in from usr local cuda 8 0 lib64 libcudart so 8 0 21 0x00007fffe30d672e in from usr local cuda 8 0 lib64 libcudart so 8 0 22 0x00007fffe30c3e8e in from usr local cuda 8 0 lib64 libcudart so 8 0 23 0x00007fffe30f417c in cudaMemGetInfo from usr local cuda 8 0 lib64 libcudart so 8 0 24 0x00007fffe652aea5 in mxnet storage GPUPooledStorageManager Alloc this 0xa5fe80 raw size 401408 at src storage pooled storage manager h 77 25 0x00007fffe652b3f9 in mxnet StorageImpl Alloc this 0x7fff6c0052d0 size 401408 ctx at src storage storage cc 86 26 0x00007fffe6010bfa in mxnet NDArray Chunk CheckAndAlloc this 0xa6c790 at include mxnet ndarray h 391 27 0x00007fffe6010bb5 in mxnet NDArray Chunk Chunk this 0xa6c790 size 100352 ctx delay alloc false dtype 0 at include mxnet ndarray h 386 It only stuck on cuda 8 0 61 I tried another machine with cuda 8 0 44 and it worked well,,"lx75249,sifmelcara,lx75249,lx75249,sifmelcara,lx75249,szha,szha",2017-05-16 20:40:26,2017-06-07 09:30:38
IS,Need help with MLP tuning,I am trying to train a simple MLP model for sentiment analysis using the IMDB dataset in MXNet I tried to duplicate the same MLP design and used the same hyper parameters as the keras example in the link below However my MXNet version does not seem to learn The training accuracy was always stuck around 50 whereas the keras sample using Tensorflow backend produced an accuracy of 89 My network has the following configurations and layers Data Embedding Flatten FullConnected 250 hidden neurons Activation ReLU FullConnected 1 hidden neuron LogisticRegressionOutput Optimizer adam learning rate 0 1 number of epochs 10 Appreciate any help on this,,,2017-06-02 21:17:11,2017-06-07 13:53:33
PR,Clearer error message for simple bind failure,,,"reminisce,piiswrong,piiswrong,reminisce,piiswrong,reminisce",2017-06-06 22:21:28,2017-06-07 16:18:04
PR,Changed make to support more gpu archs multiple toolkits reduce lib size,,,"DickJC123,piiswrong,mli,DickJC123,mli,mli",2017-06-06 03:00:35,2017-06-07 17:12:33
IS,how to use profiler,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler g nvcc Package used Python R Scala Julia python MXNet version latest Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 0 recursively pull the code from your repo 1 I modify the config mk and set USE PROFILER 1 2 then I make 3 After that I run python mxnet example profiler profiler executor py 4 It says mxnet dmlc core include dmlc logging h 300 22 33 05 src c api c api cc 85 Need to compile with USE PROFILER 1 for MXNet Profiler What have you tried to solve it 1 I try to build the source code again but same errors 2 3 Can you provide more details about how to use the MxNet profiler,,"piiswrong,ZihengJiang,tornadomeet",2017-06-07 22:36:29,2017-06-08 01:58:04
PR,Update to version 1 7 0 of CUB,Supports CUDA9 and Volta,,ptrendx,2017-06-07 20:55:28,2017-06-08 05:53:02
PR,sort data label names and data label shapes before checking the equality,data label shapes are extracted from dict so its sequence is not determined data label names are provided by user so its sequence is also not determined When checking the equality we should sort them first,,"chunyang-wen,chunyang-wen",2017-06-08 01:42:34,2017-06-08 06:21:14
PR,Remove references to deprecated API in how to docs,,,"madjam,piiswrong",2017-06-08 16:54:41,2017-06-08 20:28:57
IS,Algorithm between pooling layer and Convolutional Layer with LeNet mxnet in R,Here is the code for LeNet in R input data mx symbol Variable wouldata' first conv conv1 mx symbol Convolution data data kernel c 5 5 num filter 20 tanh1 mx symbol Activation data conv1 act type tanh pool1 mx symbol Pooling data tanh1 pool type max kernel c 2 2 stride c 2 2 second conv conv2 mx symbol Convolution data pool1 kernel c 5 5 num filter 50 tanh2 mx symbol Activation data conv2 act type tanh pool2 mx symbol Pooling data tanh2 pool type max kernel c 2 2 stride c 2 2 first fullc flatten mx symbol Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 500 tanh3 mx symbol Activation data fc1 act type tanh second fullc fc2 mx symbol FullyConnected data tanh3 num hidden 10 loss lenet mx symbol SoftmaxOutput data fc2 train array train x dim train array c 28 28 1 ncol train x test array test dim test array c 28 28 1 ncol test device cpu mx cpu model mx model FeedForward create lenet X train array y train y ctx device cpu num round 5 array batch size 100 learning rate 0 05 momentum 0 9 wd 0 00001 eval metric mx metric accuracy batch end callback mx callback log train metric 100 What is the Algorithm between pool2 layer and Second Convolutional Layer Does that all 20 first pooling layer hadamard product model arg params convolution1 weight 1 20 and sum up Thank you,,,2017-06-03 15:26:54,2017-06-09 01:28:35
PR,fix lint,fixed the lint caused by rebuilding the lint docker on ci server the reason is that pylint is updated on pip which generates more errors compared to the previous version,,mli,2017-06-08 22:21:24,2017-06-09 06:02:41
IS,Large time cost over GPU computation,I'm trying mxnet code from API n 10 a mx nd ones 1000 1000 b mx nd ones 6000 6000 mx gpu tic time time c do a n wait c print 'Time to finish the CPU workload f sec' time time tic d do b n wait d print 'Time to finish both CPU GPU workloads f sec' time time tic and the result is Time to finish the CPU workload 1 327401 sec Time to finish both CPU GPU workloads 117 675283 sec There are no error during compilation and i use the same configurations as in caffe compiled on my machine gpu 980m caffe on my PC can conduct gpu computation at once yet mxnet on my PC has to wait 100 seconds at least What is wrong with my configuration,,piiswrong,2017-06-07 08:03:48,2017-06-09 06:53:45
PR,update cub url,changed submodule cub to a squashed version to avoid cloning the huge repo each time if failed to check out the source in a CI task delete the current workspace to clean the git cache and try it again,,"mli,mli",2017-06-09 01:37:40,2017-06-09 16:48:48
PR,Add Graphviz installation instructions to installation page,Graphviz is an important component without which graph visualization related MXNet APIs would fail Adding this to the installation page will reduce the likelihood of users bumping into this issue,,"indhub,nswamy,piiswrong,nswamy,piiswrong,nswamy,indhub",2017-06-09 04:16:21,2017-06-09 18:24:41
PR,remove Python file from tutorials,madjam screen shot 2017 06 08 at 5 32 42 pm,,"nswamy,piiswrong,nswamy",2017-06-09 00:34:27,2017-06-09 18:24:57
PR,Module tutorial prerequisite section modified,piiswrong Can you merge this Seems like build failure is not due to this PR,,"Roshrini,piiswrong,nswamy,piiswrong,nswamy",2017-06-08 23:36:20,2017-06-09 18:25:43
PR,Fix a couple of print statements that were not working on Python 3,,,indhub,2017-06-07 06:36:22,2017-06-09 18:28:57
PR,extend tranpose to 6dim,piiswrong,,szha,2017-06-08 02:08:34,2017-06-09 18:30:06
PR,super resolution example in nn,piiswrong,,"szha,szha",2017-06-08 04:23:38,2017-06-09 18:44:53
PR,extend tranpose to 6dim,piiswrong,,szha,2017-06-09 18:29:59,2017-06-09 18:59:20
PR,fix error message,piiswrong,,"szha,mli,szha,mli",2017-06-07 00:03:03,2017-06-09 19:06:30
IS,FusedRNN CuDNN Illegal Memory Access Error in cudnn detail dropout fp,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu14 04 Compiler gcc g 4 8 4 nvcc7 5 17 Package used Python R Scala Julia python MXNet version commit 187e605d If you are using python package please provide Python version and distribution 2 7 6 Error Message Please paste the full error message including stack trace,,"szha,ptrendx,ptrendx,szha,szha,szha",2017-03-05 20:42:17,2017-06-09 19:07:55
PR,test remove COPY in dockerfiles,The problem of COPY sh is that any update to a sh file will invalid the docker cache and then trigger the container re build,,mli,2017-05-31 19:50:57,2017-06-09 19:27:27
PR,CI add doc build in deploy stage,,,"mli,mli",2017-06-09 19:44:36,2017-06-09 20:24:50
PR,add language model example,,,"piiswrong,szha,szha,szha,szha,piiswrong",2017-06-08 23:35:31,2017-06-09 21:56:19
PR,extend tranpose to 6dim,piiswrong,,szha,2017-06-09 19:02:40,2017-06-09 21:57:27
PR,super resolution,piiswrong,,"szha,szha",2017-06-09 18:57:01,2017-06-09 21:58:28
PR,Website improvement,1 Change API title to black and add background color webimprove2 2 For why mxnet page disable selection for Install tab 3 Modify download buttons to actually download files instead of showing files,,"kevinthesun,piiswrong,piiswrong",2017-06-08 00:07:42,2017-06-09 21:59:07
PR,OnlyImplementedInNDArray,piiswrong,,"szha,piiswrong,szha,piiswrong,mli,szha,piiswrong,szha,szha",2017-06-09 19:09:14,2017-06-09 22:59:16
PR,add prerequisites to tutorials,zackchase Please review I have added prerequisites to the main Tutorials page and also to the data iterator tutorial I feel that these are common and should be at the top of the Tutorials page instead of having them in each tutorial I will remove one of them based on the feedback I get screen shot 2017 06 06 at 8 23 36 pm screen shot 2017 06 06 at 8 23 13 pm,,"nswamy,piiswrong,piiswrong,piiswrong,madjam,nswamy,nswamy",2017-06-07 03:33:00,2017-06-09 23:43:20
IS,Recruitment of MXNet Developer and Maintainer under TuSimple,Job Description This position is responsible for TuSimple is deep learning infrastructure You will also contribute to the DMLC community You will play an important role in developing and maintaining computing platforms across GPU server to embedded devices that enable researchers and developers to implement their ideas in an efficient and effective way You are expected to have strong skills in technical design implementation and troubleshooting Responsibility Track issues and maintain high availability of machine learning frameworks Contribute new features to open source project MXNet mshadow etc Design modern build and deploy pipelines Qualifications Good C CUDA programming Machine learning background knowledge Some basics on deep learning Experienced with linux environment Bonus Points Active contributions to DMLC projects Knowledge on distributed machine learning algorithm Passions about autonomous driving technology Our company website Location Beijing Expected Salary Full Time RMB 18K 30K month Intern RMB 10K month If you are interested please contact me via email winsty gmail com and attach your CV for reference,,winstywang,2016-11-03 06:11:05,2017-06-10 03:59:52
PR,Update Tutorial prerequisites,Update Tutorial Prerequisites and add Graphviz to Installation,,"nswamy,piiswrong",2017-06-09 23:29:54,2017-06-10 06:15:47
PR,fix python debug cpp markdown format,fix markdown format the original format will not be recognize as a block,,vsooda,2017-06-10 02:00:32,2017-06-10 06:16:23
PR,CI fix doc build on ci,,,mli,2017-06-09 23:23:39,2017-06-10 06:17:50
PR,add auto infershape,GaiYu0,,piiswrong,2017-06-09 23:09:10,2017-06-10 06:20:31
IS,Anyone can help on issue 3625,i already shared the data website where i downloaded the CIFAR 10 binary data and my R script by transfer the binary data into R array format and feed into a CNN model the accuracy just could not go up was always around 0 1 3625,,thirdwing,2016-11-14 05:24:48,2017-06-10 17:32:04
IS,Execution time differ with iterations,Environment info Operating System Tested with both Arch Linux and Ubuntu 14 04 Compiler Tested with both gcc 4 8 4 and 7 1 1 Package used Python R Scala Julia None MXNet commit hash git rev parse HEAD d75ef8eb65a56ecec6613458017a0c83f7a9ed34 just a few commits behind current master If you are using python package please provide Python version and distribution tested with both Python 3 4 0 and 3 6 1 Minimum reproducible example,,"hotpxl,hotpxl",2017-06-10 23:00:07,2017-06-10 23:08:22
PR,Added ResNet v1 fp32 and double buffering of input data to both resnet scripts,,,ptrendx,2017-06-11 21:06:35,2017-06-12 00:00:22
PR,Scala add eq ne gt ge lt le to NDArray and Symbol,javelinjs would you help to review,,Ldpe2G,2017-06-12 08:52:35,2017-06-12 15:45:36
IS,infer shape fails and bind fails but infer shape partial succeeds,MXNet version 0 10 infer shape and binding fails in certain conditions when the shape should be inferable and in fact infer shape partial is able to successfully infer the shape In the below example the problem seems to stem from applying an expand dims operator after a symbol has been broadcast on an axis with size 0 Setup correctly has infer shape produce 5 2 3 and simple bind also works So concat should be passing down the shape information but it is getting lost somewhere in the expand dims This all suggests that there is some bug in the infer shape possibly in expand dims not passing down the 0 shapes but not in the partial infer shape,,"piiswrong,reminisce,reminisce,szha,reminisce,piiswrong,reminisce",2017-06-09 13:29:39,2017-06-12 16:58:54
PR,Add backward infer shape in expand dims,This PR fixes,,"reminisce,piiswrong,piiswrong,piiswrong,reminisce,reminisce,piiswrong,reminisce,piiswrong",2017-06-11 05:40:55,2017-06-12 16:58:54
IS,How to save the metrics in each epoch,I have been successfully saving metrics at the end of each batch in a batch callback function passed into fit but I want to do the same for epochs with an epoch end callback function Unfortunately the epoch callback function expects these 4 arguments epoch self symbol arg params aux params unlike the batch callback function which only requires param The batch callback metrics are easy to access since you just need to call param eval metric get name value to get the name value list How would I do the same in an epoch callback function Thanks I would really appreciate it because I have searched all over your api and could not see any similar way of accomplishing this in a callback function,,,2017-06-12 16:31:44,2017-06-12 20:08:58
PR,Spellcheck for pre trained model tutorial,Some relatively minor grammatical errors in pre trained model tutorial,,pracheer,2017-06-12 20:44:52,2017-06-12 21:57:10
PR,Truncate operator implementation,As proposed in 3201 Tested with unit tests and interactive shell,,"apaleyes,piiswrong,apaleyes,piiswrong,apaleyes,piiswrong,apaleyes",2017-06-10 00:06:57,2017-06-12 21:59:05
PR,Change method and attribute color,changecolor1,,kevinthesun,2017-06-13 00:06:44,2017-06-13 02:05:13
PR,Ssd hotfix,quick fix to address the changes in EvalMetric,,zhreshold,2017-06-12 23:52:06,2017-06-13 02:05:51
IS,saving weights,I'm trying to save the weights in each epoch batch so that I can view them I'm guessing that the best way to do this is to make a batch callback function How do you make a custom callback function For instance I want to make a batch callback function and save the weights for each batch as a different file format than params but write it to one file,,,2017-05-31 20:18:13,2017-06-13 14:25:58
IS,No Scala support for contrib CTCLoss,Trying to use MXNet with Scala and getting an error while loading a trained model that uses CTCLoss Failed loading Op contrib ctc loss0 of type contrib CTCLoss 14 00 15 src core op cc 55 Check failed op nullptr Operator contrib CTCLoss is not registered I noticed that CTCLoss was introduced in 0 10 while the Scala packages are published only at version 0 9 3a Perhaps that is the problem or perhaps something else must be done to be able to use contrib CTCLoss I would be interested in fixing this myself in the Scala package if I only knew more about the nature of the problem,,"Ldpe2G,Ldpe2G,Ldpe2G,yzhliu,yzhliu,yzhliu",2017-06-03 01:34:59,2017-06-13 14:45:42
IS,Ubuntu building R mxnet when i run 'make rpkg' procuced Error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Distributor ID Ubuntu Description Ubuntu 16 04 2 LTS Release 16 04 Compiler gcc Ubuntu 5 4 0 6ubuntu1 16 04 4 5 4 0 20160609 Package used Python R Scala Julia R MXNet version installed from source MXNet commit hash git rev parse HEAD d75ef8eb65a56ecec6613458017a0c83f7a9ed34 If you are using python package please provide Yes I using python MXNet installed from source And it is version '0 10 0' R version and distribution R version 3 4 0 2017 04 21 R sessionInfo sessionInfo R version 3 4 0 2017 04 21 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 16 04 2 LTS Matrix products default BLAS usr lib openblas base libblas so 3 LAPACK usr lib libopenblasp r0 2 18 so locale 1 LC CTYPE en US UTF 8 LC NUMERIC C 3 LC TIME en US UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 7 LC PAPER en US UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C 11 LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 compiler 3 4 0 Error Message Downloads rmxnet mxnet make rpkg mkdir p R package inst mkdir p R package inst libs cp rf lib libmxnet so R package inst libs mkdir p R package inst include cp rf include R package inst include cp rf dmlc core include R package inst include cp rf nnvm include R package inst include echo import Rcpp R package NAMESPACE echo import methods R package NAMESPACE R CMD INSTALL R package Warning invalid package R package Error ERROR no packages specified Makefile 349 recipe for target 'rpkg' failed make rpkg Error 1 Steps to reproduce I running standard examples when i run 'make rpkg' it produced the error What can i do it is because i installed python package Thanks for your help,,"thirdwing,thirdwing",2017-06-12 01:37:02,2017-06-13 14:52:26
IS,mod fit fatal error,Environment info Operating System Ubuntu 16 04 2 LTS Installed from source MXNet 0 9 5 master 05 14 2017 build from source with mx contrib sym ctc loss built in MXNet commit hash git rev parse HEAD 38f7c5584016e92ba1e0ee1b00ea6632740f67ce Python version and distribution python 2 7 Hi I tried to use mx contrib sym ctc loss as the solution for my experiment with my own data converted to rec files However I am not able to use mod fit for training BUT I am able to use mod forward mod backward and mod update to train the network Here are the code and error message Code that CAN successfully train the model,,piiswrong,2017-05-23 18:33:41,2017-06-13 16:03:08
IS,introduction to statistical learning with R,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-06-13 11:43:18,2017-06-13 17:16:52
PR,Updated mshadow submodule,This will finally fix the bug in LRN layer see,,"arikpoz,arikpoz,arikpoz",2017-06-13 06:43:23,2017-06-13 17:52:07
PR,Add BLAS3 and LAPACK routines,This brings in operators that wrap certain BLAS3 and LAPACK routines in particular trsm trmm potrf potri gemm in addition a 2 operator version of gemm and an operator for getting the sum of logs of a matrix diagonal The goal is to bring in enough operators in order to support specific commonly important applications for example Gaussian Processes With this set of operators it is possible to Use MxNet as a computing framework for lot of Gaussian Process work Develop new applications that combine neural networks and Gaussian processes An example was given at AMLC There is a lot of active work ongoing in this area In addition there are other groups that are interested specifically in the Cholesky factorization potrf All operators provide proper backward gradients such that they can be integrated easily into custom loss layers of neural networks Getting gradients is nontrivial specifically for matrix factorization so this PR is more than a simple wrapper around linear algebra routines This PR defines a framework into which we can add other LAPACK BLAS related operators in the future The plan is to first release these CPU versions and afterwards bring in the GPU ones we are already working on that Based on the outcome of the GPU work The documentation for the operators is in line with the most recent general doc updates verified the layout on a local webserver,,"asmushetzel,piiswrong,piiswrong,asmushetzel,asmushetzel,piiswrong,piiswrong,piiswrong,asmushetzel,asmushetzel,asmushetzel,piiswrong,jli05,piiswrong,asmushetzel,asmushetzel,asmushetzel,piiswrong,asmushetzel,jli05,jli05,jli05,piiswrong,jli05,asmushetzel,piiswrong,asmushetzel,asmushetzel,piiswrong,asmushetzel,jli05,jli05,asmushetzel,piiswrong,jli05,asmushetzel,jli05,asmushetzel,piiswrong,asmushetzel,piiswrong,jli05,asmushetzel",2017-06-02 12:21:10,2017-06-13 21:13:20
PR,round batch should start with first element,when set round batch True we should start with first element Current document is not correct Please refer to mxnet src iter batchloader h for details of loading strategy,,"chunyang-wen,chunyang-wen",2017-06-13 15:21:05,2017-06-14 03:42:22
PR,Minor grammatical errors in 3 of the tutorials,,,pracheer,2017-06-12 20:58:36,2017-06-14 05:15:22
IS,C Tutorial Error Model file for image classification predict cc seems like broken,Hi folks Now I'm trying to run image classification predict cc as a sample code of image classification with using pre trained model I'm facing an error at the point of creating a predictor like Steps to reproduce I tried to run this program with the command of image classification predict image apple jpg and received that error,,,2017-06-09 04:59:04,2017-06-14 10:25:01
PR,Fix for broken link,Also the link to how to predict by pretrained model is broken,,ZihengJiang,2017-06-14 06:11:39,2017-06-14 16:29:58
PR,Perl Sync with all recent changes in Python interface,,,"sergeykolychev,sergeykolychev",2017-06-11 07:15:15,2017-06-14 16:32:06
PR,Use pinned memory in IO iterator to avoid unnecessary memory copies,Using non pinned memory in asynchronous copies makes the CUDA driver perform additional unnecessary non pinned pinned memcpy That is why this PR changes the memory type used as an output from IO iterator to be pinned to avoid this additional copy,,ptrendx,2017-06-11 20:30:10,2017-06-14 16:53:45
PR,fix cpp package charRNN shape,fix infer shape error in cpp package charRNN example,,"vsooda,piiswrong,vsooda",2017-06-09 03:35:04,2017-06-14 16:56:52
PR,Update rnn md,Via L40 L45,,"leezu,piiswrong,leezu",2017-06-05 03:27:32,2017-06-14 17:05:24
PR,Update mshadow submodule to pick up CUDA 9 fp16 changes,,,"ptrendx,piiswrong,ptrendx",2017-06-08 22:28:09,2017-06-14 21:53:39
IS,Bind error of lro labels and data for image regression,I use mxnet to do image regression 4 labels by fine tuning resnet50 1 I changed SoftmaxOutput with LinearRegressionOutput in symbol 2 I changed image label into a number 3 I use metric mx metric MSE instead of training acc So the symbol is like in the last layer op FullyConnected name fc attr num hidden 4 inputs 430 0 0 431 0 0 432 0 0 op null name lro label inputs op LinearRegressionOutput name lro inputs 433 0 0 434 0 0 But when I run the code I have an error like simple bind error simple bind error Arguments lro label 36 data 36 3 227 227 Traceback most recent call last File finetune py line 59 in module for training True File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet module module py line 388 in bind state names self state names File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet module executor group py line 214 in init self bind exec data shapes label shapes shared group File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet module executor group py line 310 in bind exec shared group File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet module executor group py line 582 in bind ith exec shared buffer shared data arrays input shapes File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 1375 in simple bind raise RuntimeError isimple bind failed' RuntimeError simple bind failed It seems the error happened in mod mx module Module symbol new sym context ctx data names wouldata' label names 'lro label' mod bind data shapes wouldata' batch size 3 227 227 label shapes 'lro label' batch size for training True Input and output is not same but when I use softmax there is no such problem What happened,,,2017-06-15 12:27:10,2017-06-15 13:34:37
IS,Typo,L61 Should be resnet 101 instead of resnt 101,,"Hexiang-Hu,chunyang-wen,Hexiang-Hu",2017-06-14 20:42:01,2017-06-15 19:08:23
PR,R fix accessing variables in environment close 4282,Before,,thirdwing,2017-06-14 00:42:35,2017-06-15 20:08:03
IS,Bug in adam optimizer in R,The following code generates an error library mxnet w mx nd ones c 2 3 x mx opt create adam state x create state 1 w t2 Something new x update 2 w 1 state Error in t 1 non numeric argument to binary operator The issue can be fixed by changing line 215 in optimizer R if exists envir adam x time key with if exists envir adam x time key inherits FALSE The problem arises from the fact that the functions exists and get by default search also all parent environments In this case a variable t2 is found in the global environment which is not what is expected Unfortunately it looks like this technique of accessing variables inside environments by means of assign get exists is used in many other places of the code and possibly in other optimizers as well so a more extensive fix is required A modest suggestion could be to leverage more the and operators to access environments Instead of assign x name envir env value z z get x name envir env inherits FALSE it is also possible to write env name z z env name,,thirdwing,2016-12-18 22:01:17,2017-06-15 20:08:12
PR,Fix a typo,,,Hexiang-Hu,2017-06-15 19:07:49,2017-06-15 22:14:28
IS,lstm ocr py support data parallel in mult gpus in single host,I have tried lstm ocr py with muit gpus in data parallel mode but it does not work I just set sym context to a list of mult gpus I know i also need to update Iter and accuracy and some others Anyone have this experience can offer some suggestions,,,2017-05-19 08:32:54,2017-06-16 01:52:45
IS,lstm warp ctc ocr example failed when run with multiple gpus in data parallel mode,I try ocr recognition example now it can work well in single gpu mode But when I tried it with multiple gpus it failed when check accuracy contexts mx context gpu 2 mx context gpu 3 Above I select gpu 2 and 3 error will occure usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet model py 526 DeprecationWarning Calling initializer with init str NDArray has been deprecated please use init mx init InitDesc NDArray instead self initializer k v OCR ALP DIGITS 2017 06 06 10 25 09 627 Start training with gpu 2 gpu 3 iter Traceback most recent call last File lstm ocr alp dig py line 226 in module epoch end callback mx callback do checkpoint prefix 1 File usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet model py line 826 in fit sym gen self sym gen File usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet model py line 260 in train multi device executor manager update metric eval metric data batch label File usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet executor manager py line 424 in update metric self curr execgrp update metric metric labels File usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet executor manager py line 276 in update metric metric update labels slice texec outputs File usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet metric py line 717 in update reval self feval label pred File usr lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet metric py line 756 in feval return numpy feval label pred File lstm ocr alp dig py line 174 in Accuracy LCS p append np argmax pred k BATCH SIZE i IndexError index 1280 is out of bounds for axis 0 with size 1280 But if I just use one gpu it will work,,,2017-06-06 02:35:34,2017-06-16 01:53:32
IS,Question about random crop in ImageRecordIter,I always get the same result at two random crops of the same Image when using ImageRecorder I have found a similar issue closed but I can not figure out how to get the correct result Some of my codes,,,2017-06-15 14:33:20,2017-06-16 02:47:09
IS,Build the shared library fail for cudafe died,Environment info Operating System centos 7 0 Compiler g 4 8 2 nvcc Build the shared library MXNet commit hash git rev parse HEAD dc1078df8ffd443f16176b68414cbe1a59e1ec45 Error Message usr local cuda bin nvcc c o build src operator pooling v1 gpu o std c 11 Xcompiler D FORCE INLINES g O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 50 code compute 50 Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 I opt meituan user machineLearning mxnet mshadow I opt meituan user machineLearning mxnet dmlc core include fPIC I opt meituan user machineLearning mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMSHADOW USE CUDNN 1 I home machineLearning warp ctc include I opt meituan user machineLearning mxnet cub DMXNET USE NVRTC 0 src operator pooling v1 cu nvcc error 'cudafe' died due to signal 9 Kill signal Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 modify config mk detail for git diff diff git a make config mk b make config mk index 7a98d94 f3298ef 100644 a make config mk b make config mk 171 8 171 8 USE CPP PACKAGE 0 WARPCTC PATH HOME warp ctc MXNET PLUGINS plugin warpctc warpctc mk WARPCTC PATH HOME machineLearning warp ctc MXNET PLUGINS plugin warpctc warpctc mk 2 make j USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 add USE OPENCV 0 also died,,,2017-05-09 14:07:13,2017-06-16 03:14:49
PR,foo doc and tutorial,,,piiswrong,2017-06-15 18:17:33,2017-06-16 04:59:56
PR,R fix the predict function and optimizer,We do not need the label in prediction,,"thirdwing,piiswrong,thirdwing",2017-06-15 20:35:02,2017-06-16 06:25:51
PR,Fix data layout handling in BucketSentenceIter Fixes 5509,BucketSentenceIter did not correctly specify the used data layout in the provide data and provide label attributes This lead to the executor group always assuming an NCHW layout trying to split up the time dimension instead of the batch size dimension over all contexts in case that TNC layout was used For some reason this nevertheless worked for the provided FusedRNNCell resulting in model parallelism Interestingly this patch reduces performance by 25 as it replaces the unmeant model parallelism with the correct data parallelism Reference architecture is a simple single layer CharRNN trained on 4 GTX 1080 cards Without the patch 2000 samples sec with the patch 1500 samples sec,,"leezu,piiswrong,piiswrong,leezu",2017-06-15 15:56:10,2017-06-16 06:26:36
IS,mxnet mod BucketingModule looks like regarding default bucket key as batch size,Recently I want to use the lstm network to do a sentiment classification task I use mxnet rnn FusedRNNCell and define the sym gen seq len function but mxnet sym infer shape runs correctly However if the dataIter receive a layout NT and set args layout 'NTC' and slightly edit the sym gen seq len function the program can run It seems that the mxnet mod BucketingModule regards default bucket key as batch size,,"kevinthesun,leezu",2017-03-21 09:24:27,2017-06-16 06:26:44
IS,Infer shape error,My model below works fine when I set num hidden 1 but not when I set num hidden 2 I get the error infer shape error Arguments data 150 9723L 3L lin reg label 150 Any suggestions how to get around the error,,"piiswrong,chunyang-wen",2017-06-09 12:50:22,2017-06-16 12:43:05
PR,Update cub submodule to point to valid commit origin master,Currently the cub submodule points to a non existant commit leading to errors in git submodule operations This pull request updates cub to point to origin master mxnet compiles fine for me with the updated cub,,"leezu,piiswrong",2017-06-16 09:58:26,2017-06-16 16:55:43
PR,macOS install script Support MXNET HOME Print MXNet version Added few package dependencies,Changes to the installation script 1 The macOS installation script now supports defining MXNET HOME to any directory of choice though the default is mxnet if MXNET HOME is not defined 2 Added a few package dependencies and improved the code structure 3 Added a line to print the MXNet version Tested it on OSX El Capitan Version 10 11 6 and Sierra krishnamurthy,,"bhavinthaker,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,chunyang-wen,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker",2017-06-12 20:53:29,2017-06-16 17:39:05
PR,example text cnn classification reconstruction using mx module,The old one uses low level api and has been moved to the old directory New version uses high level api in mx module to reconstruct this example so more compact and simpler,,formath,2017-06-16 08:35:14,2017-06-16 18:02:51
PR,Add module forward reshape,This is the same change as Add support for different batch size data for forward One major use case is predicting single input or any data of which batch size is different from training data,,kevinthesun,2017-04-10 23:12:13,2017-06-16 18:44:40
PR,Add python opencv installation to Docker image,This PR adds python opencv as that was missing and user could not import cv2,,lxn2,2017-06-14 22:14:59,2017-06-16 21:54:24
IS,Cannot find Scala source code or documentation that matches the current compiled code library,Dear all I was trying to use mxnet scala API while reading its source and I could not find its source or documentation For example I was reading an example in mnist classification and I was trying to find the definition of Symbol FullyConnected so I looked at the current src but could not find it The current Scala documentation ml dmlc mxnet Symbol is faithful to the source so it does not have it either An old version I found online has it but it is not up to date In IntelliJ through sbt I directly used And the imported library i e ml dmlc mxnet has Symbol FullyConnected in there But everything I use in IntelliJ is compiled code so I cannot read the source code Could anyone direct me to the source code or documentation that actually maps the library compiled code Many thanks,,"Ldpe2G,Ldpe2G",2017-06-13 00:23:19,2017-06-16 22:37:23
PR,Fix docs typo from gpu0 cpu,Fixing typo in the code portion of the dependency engine docs referenced in 6721 We will need to get a hold of the graphic that goes right below these lines and change the code in there too,,lxn2,2017-06-16 22:16:13,2017-06-16 23:51:50
IS,about customop,I am fresh here and encountered some problems with customop url mlp mx symbol Custom data fc3 name isoftmax' op type isoftmax' I changed the name to be 'Softmax' or others and got the following errors 11 01 04 Users travis build dmlc mxnet distro mxnet build dmlc core include dmlc logging h 304 11 01 04 src c api c api symbolic cc 398 InferShapeKeyword argument name softmax label not found Candidate arguments 0 data 1 fc1 weight 2 fc1 bias 3 fc2 weight 4 fc2 bias 5 fc3 weight 6 fc3 bias 7 Softmax label Stack trace returned 4 entries bt 0 0 libmxnet so 0x00000001101a82b5 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x00000001107a597f ZN5mxnet14MatchArgumentsIN4nnvm6TShapeEEEvRKNS1 12IndexedGraphERKNSt3 113unordered mapINS6 12basic stringIcNS6 11char traitsIcEENS6 9allocatorIcEEEET NS6 4hashISD EENS6 8equal toISD EENSB INS6 4pairIKSD SE EEEEEEPNS6 6vectorISE NSB ISE EEEEPKc 3519 bt 2 2 libmxnet so 0x00000001107a3af8 MXSymbolInferShape 1448 bt 3 3 ctypes so 0x000000010f58a7f7 ffi call unix64 79 Traceback most recent call last File Users name software mxnet example numpy ops custom softmax py line 69 in module batch end callback mx callback Speedometer 100 100 File usr local lib python2 7 site packages mxnet model py line 782 in fit self init params data provide data data provide label File usr local lib python2 7 site packages mxnet model py line 502 in init params arg shapes aux shapes self symbol infer shape input shapes File usr local lib python2 7 site packages mxnet symbol py line 747 in infer shape res self infer shape impl False args kwargs File usr local lib python2 7 site packages mxnet symbol py line 871 in infer shape impl ctypes byref complete File usr local lib python2 7 site packages mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 11 01 04 src c api c api symbolic cc 398 InferShapeKeyword argument name softmax label not found Candidate arguments 0 data 1 fc1 weight 2 fc1 bias 3 fc2 weight 4 fc2 bias 5 fc3 weight 6 fc3 bias 7 Softmax label Stack trace returned 4 entries bt 0 0 libmxnet so 0x00000001101a82b5 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x00000001107a597f ZN5mxnet14MatchArgumentsIN4nnvm6TShapeEEEvRKNS1 12IndexedGraphERKNSt3 113unordered mapINS6 12basic stringIcNS6 11char traitsIcEENS6 9allocatorIcEEEET NS6 4hashISD EENS6 8equal toISD EENSB INS6 4pairIKSD SE EEEEEEPNS6 6vectorISE NSB ISE EEEEPKc 3519 bt 2 2 libmxnet so 0x00000001107a3af8 MXSymbolInferShape 1448 bt 3 3 ctypes so 0x000000010f58a7f7 ffi call unix64 79 It seems that the names of operator CustomOp and operator CustomOpProp could be any name But the name of symbol Custom must be softmax So what should I do if I want to build a new op for my research,,,2017-06-17 03:09:08,2017-06-17 03:11:30
IS,Dependency Engine Documentation typo,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you An Apache mentor Sebastian pointed out a typo in the dependency engine documentation aggregate gradient and update fc1 wgrad cpu fc1 wgrad gpu0 fc1 wgrad gpu1 fc2 wgrad cpu fc2 wgrad gpu0 fc2 wgrad gpu1 fc1 weight cpu lr fc1 wgrad gpu0 fc2 weight cpu lr fc2 wgrad gpu0 I think the last two lines should refer to the weights on the 'cpu' instead of weights on 'gpu0' and these wrong lines have also been copied to the picture below the code I can make the documentation change but also need the original file to the graphic Would you have an editable original for the graphic,,"lxn2,lxn2",2017-06-16 18:22:06,2017-06-17 03:14:44
PR,fix bug in optimizer because state is not synced with weight context when load optimizer states is true,There was a bug in Updater and Optimizer when training model from gpu0 context then loading it with gpu1 context and load optimizer states True for module load Error message is like below Check failed e cudaSuccess CUDA an illegal memory access was encountered and it causes NVRM error also from dmesg 248747 249293 NVRM Xid PCI 0000 83 00 31 Ch 00000014 engmask 00000101 intr 10000000 This bug prevents users to keep training from the previous optimizer states when attempting with different context I implemented sync state context to sync for various state returns which differs from each optimizer scheme Related issue is 5428,,"Soonhwan-Kwon,piiswrong",2017-06-16 08:29:35,2017-06-17 04:55:54
PR,Avoid pushing dangling references into engine,Please correct me if I'm wrong I think we cannot push references to local variable into the dependency engine because the dependency engine may invoke the lambda function after the lifetime of those local variable has ended However it seems that this undefined behavior has not causes any real problem,,"sifmelcara,piiswrong,sifmelcara,piiswrong",2017-06-17 05:58:44,2017-06-17 07:13:10
PR,Add operators for Deformable ConvNets DFF,,,"YuwenXiong,piiswrong,piiswrong,piiswrong,piiswrong,YuwenXiong,piiswrong,YuwenXiong,sbodenstein,piiswrong,YuwenXiong,piiswrong,piiswrong,piiswrong,YuwenXiong",2017-05-17 08:11:12,2017-06-17 17:53:37
PR,add verbose option to initializer,Add verbose option to initializer according to 5776,,"zhreshold,piiswrong,piiswrong,chunyang-wen,zhreshold,chunyang-wen,zhreshold",2017-06-15 22:05:00,2017-06-17 18:50:09
PR,fix bug in optimizer because state is not synced with weight context,Thank you for your review I accepted your review and removed sync state context from Optimizer class There was a bug in Updater and Optimizer when training model from gpu0 context then loading it with gpu1 context and load optimizer states True for module load Error message is like below Check failed e cudaSuccess CUDA an illegal memory access was encountered and it causes NVRM error also from dmesg 248747 249293 NVRM Xid PCI 0000 83 00 31 Ch 00000014 engmask 00000101 intr 10000000 This bug prevents users to keep training from the previous optimizer states when attempting with different context Related issue is 5428,,"Soonhwan-Kwon,piiswrong,Soonhwan-Kwon",2017-06-17 04:55:16,2017-06-18 06:53:45
PR,enable use of lapack by default,Changes to makefiles config to safely enable use of lapack based functionalities by default By platform Windows No issues as we get lapack with openblas automatically osx No issues as we get lapack with apple automatically Ubuntu On Ubuntu 14 04 we do not get the lapack library automatically installed with openblas On all Ubuntu versions we do not get it automatically installed when using atlas The simplest and most consistent handling is to explicitly check whether the lapack library is in usr lib or usr local lib and if not then switch off lapack support automatically In addition provided an extra variable in the config to support setting a custom path for the lapack library Just for convenience BTW The build using atlas is generally broken on Ubuntu at least Reason is that the mshadow config adds a lcblas while Ubuntu will install a blas library The latter is in contrast to the official atlas documentation which states that the library is cblas But as atlas is not providing precompiled libraries guess anyone else is free to name them differently and apparently that is what happened We have no regression build for atlas on Jenkins so we never see this Eric do you think we can simply change this setting in mshadow,,"asmushetzel,piiswrong,piiswrong,asmushetzel,piiswrong,piiswrong,asmushetzel,asmushetzel,piiswrong,liangfu,asmushetzel,liangfu,szha,jli05,jli05,asmushetzel,liangfu,asmushetzel,szha,szha",2017-06-15 13:30:39,2017-06-18 21:23:30
PR,Prerequisites for tutorials print for python3 sudo for installation,Details 1 Added pre requisites for all the tutorials For opencv directed them to opencv 3 2 2 Using print functions to make the tutorials compatible with python3 too 3 Our installation procedure was just doing pip install mxnet with no sudo and therefore failing So added that I tested only one procedure to verify the need for sudo on ubuntu ec2 instance c4 machine using pip I assume the same goes for others too Let me know if I should be testing other procedures too 4 Imagenet ImageNet,,"pracheer,piiswrong,pracheer,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,nswamy,mli,pracheer,piiswrong,nswamy,nswamy,piiswrong,pracheer",2017-06-05 18:20:11,2017-06-19 01:20:19
PR,example svm mnist use module instead of model,Current example svm mnist is based on model a deprecated API that causes a warning like this So this commit lets the example use module and removes the warning message,,mitake,2017-06-19 07:16:36,2017-06-19 07:23:25
IS,Custom operator always pop up error about num of arguments,I'm trying to write my own custom operator when I exec it it always pop up error about num of arguments I check my args setting and it seems all right What' going wrong here Error Message,,"Zehaos,Zehaos",2017-06-19 07:41:55,2017-06-19 08:16:28
IS,Extract Input parameters from mxnet model,I have saved the model using mx model save model fit dl prefix model iteration 10 and loaded later fit mx model load prefix model iteration 10 Now using object fit I want to extract the input features column names of train data How to do that,,thirdwing,2017-06-15 05:26:48,2017-06-19 08:30:56
IS,Unable to free memory when it does predict,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 15 10 Compiler gcc4 8 Package used Python R Scala Julia Python MXNet version 0 10 Or if installed from source MXNet commit hash git rev parse HEAD 5a9c3c0e2fbd7405e8210f3f274e497f4871c52c If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example I just train a model about fast style transfer and memory cannot free after I call method style transfer code shows as following Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 load model twice by calling executor init method 2 3 What have you tried to solve it 1 del executor etc objects after calling complete 2 Change the way of creating executor from class to function 3,,,2017-06-19 09:13:07,2017-06-19 11:53:36
IS,self binded assertion error in score when loading Module checkpoint,Hey for the following code I'm getting an Assertion failure for self binded in score model mx mod Module load 'xval4gpu2' epoch to load da score model score val eval metric 'acc' it seems to be working fine if using model mx model FeedForward load 'xval4gpu2' epoch to load ctx devices Thanks For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 Compiler vc14 Package used Python MXNet version 0 10 1 Or if installed from source yajiedesign precompiled 20170608 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 3 6 1 Error Message AssertionError,,,2017-06-19 09:46:19,2017-06-19 13:24:40
IS,Custom op backward Crashed when manipulate out grad,I am writing a custom op and I got stucked when writing the backward part When I call out grad 0 asnumpy or do any manipulation of the out grad the program crash without any error message I tried fill the in grad with zeros the program run smoothly but I need the grad to flow backward What is going wrong here,,"Zehaos,Zehaos",2017-06-19 13:48:48,2017-06-19 14:05:07
PR,Remove executable bit from header and cu files in src operator,,,leezu,2017-06-19 10:59:39,2017-06-19 15:56:17
PR,Fix typos in installation guide,Change python program to R program in R validation examples,,fdavidcl,2017-06-19 10:25:47,2017-06-19 15:59:19
PR,Fix pad example error,,,Zehaos,2017-06-19 10:10:34,2017-06-19 16:01:14
PR,fixed layer name,,,jonasrauber,2017-06-19 09:47:49,2017-06-19 16:01:44
PR,fixed comment,,,jonasrauber,2017-06-19 09:40:37,2017-06-19 16:02:08
PR,Fix parameter initialization,,,piiswrong,2017-06-17 00:46:05,2017-06-19 18:49:05
IS,Issues on the latest Windows for R,Building the R package from the pre build Windows releases is no longer working It works fine up to the build 20170304 but since the 20170305 build the package can be installed but several functionnalities are crashing The first moment in building the package where I noticed an anomaly is during the roxygenise step Given no change was made to base h for a while the issue is likely from somewhere else The 20170305 20170306 and 20170307 builds are all causing the same problem and I coul not see any recent commit that would have cause this Any help appreciated thanks,,"jeremiedb,piiswrong,jeremiedb,jeremiedb,thirdwing,thirdwing",2017-03-07 04:22:33,2017-06-19 20:53:39
IS,Multi source Iterator,Hi all This issue troubles me a lot of time I wrote a multi source iterator according to this API develop a new iterator document It combines two iterators one is NDArrayIter and the other one is ImageRecordIter If you want to take a look at the code it is here Any suggestions on this issues Thanks in advance,,,2017-06-19 16:19:14,2017-06-19 21:30:31
PR,Enable support for float64 dot operator,I added support for 64bit floating point in the dot and batch dot operator as well as corresponding unit tests,,stefanhenneking,2017-06-19 21:30:38,2017-06-19 22:38:02
PR,Add Deformable ConvNets link in example README md,,,YuwenXiong,2017-06-19 16:55:59,2017-06-20 03:18:21
PR,mod,Implementation is following the theano L2083 L2109 way the tensorflow L91 L94 way is shorter but can not provide the same consistency with numpy,,"szha,szha,szha,szha",2017-06-14 22:04:50,2017-06-20 06:59:40
PR,Adding functionalities from minpy model builder,Added functionalities include Binary operators for Layer An API enabling user to declare layers via syntax similar to MXNet NDArray operator API,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,mli,piiswrong",2017-06-06 03:52:48,2017-06-20 09:01:05
IS,Exception when using CPP package with GPU TitanX with mlp example,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Windows 10 64 bit Compiler Package used Python R Scala Julia cpp MXNet version Or if installed from source Latest 30 3 22017 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Result in console window make the Executor Training epoch 0 13 08 2413 08 24 C Projects MXNet Base dmlc core include dmlc logging h C Projects MXNet Base dmlc core include dmlc logging h300 13 08 24 c projects mxnet base mshadow mshadow cuda tensor gpu inl cuh 106 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function 300 13 08 24 13 08 24 c projects mxnet base mshadow mshadow cuda tensor gpu inl cuh 106 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device functionC Projects MXNet Base dmlc core include dmlc logging h 300 13 08 24 c projects mxnet base src engine threaded engine h 329 13 08 24 c projects mxnet base mshadow mshadow cuda tensor gpu inl cuh 106 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging 13 08 24 C Projects MXNet Base dmlc core include dmlc logging h 300 13 08 24 c projects mxnet base src engine threaded engine h 329 13 08 24 c projects mxnet base mshadow mshadow cuda tensor gpu inl cuh 106 Check failed err cudaSuccess 8 vs 0 Name MapPlanKernel ErrStr invalid device function An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Tried to create a Windows 2015 solution with c project As it is not supplied as was in the old separate cpp package Used the example file mlp cpp Hell of a job to get everything right als environment variables and include paths 2 When run with context CPU it works as should When run with GPU device 0 a GT640 compute capability 2 1 graphics card it works When run with GPU device 1 a NVidia TitanX board it crashes Probably when copying the ndarray to the GPU 3 With the old cpp package I had the opposite graphics card did not work Titan X did work No clue on the cause of the problem with the graphics card in this particular case What have you tried to solve it 1 Tried to debug it Somewhere awaiting the copy to the GPU the exception is caused Due to multiple threads could not find the real place the eception is generated 2 Looked at all issues as envrironment variables 3 In contrast to the previous version of the c api cpp package it is quite time consuming to fix everything Maybe someone with skills can get it to the same level is the previous version Has anyone been able to run the cpp examples with the new mxnet version and c api on a windows machine,,ptrendx,2017-03-30 11:19:44,2017-06-20 19:09:03
IS,how to build im2rec from im2rec cc on a Windows machine,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 x64 Compiler VS2015 Package used Python R Scala Julia c api MXNet version Or if installed from source 0 9 4 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message I have created a console solution using im2rec cc and linking it with libmxnet lib However the symbols used in im2rec cc are not exported by libmxnet lib producing linker errors Has anyone being able to build im2rec on Windows and how should it be done Possible to share the project file 1 of the 14 unresolved externals Severity Code Description Project File Line Suppression State Error LNK2019 unresolved external symbol public static class dmlc InputSplit cdecl dmlc InputSplit Create char const unsigned int unsigned int char const Create InputSplit dmlc SAPEAV12 PEBDII0 Z referenced in function main Bin2Rec C Projects MXNet Examples Bin2Rec Bin2Rec im2rec obj 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-04-25 20:02:48,2017-06-20 19:10:12
PR,set layout attribute for FusedRNNCell output states,In FusedRNNCell default layout N is not possible for its output states which should be 'LNC',,"formath,leezu,piiswrong,piiswrong,formath",2017-06-19 10:59:07,2017-06-20 22:31:59
PR,Reformulate some comments in executor manager and executor group,,,leezu,2017-06-19 08:54:28,2017-06-20 22:32:39
PR,Env variables doc modified,piiswrong,,"Roshrini,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker",2017-06-19 19:58:22,2017-06-20 22:38:27
PR,MKL 1D ReLU fix,This is a fix for 6615,,"piiswrong,mli",2017-06-14 23:14:50,2017-06-20 22:39:29
PR,refactor cachedop,sergeykolychev I refactored CachedOp and revered changes in perl,,"piiswrong,sergeykolychev,sergeykolychev,sergeykolychev,piiswrong,sergeykolychev,sergeykolychev",2017-06-18 22:17:09,2017-06-21 03:33:40
PR,Use the latest MKLML release,This new mklml release contains some more optimizations for small batch sizes on SKX and KNL machines If you want to use dmlc is git repository please upload the mklml file and let me know the path Then I will update the pull request Thanks,,"szha,szha,szha",2017-06-07 00:42:59,2017-06-21 03:54:34
IS,predict with finetuned model raise shape mismatch,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 15 04 Compiler gcc4 8 9 Package used Python R Scala Julia python MXNet version 0 10 Or if installed from source MXNet commit hash git rev parse HEAD 5a9c3c0e2fbd7405e8210f3f274e497f4871c52c If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace As well as we know outputs of relu5 3 in vgg16 are in size 1 512 14 14 and I add a conv layer k 3 s 1 and the outputs of this will in size 1 512 12 12 so the size of weights between fc1 and the conv layer will be in size 4096 512x12x12 4096 25088 I just no idea why the fc1 weight is in size 4096x2048 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Start from different layers such as flatten 0 or pool5 2 Re download vgg16 symbol and pretrain weights 3 I find 'workspace 1024' in vgg symbol this argument may not cause the error,,,2017-06-21 12:49:32,2017-06-21 13:26:12
IS,How do you save BOTH the validation and training metrics in each batch or epoch,Environment info Operating System macOS Package used Python R Scala Julia Python MXNet version 0 10 Python version and distribution Python 3 6 1 What have you tried to solve it I wrote my own batch end callback function which I passed into the fit method I have an option to save the file for the specified period of batches or epochs My method is like this In this example I'm saving the metrics every period of batches I'm successfully saving the metrics to a file but I do not know if the metrics are metrics on the training data or the validation data I would like to be able to save both But after looking through the docs and callback py I do not understand how you can save both the validation and training metrics in the same callback function Has this issue been resolved or is there a workaround Can someone help me Thanks,,,2017-06-14 17:04:44,2017-06-21 15:43:54
PR,RFC HybridLayer,Rename API autograd train section record autograd test section pause Added HybridLayer,,piiswrong,2017-06-21 17:17:51,2017-06-21 19:59:48
IS,Installation problems mxnet for R 3 1 3 on windows8 1 x64,I run following codes to install mxnet for R 3 1 3 on windows8 1 x64 install packages drat repos drat addRepo dmlc install packages mxnet The error messages are install packages drat repos Installing package into C Users turbo1 Documents R win library 3 1 as lib is unspecified trying URL '' Content type 'application zip' length 70200 bytes 68 KB opened URL downloaded 68 KB package drat successfully unpacked and MD5 sums checked The downloaded binary packages are in C Users turbo1 AppData Local Temp Rtmp2x1mBS downloaded packages drat addRepo dmlc install packages mxnet Warning in install packages cannot open HTTP status was '404 Not Found' Warning in install packages cannot open HTTP status was '404 Not Found' Warning in install packages unable to access index for repository Installing package into C Users turbo1 Documents R win library 3 1 as lib is unspecified Warning in install packages cannot open HTTP status was '404 Not Found' Warning in install packages cannot open HTTP status was '404 Not Found' Warning in install packages unable to access index for repository package mxnet is available as a source package but not as a binary Warning in install packages package mxnet is not available as a binary package for R version 3 1 3 Please help me,,"thirdwing,thirdwing",2017-06-14 08:59:40,2017-06-21 21:12:59
PR,Multi precision SGD,In this PR I add additional type of optimizer MultiPrecisionSGD which takes 16 bit weights and gradients but internally keeps a 32 bit master copy of the weights and uses it to increase the precision of the update It is the approach that was used in the alexnet and resnet v1 fp16 examples using explicit casts but it achieves better performance by omitting storing and loading casted values from memory I also simplified alexnet and resnet v1 fp16 examples by removing explicit casts they are now intended to be used with the proposed MultiPrecisionSGD optimizer,,"ptrendx,piiswrong,ptrendx,piiswrong,ptrendx,ptrendx,piiswrong,ptrendx,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2017-05-24 20:46:00,2017-06-21 23:35:39
PR,Fix MacOS install instructions for venv and python,Incorrect documentation for install steps of MxNET on MacOS,,robdefeo,2017-06-21 21:13:11,2017-06-22 02:36:15
IS,DOC Small error in MacOS install instructions,On page the instructions on step 5 are incorrect,,robdefeo,2017-06-21 20:42:35,2017-06-22 02:36:23
PR,Support LSTM callbacks Continuation of training R Package,LSTM training supports end of batch and end of cycle callbacks This can be used to check progress and to save checkpoints In addition one can now continue the training by providing an existing more and arguments,,"offerm,thirdwing",2017-06-11 11:45:49,2017-06-22 05:06:37
PR,Module forward reshape,Support module forward reshape Resize executor if data batch shape changes,,"kevinthesun,piiswrong,piiswrong,piiswrong,kevinthesun,piiswrong,piiswrong",2017-06-16 18:44:00,2017-06-22 05:50:49
IS,Pascal TitanX too many resources requested for launch,Environment info Operating System Ubuntu 16 04 Compiler GCC 5 4 Package used Python R Scala Julia Python Or if installed from source MXNet commit hash git rev parse HEAD 0418aae16c2c6a01bf2e937d6e05596ec21e9087 8713d257cde97a660a459aa8a50a780944cf823c 0 10 release Python version and distribution 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 Error Message 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet src executor graph executor cc 558 Bucketing data gt boxes has a shape 1 123 5 which is larger than already allocated shape 1 100 5 Need to re allocate Consider putting default bucket key to be the bucket taking the largest input for better memory sharing 19 20 36 code mxnet dmlc core include dmlc logging h 304 19 20 36 code mxnet mshadow mshadow cuda tensor gpu inl cuh 110 Check failed err cudaSuccess 7 vs 0 Name MapPlanKernel ErrStr too many resources requested for launch Stack trace returned 9 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f81998b95dc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow4cuda7MapPlanINS 2sv6plustoENS 6TensorINS 3gpuELi2EfEENS 4expr14Broadcast1DExpINS4 IS5 Li1EfEEfLi2ELi1EEEfEEvNS7 4PlanIT0 T2 EERKNSB IT1 SD EENS 5ShapeILi2EEEP11CUstream st 0x1bc 0x7f819a61351c bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet2op16FullyConnectedOpIN7mshadow3gpuEfE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x972 0x7f819a614062 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x6f7c19 0x7f8199949c19 bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f819992b337 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x78 0x7f819992fab8 bt 6 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7f81eb1d2c80 bt 7 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7f81ef2326ba bt 8 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f81eef6882d 19 20 36 code mxnet dmlc core include dmlc logging h 304 19 20 36 code mxnet src engine threaded engine h 329 19 20 36 code mxnet mshadow mshadow cuda tensor gpu inl cuh 110 Check failed err cudaSuccess 7 vs 0 Name MapPlanKernel ErrStr too many resources requested for launch Stack trace returned 9 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f81998b95dc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow4cuda7MapPlanINS 2sv6plustoENS 6TensorINS 3gpuELi2EfEENS 4expr14Broadcast1DExpINS4 IS5 Li1EfEEfLi2ELi1EEEfEEvNS7 4PlanIT0 T2 EERKNSB IT1 SD EENS 5ShapeILi2EEEP11CUstream st 0x1bc 0x7f819a61351c bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet2op16FullyConnectedOpIN7mshadow3gpuEfE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x972 0x7f819a614062 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x6f7c19 0x7f8199949c19 bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f819992b337 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x78 0x7f819992fab8 bt 6 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7f81eb1d2c80 bt 7 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7f81ef2326ba bt 8 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f81eef6882d An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f81998b95dc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x31a 0x7f819992b5ca bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x78 0x7f819992fab8 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7f81eb1d2c80 bt 4 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7f81ef2326ba bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f81eef6882d terminate called after throwing an instance of wouldmlc Error' what 19 20 36 code mxnet src engine threaded engine h 329 19 20 36 code mxnet mshadow mshadow cuda tensor gpu inl cuh 110 Check failed err cudaSuccess 7 vs 0 Name MapPlanKernel ErrStr too many resources requested for launch Stack trace returned 9 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f81998b95dc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow4cuda7MapPlanINS 2sv6plustoENS 6TensorINS 3gpuELi2EfEENS 4expr14Broadcast1DExpINS4 IS5 Li1EfEEfLi2ELi1EEEfEEvNS7 4PlanIT0 T2 EERKNSB IT1 SD EENS 5ShapeILi2EEEP11CUstream st 0x1bc 0x7f819a61351c bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet2op16FullyConnectedOpIN7mshadow3gpuEfE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x972 0x7f819a614062 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x6f7c19 0x7f8199949c19 bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f819992b337 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x78 0x7f819992fab8 bt 6 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7f81eb1d2c80 bt 7 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7f81ef2326ba bt 8 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f81eef6882d An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f81998b95dc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x31a 0x7f819992b5ca bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x78 0x7f819992fab8 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7f81eb1d2c80 bt 4 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7f81ef2326ba bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f81eef6882d Minimum reproducible example import argparse import pprint import mxnet as mx import numpy as np import glob import sys sys path append ' code mxnet example rcnn ' from rcnn logger import logger from rcnn config import config default generate config from rcnn symbol import from rcnn core import callback metric from rcnn core loader import AnchorLoader from rcnn core module import MutableModule from rcnn utils load data import load gt roidb merge roidb filter roidb from rcnn utils load model import load param from rcnn dataset imdb import IMDB import xmltodict from PIL import Image import cPickle import os classes 'human person' 'human rider bicyclist' 'human rider motorcyclist' 'human rider other rider' 'object pothole' 'object street light' 'object traffic light' 'object traffic sign back' 'object traffic sign front' 'object vehicle bicycle' 'object vehicle boat' 'object vehicle bus' 'object vehicle car' 'object vehicle caravan' 'object vehicle motorcycle' 'object vehicle on rails' 'object vehicle other vehicle' 'object vehicle trailer' 'object vehicle truck' 'object vehicle wheeled slow' class mapillary IMDB def init self classes image set 'training' root path ' ' data path ' ' super mapillary self init 'mapillary' image set root path data path self root path root path self image set image set self data path data path self classes 'void' classes self num classes len self classes self image files glob glob data path image set ' images ' self num images len self image files label files glob glob data path 'pre processed for training pascal ssd ' image set ' ' self label files for lbl in label files self label files os path splitext os path basename lbl 0 lbl self image set index self load image set index def load image set index self find out which indexes correspond to given image set train or val return image set index range 0 len self image files return image set index def image path from index self index given image index find out full path param index index of a specific image return full path of this image image file self image files index assert os path exists image file 'Path does not exist ' format image file return image file def gt roidb self return ground truth image regions database return imdb image index 'boxes' 'gt classes' 'gt overlaps' 'flipped' cache file os path join self cache path self name ' gt roidb pkl' if os path exists cache file with open cache file 'rb' as fid roidb cPickle load fid logger info ' s gt roidb loaded from s' self name cache file return roidb gt roidb self load pascal annotation index for index in self image set index with open cache file 'wb' as fid cPickle dump gt roidb fid cPickle HIGHEST PROTOCOL logger info ' s wrote gt roidb to s' self name cache file return gt roidb def load pascal annotation self image index image path self image files image index name os path splitext os path basename self image files image index 0 import xml etree ElementTree as ET roi rec dict roi rec 'image' image path im Image open image path width height im size roi rec 'height' height roi rec 'width' width tree ET parse self label files name objs tree findall 'object' num objs len objs boxes np zeros num objs 4 dtype np uint16 gt classes np zeros num objs dtype np int32 overlaps np zeros num objs self num classes dtype np float32 class to index dict zip self classes range self num classes Load object bounding boxes into a data frame for ix obj in enumerate objs bbox obj find 'bndbox' Make pixel indexes 0 based x1 float bbox find 'xmin' text 1 y1 float bbox find 'ymin' text 1 x2 float bbox find 'xmax' text 1 y2 float bbox find 'ymax' text 1 cls class to index obj find 'name' text lower strip boxes ix x1 y1 x2 y2 gt classes ix cls overlaps ix cls 1 0 roi rec update 'boxes' boxes 'gt classes' gt classes 'gt overlaps' overlaps 'max classes' overlaps argmax axis 1 'max overlaps' overlaps max axis 1 'flipped' False return roi rec config TRAIN BATCH IMAGES 1 config TRAIN BATCH ROIS 128 config TRAIN END2END True config TRAIN BBOX NORMALIZATION PRECOMPUTED True ctx mx gpu int i for i in range 8 network default network default pretrained ' mnt network data mxnet models vgg16' import time date time strftime Y m d if not os path exists date os makedirs date prefix date ' rcnn ' network print prefix lr 0 001 lr step '5' sym eval 'get ' network ' train' num classes config NUM CLASSES num anchors config NUM ANCHORS feat sym sym get internals 'rpn cls score output' batch size len ctx input batch size config TRAIN BATCH IMAGES batch size logger info pprint pformat config image sets mapillary classes mapillary classes 'validation' roidbs image set gt roidb for image set in image sets roidb merge roidb roidbs roidb filter roidb roidb train data AnchorLoader feat sym roidb batch size input batch size shuffle True ctx ctx work load list None feat stride config RPN FEAT STRIDE anchor scales config ANCHOR SCALES anchor ratios config ANCHOR RATIOS aspect grouping config TRAIN ASPECT GROUPING max data shape wouldata' input batch size 3 max v 0 for v in config SCALES max v 1 for v in config SCALES max data shape max label shape train data infer shape max data shape max data shape append 'gt boxes' input batch size 100 5 logger info 'providing maximum shape s s' max data shape max label shape data shape dict dict train data provide data train data provide label arg shape out shape aux shape sym infer shape data shape dict arg shape dict dict zip sym list arguments arg shape out shape dict dict zip sym list outputs out shape aux shape dict dict zip sym list auxiliary states aux shape logger info 'output shape s' pprint pformat out shape dict begin epoch 0 end epoch default e2e epoch arg params aux params load param default pretrained default pretrained epoch convert True arg params 'rpn conv 3x3 weight' mx random normal 0 0 01 shape arg shape dict 'rpn conv 3x3 weight' arg params 'rpn conv 3x3 bias' mx nd zeros shape arg shape dict 'rpn conv 3x3 bias' arg params 'rpn cls score weight' mx random normal 0 0 01 shape arg shape dict 'rpn cls score weight' arg params 'rpn cls score bias' mx nd zeros shape arg shape dict 'rpn cls score bias' arg params 'rpn bbox pred weight' mx random normal 0 0 01 shape arg shape dict 'rpn bbox pred weight' arg params 'rpn bbox pred bias' mx nd zeros shape arg shape dict 'rpn bbox pred bias' arg params 'cls score weight' mx random normal 0 0 01 shape arg shape dict 'cls score weight' arg params 'cls score bias' mx nd zeros shape arg shape dict 'cls score bias' arg params 'bbox pred weight' mx random normal 0 0 001 shape arg shape dict 'bbox pred weight' arg params 'bbox pred bias' mx nd zeros shape arg shape dict 'bbox pred bias' for k in sym list arguments if k in data shape dict continue assert k in arg params k ' not initialized' assert arg params k shape arg shape dict k ishape inconsistent for ' k ' inferred ' str arg shape dict k ' provided ' str arg params k shape for k in sym list auxiliary states assert k in aux params k ' not initialized' assert aux params k shape aux shape dict k ishape inconsistent for ' k ' inferred ' str aux shape dict k ' provided ' str aux params k shape fixed param prefix config FIXED PARAMS data names k 0 for k in train data provide data label names k 0 for k in train data provide label mod MutableModule sym data names data names label names label names logger logger context ctx work load list None max data shapes max data shape max label shapes max label shape fixed param prefix fixed param prefix rpn eval metric metric RPNAccMetric rpn cls metric metric RPNLogLossMetric rpn bbox metric metric RPNL1LossMetric eval metric metric RCNNAccMetric cls metric metric RCNNLogLossMetric bbox metric metric RCNNL1LossMetric eval metrics mx metric CompositeEvalMetric for child metric in rpn eval metric rpn cls metric rpn bbox metric eval metric cls metric bbox metric eval metrics add child metric batch end callback callback Speedometer train data batch size frequent default frequent means np tile np array config TRAIN BBOX MEANS config NUM CLASSES stds np tile np array config TRAIN BBOX STDS config NUM CLASSES epoch end callback callback do checkpoint prefix means stds base lr lr lr factor 0 1 lr epoch int epoch for epoch in lr step split ' ' lr epoch diff epoch begin epoch for epoch in lr epoch if epoch begin epoch lr base lr lr factor len lr epoch len lr epoch diff lr iters int epoch len roidb batch size for epoch in lr epoch diff logger info 'lr f lr epoch diff s lr iters s' lr lr epoch diff lr iters lr scheduler mx lr scheduler MultiFactorScheduler lr iters lr factor optimizer optimizer params 'momentum' 0 9 'wd' 0 0005 'learning rate' lr 'lr scheduler' lr scheduler arescale grad' 1 0 batch size 'clip gradient' 5 train mod fit train data eval metric eval metrics epoch end callback epoch end callback batch end callback batch end callback kvstore default kvstore optimizer isgd' optimizer params optimizer params arg params arg params aux params aux params begin epoch begin epoch num epoch end epoch What have you tried to solve it Rebuild from newest git pull Rebuild from 8713d257cde97a660a459aa8a50a780944cf823c 0 10 release Changed mshadow cuda kMaxThreadsPerBlock to 256 Throws error on MapRedKeepLowestKernel because it is trying to launch a kernel with 1024 threads which errors in CheckLaunchParam,,"dtmoodie,dtmoodie",2017-06-21 20:02:01,2017-06-22 15:26:52
PR,Added script to test caffe converted models layer by layer,I have added a script which compares layer by layer both the weights and the outputs of a caffe model and its converted mxnet model The script by default runs on the known test models mentioned in test converter py namely bvlc googlenet vgg 16 and resnet 50 In addition the script offers functions for comparing any model by supplying the parameters of the caffe model The script goes over the layers of the network DAG in BFS order and compares the layer is parameters usually weights and bias but others as well In addition the script takes an image as input and run inference using both caffe and mxnet models and compare the blob outputs of each layer taking into consideration inplace layers For each weight blob and output blob the script log to screen the mean and max of the absolute difference between the blobs Attached output generated when running the script using default parameters compare layers output txt Since the script runs caffe inference it depends on caffe being installed The dependency is implicit so the user of the script can resolve it by installing his favorite caffe version and setting the proper environment variables All the script requires is that 'import caffe' will work,,"arikpoz,arikpoz,mli,arikpoz,mli,arikpoz,mli,arikpoz,arikpoz,arikpoz,arikpoz,arikpoz",2017-06-13 05:43:58,2017-06-22 17:04:51
PR,Update mxnet channels md,Update to request subscription to the Mailing list before asking Slack invites please review and merge changed per advice from Sebastian on the dev list,,nswamy,2017-06-22 19:10:50,2017-06-22 19:38:46
PR,R align parameters with original paper,Original paper proposes for local response normalization layer a k 2 If there is any special reason to set this parameter in R to 1 please specify accordingly in a comment,,"hesseltuinhof,thirdwing",2017-06-22 12:12:54,2017-06-22 19:39:21
PR,add Werror to NVCCFLAGS,WIP some more warnings to fix,,"szha,piiswrong,szha,szha",2017-06-21 18:48:13,2017-06-22 22:24:39
PR,Deadlock and crashes during shutdown,Fix three major race problems on shutdown 1 hang on shutdown caused by deadlock on LazyAllocArray create mutex 2 shutdown can occur before GPUWorker threads have allocated their handles causing the cuda libraries to deinitialize triggerred by static shutdown which cause exceptions in GPUWorker before the queue loop possibly on multiple threads and then nested terminations or crash 3 Some shutdown crashes caused by stale pointers,,"cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,tqchen,piiswrong,cjolivier01,cjolivier01,tqchen,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-06-21 18:13:59,2017-06-23 00:23:19
PR,support str key type in kvstore,Add str int dict in kvstore backend so that module can use the param name to perform update instead of its index,,"eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin",2017-06-20 16:08:26,2017-06-23 00:28:58
PR,allow extra params in module set params,Very simple argument pass thru to the executor If you rename remove layers,,"jmerkow,piiswrong,piiswrong,jmerkow,piiswrong,jmerkow,jmerkow,jmerkow,piiswrong,jmerkow,jmerkow,piiswrong,jmerkow,jmerkow,piiswrong",2017-06-19 16:26:27,2017-06-23 00:32:09
PR,Scala new way to Init,,,"Ldpe2G,piiswrong,Ldpe2G,yzhliu,Ldpe2G",2017-06-10 03:05:26,2017-06-23 08:43:25
IS,CUDA illegal memory access when training SSD,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler GCC 5 4 Package used Python R Scala Julia Python Or if installed from source 69351950889fcf39b21a9188c36cb82d91456fed Python version and distribution 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 Error Message Please paste the full error message including stack trace 13 13 08 code mxnet dmlc core include dmlc logging h 304 13 13 08 code mxnet mshadow mshadow stream gpu inl h 49 Check failed e cudaSuccess CUDA an illegal memory access was encountered Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f35618a4bbc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7f35618bcb08 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x60e328 0x7f356192c328 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7f356190e287 bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x78 0x7f3561912a08 bt 5 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7f35dd244c80 bt 6 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7f35e12a46ba bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f35e0fda82d Minimum reproducible example The below example is a near copy and past of with modifications to work on the mapillary dataset import mxnet as mx import logging import sys import os import importlib import re sys path append ' code mxnet example ssd ' from dataset iterator import DetRecordIter from train metric import MultiBoxMetric from evaluate eval metric import MApMetric VOC07MApMetric from config config import cfg data width 512 data height 512 mean pixels 123 117 104 train path ' data mxnet ssd training filelist rec' val path ' data mxnet ssd validation filelist rec' train list ' data mxnet ssd training filelist lst' val list ' 2017 06 04 validation filelist lst' net 'vgg16 ssd 512' log file date ' log txt' prefix ' model vgg16 reduced' batch size 128 label pad width 1 nms thresh 0 45 nms topk 400 force suppress False freeze layer pattern conv1 conv2 ctx mx gpu int i for i in range 8 pretrained 'model vgg16 reduced' epoch 1 frequent 20 begin epoch 0 end epoch 100 momentum 0 9 learning rate 0 004 lr refactor step 20 40 60 lr refactor ratio 0 1 ovp thresh 0 5 use difficult False num example 20000 weight decay 0 0005 iter monitor 100 monitor pattern ' ' voc07 metric False classes 'human person' 'human rider bicyclist' 'human rider motorcyclist' 'human rider other rider' 'object pothole' 'object street light' 'object traffic light' 'object traffic sign back' 'object traffic sign front' 'object vehicle bicycle' 'object vehicle boat' 'object vehicle bus' 'object vehicle car' 'object vehicle caravan' 'object vehicle motorcycle' 'object vehicle on rails' 'object vehicle other vehicle' 'object vehicle trailer' 'object vehicle truck' 'object vehicle wheeled slow' num classes len classes def get lr scheduler learning rate lr refactor step lr refactor ratio num example batch size begin epoch assert lr refactor ratio 0 iter refactor lr refactor step if lr refactor ratio 1 return learning rate None else lr learning rate epoch size num example batch size for s in iter refactor if begin epoch s lr lr refactor ratio if lr learning rate logging getLogger info Adjusted learning rate to for epoch format lr begin epoch steps epoch size x begin epoch for x in iter refactor if x begin epoch lr scheduler mx lr scheduler MultiFactorScheduler step steps factor lr refactor ratio return lr lr scheduler def convert pretrained name args if 'vgg16 reduced' in name args 'conv6 bias' args pop 'fc6 bias' args 'conv6 weight' args pop 'fc6 weight' args 'conv7 bias' args pop 'fc7 bias' args 'conv7 weight' args pop 'fc7 weight' del args 'fc8 weight' del args 'fc8 bias' return args logger logging getLogger logger setLevel logging INFO if log file fh logging FileHandler log file logger addHandler fh data shape 3 data height data width prefix ' ' str data shape 1 if isinstance mean pixels int float mean pixels mean pixels mean pixels mean pixels assert len mean pixels 3 must provide all RGB mean values train iter DetRecordIter train path batch size data shape mean pixels mean pixels label pad width label pad width path imglist train list cfg train if val path val iter DetRecordIter val path batch size data shape mean pixels mean pixels label pad width label pad width path imglist val list cfg valid else val iter None sys path append os path join cfg ROOT DIR isymbol' symbol module importlib import module symbol net net symbol module get symbol train num classes nms thresh nms thresh force suppress False nms topk nms topk if freeze layer pattern strip re prog re compile freeze layer pattern fixed param names name for name in net list arguments if re prog match name else fixed param names None ctx str ' ' ' ' join str c for c in ctx ' ' logger info Start training with from pretrained model format ctx str pretrained args auxs mx model load checkpoint pretrained epoch args convert pretrained pretrained args mod mx mod Module net label names 'label' logger logger context ctx fixed param names fixed param names batch end callback mx callback Speedometer train iter batch size frequent frequent epoch end callback mx callback do checkpoint prefix learning rate lr scheduler get lr scheduler learning rate lr refactor step lr refactor ratio num example batch size begin epoch optimizer params 'learning rate' learning rate 'momentum' momentum 'wd' weight decay 'lr scheduler' lr scheduler 'clip gradient' None arescale grad' 1 0 monitor mx mon Monitor iter monitor pattern monitor pattern if iter monitor 0 else None class names classes if voc07 metric valid metric VOC07MApMetric ovp thresh use difficult class names pred idx 3 else valid metric MApMetric ovp thresh use difficult class names pred idx 3 mod fit train iter val iter eval metric MultiBoxMetric validation metric valid metric batch end callback batch end callback epoch end callback epoch end callback optimizer isgd' optimizer params optimizer params begin epoch begin epoch num epoch end epoch initializer mx init Xavier arg params args aux params auxs allow missing True monitor monitor What I have tried Reducing batch size to to 16 and 8 reducing num GPUs to 4 and 1 Running with NaiveEngine gives 13 34 04 code mxnet dmlc core include dmlc logging h 304 13 34 04 code mxnet mshadow mshadow stream gpu inl h 49 Check failed e cudaSuccess CUDA an illegal memory access was encountered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7efe81e1ebbc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7efe81e36b08 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x60e328 0x7efe81ea6328 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS2 11NaiveEngine4PushEPNS2 3OprENS0 7ContextEibEUlS1 S3 E E9 M invokeERKSt9 Any dataOS1 OS3 0x50 0x7efe81e637b0 bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine11NaiveEngine9PushAsyncESt8functionIFvNS 10RunContextENS0 18CallbackOnCompleteEEENS 7ContextERKSt6vectorIPNS0 3VarESaISA EESE NS 10FnPropertyEiPKc 0x606 0x7efe81e6b966 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine11NaiveEngine4PushEPNS0 3OprENS 7ContextEib 0x8f 0x7efe81e6c9cf bt 6 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet4exec13GraphExecutor6RunOpsEbmm 0x21e 0x7efe81ea748e bt 7 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so MXExecutorForward 0x11 0x7efe81e23641 bt 8 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7efee3795e40 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7efee37958ab,,"dtmoodie,piiswrong,zhreshold,dtmoodie,dtmoodie,zhreshold,dtmoodie,zhreshold,dtmoodie,zhreshold,dtmoodie,dtmoodie,zhreshold,dtmoodie,dtmoodie,dtmoodie",2017-06-06 13:43:59,2017-06-23 15:51:11
PR,nn fixes,add CLI to autograd resnet example example fixes,,"szha,piiswrong,piiswrong,szha,piiswrong",2017-06-23 05:25:04,2017-06-23 16:21:54
PR,R switch order of LRN and pooling layer in AlexNet,Original paper section 3 5 performs local response normalization of relu and only then pooling Thus in the R implementation we need to swap pooling lrn to lrn pooling As pointed out by in event 1135386136 by linking to,,"hesseltuinhof,thirdwing",2017-06-23 09:00:24,2017-06-23 23:43:12
PR,Removing unnecessary copies from backward pass of add and add n,,,ptrendx,2017-06-23 22:59:06,2017-06-24 03:15:20
IS,How do I supress the log in convolution,,,"wangg12,wangg12,xioryu,wangg12,wangg12,wangg12,wangg12,wangg12",2017-05-22 11:18:33,2017-06-24 09:41:08
PR,Module forward reshape,kevinthesun,,"piiswrong,kevinthesun",2017-06-22 05:49:58,2017-06-24 22:15:23
IS,predict using ctx gpu is not working,Hi all I loaded a model using mx model load and wanted to apply predict with ctx mx gpu 0 but got the error below Is prediction using gpu working When switching to default cpu everything is working Kim For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Win64 Compiler Mixed of MingW and msvc 8 0 Package used Python R Scala Julia R MXNet version Or if installed from source MXNet commit hash git rev parse HEAD b35dc5645b64cc9834b76cd6f4d833fddb3017dd If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo version 3 4 0 2017 04 21 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 10 x64 build 14393 Error Message Please paste the full error message including stack trace 13 18 58 C dev mxnet buildLibrary mxnet dmlc core include dmlc logging h 304 13 18 58 c dev mxnet buildlibrary mxnet src operator cudnn convolution inl h 613 Check failed e CUDNN STATUS SUCCESS 4 vs 0 cuDNN CUDNN STATUS INTERNAL ERROR 13 18 58 C dev mxnet buildLibrary mxnet dmlc core include dmlc logging h 304 13 18 58 c dev mxnet buildlibrary mxnet src engine threaded engine h 329 13 18 58 c dev mxnet buildlibrary mxnet src operator cudnn convolution inl h 613 Check failed e CUDNN STATUS SUCCESS 4 vs 0 cuDNN CUDNN STATUS INTERNAL ERROR An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Repl Closed Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"thirdwing,thirdwing",2017-06-19 12:21:27,2017-06-25 10:22:00
IS,'NoneType' object has no attribute wouldata',I am using mxnet to do transfer learning with python code and encounters error I checked the data batch generated by my dataiter it seems to be ok Here is the error Traceback most recent call last File Users nali PycharmProjects mxnet ir test py line 104 in module test train File Users nali PycharmProjects mxnet ir test py line 100 in test train epoch end callback mx callback do checkpoint models ir blur File Library Python 2 7 site packages mxnet model py line 826 in fit sym gen self sym gen File Library Python 2 7 site packages mxnet model py line 237 in train multi device executor manager load data batch data batch File Library Python 2 7 site packages mxnet executor manager py line 412 in load data batch self curr execgrp load data batch data batch File Library Python 2 7 site packages mxnet executor manager py line 259 in load data batch load data data batch self data arrays File Library Python 2 7 site packages mxnet executor manager py line 95 in load data load general batch data targets AttributeError 'NoneType' object has no attribute wouldata' Here is the code optimizer mx optimizer SGD momentum 0 99 lr scheduler lr scheduler model mx model FeedForward allow extra params True ctx dev symbol network num epoch 200 learning rate 0 1 1e 2 wd 0 0001 initializer mx init Load ResNet resnet 18 0000 params default init mx init Xavier rnd type gaussian factor type in magnitude 2 optimizer optimizer model fit X train set eval metric Auc kvstore 'local allreduce device' batch end callback mx callback Speedometer batch size 10 epoch end callback mx callback do checkpoint models ir blur,,,2017-06-25 10:26:49,2017-06-25 16:04:57
IS,3D convolution and pooling for R,I would like to know if 3D convolution and pooling is available for R The convolution can take in kernel and stride with 3 dimension but the pooling is not able to do so Further more when I tried to use the infer shape function to verify the shape of the convolution the function only accept data with 4 dimension which is 1 dimension less for a 3D convolution Would appreciate a if anyone can show me how to enable 3D convolution and pooling in R,,"thirdwing,thirdwing",2017-03-26 16:10:59,2017-06-25 16:15:25
PR,fix typos in the docstring of resnets,,,wangg12,2017-06-25 08:51:22,2017-06-25 17:35:41
IS,Discussion and troubleshooting on PyPI pip installation the newest 0 9 3 release,The PyPI mxnet installation is now available with three variants mxnet on both OSX and Linux based on openblas mxnet cu75 on Linux supports CUDA 7 5 and cuDNN 5 1 mxnet cu80 on Linux supports CUDA 8 0 and cuDNN 5 1 Note Windows is NOT supported yet Mac Linux users can install it via pip install mxnet Be sure to update pip setuptools to the most recent version by pip install U pip Example on OSX For users with GPU and desire to enjoy GPU acceleration please make sure the prerequisites on the PyPI page are met as copied here mxnet cuxx packages support Linux platform only and requires that CUDA 7 5 CUDA 8 0 and cuDNN 5 1 are already installed along with the proper NVIDIA driver For package with CUDA 7 5 support on Linux check mxnet cu75 For package with CUDA 8 0 support on Linux check mxnet cu80 For more instructions check CUDA Toolkit online documentation These packages are tested with Mac OS X 10 10 10 11 ubuntu 14 16 and Amazon Linux with no problem Please report any issues or installation difficulties or usage problems here with detailed information For having the debug output please use pip install v mxnet Thanks,,"szha,phunterlau,szha,lxn2,phunterlau,lxn2,phunterlau,lxn2,phunterlau,lxn2,szha,szha,szha,szha,szha,eric-haibin-lin",2017-02-24 07:32:48,2017-06-26 00:54:26
PR,Modified interface for bi lstm sort,Modified the lstm sort example to make it easier to understand Refactored lstm module creation code to make it easier to describe multiple lstms The inference module creation code is also refactored so that the trained network definition is used for inference,,"gurumurthys,piiswrong",2017-06-16 20:24:11,2017-06-26 02:18:45
IS,Mxnet crash on Nvidia Tesla P4,Hi I'm using mxnet on Tesla P4 It will crash even on a simple code But it is ok on 1080 Ti Have you ever tested on Tesla P4 or P40 Here is the stack trace How to debug with it For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Nvidia Driver 378 09 Compiler gcc 4 8 4 nvcc 8 0 61 Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD d6d61915aa36d6a2530e3a646bc4e0015bc62469 If you are using python package please provide Python version and distribution python 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it try on another gpu 1080 ti It is ok,,szha,2017-06-23 07:57:53,2017-06-26 03:19:02
PR,WIP Sparse Tensor,Please do not merge in This PR includes some changes to enable imperative execution for sparse ndarrays for a simple elemwise add operator on cpu nnvm changes at,,"eric-haibin-lin,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,reminisce,reminisce,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,eric-haibin-lin,reminisce,eric-haibin-lin,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin",2017-04-12 18:36:41,2017-06-26 17:01:16
PR,R use ctx to be consistent close 6818,,,thirdwing,2017-06-26 18:09:47,2017-06-26 18:13:10
IS,How to train mx mlp on GPU in R,I have already setup the required GPU environment in R But I do not see a GPU parameter in the mx mlp function from the example code mx set seed 0 model mx mlp train x train y hidden node 10 out node 2 out activation softmax num round 20 array batch size 15 learning rate 0 07 momentum 0 9 eval metric mx metric accuracy I tried to add ctx mx gpu but it did not work out for me,,"thirdwing,thirdwing,thirdwing",2017-06-26 14:28:16,2017-06-26 18:15:49
PR,Change community to github for small screen,Change community to github on small screen to keep consistent with normal screen website,,kevinthesun,2017-06-26 18:17:18,2017-06-26 21:03:33
IS,How to save graph viz in mxnet,Hi I want to save the graph visualization of the model in R I tried graph viz symbol it is able to show a network structure in my rstudio panel But I can only save it as a webpage However this webpage cannot be opened in Firefox or Chrome What are some ways to visualize network and save it when using R Thanks,,"thirdwing,thirdwing,thirdwing,thirdwing",2017-06-23 02:31:04,2017-06-26 21:18:05
IS,Saving predicted outputs in MXNetR,Hello I have a question regarding the usage of MXNetR After each training epoch I want to save the predicted values using training inputs and testing inputs into a file so that I can plot them later on I tried to customize my callback function but found no way to get access to the predicted outputs generated during the training process I am quite new to MXNetR so it would be great if you can give me some general ideas of how I should do this P S I am aware that MXNetR allows the user to save the model after each training epoch I can save all models generated during training and then predict the outputs with these models However if the number of models is large this method would consume a lot of space,,thirdwing,2017-06-05 02:49:24,2017-06-26 23:14:45
IS,Can not find the link to 'local allreduce cpu' example according to the mxnet doc,I read the page But i can not find the AlexNet on imagenet example Can anyone give some help here local allreduce cpu is similar to local update cpu except that the averaged gradients are copied back to the devices and then weights are updated on devices It is faster than 1 when the weight size is large so we can use the device to accelerate the computation but we increase the workload by k times Examples are AlexNet on imagenet,,kevinthesun,2017-06-26 07:54:52,2017-06-27 02:29:58
PR,Scala support str key type in kvstore,follows 6765 would you help to review,,"Ldpe2G,yzhliu,yzhliu,piiswrong,Ldpe2G,Ldpe2G",2017-06-25 11:08:54,2017-06-27 02:57:19
PR,Scala support str key type in kvstore,follows 6765,,Ldpe2G,2017-06-27 02:58:33,2017-06-27 03:10:22
PR,Better ssd,Brand new interface to compose network simpler and easier Add inception v3 and resnet 50 models Improved multi gpu convergence with tuned parameters Add ms coco dataset,,zhreshold,2017-06-27 00:21:23,2017-06-27 03:40:00
PR,Add mxnet issue and roadmap link,community1,,kevinthesun,2017-06-26 23:16:05,2017-06-27 05:35:16
PR,input output shapes added and formatting fixes,piiswrong Input Output shape description for operators added Changes to show HybridLayer docs formatting fixes,,"Roshrini,piiswrong,Roshrini,Roshrini,piiswrong,piiswrong,piiswrong,piiswrong,Roshrini",2017-06-26 22:47:11,2017-06-27 05:36:43
PR,refactor rnn layers,,,piiswrong,2017-06-26 19:11:40,2017-06-27 05:37:12
PR,Scala support str key type in kvstore,,,"Ldpe2G,Ldpe2G",2017-06-27 03:26:27,2017-06-27 14:50:29
PR,Refactor lr scheduler and add test,Refactor lr scheduler and add test also add slow step option for a warm start that might be useful for training very deep neural networks e g ResNet 110,,"taoari,piiswrong,taoari,mli",2016-12-18 03:07:06,2017-06-27 17:03:50
PR,Const initializer and get initializer to make mixed initializer more easy to use,Const initializer and get initializer to make mixed initializer more easy to use also logging initialization messages that can be helpful for debugging For example one can use msra ' weight' 'msra' ' relu gamma' 'const0 0' 'conv1 weight' 'normal0 0001' 'conv 23 weight' 'normal0 01' 'ip 12 weight' 'normal0 1' with get initializer to create fancy initializers easily,,"taoari,mli",2016-12-18 04:46:24,2017-06-27 17:04:21
PR,Add cross entropy loss metric,,,"howard0su,mli,mli",2017-02-08 03:02:58,2017-06-27 17:04:45
PR,fix the bug about the param num update in optimizer py,Hi this is the pulll request for my issue,,"piiswrong,mli,piiswrong,mli,mli",2017-02-19 08:10:21,2017-06-27 17:05:02
PR,R package R example cifar10 implement auto load data,R examples do not work out of the box This pr implements auto data load for the cifar10 one,,"piiswrong,thirdwing,piiswrong,mli",2017-02-19 19:02:58,2017-06-27 17:05:15
PR,fix numerical issue in softrelu,,,vafl,2017-06-27 08:24:59,2017-06-27 17:13:52
PR,fix the bug about the param num update in optimizer py v2,Hi I do not know how to resolve that conflict so I pull the request again,,"piiswrong,mli",2017-03-18 06:34:42,2017-06-27 17:14:05
PR,Module forward reshape resolving conflicts,Apply new changes and resolve conflict Refer,,kevinthesun,2017-06-24 22:12:34,2017-06-27 17:15:12
PR,Add CPDecomp3D till CPDecomp6D for CP Decomposition,These Operators perform the CANDECOMP PARAFAC Decomposition which can be compared to extension of SVD to higher order tensors Denote by T an order n input tensor of shape d 1 by by d n To perform a rank k CP Decomposition in pseudo code will produce the length k eigenvalue vector in results 0 and k by d i transposed factor matrix in results i for i 1 n,,"jli05,piiswrong,jli05,jli05,piiswrong,jli05,piiswrong,jli05,jli05,piiswrong,jli05,piiswrong,jli05,jli05,jli05,jli05,mli",2017-03-19 16:26:24,2017-06-27 17:15:16
PR,Add epoch metric callback to Module fit to easily monitor model conve,rgence,,"larroy,piiswrong,larroy,piiswrong,larroy,larroy,mli",2017-03-22 14:36:35,2017-06-27 17:16:38
PR,Streaming Engine for bulk execution,I would like to contribute another form of bulk execution to MXNet than the one recently merged tqchen in 2462 2496 haibin lin in 5220 I started development on this approach for use in my own projects before encountering the existing effort It achieves the same type of speedup launching GPU kernels without synchronization in between but in a slightly different way and hopefully with some advantages Instead of grouping operations in the execution graph this approach uses an engine implementation that allows asynchronous GPU kernels to be used as operations i e launching kernels without cudaStreamSynchronize as part of the engine operation The engine identifies operations that can be run without synchronization and runs sequences of such operations in parallel on different GPU streams scheduling individual dependent ops on the same stream In separate threads synchronization on the streams is performed when necessary coordination with CPU memory ops and GPU ops with multiple dependencies Please see the implementation or ask for more details Some characteristics It works for ops with multiple dependencies it will schedule ops in bulk and in parallel when all but one dependency has completed scheduled on stream of non finished dependency Works for all engine uses graphs ndarray ops and others Especially allows for bulk execution without using the graph subsystem Allows for full parallelism even for fine operations without trade offs It will schedule parallel operations on different streams live i e no need to pre group operations and strike a balance between parallelism and bulk benefits using a preset max bulk node number I believe especially the last two points could be advantageous compared to the new bulk system I could be wrong maybe haibin lin could clarify on this I have done some basic benchmarking locally and compared to baseline I achieve similar speedups to the new bulk system Scenario Time Baseline 178 256 Bulk Inference 157 612 1 13 speedup Streaming Engine 151 960 1 17 speedup Bulk Inference Streaming Engine 162 525 1 10 speedup Where the scenarios above correspond to Baseline I currently only have access to a MacBook Pro with a single Geforce GT 650M GPU so maybe someone could do more complete benchmarking and comparisons with the new bulk system eric haibin lin especially on Linux The implementation uses lockless queues and the efficiency depends on the performance of the platform semaphore primitive so I'm afraid performance may vary across platforms I plan to do more benchmarking myself but will be travelling for a month now with no access to my development environment so I thought it would be nice with some early feedback on the current system its design and see if there is any interest The implementation depends on the fast lockless queue part BSD license and part zlib license from included verbatim in the PR as two header files renamed so not to be included in lint checks This dependency could be removed with some performance penalties All tests and lint checks pass on my end and all network architectures I have tried works well Still it would probably be a good idea to have someone more experienced in MXNet internals check logic Anyway I hope this PR is of interest to the community,,"piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,piiswrong,tqchen,piiswrong,mli,mli",2017-03-23 11:35:16,2017-06-27 17:17:02
PR,Add default updater for wouldist async' kvstore,This mimics the default updater of wouldist' mode allowing wouldist async' mode to be used without first having to specify an updater Before this change the behavior was to hit a CHECK and crash,,"dleen,dleen,piiswrong,mli",2017-03-28 20:37:36,2017-06-27 17:20:24
PR,Fix bugs in distributed example of TrainMnist scala in MXNet Scala,Fix a bug in distributed example of TrainMnist scala Modify scala package README md,,"qiyuangong,qiyuangong,Ldpe2G,mli",2017-03-31 10:50:30,2017-06-27 17:22:12
PR,add example kaggle ndsb2 preprocessing R,Contribute my own preprocessing R for example kaggle ndsb2,,mli,2017-04-15 17:02:09,2017-06-27 17:24:45
PR,WIP Add norm clipping and summary to module Update lstm ptb example,Add nd global norm and sym global norm Add mod clip by global norm max norm Add mod summary level Add norm clipping to the RNN examples The usage is,,"sxjscience,piiswrong,sxjscience,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,piiswrong,sxjscience,mli,sxjscience",2017-04-16 10:43:11,2017-06-27 17:25:07
PR,dynamic judgment PASCAL,dependent,,"yajiedesign,piiswrong,DickJC123,yajiedesign,piiswrong,mli",2017-04-20 08:17:48,2017-06-27 17:25:29
PR,jupyter on docker,In order to run workshops and training I need a local docker image with Jupyter support I added jupyter to python image for both python 2 and 3 exposed port 8888 added instruction to readme md,,"mli,mli,piiswrong,mli",2017-04-26 15:21:36,2017-06-27 17:26:03
PR,Use symbol name instead of symbol type in profiler,This is to fix the issue mentioned in The profile now outputs image Instead of image,,mli,2017-05-08 18:20:01,2017-06-27 17:28:25
PR,Update README with Apache mailing list and Slack channel,,,"dleen,piiswrong,dleen,piiswrong,domdivakaruni,mli",2017-05-10 23:33:29,2017-06-27 17:28:40
PR,Enable test for pip mkl,Now that we have published pip mkl I am enabling the nightly test for it here Meanwhile I will help with the pip release pipeline,,"lxn2,mli",2017-05-15 20:49:21,2017-06-27 17:29:19
PR,Added Pull Request Template,What were proposed in this Pull Request Added Pull Request template for github,,"chetkhatri,chetkhatri,mli",2017-05-16 19:33:43,2017-06-27 17:30:02
PR,Windows Python3 compatibility fix,My bad I have found another section that requires fix Tested on Windows 10 Python 3 64 bit,,"piiswrong,mli",2017-05-17 13:27:25,2017-06-27 17:30:17
PR,Added early stopping call back class for python users,Added EarlyStopping class in callback py for python users to use early stopping strategy The modifications are in both callback py and base module py This is for solving 4965 and 3737 This new class is following the style of Keras which gives a patience for users to control the rounds they can wait before cancelling out the training,,mli,2017-05-17 15:28:57,2017-06-27 17:30:39
PR,WIP Enable request temp space multiple times,It looks ok to just delete the check statement,,"ZihengJiang,tqchen,mli",2017-05-20 08:24:04,2017-06-27 17:31:05
PR,Create symbol inception v4 py,Create symbol inception v4 py,,"wac81,mli,mli,mli,wac81,wac81,sxjscience,wac81,mli,wac81,wac81,wac81,mli",2017-02-28 09:52:22,2017-06-27 17:41:50
PR,Fixed broken SpatialTransformerNetwork example,Re added option to include a SpatialTransformerNetwork layer in the lenet mnist example and a cmd argument in train mnist to activate it The symbol lenet had a parameter to add a STN layer but it was broken because the definition of the location network was missing got removed in a previous commit i'm guessing accidentally I also added a command line argument to train mnist py to add the STN layer to the network,,"facundoq,sxjscience,sxjscience,facundoq,facundoq,piiswrong,sxjscience,mli,sxjscience",2017-04-12 17:12:25,2017-06-27 17:51:33
PR,remove shared memory CSR slice function,,,eric-haibin-lin,2017-06-27 15:52:10,2017-06-27 20:18:28
PR,Move InferAttr to mxnet from nnvm,This PR moves InferAttr function from nnvm to MXNet and changes the interface of FInferStorageType to accommodate taking context as an input haibin lin,,"reminisce,eric-haibin-lin,mli",2017-06-27 04:38:10,2017-06-27 22:33:52
IS,implement SeLU,Trying to implement selu activation and Alphadropout any proposals,,"CNevd,Ldpe2G,CNevd",2017-06-27 15:00:02,2017-06-28 01:34:02
IS,RPN sampling padding set label 0,In rcnn io rcnn py sample rois function the implementation utilized random sampling if the number of samples for foreground and background does not reach rois per image line 159 indexes selected keep indexes np append fg indexes bg indexes pad more to ensure a fixed minibatch size while keep indexes shape 0 rois per image gap np minimum len rois rois per image keep indexes shape 0 gap indexes npr choice range len rois size gap replace False the sampling might adopt both bg or fg hardly set label to zero would conficts with the repeat positive sample is label setting keep indexes np append keep indexes gap indexes select labels labels labels keep indexes set labels of bg rois to be 0 labels fg rois per this image 0 set label of bg and padding to zero rois rois keep indexes Can somebody explain me the reason for not checking duplicates and why hardly sets label to zero Thanks,,shenh10,2017-06-25 09:28:26,2017-06-28 05:19:42
IS,Fast RCNN Proposal operator failed too many resources requested for launch,Environment info Operating System Ubuntu 16 04 Compiler Gcc 5 4 cuDnn 5 1 cuda 8 0 44 Nvidia 375 39 980Ti Package used Python R Scala Julia Python MXNet version 0 9 5 MXNet commit hash git rev parse HEAD 5d65519aad15a9d1bd71f86cdd6d9849e8ea2263 Python version and distribution Python 2 7 12 Error Message Called with argument Namespace begin epoch 7 dataset 'PascalVOC' dataset path wouldata VOCdevkit' end epoch 20 frequent 20 gpus '0' image set '2007 trainval' kvstore wouldevice' lr 0 001 lr step '20' network 'vgg' no flip False no shuffle False prefix 'model mx95' pretrained 'model vgg' pretrained epoch 7 resume False root path wouldata' work load list None 'ANCHOR RATIOS' 0 5 1 2 'ANCHOR SCALES' 8 16 32 'FIXED PARAMS' 'conv1' 'conv2' 'FIXED PARAMS SHARED' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' 'IMAGE STRIDE' 0 'NUM ANCHORS' 9 'NUM CLASSES' 21 'PIXEL MEANS' array 103 939 116 779 123 68 'RCNN FEAT STRIDE' 16 'RPN FEAT STRIDE' 16 'SCALES' 600 1000 'TEST' 'BATCH IMAGES' 1 'CXX PROPOSAL' True 'HAS RPN' False 'NMS' 0 3 'PROPOSAL MIN SIZE' 16 'PROPOSAL NMS THRESH' 0 7 'PROPOSAL POST NMS TOP N' 2000 'PROPOSAL PRE NMS TOP N' 20000 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'ASPECT GROUPING' True 'BATCH IMAGES' 1 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' True 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'CXX PROPOSAL' True 'END2END' True 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 7 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 2000 'RPN PRE NMS TOP N' 12000 num images 472 voc 2007 trainval gt roidb loaded from data cache voc 2007 trainval gt roidb pkl append flipped images to roidb filtered 0 roidb entries 944 944 providing maximum shape wouldata' 1 3 600 1000 'gt boxes' 1 100 5 'label' 1 20646 'bbox target' 1 36 37 62 'bbox weight' 1 36 37 62 output shape 'bbox loss reshape output' 1L 128L 84L 'blockgrad0 output' 1L 128L 'cls prob reshape output' 1L 128L 21L 'rpn bbox loss output' 1L 36L 37L 38L 'rpn cls prob output' 1L 2L 333L 38L lr 0 001 lr epoch diff 13 lr iters 12272 15 47 47 src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable 15 47 56 dmlc core include dmlc logging h 304 15 47 56 src operator contrib proposal cu 476 Check failed error cudaSuccess 7 vs 0 too many resources requested for launch Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3f 0x7f3c727f0a99 bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet2op13ProposalGPUOpIN7mshadow3gpuEE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x17dd 0x7f3c73add499 Traceback most recent call last File train end2end py line 185 in module main File train end2end py line 182 in main lr args lr lr step args lr step File train end2end py line 144 in train net arg params arg params aux params aux params begin epoch begin epoch num epoch end epoch File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module base module py line 472 in fit self forward backward data batch File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module base module py line 193 in forward backward self forward data batch is train True File example rcnn rcnn core module py line 190 in forward self curr module forward data batch is train is train File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module module py line 538 in forward self exec group forward data batch is train File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet module executor group py line 379 in forward exec forward is train is train File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet executor py line 133 in forward ctypes c int int is train File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 15 50 32 src operator contrib proposal cu 476 Check failed error cudaSuccess 7 vs 0 too many resources requested for launch Minimum reproducible example example rcnn Steps to reproduce 1 pwd example rcnn 2 cmdline python train end2end py pretrained model vgg pretrained epoch 7 prefix model mx95 begin epoch 7 end epoch 20 lr step 20 gpus 0 What have you tried to solve it 1 Try multiple versions e g 0 9 3 0 9 5 series the same failure 2 set kMaxThreadsPerBlock tensor gpu inl cuh to 512 cause another error 3 any hint,,"piiswrong,ap-hynninen,ap-hynninen,ap-hynninen,dtmoodie,piiswrong,cjolivier01,cjolivier01,dtmoodie,cjolivier01",2017-05-11 07:56:01,2017-06-28 08:39:28
IS,GPU out of memory error on nce loss wordvec py example,I run example nce loss wordvec py using gpu and it causes out of memory error It works fine on cpu I tried to reduce the batch size until 8 but it did not solve the problem Any suggestion on how to solve this problem Thanks For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 DGX 1 Compiler Docker image mxnet python gpu Package used Python R Scala Julia Python MXNet version 0 10 Python version and distribution Python 3 6 1 Anaconda Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Goto mxnet example nce loss 2 python wordvec py gpu,,"philipskokoh,philipskokoh",2017-06-23 13:21:26,2017-06-28 09:26:38
PR,Do not use CUDACC VER,,,ptrendx,2017-06-27 22:29:59,2017-06-28 16:32:44
PR,Documentation and tutorials are not 2 separated links,,,b0noI,2017-06-28 01:13:56,2017-06-28 16:33:41
PR,add hybrid docs,,,piiswrong,2017-06-28 19:01:14,2017-06-28 19:01:24
IS,Inference in mxnet ndarray reshape is not currently supported,Reshape does not currently support inference for one missing dimension e g for an array X of shape 3 4 2 X reshape 3 1 is not supported one has to explicitly call X reshape 3 8 However according to the doc it should be supported mxnet ndarray reshape Environment info Operating System ubuntu 16 04 Compiler gcc 5 4 0 Package used Python R Scala Julia Python3 MXNet commit hash git rev parse HEAD ecbcd1f50900f4edbc6ff5dd69ac287efaf35a9b Python version and distribution python3 anaconda Error Message MXNetError Traceback most recent call last ipython input 11 e2d0db8b0e35 in module 1 X reshape 3 1 home ubuntu mxnet python mxnet ndarray py in reshape self shape 487 len shape 488 c array ctypes c int shape 489 ctypes byref handle 490 return NDArray handle handle writable self writable 491 home ubuntu mxnet python mxnet base py in check call ret 76 77 if ret 0 78 raise MXNetError py str LIB MXGetLastError 79 80 if sys version info 0 3 MXNetError 00 35 24 include mxnet ndarray h 314 Check failed shape Size shape Size 24 vs 12884901885 NDArray Reshape target shape size is different from current shape Stack trace returned 10 entries bt 0 home ubuntu mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f1e42f6dacc bt 1 home ubuntu mxnet python mxnet lib libmxnet so MXNDArrayReshape 0xf13 0x7f1e43b21313 bt 2 home ubuntu anaconda3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ffi call unix64 0x4c 0x7f1e704e25a0 bt 3 home ubuntu anaconda3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ffi call 0x1f5 0x7f1e704e1d45 bt 4 home ubuntu anaconda3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ctypes callproc 0x3dc 0x7f1e704d988c bt 5 home ubuntu anaconda3 lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so 0x9df3 0x7f1e704d1df3 bt 6 home ubuntu anaconda3 bin lib libpython3 6m so 1 0 PyObject FastCallDict 0x9e 0x7f1e7718caae bt 7 home ubuntu anaconda3 bin lib libpython3 6m so 1 0 0x1482bb 0x7f1e772692bb bt 8 home ubuntu anaconda3 bin lib libpython3 6m so 1 0 PyEval EvalFrameDefault 0x26fd 0x7f1e7726c15d bt 9 home ubuntu anaconda3 bin lib libpython3 6m so 1 0 0x145e74 0x7f1e77266e74 Minimum reproducible example,,"JeanKossaifi,szha,JeanKossaifi,JeanKossaifi,JeanKossaifi",2017-03-25 00:52:08,2017-06-28 21:10:46
PR,lstm bucketing now shows more information if the training file is not present,fix for the 6856 Before python lstm bucketing py leads to the following error Traceback most recent call last File lstm bucketing py line 56 in invalid label invalid label File lstm bucketing py line 35 in tokenize text lines open fname readlines IOError Errno 2 No such file or directory ' data ptb train txt' After python lstm bucketing py Traceback most recent call last File lstm bucketing py line 59 in module invalid label invalid label File lstm bucketing py line 37 in tokenize text raise IOError Please use get ptb data sh to download requied file data ptb train txt IOError Please use get ptb data sh to download requied file data ptb train txt Manually tests with and without the data ptb train txt file,,b0noI,2017-06-28 21:06:03,2017-06-28 22:35:06
PR,add Adamax Nadam,1 fix Ftrl 2 add Adamax and Nadam,,"CNevd,sxjscience,piiswrong,CNevd,piiswrong,leezu,leezu,CNevd,leezu,piiswrong,CNevd,leezu,sxjscience",2017-06-22 15:43:34,2017-06-28 22:56:21
PR,R instruction to build GPU enabled pkg on Windows close 3902,sandeep krishnamurthy,,"thirdwing,sandeep-krishnamurthy,thirdwing,sandeep-krishnamurthy,thirdwing",2017-06-27 18:45:05,2017-06-28 23:00:38
IS,Instructions Building MXnet on Windows with CUDA 8 and R support,I put together a quick gist with instructions on how to get a working R package using CUDA 8 for all of you who have Pascal architecture devices Please let me know if there are any problems clarifications and I can update it,,"piiswrong,sandeep-krishnamurthy,thirdwing,sandeep-krishnamurthy,thirdwing",2016-11-20 00:12:38,2017-06-28 23:00:44
PR,R add R test into jenkins add rpkgtest into makefile,The R test will exceed the time limit on Travis CI if more tests are added Adding R task on the Jenkins server might be better Do we have R installed on the Jenkins servers,,"thirdwing,piiswrong,thirdwing,thirdwing,piiswrong,szha,thirdwing,thirdwing,piiswrong,thirdwing,thirdwing,mli,thirdwing,piiswrong",2017-06-17 18:42:16,2017-06-28 23:01:23
IS,R add R into the Jenkins test,add R into Jenkins R test for NDArray R test for iter R test for symbol R test for model,,"thirdwing,thirdwing",2017-06-02 16:48:05,2017-06-28 23:05:15
PR,R require R 3 3 0 close 6539,Another reason to require a more recent version of R is for the prebuilt pkg for OSX The R OSX will use a different version of toolchain,,thirdwing,2017-06-27 16:40:04,2017-06-28 23:07:34
PR,Updated installation instruction,Do not use sudo needlessly Use pip rather than python setup py,,"JeanKossaifi,piiswrong,JeanKossaifi",2017-06-28 19:22:49,2017-06-28 23:08:02
PR,R doc update for regression and classification,remind user that softmax output is zero indexed close 3086 linear regression with multiple outputs close 2138,,thirdwing,2017-06-26 23:49:48,2017-06-28 23:08:56
PR,R AWS S3 to host all pre built binary packages,Please do not merge I need opinions from R users I am considering using S3 to host all the pre built pkgs for Windows OSX users because they are too large to host on github The GPU enabled pkg for Windows is more than 180 MB For CPU only pkg,,"thirdwing,mli,thirdwing,szha,thirdwing,thirdwing",2017-06-16 18:27:22,2017-06-28 23:11:07
IS,R place to host the prebuilt R package for OSX and Windows,For several reasons we can not put the mxnet package on CRAN So we need to find a good way to host the prebuilt package for OSX Windows users We currently use github However just the binary package for Windows with GPU is more than 180 MB all the dlls are included it is a little too large to host on github,,"thirdwing,thirdwing,thirdwing,thirdwing,thirdwing",2017-06-02 17:14:01,2017-06-28 23:11:13
PR,Improved caffe converter,Extended caffe to mxnet converter and improved converter test Added support for networks which uses batch normalization without a scale layer following the batch norm i e gamma is fixed to 1 Extended naming convention used when implementing batch normalization in caffe Added support for old caffe versions where dilation did not exist This is needed to convert models which depends on old caffe Added support for deconvolution layer Added support for older version of caffe where kernel size pad and stride parameters were not iterable Fixed crash happening when a bottom layer does not exist in the internal top to layers dictionary this can happen if the name of the input is not wouldata' Added ignore by design support for converting 'Crop' layers Fixed batch norm layer comparison to take into account the rescaling factor Added careful condition in tester to swap RGB BGR input channels only if they are of size 3 or 4 which is the same check the conversion does Allow comparing layers of models with no mean file Added support for comparing the parameters of deconvolution layers,,"arikpoz,piiswrong,arikpoz,arikpoz,piiswrong,arikpoz,piiswrong,arikpoz,piiswrong,arikpoz,mli,arikpoz",2017-06-26 19:07:10,2017-06-28 23:11:52
IS,R require R 3 3 0,I want to get opinions from R users on this Do you think it is reasonable to require R 3 3 0 There are two reasons I want to set the requirement 1 currently we host the pre built R package on github Less packaging work if we just support the latest two versions of R 2 some utility functions we wrote by ourselves have been provide by the base package from R 3 3,,"thirdwing,thirdwing,hetong007,jeremiedb,miguelgfierro,thirdwing",2017-06-02 16:44:12,2017-06-29 00:27:02
PR,add detection to mx image,add ForceResizeAug fix parentheses rename more on ImageDetIter fix typo add batch size fix typo full argument list for ImageDetIter fix numpy nd fix bugs fix reshape ImageDetIter init random cropping logic fix random crop and pad fix bug extend makeBorder value option add draw function as member function fix lint,,"zhreshold,piiswrong,piiswrong,piiswrong,piiswrong",2017-06-27 19:24:36,2017-06-29 04:14:10
IS,cpp package SimpleBind every minibatch,In the example mlp gpu cpp L91 seems executor is got from SimpleBind every minibatch why and,,"CNevd,lx75249,CNevd",2017-06-29 08:07:26,2017-06-29 08:35:17
PR,Install mklml on a local dir in jenkins,,,mli,2017-06-29 17:08:06,2017-06-29 17:26:36
IS,help with setting output grads in R package mx exec backward,I have been puzzled about this I can not seem to set the output grads in this example Could someone please take a quick look at the code below and tell me why it does not matter what output grad is in mx exec backwards,,thirdwing,2016-09-07 19:21:23,2017-06-29 18:47:49
PR,update mklml path in jenkins docker,,,mli,2017-06-29 17:20:20,2017-06-29 19:14:45
IS,Error in mx io CSVIter,I am trying to run a VDCNN on text based on code here Everytime I try to run mx io CSVIter in CustomCSVIter or by itself I get a no file or directory error and the stacktrace below That folder definitely exists I have tried this on two different Macbooks running 0 9 4 and 0 10 0 Same result but it is not consistent One time I was able to get past this point in the CNN demo with amazon product review sample data Any ideas how to make it work consistently I have tried re installing mxnet but same result Code Example,,thirdwing,2017-06-28 13:23:38,2017-06-29 19:35:21
PR,Change idx type switch for aux data,Changed MSHADOW INT TYPE SWITCH to MSHADOW IDX TYPE SWITCH in MXNet Waiting for this PR to be merged to check in mshadow commit,,"reminisce,reminisce",2017-06-28 19:07:16,2017-06-29 20:42:18
PR,nn mnist tutorial,piiswrong Mnist tutorial follows our existing Mnist tutorial with MLP and CNN just using nn interface Some other fixes,,"Roshrini,piiswrong",2017-06-29 23:03:04,2017-06-29 23:22:19
IS,mx nd array VS np array,mxnet 0 10 x np ones 2 np ones 2 y mx nd array x Code above can pass but Code below failed x mx nd ones 2 md nd ones 2 y mx nd array x home jlxie anaconda2 envs mxnet 0 10 lib python2 7 site packages mxnet ndarray pyc in array source array ctx dtype 1110 source array np array source array dtype dtype 1111 except 1112 raise TypeError isource array must be array like object' 1113 arr empty source array shape ctx dtype 1114 arr source array TypeError source array must be array like object Is it a bug,,reminisce,2017-06-29 02:57:36,2017-06-30 01:26:17
PR,R fix concat for symbol and NDArray close 6650,This needs more careful tests I need to make sure I did not break anything,,"thirdwing,thirdwing",2017-06-14 19:58:18,2017-06-30 02:17:55
IS,R mx nd concat is expecting a MXNDArray instead of a list of MXNDArray,I think there might be a issue with the expected format of the data argument in mx nd concat An error is returned if a list of MXNDArray is passed as argument mentionning The equivalent code using mx symbol concat works fine,,"jeremiedb,jeremiedb,jeremiedb,thirdwing,thirdwing,jeremiedb",2017-06-10 23:50:44,2017-06-30 02:17:59
IS,Latest git pull built error on Mac Pro,Environment info Operating System Mac OS Compiler Configured with prefix Library Developer CommandLineTools usr with gxx include dir usr include c 4 2 1 Apple LLVM version 8 1 0 clang 802 0 42 Target x86 64 apple darwin16 5 0 Thread model posix Package used Python R Scala Julia Python MXNet version 0 10 Or if installed from source MXNet commit hash git rev parse HEAD 75aa6e0ef8e0b7e666c0b6f916f1b6f7c988bacc If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 80e650266392 mxnet zhijunz make g std c 11 c DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I Users zhijunz mxnet mshadow I Users zhijunz mxnet dmlc core include fPIC I Users zhijunz mxnet nnvm include I Users zhijunz mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include I Users zhijunz warp ctc include DMXNET USE NVRTC 0 MMD c src executor graph executor cc o build src executor graph executor o src executor graph executor cc 249 17 error too many arguments to function call expected at most 8 have 9 zero ops copy Users zhijunz mxnet nnvm include nnvm pass functions h 137 1 note 'Gradient' declared here inline Graph Gradient 1 error generated make build src executor graph executor o Error 1 What have you tried to solve it Remove the last parameter in src engine executor graph executor cc line 249 nnvm Graph g grad nnvm pass Gradient g symbol outputs xs head grad entry AggregateGradient need mirror nullptr zero ops copy zero ops,,szha,2017-06-29 08:42:29,2017-06-30 02:51:03
PR,cpp package add lr scheduler,1 add lr scheduler to optimizer 2 move to optimizer 3 fix examples,,CNevd,2017-06-30 03:31:32,2017-06-30 03:37:25
PR,cpp package add lr scheduler,1 add lr scheduler to optimizer 2 move learning rate and weight decay to optimizer 3 fix examples,,"CNevd,lx75249,CNevd",2017-06-30 03:38:55,2017-06-30 03:43:13
PR,fix warnings,fix warnings in mshadow op h by handling unsigned types,,szha,2017-06-29 23:39:40,2017-06-30 04:55:45
PR,Sparse dot enhancement,Enhancement motivated by the benchmark results of cast storage issuecomment 310571404 1 Enabled dot csr T dns to output an rsp format matrix This avoids the step of casting a dense matrix to a rowsparse one after dot csr T dns dns is performed We identified that casting operation is a bottleneck in the network 2 Used STORAGE TYPE ASSIGN CHECK in storage type inference for sparse dot 3 Added fallback function for sparse dot forward and backward 4 Fixed a unittest typo haibin lin,,"reminisce,eric-haibin-lin,reminisce,reminisce,eric-haibin-lin,reminisce",2017-06-27 19:10:24,2017-06-30 05:05:18
PR,fix pinned mem USE CUDA 1 on host with no device,This change uses PinnedMemoryStorage only when there actually is GPU Previously if the libmxnet so is built with USE CUDA 1 even if the context does not involve GPU it will still throw a CUDA error for counting GPUs,,"szha,mli,szha",2017-06-29 01:16:33,2017-06-30 05:06:14
PR,create inception v4,create inception v4,,"wac81,mli,chunyang-wen",2017-06-28 04:13:06,2017-06-30 05:11:33
PR,Update deconvolution inl h Change the condition control statement in InferPad Merge differences between v0 8 and v0 9,There seems to be a bug at the condition control statement in InferPad Original version v0 9 only use target shape ndim to decide the calculation of pad But we find even the target shape ndim is not zero values of target shape can be zero When we load v0 8 model with v0 9 this may cause error calculation of pad,,"piiswrong,piiswrong,mli,piiswrong",2017-04-28 02:58:10,2017-06-30 05:12:11
PR,kvstore profile push and pull with the profiling mechanism of engine,This commit adds profiling to push and pull of local kvstore It reuses the profilng infrastructure of engine The purpose of this profiling is understanding how workers are consuming time in the communication If they are spending too much time in the paths optimizing it with RDMA based transport would be hopeful I'm starting it with local kvs because I'm very new to mxnet so reusing the proriler of the engine is correct or not If someone can review this strategy I'm very glad,,"mitake,mitake,mitake",2017-06-29 07:47:33,2017-06-30 08:03:32
IS,advise for implement a C layer,hi I'm writing a NCE loss in C the forward is easy but backward is some hard to me the formula as below ograd i j k y i j k y i j k k pn i j k ograd 0 1 0 I'm writing with softmax output as template using the old style previous 0 9 0 as here operators in mxnet because the tutorial c told little about how to make a loss layer I'm a newbie to mxnet and cuda cudnn and only know 2 possible solution 1 using cuda and openmp to write for loop 1 using mshadow just guess are there any better solution where can I find document about how to do this more quickly or which can I take as example thanks for any help,,piiswrong,2017-06-29 14:53:32,2017-06-30 08:31:31
IS,Add meaningful information in the failure in RNN tutorial when the file with the training data is not present,error example python lstm bucketing py leads to the following error Traceback most recent call last File lstm bucketing py line 56 in module invalid label invalid label File lstm bucketing py line 35 in tokenize text lines open fname readlines IOError Errno 2 No such file or directory ' data ptb train txt' It would be nice to have here the information that need to be used in order to download the file,,"b0noI,kevinthesun",2017-06-28 16:15:15,2017-06-30 23:38:31
PR,Fix 404 links on rnn page,Fix for the 6857,,b0noI,2017-07-01 00:26:03,2017-07-01 03:48:21
PR,Link to install XCode from the App Store has been added,,,"b0noI,piiswrong",2017-07-01 00:31:40,2017-07-01 03:49:40
PR,Add inplace identity,tqchen,,"piiswrong,asmushetzel",2017-06-30 18:11:09,2017-07-01 03:50:11
PR,Change mxnet io home page and api subtitle color,newhomepage1,,kevinthesun,2017-06-30 22:40:37,2017-07-01 04:04:23
PR,add allow extra parameter to module,piiswrong After update base module with allow extra parameter 6753 rcnn module has to be updated,,"fullfanta,piiswrong",2017-07-01 03:20:08,2017-07-01 04:30:35
PR,added docs for output of ctc loss layer,In response to 6871,,sbodenstein,2017-07-01 15:29:20,2017-07-01 18:57:48
IS,Revert on old mxnet version v0 9 5,Since the change of the cub module from NVlab to dmlc I was not able to go on previous version due to issues with commit hash Did you know how I can go back to v0 9 5,,piiswrong,2017-06-30 12:24:50,2017-07-02 09:54:02
IS,I want to use opencv in a C layer,Hi I sort of want to use opencv in my custom c layer To be more specific I want to use resize function on some matrices But I am curious how to adapt the Tensor to the cv mat kind so that I can use resize function Any suggestions Thanks,,piiswrong,2017-06-27 14:04:56,2017-07-02 12:38:30
IS,How to debug a C layer,Hi I am new to Mxnet I am interested in implementing a new C layer but I am wondering how to debug during the process Do I have to recompile the whole framework to test it Or is there any other simpler way It seems like recompiling takes way too much time I did not find any useful info in the tutorial Hope to get some hints Thanks,,,2017-06-23 10:48:55,2017-07-02 12:39:34
IS,I'm sorry that i cant find the tools,where the tools in your project,,,2017-07-03 05:31:36,2017-07-03 05:32:02
PR,R fix the operations between MXSymbol and scalar close 4994,,,"thirdwing,jeremiedb,thirdwing",2017-06-20 00:29:57,2017-07-03 15:33:25
IS,R mxnet binary operations only work in one direction,From For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error,,thirdwing,2017-02-12 19:56:48,2017-07-03 15:33:37
PR,Fix a bug in deformable convolution operator,,,"Oh233,piiswrong,YuwenXiong",2017-07-03 04:34:39,2017-07-03 17:45:49
PR,Fix Python 3 compatibilities,,,kkk669,2017-07-03 01:29:17,2017-07-03 17:46:40
IS,R Logistic Regression in mxnetR does not learn the parameters,I'm doing a logistc regression for a biomedical data to predict an adverse event I want to adjust a simple logistic regression model with mxnetR but the model does not learn anything in my data I supos that my architecture is wrong I put here an simulated example and I fitted a simple glm model with the base R library and it works Do you know whats the problem,,thirdwing,2017-06-02 12:00:50,2017-07-03 18:23:49
PR,fix label bug which harms voc accuracy,the problem is stated at 6809 and 6848,,"shenh10,piiswrong,shenh10,shenh10,shenh10,Zehaos,shenh10,precedenceguo,shenh10",2017-06-28 05:58:47,2017-07-03 18:37:28
PR,Implement dot csr rsp dns and dot csr T rsp rsp and refactor,1 Move all the functions related to dot and batch dot operators from matrix op inl h matrix op cc and matrix op cu to matrix dot inl h matrix dot inl cuh matrix dot cc and matrix dot cu 2 dot csr rsp dns and dot csr T rsp rsp are needed in distributed training for reducing host device I O workload Hardware p2 xlarge Device cpu 4 omp threads Commit 8f5bb98 Compile flags USE CUDA 0 USE OPENMP 1 USE BLAS openblas haibin lin Table 1 Benchmark results of dot csr rsp dns vs dot csr dns dns The right hand side matrix rsp and dns in both formulas are same in their dense format lhs shape rhs shape nnr of rhs rsp storage shape 0 rsp shape 0 density of lhs density of rhs dot csr rsp dns ms dot csr dns dns ms speedup 512 100 000 100 000 100 1 029 1 029 0 01 0 01 3 738 17 006 4 550 512 100 000 100 000 100 4 982 4 982 0 01 0 05 7 8249 17 7145 2 264 512 100 000 100 000 100 9 983 9 983 0 01 0 1 6 992 17 658 2 525 512 100 000 100 000 100 20 009 20 009 0 01 0 2 10 635 20 243 1 904 512 100 000 100 000 100 50 391 50 391 0 01 0 5 22 352 19 391 0 868 Table 2 Benchmark results of dot csr T rsp rsp vs dot csr T dns rsp The right hand side matrix rsp and dns in both formulas are same in their dense format lhs shape rhs shape nnr of rhs rhs storage shape 0 rhs shape 0 density of lhs density of rhs dot csr T rsp rsp ms dot csr T dns rsp ms speedup 512 100 000 512 100 4 0 781 0 01 0 01 10 556 29 346 2 780 512 100 000 512 100 31 6 055 0 01 0 05 12 160 33 245 2 734 512 100 000 512 100 43 8 398 0 01 0 1 12 418 30 102 2 424 512 100 000 512 100 101 19 727 0 01 0 2 19 770 30 472 1 541 512 100 000 512 100 275 53 711 0 01 0 5 25 369 31 135 1 227 Benchmark script,,"reminisce,piiswrong,reminisce,eric-haibin-lin",2017-07-01 02:48:05,2017-07-04 04:10:34
PR,LibsvmIter fix,fix a bug in libsvm iter add more tests,,eric-haibin-lin,2017-06-30 22:02:05,2017-07-04 04:13:09
IS,why MKL fullyconnected operator not available in 0 10,Environment info Operating System Centos7 3 Compiler g 4 8 5 Package used Python R Scala Julia python MXNet version 0 10 0 Or if installed from source source If you are using python package please provide Python version and distribution 2 7 5 I use mxnet with MKL in KNL machine But I found mkl fullyconnected opeator did not used in version 0 10,,szha,2017-06-26 12:25:58,2017-07-04 09:17:34
PR,R install scripts update To use the latest R on ubuntu,,,thirdwing,2017-06-30 20:33:57,2017-07-04 17:55:36
PR,convert to long int for direct comparison,for direct comparison,,"liangfu,piiswrong,liangfu,piiswrong,liangfu",2017-06-29 08:52:51,2017-07-04 19:20:10
PR,Fix Python 3 compatibilities,In Python 3 values method returns a dict values instead of a list,,kkk669,2017-07-05 01:36:47,2017-07-05 01:38:41
PR,example add bucketing and batchnorm scheme for speech recognition example,add bucketing and batch norm scheme for speech recognition example to improve training performance add flexibility to support various optimizer and it solves performance issue speed related to 6418 and may related to issues 3076 6115 2663 when using both bucketing variable length and batch norm although it is not a solution for time step wise but layer wise approach for batch normalization,,Soonhwan-Kwon,2017-07-04 02:12:52,2017-07-05 02:25:41
PR,R add fixed param close 3906,,,thirdwing,2017-07-03 23:54:09,2017-07-05 02:35:52
IS,use constant weight matrix,I think this must be trivial but I can not work to how to do it I would like to have one weight matrix manually entered and not trained I am using R I have tried data mx symbol Convolution data data weight 1 4 kernel c 1 1 stride c 1 1 pad c 0 0 no bias TRUE num filter 1L and extWeight mx symbol Blockgrad mx symbol Variable 'extWeight' data mx symbol Convolution data data weight extWeight kernel c 1 1 stride c 1 1 pad c 0 0 no bias TRUE num filter 1L with arg params list extWeight 1 4 in the call to mx model Feedforward create I do not think any of this is working Can someone help me out,,"piiswrong,piiswrong,thirdwing",2016-11-20 19:18:02,2017-07-05 02:35:57
PR,Add ConvRNN Cell ConvLSTM Cell,Reference Xingjian et al Convolutional LSTM network A machine learning approach for precipitation nowcasting,,"dsqx71,sxjscience,sxjscience,sxjscience,sxjscience,dsqx71,sxjscience,sxjscience,dsqx71,sxjscience,sxjscience,dsqx71,sxjscience,piiswrong,sxjscience,sxjscience,szha,dsqx71,szha,dsqx71",2017-06-27 06:31:45,2017-07-05 02:40:38
PR,Perl Sync with upstream,reworked cachedop kvstore is indexed via strings not ints from now on added two more optimizers and reworked sgd optimizer auto reshape for module forward miscellaneous changes,,"sergeykolychev,sergeykolychev",2017-07-03 03:46:07,2017-07-05 02:41:12
IS,Stop metric made increasing memory usage,Environment info Ubuntu 16 04 GCC 5 4 0 MXNet 0 9 5 Python 2 7 Error Message I would like to train a model without a metric so I use the code just as follows But I find that the memory usage keeps increasing until it is full Luckily the memory usage does not increase anymore But I want to know why I must call asnumpy Latest I find that whether I use train data mx io PrefetchingIter train data or not the memory keeps increasing Similarly if I use multi process to read data in a queue for acceleration this problem will also appear The latest version of MXNet does not fix my bug,,"Zehaos,CNevd,CNevd",2017-06-30 13:03:29,2017-07-05 03:14:48
PR,Fix smooth l1 comment,,,Zehaos,2017-07-05 05:58:54,2017-07-05 16:11:55
IS,CTC build failed for speech recognition example,Environment info Operating System Ubuntu 14 04 Compiler Package used Python R Scala Julia python MXNet version 0 10 0 Or if installed from source git clone mxnet recursive Error Message There are 3 Errors with different build schedules git clone with MxNet modify the config mk with USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 three different modifications as follows and then run the example of speech recognition 1 without MXNET PLUGINS plugin warpctc warpctc mk net mx sym WarpCTC data net label label label length num label input length seq len AttributeError 'module' object has no attribute 'WarpCTC' 2 with MXNET PLUGINS plugin warpctc warpctc mk terminate called after throwing an instance of istd runtime error' what Error compute ctc loss stat cuda memcpy or memset failed 3 with MXNET PLUGINS plugin warpctc warpctc mk and delete all the new ctc loss files the folder ctc include ctc loss cc ctc loss cu and ctc loss inl h as mentioned in the previous issue terminate called after throwing an instance of istd runtime error' what Error compute ctc loss stat execution failed Aborted core dumped Minimum reproducible example As mentioned above modify the config mk in different ways then run the example Steps to reproduce after make the mxnet and python installed run the example of speech recognition with the deepspeech cfg python main py configfile deepspeech cfg From the three different builds and errors it shows that the mxnet need the wrap ctc plugin and the problem is still the ctc of mxnet in the src operator contrib folder Would you please help me to dig into this issue How can I fix it Many thanks Xin q,,"piiswrong,sbodenstein,alues,piiswrong,sbodenstein,alues,piiswrong,sbodenstein,piiswrong,piiswrong,piiswrong",2017-06-07 05:59:47,2017-07-05 16:13:44
PR,Fix for WarpCTC conflict,This seems enough to fix,,"sbodenstein,sbodenstein,piiswrong,alues,sbodenstein,sbodenstein,alues",2017-07-01 15:11:56,2017-07-05 16:13:44
PR,pulled update to mshadow,Pulling in the latest changes to mshadow mshadow base h so that new MXNet dot kernels on GPU which require those changes can be added haibin lin,,"stefanhenneking,piiswrong,reminisce,stefanhenneking",2017-07-05 17:31:46,2017-07-05 17:39:39
PR,Fix rcnn sample rois potential error when filling neg gap,If there not enough neg samples to fill the gap the mini batch size will change,,Zehaos,2017-07-06 03:26:16,2017-07-06 03:34:12
IS,sample rois function bug which harms accuracy of Faster RCNN,Previous issue 6809 It has been proved to be a bug which harms the accuracy of VOC detection results Before fixing the bug with ResNet VOC2007 2012 as training data VOC2007test as test data the mAP can only reach 0 726 By change labels fg rois per this image 0 to labels overlaps keep indexes config TRAIN FG THRESH 0 the mAP reach 0 795 which is even a little higher than reported result Cheers Han,,"shenh10,Zehaos,shenh10,Zehaos,shenh10,shenh10,Zehaos,shenh10",2017-06-28 05:19:15,2017-07-06 09:06:06
IS,retrive saved error during training in Python,It is related to 1274 so I want to plot the training and validation metrics and I have used mx callback log train metric 5 which works I am just wondering to which variable is it saved and how do I load it after training Or could I save it directly to some file during training Thanks a lot,,,2017-06-28 08:40:34,2017-07-06 14:35:45
PR,Fix Mistake,,,alues,2017-07-06 07:34:34,2017-07-06 17:16:48
PR,Page title for installing from source has been udpated,Fix for the 6877,,b0noI,2017-07-06 17:25:09,2017-07-06 17:26:55
PR,Add prerequisites for linear regression and MNIST tutorials,,,"indhub,nswamy,piiswrong,nswamy,nswamy,indhub,indhub,indhub,piiswrong,piiswrong,indhub,indhub,indhub",2017-06-08 21:12:11,2017-07-06 17:31:49
PR,Failing with a more descriptive error message in infer shape,Given this example,,"tdomhan,tdomhan",2017-06-30 14:39:13,2017-07-06 17:34:13
PR,Fix Python 3 compatibilities,In Python 3 values method returns a dict values instead of a list,,kkk669,2017-07-05 01:45:18,2017-07-06 17:37:39
IS,Same prediction values for linear regression while using mxnet in R,I am new to mxnet and I'm trying to solve a linear regression My training data contains 1460 observations and 303 variables I am using R v3 4 0 Since mxnet is not updated for R v3 4 0 I have installed mxnet using install packages repos NULL as mentioned by jeremiedb in 5948 I created following frame for the neural network but the predictions I get have a constant value I tried different values for batch size num rounds but I still get constant values When I change the activation type to Relu or softrelu I get Nan while training Even using different optimizers is not helping Can anyone please point out where I am going wrong Thanks in advance data mx symbol Variable data fc1 mx symbol FullyConnected data name fc1 num hidden 300 act1 mx symbol Activation fc1 name sigm1 act type sigmoid fc2 mx symbol FullyConnected act1 name fc2 num hidden 300 act2 mx symbol Activation fc2 name sigm2 act type sigmoid fc3 mx symbol FullyConnected act2 name fc3 num hidden 300 act3 mx symbol Activation fc3 name sigm3 act type sigmoid fc4 mx symbol FullyConnected act3 name fc4 num hidden 1 linear reg output mx symbol LinearRegressionOutput fc4 mx set seed 0 model mx model FeedForward create symbol linear reg output X train x y train y ctx mx cpu num round 200 array batch size 50 learning rate 1e 6 eval data list data valid x label valid y momentum 0 05 optimizer adam optimizer rmsprop eval metric mx metric rmse preds predict model valid x sqrt mean preds valid y 2 head preds 119142 6 119142 6 119142 6 119142 6 119142 6 119142 6 119142 6 119142 6 119142 6 119142 6,,"Soonhwan-Kwon,Soonhwan-Kwon,Soonhwan-Kwon,thirdwing,thirdwing",2017-04-25 23:38:29,2017-07-06 19:36:47
IS,Use R how to manually confficients predict y make the result equal to the function predict,The question This is not just a question of R but I use R I hope to know how predict MXFeedForwardModel work I set three layers linear FeedForward neural networks it like this data mx symbol Variable data fc1 mx symbol FullyConnected data num hidden num hidden first name fc1 act1 mx symbol Activation fc1 act type relu name relu1 fc2 mx symbol FullyConnected act1 num hidden 28 name fc2 act2 mx symbol Activation fc2 act type relu name relu2 fc3 mx symbol FullyConnected act2 num hidden 1 name fc3 mlp mx symbol LinearRegressionOutput fc3 name mlp It outputs three sets variable coefficients Each set contains weight and bias dim model arg params fc1 weight 39 64 length model arg params fc1 bias 64 dim model arg params fc2 weight 39 28 length model arg params fc2 bias 28 dim model arg params fc3 weight 39 1 dim model arg params fc3 bias 1 I try to manually substitution confficients to predict the y make the result equal to the function predict model data Try the way like this mx predict function model data W1 as array model arg params fc1 weight b1 as array model arg params fc1 bias W2 as array model arg params fc2 weight b2 as array model arg params fc2 bias W3 as array model arg params fc3 weight b3 as array model arg params fc3 bias data as matrix data pred data W1 for i in length b1 pred i pred i b1 i pred pred W2 for i in length b2 pred i pred i b2 i pred pred W3 for i in length b3 pred i pred i b3 i return pred But the result is far greater than the predict model data I know that wrong But please tell me which way to use the coefficient to get the correct results I try to view the predict MXFeedForwardModel code but found that its core part seems to involve c I am not familiar with c Who can help me understand how predict MXFeedForwardModel works and help me get the right result with the R function Thanks,,"jeremiedb,thirdwing",2017-07-05 15:31:35,2017-07-06 20:58:52
IS,How to read rec files in R,I use im2rec py in mxnet to generate rec files for training and validation I am wondering how to access the content of the rec files such as the indices labels or images in R Say val is the variable name for the rec file I generated and loaded in R I used the command val xData trying to see if I can get the indices or labels my R crashes,,"thirdwing,thirdwing,thirdwing,thirdwing",2017-07-06 20:50:00,2017-07-06 21:14:00
IS,R package core dumped,After training some simple net on MNIST I get this when I quit R when the script finishes Quick google shows what the issue might be The code I was running is here and the output here I was running on EC2 P2 Tesla K80 software versions here,,"thirdwing,thirdwing,thirdwing,thirdwing",2016-11-01 03:30:23,2017-07-07 18:55:52
IS,Fail Install Mxnet Windows 10,Fail of install I use this guide Fail on command R CMD INSTALL no multiarch R package C GPU mxnet R CMD INSTALL no multiarch R package installing to library 'C Users Statislove Documents R win library 3 4' installing source package 'mxnet' libs make Nothing to be done for all' installing to C Users Statislove Documents R win library 3 4 mxnet libs x64 R demo inst preparing package for lazy loading help No man pages found in package 'mxnet' installing help indices building package indices installing vignettes testing if installed package can be loaded Error package or namespace load failed for 'mxnet' onLoad failed in loadNamespace for 'mxnet' details call inDL x as logical local as logical now error 'C Users Statislove Documents R win library 3 4 mxnet libs x64 libmxnet dll' LoadLibrary failure ERROR loading failed removing 'C Users Statislove Documents R win library 3 4 mxnet' What is mistake,,"thirdwing,thirdwing",2017-07-05 10:18:50,2017-07-07 18:58:40
PR,Fix for pinned memory never being used,,,"ptrendx,piiswrong,szha",2017-07-07 18:05:37,2017-07-07 20:04:33
PR,rename load data to split and load,,,piiswrong,2017-07-06 00:11:16,2017-07-07 21:01:06
IS,mxnetR 0 7 bug,mx symbol Convolution instruction from the R documentation indicated that weight and bias parameters can be added mx symbol Convolution mxnet R Documentation Apply convolution to input then add a bias Description Apply convolution to input then add a bias Usage mx symbol Convolution Arguments data Symbol Input data to the ConvolutionOp weight Symbol Weight matrix bias Symbol Bias parameter kernel Shape tuple required convolution kernel size y x stride Shape tuple optional default 1 1 convolution stride y x dilate Shape tuple optional default 1 1 convolution dilate y x pad Shape tuple optional default 0 0 pad for convolution y x num filter int non negative required convolution filter channel number num group int non negative optional default 1 Number of groups partition This option is not supported by CuDNN you can use SliceChannel to num group apply convolution and concat instead to achieve the same need workspace long non negative optional default 512 Tmp workspace for convolution MB no bias boolean optional default False Whether to disable bias parameter name string optional Name of the resulting symbol Value out The result mx symbol but when add weight and bias in this command the error pop out conv1 mx symbol Convolution data input data kernel c 5 5 pad c 2 2 weight 0 001 num filter 32 name conv1 Error Cannot find argument 'weight' Possible Arguments kernel Shape tuple required convolution kernel size y x stride Shape tuple optional default 1 1 convolution stride y x dilate Shape tuple optional default 1 1 convolution dilate y x pad Shape tuple optional default 0 0 pad for convolution y x num filter int non negative required convolution filter channel number num group int non negative optional default 1 Number of groups partition This option is not supported by CuDNN you can use SliceChannel to num group apply convolution and concat instead to achieve the same need workspace long non negative optional default 512 Tmp workspace for convolution MB no bias boolean optional default False Whether to disable bias parameter,,"thirdwing,thirdwing",2016-10-05 02:45:12,2017-07-07 21:12:32
IS,CPP inference example is broken,I tried to build and run and I get the following error pretrained models inception bn Inception BN symbol json 116922 bytes pretrained models inception bn Inception BN 0126 params 45284780 bytes 21 26 38 include dmlc logging h 300 21 26 38 src core op cc 55 Check failed op nullptr Operator Convolution is not registered Stack trace returned 14 entries bt 0 home ubuntu mxnet lib libmxnet so ZN4nnvm2Op3GetERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x38a 0x7f83791543da bt 1 home ubuntu mxnet lib libmxnet so 0x19d166c 0x7f837912b66c bt 2 home ubuntu mxnet lib libmxnet so 0x19d1b03 0x7f837912bb03 bt 3 home ubuntu mxnet lib libmxnet so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x12a 0x7f8379133e3a bt 4 home ubuntu mxnet lib libmxnet so 0x19cfeb4 0x7f8379129eb4 bt 5 home ubuntu mxnet lib libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataOS1 0x111 0x7f83784e0ca1 bt 6 home ubuntu mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaIS7 EE 0x342 0x7f837913e932 bt 7 home ubuntu mxnet lib libmxnet so ZN4nnvm4pass8LoadJSONERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x986 0x7f83787f9bc6 bt 8 home ubuntu mxnet lib libmxnet so MXPredCreatePartialOut 0x118 0x7f83787efce8 bt 9 home ubuntu mxnet lib libmxnet so MXPredCreate 0x1d 0x7f83787f4e2d bt 10 src Inference ZN14MXNetPredictor7predictERKN2cv3MatESt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaISA EERS4 IS4 IfSaIfEESaISE EE 0x214 0x40d26c bt 11 src Inference main 0x26d 0x410707 bt 12 lib x86 64 linux gnu libc so 6 libc start main 0xf0 0x7f837631b830 bt 13 src Inference start 0x29 0x40cc99 Segmentation fault core dumped This used to work before the nnvm merge Environment info Operating System Ubuntu Compiler gcc MXNet commit hash git rev parse HEAD 1ae2905cc3c2e35541d45edab18ddffb9cf455da Error Message pretrained models inception bn Inception BN symbol json 116922 bytes pretrained models inception bn Inception BN 0126 params 45284780 bytes 21 26 38 include dmlc logging h 300 21 26 38 src core op cc 55 Check failed op nullptr Operator Convolution is not registered Stack trace returned 14 entries bt 0 home ubuntu mxnet lib libmxnet so ZN4nnvm2Op3GetERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x38a 0x7f83791543da bt 1 home ubuntu mxnet lib libmxnet so 0x19d166c 0x7f837912b66c bt 2 home ubuntu mxnet lib libmxnet so 0x19d1b03 0x7f837912bb03 bt 3 home ubuntu mxnet lib libmxnet so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x12a 0x7f8379133e3a bt 4 home ubuntu mxnet lib libmxnet so 0x19cfeb4 0x7f8379129eb4 bt 5 home ubuntu mxnet lib libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataOS1 0x111 0x7f83784e0ca1 bt 6 home ubuntu mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaIS7 EE 0x342 0x7f837913e932 bt 7 home ubuntu mxnet lib libmxnet so ZN4nnvm4pass8LoadJSONERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x986 0x7f83787f9bc6 bt 8 home ubuntu mxnet lib libmxnet so MXPredCreatePartialOut 0x118 0x7f83787efce8 bt 9 home ubuntu mxnet lib libmxnet so MXPredCreate 0x1d 0x7f83787f4e2d bt 10 src Inference ZN14MXNetPredictor7predictERKN2cv3MatESt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaISA EERS4 IS4 IfSaIfEESaISE EE 0x214 0x40d26c bt 11 src Inference main 0x26d 0x410707 bt 12 lib x86 64 linux gnu libc so 6 libc start main 0xf0 0x7f837631b830 bt 13 src Inference start 0x29 0x40cc99 Segmentation fault core dumped Minimum reproducible example,,"indhub,piiswrong",2017-01-05 21:49:43,2017-07-08 00:14:35
IS,CSVIter segfault,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Amzn Ubuntu DL AMI Package used Python R Scala Julia MXNet commit hash git rev parse HEAD cde53611167c54ad7ce726a6b775faffcbdb0258 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error I was following the steps in CSVIter documentation but it did not work for me Context of csv file What have you tried to solve it 1 2 3,,"eric-haibin-lin,eric-haibin-lin",2017-05-23 01:18:36,2017-07-08 00:39:37
IS,DOC KVStore,Currently kvstore api doc only shows one of the many options to create kvstore mxnet kvstore create Actually there are many more and detailed description at this page multiple devices on a single computer Would be great if these options appear in kvstore doc or at least have a link for reference,,"eric-haibin-lin,eric-haibin-lin",2017-04-23 18:14:56,2017-07-08 00:40:22
PR,R operators between ndarray close 6768,,,thirdwing,2017-06-30 05:30:49,2017-07-08 03:24:04
IS,R negate is not implemented for NDArray,,,"thirdwing,thirdwing,thirdwing,thirdwing",2017-06-21 00:37:57,2017-07-08 03:24:09
PR,Fix broken link in example svm mnist README md,Fix broken link in example svm mnist README md,,yanboliang,2017-07-10 04:53:54,2017-07-10 06:05:40
PR,ctc example,,,"yajiedesign,piiswrong,piiswrong,yajiedesign",2017-07-07 09:22:20,2017-07-10 06:16:05
PR,rename nn Layer to foo Layer,,,piiswrong,2017-07-08 00:22:41,2017-07-10 16:20:03
PR,rename Foo to Gluon,,,piiswrong,2017-07-10 17:53:45,2017-07-10 17:55:46
PR,Mention mxnet version in tutorial prereq,,,madjam,2017-07-10 18:48:04,2017-07-10 18:50:48
PR,Fix linear regression tutorial,Print training logs in notebook Switch from accuracy to mse for validation metrics Accuracy does not make much sense for regression Lower the number of epochs to 50 Number of epochs in unnecessarily huge 1000 consuming too much time Training converges in around 30 epochs,,indhub,2017-07-10 08:25:16,2017-07-10 19:26:08
PR,Change slave labels and add nightly Jenkinsfile,In this PR I Change the slave labels in the main Jenkinsfile to reflect the labels we give to our Apache slaves mxnetlinux and mxnetjenkins Our own infrastructure is slave also have the same labels now Ex The fork of MXNet builds on this exact Jenkinsfile Add a dummy Jenkinsfile for nightly builds It will be populated with more builds and tests later,,"lxn2,mli",2017-07-07 22:25:32,2017-07-10 19:36:17
PR,Fix im2col h,,,ptrendx,2017-07-10 20:45:58,2017-07-10 20:49:09
PR,backend changes from foo,,,piiswrong,2017-07-10 18:19:44,2017-07-11 00:34:44
IS,Process get killed with out raising any error,The model is loaded successfully but the process get killed when running performance test The machine is not responding to any command before it get killed The model is similar to Inception resnet v2 is it caused by out of memory issue 32GB memory,,piiswrong,2017-07-10 07:49:47,2017-07-11 01:47:18
IS,NDArray asnumpy very slow,I want to get network output in numpy array which can be achieved with the NDArray asnumpy method But I found this very slow since it copies all the elements The computation takes only a few ms while the NDArray asnumpy call takes 100 ms How can I obtain the numpy array more efficiently I also tried to directly manipulate the mxnet NDArray I used slice or slice axis since multi dimension indexing is not supported But both methods only support slicing a contiguous region This makes a big trouble for me BTW I run my program on CPU and the arrays are of size 2000x6,,"II-Matto,piiswrong,II-Matto",2017-07-10 07:18:45,2017-07-11 01:56:21
PR,support negative axes values in ReduceAxes ops,enable support for negative axes values,,"szha,piiswrong,piiswrong,piiswrong,szha,piiswrong",2017-07-07 04:20:18,2017-07-11 05:03:50
PR,R custom iter in model training,I am working to let people use a customized iter in model training and it provides more flexibility in inferencing the shape see 4462 A simple MF demo with a customized iter is also provided Can you give me some opinions on this,,"thirdwing,jeremiedb,thirdwing",2017-06-12 22:59:56,2017-07-11 17:19:23
IS,Matrix Factorization example in R,Hi Is it possible to reproduce Matrix Factorization python example in R I am having issues as it keeps asking for data input layer but I would need multiple input layers for Matrix Factorization,,"thirdwing,thirdwing",2017-02-08 07:35:23,2017-07-11 17:19:36
IS,R infers shape with rigid argument,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Referencing to StackOverflow question How bind to names in MXNET mx io internal arrayiter output Environment info Operating System ubuntu xenial 16 04 amd64 Compiler gcc version 5 4 0 20160609 Package used Python R Scala Julia R MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 0a0341793f0a5fce887e48a31ec5079f6d2c539c If you are using R package please provide R sessionInfo R version 3 2 3 2015 12 10 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 16 04 1 LTS locale 1 LC CTYPE en US UTF 8 LC NUMERIC C 3 LC TIME en US UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 7 LC PAPER en US UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C 11 LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 mxnet 0 7 loaded via a namespace and not attached 1 Rcpp 0 12 8 codetools 0 2 14 visNetwork 1 0 3 digest 0 6 10 5 plyr 1 8 4 jsonlite 1 1 magrittr 1 5 scales 0 4 1 9 stringi 1 1 2 data table 1 10 0 rstudioapi 0 6 DiagrammeR 0 8 4 13 tools 3 2 3 stringr 1 1 0 htmlwidgets 0 8 munsell 0 4 3 17 igraph 1 0 1 influenceR 0 1 0 colorspace 1 3 2 htmltools 0 3 5 Error Message Please paste the full error message including stack trace 19 28 45 root mxnet dmlc core include dmlc logging h 235 19 28 45 src symbol symbol cc 155 Symbol InferShapeKeyword argument name data not found Candidate arguments 0 user 1 user1 weight 2 item 3 item1 weight 4 label Error in symbol infer shape list basic string resize traceback 6 stop list message basic string resize call symbol infer shape list cppstack NULL 5 External list name CppMethod invoke notvoid address pointer 0x11caef0 dll list name Rcpp path usr local lib R site library Rcpp libs Rcpp so dynamicLookup TRUE handle pointer 0x1404520 info pointer 0x7f348a598d80 numParameters 1L pointer 0x3835820 pointer 0x3836540 pointer 4 symbol infer shape list 3 mx symbol infer shape symbol data input shape 2 mx model init params symbol input shape initializer mx cpu 1 mx model FeedForward create pred3 X mx io arrayiter data DF mat x label t data matrix data frame score DF y ctx devices num round 10 array batch size 10 verbose T initializer mx init uniform 0 07 learning rate 0 07 eval metric mx metric rmse momentum 0 9 epoch end callback mx callback log train metric 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error library mxnet DF read delim data ml 100k u data header F names DF c user item score time max user max DF user max item max DF item DF mat x data matrix t sapply DF 1 2 as numeric DF y DF 3 k 64 user mx symbol Variable user item mx symbol Variable item score mx symbol Variable label user1 mx symbol Embedding data user input dim max user output dim k name user1 item1 mx symbol Embedding data item input dim max item output dim k name item1 pred user1 item1 pred1 mx symbol sum axis pred axis 1 name pred1 pred2 mx symbol Flatten pred1 name pred2 pred3 mx symbol LinearRegressionOutput data pred2 label score name pred3 devices mx cpu mx set seed 123 mx model FeedForward create pred3 X mx io arrayiter data DF mat x label t data matrix data frame score DF y ctx devices num round 10 array batch size 10 verbose T array layout rowmajor initializer mx init uniform 0 07 learning rate 0 07 eval metric mx metric rmse momentum 0 9 epoch end callback mx callback log train metric 1 What have you tried to solve it 1 Changing user mx symbol Variable user to user mx symbol Variable data which returns error Error in mx model init params symbol input shape initializer mx cpu Not enough information to get shape The argument data input shape seems to be the culprit here It should be more like Python is DataIter which allows dynamic arguments like user item and label in this case rather than just data,,"lxn2,piiswrong,thirdwing,thirdwing",2016-12-30 20:11:36,2017-07-11 17:20:08
IS,Error in mx io internal arrayiter,I rewrote an example available here link to data in document Environment info Operating System Ubuntu 14 04 Package used R MXNet commit hash git rev parse HEAD mxnet git rev parse HEAD 9d066eebfb4f42f75e18ea8925c28d5526153576 R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 14 04 5 LTS locale 1 C attached base packages 1 stats graphics grDevices utils datasets methods base,,"thirdwing,thirdwing",2016-12-26 22:19:32,2017-07-11 17:20:38
PR,example add bucketing batchnorm and improved performance for speech recognition example,add bucketing and batch norm scheme for speech recognition example to improve training performance add flexibility to support various optimizer fix bug in generating bi graphemes dictionray add option not to save csv file to improve performane and reduce disk space requirement and it solves performance issue speed related to 6418 and may related to issues 3076 6115 2663 when using both bucketing variable length and batch norm although it is not a solution for time step wise but layer wise approach for batch normalization,,"Soonhwan-Kwon,Soonhwan-Kwon,piiswrong",2017-07-10 04:20:29,2017-07-11 17:56:25
PR,Fix tutorial test,Since tutorial python scripts have been removed on mxnet io just test notebooks,,kevinthesun,2017-07-10 23:15:51,2017-07-11 17:56:56
PR,CMake based compilation for Mac OS X,,,"tdomhan,tdomhan,tdomhan,tdomhan",2017-06-29 09:23:19,2017-07-11 17:57:54
PR,Optimized gpu dot kernels,Implementation of optimized GPU kernels for dot csr dns dns and dot csr T dns dns Different kernel versions are included for both cases which can be switched depending on the input attributes for best performance for example the dot csr dns vector kernel performs better than the scalar kernel in most cases for SpMV or when dense matrix is tall and skinny see e g SpMV paper for more details and terminology The old kernels are so called scalar kernels 1 the new kernels are numbered 2 4 Benchmark results for SpMV and SpMM and the transpose case with uniformly distributed non zeros in the CSR matrix are shown below for Kernels 1 and 2 of each case Benchmark conducted on K 80 p2 xlarge with compile flags USE OPENCV 1 USE BLAS openblas USE CUDA 1 USE CUDNN 1 Highlights for this particular benchmark Performance of SpMV SpMM dot csr dns improved by up to 40 Performance of SpMV SpMM tranpose dot csr T dns improved by up to 500 haibin lin Benchmark Table for dot csr dns dns with m x k k x n m x n density n m k 1 Scalar Kernel t dense t sparse t dense t sparse 2 Vector Kernel t dense t sparse t dense t sparse 20 1 512 50000 0 096 0 0037 0 0384 3 638 0 0037 0 0010 10 1 512 50000 0 153 0 0033 0 0214 4 991 0 0033 0 0007 5 1 512 50000 0 298 0 0029 0 0099 7 423 0 0033 0 0004 2 1 512 50000 0 717 0 0027 0 0038 12 352 0 0033 0 0003 1 1 512 50000 1 391 0 0027 0 0020 18 655 0 0033 0 0002 0 5 1 512 50000 2 625 0 0027 0 0010 26 777 0 0033 0 0001 0 1 1 512 50000 10 734 0 0027 0 0003 43 958 0 0033 0 0001 20 4 512 50000 0 187 0 0031 0 0165 0 567 0 0037 0 0065 10 4 512 50000 0 316 0 0024 0 0077 0 844 0 0033 0 0039 5 4 512 50000 0 624 0 0024 0 0039 1 467 0 0033 0 0023 2 4 512 50000 1 449 0 0024 0 0017 3 318 0 0033 0 0010 1 4 512 50000 2 762 0 0024 0 0009 6 133 0 0033 0 0005 0 5 4 512 50000 4 883 0 0024 0 0005 10 515 0 0033 0 0003 0 1 4 512 50000 17 888 0 0024 0 0001 27 302 0 0033 0 0001 20 64 512 50000 0 301 0 0085 0 0283 0 068 0 0085 0 1263 10 64 512 50000 0 609 0 0075 0 0123 0 126 0 0081 0 0649 5 64 512 50000 1 148 0 0068 0 0059 0 218 0 0058 0 0265 2 64 512 50000 2 716 0 0061 0 0023 0 568 0 0056 0 0099 1 64 512 50000 5 085 0 0061 0 0012 1 120 0 0056 0 0050 0 5 64 512 50000 9 270 0 0061 0 0007 2 149 0 0056 0 0026 0 1 64 512 50000 31 551 0 0061 0 0002 8 268 0 0056 0 0007 Benchmark Table for dot csr T dns dns with k x m m x n k x n density n m k 1 Scalar Kernel t dense t sparse t dense t sparse 2 Warp Kernel t dense t sparse t dense t sparse 20 1 512 50000 0 223 0 0031 0 0141 3 027 0 0027 0 0009 10 1 512 50000 0 212 0 0028 0 0130 4 681 0 0023 0 0005 5 1 512 50000 0 233 0 0028 0 0118 8 081 0 0023 0 0003 2 1 512 50000 0 267 0 0028 0 0104 14 382 0 0023 0 0002 1 1 512 50000 0 291 0 0028 0 0095 20 981 0 0023 0 0001 0 5 1 512 50000 0 323 0 0028 0 0086 29 041 0 0023 0 0001 0 1 1 512 50000 0 421 0 0028 0 0065 51 109 0 0023 0 0000 20 4 512 50000 0 060 0 0032 0 0525 0 851 0 0039 0 0046 10 4 512 50000 0 057 0 0028 0 0485 1 365 0 0035 0 0026 5 4 512 50000 0 063 0 0025 0 0402 2 278 0 0035 0 0015 2 4 512 50000 0 071 0 0025 0 0353 5 318 0 0035 0 0007 1 4 512 50000 0 077 0 0025 0 0323 9 666 0 0035 0 0004 0 5 4 512 50000 0 085 0 0025 0 0293 16 151 0 0035 0 0002 0 1 4 512 50000 0 112 0 0025 0 0222 38 063 0 0035 0 0001 20 64 512 50000 0 006 0 0032 0 5154 0 017 0 0045 0 2645 10 64 512 50000 0 005 0 0021 0 4498 0 022 0 0026 0 1183 5 64 512 50000 0 005 0 0021 0 4199 0 049 0 0026 0 0537 2 64 512 50000 0 006 0 0021 0 3778 0 157 0 0026 0 0167 1 64 512 50000 0 006 0 0021 0 3468 0 411 0 0026 0 0064 0 5 64 512 50000 0 007 0 0021 0 3156 1 055 0 0026 0 0025 0 1 64 512 50000 0 009 0 0021 0 2414 5 130 0 0026 0 0005,,"stefanhenneking,piiswrong,stefanhenneking,piiswrong,ptrendx,ptrendx,stefanhenneking,ptrendx,eric-haibin-lin,stefanhenneking,eric-haibin-lin,stefanhenneking,piiswrong,eric-haibin-lin,stefanhenneking,piiswrong",2017-07-06 01:48:51,2017-07-11 18:11:08
PR,Update mshadow,,,ZihengJiang,2017-07-05 20:42:44,2017-07-11 18:11:40
PR,superre fix,piiswrong,,szha,2017-07-08 04:43:28,2017-07-11 18:11:58
PR,Implemeneted reciprocal operator,As suggested in 3201 Besides a few minor changes Improved documentation for negative operator to be in line with numpy Adjusted indentation in one of the unit tests for operators,,"apaleyes,piiswrong,piiswrong,piiswrong,apaleyes",2017-06-16 10:44:39,2017-07-11 18:14:52
IS,R package handling of incomplete batches by mx io arrayiter,Hello as far as I understand the python version of the array data iter mxnet io NDarrayiter provides an argument for determining the handlig of the last batch last batch handle The R version mx io arrayiter seems not to provide this This raises two questions for me 1 Is it correct that in R the last batch is always padded 2 If yes does the default function mx model train train on the random padded data since it does not seem to discard the last batch,,"thirdwing,thirdwing",2017-07-06 14:39:34,2017-07-11 20:39:01
PR,R update the cats vs dogs example close 6788,,,thirdwing,2017-07-06 00:32:25,2017-07-11 22:02:26
IS,mx model init params has one extra parameter in mxnet0 10 0 compared to previous versions,I am trying to reproduce and find the following issue with the version of mxnet Its function mx model init params has one extra parameter output shape The original function is as follows arg params new mxnet mx model init params symbol new soft input shape c 224 224 3 8 initializer mxnet mx init uniform 0 1 ctx mx gpu 0 arg params Using mxnet0 10 0 the error complains that argument output shape is missing with no default I cannot find references on what this output shape refers to What does this parameter output shape mean here Should it be 8 which is the batch size Also only the following would work 8 not c 8 or 8 arg params new mxnet mx model init params symbol new soft input shape c 224 224 3 8 output shape 8 initializer mxnet mx init uniform 0 1 ctx mx gpu 0 arg params,,"thirdwing,thirdwing",2017-06-23 00:52:14,2017-07-11 22:02:36
IS,Fine tune with R API,Hi Many thanks for this library which provides almost only way to do deep learning in R Current R documentation is not very comprehensive but examples and discussions in issues help a lot Now I got stuck when looking for R equivalent of get internals method in Python Unfortunately model symbol get internals does not return any adoptable R object so modifying symbol in pre trained model seems to be impossible in R API without direct editing of symbol json file Is it possible to add in the R package capabilities for writing code like this,,"statist-bhfz,thirdwing,jeremiedb,statist-bhfz,statist-bhfz,jeremiedb,jeremiedb",2017-01-27 18:19:51,2017-07-11 22:03:12
IS,Fine tune in R end to end example and some troubles,Hi I asked about fine tune in R and due to jeremiedb s answers I finally was able to write complete R example Dogs vs Cats classification with mxnet and R Please add this link to the list of tutorials and or During this work I faced with new trouble Unfortunately all resnet models from completely failed after beginning of the training with some kind of memory allocation error nvidia smi utility shows that all memory 4Gb is used even for batch size 1 Then I tried inception bn It works fine but my R session crashed after each iteration and checkpoint save I also noticed that iterators uses almost all available RAM and swap immediately after creating Is it normal behavior,,"statist-bhfz,statist-bhfz,thirdwing",2017-02-25 18:54:38,2017-07-11 22:03:20
PR,Fix for codes not using pinned memory,Fixes issue 7000,,"ptrendx,piiswrong,ptrendx",2017-07-11 20:46:46,2017-07-11 23:43:36
IS,Mac OS installation tutorial includes Ubuntu apt get command,On the page click MacOS Observer the following commands on the page sudo apt get install graphviz pip install graphviz There should be no apt get command for Mac OS users,,"b0noI,kevinthesun",2017-06-29 22:21:28,2017-07-11 23:52:46
PR,kvstore push row sparse with data sharding,Partition a row sparse weight across servers,,"eric-haibin-lin,reminisce,reminisce,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong",2017-06-29 23:15:15,2017-07-12 01:01:46
IS,Is mxnet symbol support conditional control flow,I want to add some conditional control in my symbol it seems that if else is evaluated in symbol construction time But I want it to evaluated in symbol run time TensorFlow provides the tf cond operator to deal with it is there a counterpart in mxnet,,"Zehaos,CNevd,Zehaos,CNevd,Zehaos,Godricly,Zehaos,Godricly",2017-07-09 03:10:11,2017-07-12 07:49:43
IS,mx sym ctc loss and mx sym WarpCTC issues for latest build,Environment info Operating System Amazon Linux I tested on Ubuntu also Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source from source MXNet commit hash git rev parse HEAD c33649eb78dcee258a855e7f618c3d4988b6b140 Would you review this issue Thank you for reading in advance There are two issues related to the merge of 5834 First when I use the mx sym ctc loss as last layer of speech recognition example 5954 for newly implemented mx sym ctc switch the code example speech recognition stt layer warpctc py as below The example works fine on above commit hash so it is not the environmental issue For additional infomation I tested your unit test codes it works well on the lastest build but for the module implementation it may needs more implementation as a loss function,,"Soonhwan-Kwon,piiswrong,sbodenstein,Soonhwan-Kwon,piiswrong,piiswrong,Soonhwan-Kwon,piiswrong,sbodenstein,Soonhwan-Kwon",2017-05-04 09:39:02,2017-07-12 08:11:51
PR,Refactor Stateful operator and custom op,tqchen haibin lin Refactored CreateLayerOp to return any instead of Operator Use nnvm interface instead of Operator Forward Backward Added kLocal exec type to run operators in main thread without pushing to engine Refactored Custom op to use new interface,,"piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,tqchen,piiswrong,tqchen,piiswrong,tqchen",2017-07-05 05:19:15,2017-07-12 17:04:40
PR,CI remove pyc files before python ut,,,"mli,piiswrong",2017-07-10 20:01:59,2017-07-12 18:18:33
PR,Gluon,,,piiswrong,2017-07-10 18:26:41,2017-07-12 19:27:33
PR,Add Icon to Homepage,1 Increase splash height 2 Add icon to examples and model zoo addicon1,,kevinthesun,2017-07-12 18:44:48,2017-07-12 19:32:34
PR,Add python pip to the build from source install prerequisites,This also fixes the failing jenkins tests for the installation guide install doc changes,,anirudh2290,2017-07-11 01:00:36,2017-07-12 19:33:12
PR,add kl div loss,,,"szha,szha",2017-07-08 20:50:23,2017-07-12 19:51:16
PR,Remove setting of CUDNN AUTOTUNE in image classification example,1 is already the default and setting this here overwrites user is choice,,ptrendx,2017-07-12 20:04:18,2017-07-12 20:25:39
PR,WIP similarity tree lstm model childsum,First draft Note 1 the tree lstm assumes structure of input argument tree to have attr idx for index of input data for current node as well as attr children with value None or and iterable for child tree nodes,,"szha,szha,szha",2017-07-03 21:04:59,2017-07-12 20:55:26
PR,refactor vision models,refactored existing vision models and added VGG example using nn interface,,"szha,piiswrong,szha,piiswrong,piiswrong,mli,szha,szha,szha",2017-06-27 04:48:53,2017-07-12 20:57:05
IS,Unable to use multi gpu for training,Summary I am unable to use multi GPUs for training However with 1 GPUs training is successful After some debug I found out that this commit is introducing the failure Details OS Ubuntu 16 04 MXNet Build from source Number of GPUs 4 code CIFAR RESNet50 Stacktrace Epoch 1 10 17 26 52 src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable 17 26 56 home ubuntu mxnet dmlc core include dmlc logging h 304 17 26 56 home ubuntu mxnet mshadow mshadow stream gpu inl h 55 Check failed e cudaSuccess CUDA an illegal memory access was encountered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7fefc91eb278 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x1348b1c 0x7fefc91deb1c bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataOS1 OS3 0x3d 0x7fefc90b768d bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7fefc94f5807 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2 23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlS5 E E9 M invokeERKSt9 Any dataOS5 0x100 0x7fefc94fe360 bt 6 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt6thread5 ImplISt12 Bind simpleIFSt8functionIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8 EEE6 M runEv 0x4e 0x7fefc94f7b5e bt 7 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fefdd37ac80 bt 8 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fefe21626ba bt 9 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fefe1e983dd 17 26 56 home ubuntu mxnet dmlc core include dmlc logging h 304 17 26 56 home ubuntu mxnet mshadow mshadow stream gpu inl h 55 Check failed e cudaSuccess CUDA an illegal memory access was encountered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7fefc91eb278 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x1348b1c 0x7fefc91deb1c bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataOS1 OS3 0x3d 0x7fefc90b768d bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7fefc94f5807 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2 23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlS5 E E9 M invokeERKSt9 Any dataOS5 0x100 0x7fefc94fe360 bt 6 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt6thread5 ImplISt12 Bind simpleIFSt8functionIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8 EEE6 M runEv 0x4e 0x7fefc94f7b5e bt 7 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fefdd37ac80 bt 8 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fefe21626ba bt 9 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fefe1e983dd 17 26 56 home ubuntu mxnet dmlc core include dmlc logging h 304 17 26 56 src storage pooled storage manager h 84 cudaMalloc failed an illegal memory access was encountered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet7storage23GPUPooledStorageManager5AllocEm 0x3c9 0x7fefc9505749 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet11StorageImpl5AllocEmNS 7ContextE 0x69 0x7fefc9508059 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet7NDArrayC2ERKN4nnvm6TShapeENS 7ContextEbi 0x72e 0x7fefc911b39e bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x16e2a13 0x7fefc9578a13 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet4exec13GraphExecutor13InitArgumentsERKN4nnvm12IndexedGraphERKSt6vectorINS2 6TShapeESaIS7 EERKS6 IiSaIiEERKS6 INS 7ContextESaISG EESK SK RKS6 INS 9OpReqTypeESaISL EERKSt13unordered setINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESt4hashISW ESt8equal toISW ESaISW EEPKNS 8ExecutorEPSt13unordered mapISW NS 7NDArrayESY S10 SaISt4pairIKSW S19 EEEPS6 IS19 SaIS19 EES1I S1I 0xcd6 0x7fefc9579706 bt 6 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet4exec13GraphExecutor4InitEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSt13unordered mapISD NS2 6TShapeESt4hashISD ESt8equal toISD ESaISG ISH ST EEERKSS ISD iSV SX SaISG ISH iEEERKSN INS 9OpReqTypeESaIS18 EERKSt13unordered setISD SV SX SaISD EEPSN INS 7NDArrayESaIS1I EES1L S1L PSS ISD S1I SV SX SaISG ISH S1I EEEPNS 8ExecutorERKSS INS2 9NodeEntryES1I NS2 13NodeEntryHashENS2 14NodeEntryEqualESaISG IKS1S S1I EEE 0x88b 0x7fefc95814cb bt 7 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet8Executor10SimpleBindEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ RKSt13unordered mapISC NS1 6TShapeESt4hashISC ESt8equal toISC ESaISF ISG SS EEERKSR ISC iSU SW SaISF ISG iEEERKSM INS 9OpReqTypeESaIS17 EERKSt13unordered setISC SU SW SaISC EEPSM INS 7NDArrayESaIS1H EES1K S1K PSR ISC S1H SU SW SaISF ISG S1H EEEPS0 0x233 0x7fefc9581b73 bt 8 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so MXExecutorSimpleBind 0x2d4a 0x7fefc951bd4a bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7fef48364e40 17 26 56 home ubuntu mxnet dmlc core include dmlc logging h 304 17 26 56 src engine threaded engine h 329 17 26 56 home ubuntu mxnet mshadow mshadow stream gpu inl h 55 Check failed e cudaSuccess CUDA an illegal memory access was encountered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7fefc91eb278 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x1348b1c 0x7fefc91deb1c bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataOS1 OS3 0x3d 0x7fefc90b768d bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7fefc94f5807 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2 23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlS5 E E9 M invokeERKSt9 Any dataOS5 0x100 0x7fefc94fe360 bt 6 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt6thread5 ImplISt12 Bind simpleIFSt8functionIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8 EEE6 M runEv 0x4e 0x7fefc94f7b5e bt 7 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fefdd37ac80 bt 8 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fefe21626ba bt 9 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fefe1e983dd An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 7 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x317 0x7fefc94f5a97 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2 23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlS5 E E9 M invokeERKSt9 Any dataOS5 0x100 0x7fefc94fe360 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt6thread5 ImplISt12 Bind simpleIFSt8functionIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8 EEE6 M runEv 0x4e 0x7fefc94f7b5e bt 4 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fefdd37ac80 bt 5 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fefe21626ba bt 6 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fefe1e983dd terminate called after throwing an instance of wouldmlc Error' what 17 26 56 src engine threaded engine h 329 17 26 56 home ubuntu mxnet mshadow mshadow stream gpu inl h 55 Check failed e cudaSuccess CUDA an illegal memory access was encountered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN7mshadow6StreamINS 3gpuEE4WaitEv 0xd8 0x7fefc91eb278 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x1348b1c 0x7fefc91deb1c bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataOS1 OS3 0x3d 0x7fefc90b768d bt 4 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x87 0x7fefc94f5807 bt 5 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2 23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlS5 E E9 M invokeERKSt9 Any dataOS5 0x100 0x7fefc94fe360 bt 6 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt6thread5 ImplISt12 Bind simpleIFSt8functionIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8 EEE6 M runEv 0x4e 0x7fefc94f7b5e bt 7 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fefdd37ac80 bt 8 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fefe21626ba bt 9 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fefe1e983dd An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 7 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fefc852b7cc bt 1 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x317 0x7fefc94f5a97 bt 2 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEZZNS2 23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlS5 E E9 M invokeERKSt9 Any dataOS5 0x100 0x7fefc94fe360 bt 3 usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNSt6thread5 ImplISt12 Bind simpleIFSt8functionIFvSt10shared ptrIN5mxnet6engine10ThreadPool11SimpleEventEEEES8 EEE6 M runEv 0x4e 0x7fefc94f7b5e bt 4 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fefdd37ac80 bt 5 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fefe21626ba bt 6 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fefe1e983dd Aborted core dumped,,"sandeep-krishnamurthy,sandeep-krishnamurthy,ptrendx,ptrendx,ptrendx,ptrendx,sandeep-krishnamurthy,sandeep-krishnamurthy,ptrendx,ptrendx,sandeep-krishnamurthy,ptrendx,sandeep-krishnamurthy",2017-07-11 17:28:14,2017-07-12 21:44:26
PR,Change Gluon API style,,,kevinthesun,2017-07-12 21:44:57,2017-07-12 22:14:10
PR,interpret negative values in ReduceAxes ops,,,"szha,piiswrong,piiswrong,szha,szha",2017-07-11 05:07:11,2017-07-13 00:46:53
PR,Add python pip for GPU installation steps too,This is related to 6989 Missed adding python pip to installation prereqs for GPU Adding it now img width 784 alt screen shot 2017 07 12 at 6 04 04 pm src,,anirudh2290,2017-07-13 01:11:00,2017-07-13 02:45:20
PR,Refactor sparse tensor code,This PR refactors the sparse tensor code based upon the comments here 1 In the imperative mode returns stypes of output ndarrays together with their handles to the front end to avoid the c api for the stypes of those handles when constructing python NDArrays 2 Saves stype attribute in the python NDArray class to avoid c api calls to the backend for retrieving the stype from an NDArray handle 3 Created a folder named ndarray under src python mxnet as a subpackage for ndarray py sparse ndarray py ndarray utils py op py and internal py The folder acts as the namespace mxnet ndarray for all ndarray related functions 4 Moved the parallel cpu implementation of element wise sum of a vector of row sparse matrices to src ndarray ndarray function cc 5 Coding style fixes haibin lin,,"reminisce,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,piiswrong,reminisce,eric-haibin-lin,reminisce,piiswrong,reminisce,piiswrong",2017-07-07 20:06:39,2017-07-13 04:48:10
PR,doc Evaluate the python blocks on markdown files and append the results after,for example,,mli,2017-07-13 05:02:50,2017-07-13 15:42:17
PR,R To ignore R pkg when releasing on github,The R pkg will be ignored when releasing,,thirdwing,2017-07-12 17:12:55,2017-07-13 16:44:10
IS,libhdfs so 0 0 0 can not be located when install mxnet package on R 3 3 1,I compiled a mxnet with hdfs support and it seems successful when installed on python but while I tried to install on R in the final step error rose and said libhdfs so 0 0 0 cannot open when load libmxnet so in libPaths Detailed is illustrated as the screenshot below image I did these on Ubuntu16 04 with locally compiled R 3 3 1 and mkl linked Also linked mkl to mxnet while compiling the libmxnet so I have export HADOOP HOME lib native in which libhdfs so 0 0 0 located to LD LIBRARY PATH in both the system vars and Rprofile site but does not work Hope for a solution Thx,,"miguelgfierro,miguelgfierro,miguelgfierro",2016-11-25 04:21:42,2017-07-13 17:09:40
PR,Use pinned memory in benchmark mode,,,ptrendx,2017-07-13 17:06:13,2017-07-13 17:36:30
PR,minor bug fix in warp synchronous code,piiswrong,,stefanhenneking,2017-07-13 17:34:12,2017-07-13 17:37:02
PR,super resolution fix,,,"szha,piiswrong",2017-07-13 00:36:42,2017-07-13 17:37:54
PR,add pearsonr as metric,,,"szha,piiswrong,szha",2017-07-08 20:44:29,2017-07-13 17:54:44
PR,Some fixes for gluon,,,"piiswrong,mli",2017-07-12 22:12:30,2017-07-13 20:31:50
IS,R Conflict between resize and mean img in mx io ImageRecordIter make R crash,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia R MXNet version 0 9 4 R sessionInfo Error Message R session aborted no stack trace Minimum reproducible example library mxnet data shape c 56 56 3 was 28 28 3 train mx io ImageRecordIter path imgrec home user data image train rec batch size 128 data shape data shape resize 56 rand crop TRUE rand mirror TRUE mean img home user data mean bin Steps to reproduce 1 if i use data shape c 28 28 3 it works perfectly 2 all other values c 56 56 3 c 16 16 3 c 27 27 3 make R session crash What have you tried to solve it 1 If i remove the mean img parameter it does not crash anymore,,thirdwing,2017-03-02 19:10:11,2017-07-13 21:40:37
IS,warpctc module object has no at tribute 'WarpCTC',I compiled warpctc project followed the description of README but when run toy ctc py the error is no attribute 'WarpCTC' seems just revised config mk can be done no others need so why warpCTC is failed regiserter to mxnet,,"Soonhwan-Kwon,Soonhwan-Kwon,Soonhwan-Kwon",2017-07-11 22:51:48,2017-07-13 22:35:52
IS,Dimension mismatch error when running mxnetR,I am trying to build a Feed Forward neural network with MXNetR My input is a data frame with 6380 rows and 180 columns My training and testing outputs are one dimensional vectors with 319 elements each I run the model with the batch size set to 1 and the number of neurons at the output layer set to 319 So for each batch I expected to get a vector with 319 elements I aim to minimize my loss function which is the correlation between my predicted output vector and actual output vector Below is my code At the moment I am clueless about how I should fix this error I have been searching for a way to reshape my data sets so that they are 4D tensors but could not find any I do not look for an explicit solution for my problem but any suggestions on how I should tackle this error would be greatly appreciated,,"offerm,offerm,offerm,offerm,offerm,offerm,thirdwing",2017-06-06 21:22:40,2017-07-13 22:59:53
PR,fix mkl,,,piiswrong,2017-07-13 20:57:32,2017-07-13 23:10:44
PR,Dcgan fix,Fix the following for the DC GAN 1 reset the train iter in each epoch 2 add code to monitor the training,,piiswrong,2017-07-13 23:08:53,2017-07-14 03:23:19
PR,metric shape check fix,,,szha,2017-07-13 23:37:37,2017-07-14 03:25:12
PR,kldiv loss,this is an elementwise loss for regression tasks,,"szha,piiswrong,piiswrong,szha",2017-07-12 23:02:57,2017-07-14 04:27:23
PR,Nn docs cleanup,piiswrong,,"Roshrini,piiswrong,piiswrong,Roshrini",2017-07-13 22:06:21,2017-07-14 04:58:52
IS,I used pip and anaconda2 to install MXNet and now how can I install torch plugin,When I install MXNet I used anaconda2 and pip I also install torch correctly Now I want to use MXNet as the frontend of torch how can I install the torch plugin That is how I can connect MXNet with torch Thanks a lot,,,2017-07-11 03:21:41,2017-07-14 05:53:12
PR,gluon docs,piiswrong,,Roshrini,2017-07-14 04:58:09,2017-07-14 06:34:04
PR,fix ndarray slicing,,,piiswrong,2017-07-14 03:27:21,2017-07-14 07:04:54
PR,similarity tree lstm model childsum,piiswrong Needs 6966 7014 to work,,"szha,mli",2017-07-13 06:42:02,2017-07-14 07:07:26
PR,revert,,,piiswrong,2017-07-14 13:49:39,2017-07-14 14:10:27
PR,Change node labels to match new slave configurations,For our new CI on Apache we configured slaves with more specific names Without changing the node labels in this file to match the configuration builds will fail,,lxn2,2017-07-14 17:21:09,2017-07-14 18:20:30
PR,Add len to NDArray,This fixes the problem in 6966 7043 reported in 7040,,szha,2017-07-14 16:29:43,2017-07-14 18:20:55
IS,Segmentation Fault when training in batches v0 10,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu Package used Python R Scala Julia Python MXNet version 0 10 0 Python version and distribution 2 7 Error Message loading data X Y New Thread 0x7ffff0b9d700 LWP 4558 New Thread 0x7ffff339e700 LWP 4559 New Thread 0x7ffff3b9f700 LWP 4560 New Thread 0x7fff803ab700 LWP 4561 New Thread 0x7fff6a238700 LWP 4562 iterator made binding New Thread 0x7fff69a37700 LWP 4563 New Thread 0x7fff69236700 LWP 4564 New Thread 0x7fff63db1700 LWP 4565 New Thread 0x7fff631df700 LWP 4567 New Thread 0x7fff629de700 LWP 4568 starting to train 22 20 25 home ubuntu src mxnet dmlc core include dmlc logging h 304 22 20 25 home ubuntu src mxnet mshadow mshadow stream gpu inl h 102 Check failed err CUBLAS STATUS SUCCESS 1 vs 0 Create cublas handle failed Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fffe12a5fdc bt 1 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so 0x105cfaa 0x7fffe1c96faa bt 2 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0xa7 0x7fffe1c9af17 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7fffc4dc3a60 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7ffff7bc4184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7ffff78f1bed 22 20 25 home ubuntu src mxnet dmlc core include dmlc logging h 304 22 20 25 home ubuntu src mxnet mshadow mshadow stream gpu inl h 102 Check failed err CUBLAS STATUS SUCCESS 1 vs 0 Create cublas handle failed Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fffe12a5fdc bt 1 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so 0x105cfaa 0x7fffe1c96faa bt 2 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0xa7 0x7fffe1c9af17 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7fffc4dc3a60 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7ffff7bc4184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7ffff78f1bed terminate called after throwing an instance of wouldmlc Error' terminate called recursively what 22 20 25 home ubuntu src mxnet mshadow mshadow stream gpu inl h 102 Check failed err CUBLAS STATUS SUCCESS 1 vs 0 Create cublas handle failed Stack trace returned 6 entries bt 0 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fffe12a5fdc bt 1 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so 0x105cfaa 0x7fffe1c96faa bt 2 usr local lib python2 7 dist packages mxnet 0 10 0 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0xa7 0x7fffe1c9af17 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7fffc4dc3a60 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7ffff7bc4184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7ffff78f1bed Program received signal SIGABRT Aborted Switching to Thread 0x7fff69a37700 LWP 4563 0x00007ffff782ac37 in GI raise sig sig entry 6 at nptl sysdeps unix sysv linux raise c 56 56 nptl sysdeps unix sysv linux raise c No such file or directory gdb DETAILED INFORMATION I am running an experiment using the Deep Learning AMI Ubuntu p2 machine I was running the code before the latest release of MXNet and it was working fine However it returns segmentation faults when I do the following for b in tr iter model forward b is train True model backward model update The error occurs in the model forward part itself tr iter is the training data iterator with each batch of data being 128 X 21494 size model mxnet module module Module at 0x7fa04c595590 model bind data shapes tr iter provide data label shapes tr iter provide label init mx initializer Mixed 'fcout bias' ' ' mx init Normal 0 01 mx init Normal 0 01 model init params initializer init model init optimizer optimizer 'Adam' optimizer params 'learning rate' 0 001 'clip gradient' 1 The entire code runs fine with v0 9 3 but 0 10 throws this error My current solution is to use 0 9 3 itself but I think this is a pretty important issue that needs to be sorted out,,"chunyang-wen,Ldpe2G,sandeep-krishnamurthy",2017-06-12 14:03:30,2017-07-14 20:57:38
PR,Fix tutorial notebook names,,,kevinthesun,2017-07-14 18:12:34,2017-07-14 21:55:11
PR,Remove empty commit file,This file should not have been committed,,lxn2,2017-07-14 17:51:30,2017-07-14 21:55:58
PR,V0 10 0 fix for dropout and crash issue,v0 10 0 patches to fix the two issues 6960 and 6667,,"anirudh2290,anirudh2290",2017-07-14 22:55:56,2017-07-14 23:22:24
PR,move storage type vector from nnvm to mxnet,Requires,,eric-haibin-lin,2017-07-14 23:04:43,2017-07-15 02:42:07
PR,Improve copy sparse tensors,The CopyFromToImpl function for sparse tensors was implemented to create a temporary casted nd of the same stype of the destination This PR removes the this temporary casted nd if source ctx dest ctx and source stype dest stype by calling cast storage directly haibin lin,,"reminisce,eric-haibin-lin,reminisce",2017-07-11 23:26:36,2017-07-15 03:39:54
PR,R fix optimizer with multi GPU close 5296,jeremiedb,,thirdwing,2017-07-15 01:14:46,2017-07-15 05:18:29
IS,KVStore error with certain optimizer settings R package,When running on multi device certain configuration of the optimizer return an error relating to external pointer Below is the result when training the MNIST example and setting the momentum to 0 No bug occurs when running the same examples on a single GPU not using kvstore Running on Ubuntu 16 04 mxnet 0 9 4 as of 20170302 2 GTX 1080,,"jeremiedb,thirdwing",2017-03-07 15:58:31,2017-07-15 05:18:35
IS,all optimisers except sgd not working in R,Training fails to start with following error Error in eval substitute expr envir enclos expecting an external pointer I have looked through the r code for all the optimisers and I do not see any reason it should work for isgd' but not any of the others,,"thirdwing,thirdwing,miguelgfierro,thirdwing",2016-10-24 17:51:51,2017-07-15 05:18:56
PR,revert accidental changes,,,piiswrong,2017-07-15 00:07:37,2017-07-15 06:11:24
PR,Scala add arange to Symbol and NDArray,,,"Ldpe2G,yzhliu,Ldpe2G,Ldpe2G,Ldpe2G",2017-07-01 14:22:53,2017-07-16 05:08:31
IS,How do I create myself float data file,I wanna train a CNN with my self data which has 1 2million samples and per sample has 57600 6 float data how can I convert them to MxNet by myself I means create a MxNet data file using my own C program whithout any MxNet functions I need know the MxNet file format,,,2017-07-16 12:57:40,2017-07-16 12:58:07
PR,avoid runtime crash caused by error context of gradient storage when using context group,Consider following code which would cause a CUDA runtime error because of memory access violation In test model parallel py if we modify the mx cpu 1 context to mx gpu this test would also crash The reason is that after AssignContext we should check again if the gradient storage is on the same device of backward operator,,"piiswrong,piiswrong",2017-07-14 10:50:09,2017-07-16 21:37:59
PR,Improve PTB results,I would suggest changing the default LSTM forget bias 0 I do not think it is useful for speech and NLP task,,"yzhang87,piiswrong,piiswrong,yzhang87,yzhang87,piiswrong,piiswrong,piiswrong,yzhang87,piiswrong,yzhang87,piiswrong,piiswrong",2017-07-15 18:58:29,2017-07-16 21:41:53
PR,pretty print structure of gluon layers,piiswrong example,,"szha,piiswrong,szha",2017-07-15 22:55:37,2017-07-16 21:43:59
PR,Optimized sequence reverse operator,piiswrong This implementation provides some degree of parallelism on the GPU The version it replaces would do 1 kernel launch per DType instance int float to be copied which would make perf really bad on the GPU kernel launch latency and no parallelism In Sockeye this would take up 20 of the timeline With this change the time consumed went down to less than 0 5 I also added tests since this operator did not have either CPU or GPU tests,,"mkolod,tdomhan,mkolod,piiswrong,mkolod,piiswrong,fhieber,mkolod,fhieber,mkolod,fhieber,mkolod,mkolod,mkolod,piiswrong,mkolod,fhieber,mkolod,mkolod,fhieber",2017-07-07 00:23:16,2017-07-17 00:41:33
PR,Update documentation for correlation operation,Please review and merge,,dsqx71,2017-07-17 06:55:16,2017-07-17 16:31:31
PR,fix cub,,,szha,2017-07-17 22:50:38,2017-07-17 23:25:49
IS,compile Error undefined reference to mxnet Operator,build src operator instance norm o In function mxnet op InstanceNormProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' instance norm cc text 0x273 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op InstanceNormParam int ' build src operator pooling o In function mxnet op PoolingProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' pooling cc text 0xfdc undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op PoolingParam int ' build src operator crop o In function mxnet op CropProp CreateOperator mxnet Context const' crop cc text 0x7da undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op CropParam ' build src operator sequence reverse o In function mxnet op SequenceReverseProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' sequence reverse cc text 0x652 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SequenceReverseParam int ' build src operator spatial transformer o In function mxnet op SpatialTransformerProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' spatial transformer cc text 0x898 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SpatialTransformerParam int ' build src operator deconvolution o In function mxnet op DeconvolutionProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' deconvolution cc text 0x811 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op DeconvolutionParam int std vector nnvm TShape std allocator nnvm TShape std vector nnvm TShape std allocator nnvm TShape mxnet Context ' build src operator pad o In function mxnet op PadProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' pad cc text 0x14778 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op PadParam int ' build src operator rnn o In function mxnet op RNNProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' rnn cc text 0x70e undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op RNNParam int ' build src operator concat o In function mxnet op ConcatProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' concat cc text 0x3da undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op ConcatParam int ' build src operator lrn o In function mxnet op LocalResponseNormProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' lrn cc text 0x2b7 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op LRNParam int ' build src operator correlation o In function mxnet op CorrelationProp CreateOperator mxnet Context const' correlation cc text 0x3e7 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op CorrelationParam ' build src operator batch norm v1 o In function mxnet op BatchNormV1Prop CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' batch norm v1 cc text 0x481 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op BatchNormV1Param int ' build src operator sequence mask o In function mxnet op SequenceMaskProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' sequence mask cc text 0x672 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SequenceMaskParam int ' build src operator make loss o In function mxnet op MakeLossProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' make loss cc text 0x606 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op MakeLossParam int ' build src operator grid generator o In function mxnet op GridGeneratorProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' grid generator cc text 0x4e5 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op GridGeneratorParam int ' build src operator pooling v1 o In function mxnet op PoolingV1Prop CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' pooling v1 cc text 0x1115 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op PoolingV1Param int ' build src operator leaky relu o In function mxnet op LeakyReLUProp CreateOperator mxnet Context const' leaky relu cc text 0x325 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op LeakyReLUParam ' build src operator identity attach KL sparse reg o In function mxnet op IdentityAttachKLSparseRegProp CreateOperator mxnet Context const' identity attach KL sparse reg cc text 0x323 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op IdentityAttachKLSparseRegParam ' build src operator upsampling o In function mxnet op UpSamplingProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' upsampling cc text 0x1805 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op UpSamplingParam int ' build src operator svm output o In function mxnet op SVMOutputProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' svm output cc text 0x696 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SVMOutputParam int ' build src operator bilinear sampler o In function mxnet op BilinearSamplerProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' bilinear sampler cc text 0x6a6 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op BilinearSamplerParam int ' build src operator softmax output o In function mxnet op SoftmaxOutputProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' softmax output cc text 0x766a undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SoftmaxOutputParam int ' build src operator convolution o In function mxnet op ConvolutionProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' convolution cc text 0x7ad undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op ConvolutionParam int std vector nnvm TShape std allocator nnvm TShape std vector nnvm TShape std allocator nnvm TShape mxnet Context ' build src operator roi pooling o In function mxnet op ROIPoolingProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' roi pooling cc text 0xad9 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op ROIPoolingParam int ' build src operator slice channel o In function mxnet op SliceChannelProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' slice channel cc text 0x428 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SliceChannelParam int ' build src operator fully connected o In function mxnet op FullyConnectedProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' fully connected cc text 0x4d9 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op FullyConnectedParam int std vector nnvm TShape std allocator nnvm TShape std vector nnvm TShape std allocator nnvm TShape mxnet Context ' build src operator regression output o In function mxnet op RegressionOutputProp mxnet op reg enum RegressionOutputType 1 CreateOperator mxnet Context const' regression output cc text ZNK5mxnet2op20RegressionOutputPropILNS0 8reg enum20RegressionOutputTypeE1EE14CreateOperatorENS 7ContextE ZNK5mxnet2op20RegressionOutputPropILNS0 8reg enum20RegressionOutputTypeE1EE14CreateOperatorENS 7ContextE 0x13 undefined reference to mxnet Operator mxnet op CreateRegressionOutputOp mshadow gpu mxnet op reg enum RegressionOutputType mxnet op RegressionOutputParam ' build src operator regression output o In function mxnet op RegressionOutputProp mxnet op reg enum RegressionOutputType 2 CreateOperator mxnet Context const' regression output cc text ZNK5mxnet2op20RegressionOutputPropILNS0 8reg enum20RegressionOutputTypeE2EE14CreateOperatorENS 7ContextE ZNK5mxnet2op20RegressionOutputPropILNS0 8reg enum20RegressionOutputTypeE2EE14CreateOperatorENS 7ContextE 0x13 undefined reference to mxnet Operator mxnet op CreateRegressionOutputOp mshadow gpu mxnet op reg enum RegressionOutputType mxnet op RegressionOutputParam ' build src operator regression output o In function mxnet op RegressionOutputProp mxnet op reg enum RegressionOutputType 0 CreateOperator mxnet Context const' regression output cc text ZNK5mxnet2op20RegressionOutputPropILNS0 8reg enum20RegressionOutputTypeE0EE14CreateOperatorENS 7ContextE ZNK5mxnet2op20RegressionOutputPropILNS0 8reg enum20RegressionOutputTypeE0EE14CreateOperatorENS 7ContextE 0x10 undefined reference to mxnet Operator mxnet op CreateRegressionOutputOp mshadow gpu mxnet op reg enum RegressionOutputType mxnet op RegressionOutputParam ' build src operator l2 normalization o In function mxnet op L2NormalizationProp CreateOperator mxnet Context const' l2 normalization cc text 0x26d undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op L2NormalizationParam ' build src operator activation o In function mxnet op ActivationProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' activation cc text 0xdc4 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op ActivationParam int nnvm TShape const ' build src operator batch norm o In function mxnet op BatchNormProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' batch norm cc text 0x7cd undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op BatchNormParam int nnvm TShape const ' build src operator dropout o In function mxnet op DropoutProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' dropout cc text 0x48b undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op DropoutParam int ' build src operator softmax activation o In function mxnet op SoftmaxActivationProp CreateOperator mxnet Context const' softmax activation cc text 0xfbc undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SoftmaxActivationParam ' build src operator convolution v1 o In function mxnet op ConvolutionV1Prop CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' convolution v1 cc text 0xa6d undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op ConvolutionV1Param int std vector nnvm TShape std allocator nnvm TShape std vector nnvm TShape std allocator nnvm TShape mxnet Context ' build src operator swapaxis o In function mxnet op SwapAxisProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' swapaxis cc text 0x7ba undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SwapAxisParam int ' build src operator sequence last o In function mxnet op SequenceLastProp CreateOperatorEx mxnet Context std vector nnvm TShape std allocator nnvm TShape std vector int std allocator int const' sequence last cc text 0x692 undefined reference to mxnet Operator mxnet op CreateOp mshadow gpu mxnet op SequenceLastParam int ' collect2 error ld returned 1 exit status make bin im2rec Error 1,,madjam,2017-07-17 09:55:17,2017-07-18 03:01:49
IS,MXNet installation error on mac is R 3 4 0,Hi I am trying to install the recently released MXNet 0 10 0 on my mac is R from source using the tar gz file uploaded here However I am getting the following error install packages Downloads mxnet 0 10 0 tar gz repos NULL type source Warning in untar2 tarfile files list exdir restore times skipping pax global extended headers ERROR cannot extract package from Downloads mxnet 0 10 0 tar gz Warning in install packages installation of package Downloads mxnet 0 10 0 tar gz had non zero exit status My R is sessionInfo is the following sessionInfo R version 3 4 0 2017 04 21 Platform x86 64 apple darwin16 1 0 64 bit Running under macOS Sierra 10 12 1 Matrix products default BLAS System Library Frameworks Accelerate framework Versions A Frameworks vecLib framework Versions A libBLAS dylib LAPACK usr local Cellar openblas 0 2 19 1 lib libopenblas haswellp r0 2 19 dylib locale 1 en US UTF 8 en US UTF 8 en US UTF 8 C en US UTF 8 en US UTF 8 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 compiler 3 4 0 tools 3 4 0 I built R with openMP support so that should not be a problem Minimum reproducible example install packages Downloads mxnet 0 10 0 tar gz repos NULL type source What have you tried to solve it I had also tried to install it following this link is suggestion It resulted in the following error install packages mxnet Installing package into Library R 3 4 0 library as lib is unspecified trying URL '' Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Error in download file url destfile method mode wb cannot open URL '' Warning in install packages download of package mxnet failed Could you please look into the error Thanks SG,,"thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing",2017-07-06 00:55:23,2017-07-18 04:11:46
IS,Ask Prediction with single image with API Module,hello I was wondering if it is possible to predict on a picture without creating and saving a rec and lst file with API Module I know it is possible with mxnet amalgamation but using gpu is impossible with this version of mxnet,,sergeykolychev,2017-03-24 11:45:24,2017-07-18 14:02:38
IS,mxnet fine tuned model result wrong,Hello I would like to fine tuned a pre trained model VGG16 pretrained on different dataset I try to fine tuned it fixing all the layers excepted the full connected layers But I obtain wrong result In fact I obtain the same validation result for each epoch Have I forgot some parameters Where am I wrong I change the value of wd and learning rate but without any success,,,2017-03-22 15:11:40,2017-07-18 14:02:48
IS,loading weight in pretained model error,Hello I am trying to adapt a pre trained model loading the weight of all the hidden layers but not the full connected layers The goal is to fine tuned the full connected layers and reduce the dimension of the output What I did params is the file model 0001 params I think the problem is that I do not indicate what weight to load load params seems to load all the weight that why I have an error How can I solve this problem Error message RuntimeError fc8 weight is not presented mod mx mod Module Create modele mod bind data shapes wouldata' 1 3 224 224 label shapes isoftmax label' 1 mod load params params def Create modele data mx sym Variable wouldata' conv1 conv1 1 mx sym Convolution data data kernel 3 3 pad 1 1 num filter 64 name conv1 1 relu1 1 mx sym Activation data conv1 1 act type relu name relu1 1 conv1 2 mx sym Convolution data relu1 1 kernel 3 3 pad 1 1 num filter 64 name conv1 2 relu1 2 mx sym Activation data conv1 2 act type relu name relu1 2 pool1 mx sym Pooling data relu1 2 kernel 2 2 stride 2 2 pool type max name pool1 conv2 conv2 1 mx sym Convolution data pool1 kernel 3 3 pad 1 1 num filter 128 name conv2 1 relu2 1 mx sym Activation data conv2 1 act type relu name relu2 1 conv2 2 mx sym Convolution data relu2 1 kernel 3 3 pad 1 1 num filter 128 name conv2 2 relu2 2 mx sym Activation data conv2 2 act type relu name relu2 2 pool2 mx sym Pooling data relu2 2 kernel 2 2 stride 2 2 pool type max name pool2 conv3 conv3 1 mx sym Convolution data pool2 kernel 3 3 pad 1 1 num filter 256 name conv3 1 relu3 1 mx sym Activation data conv3 1 act type relu name relu3 1 conv3 2 mx sym Convolution data relu3 1 kernel 3 3 pad 1 1 num filter 256 name conv3 2 relu3 2 mx sym Activation data conv3 2 act type relu name relu3 2 conv3 3 mx sym Convolution data relu3 2 kernel 3 3 pad 1 1 num filter 256 name conv3 3 relu3 3 mx sym Activation data conv3 3 act type relu name relu3 3 pool3 mx sym Pooling data relu3 3 kernel 2 2 stride 2 2 pool type max name pool3 conv4 conv4 1 mx sym Convolution data pool3 kernel 3 3 pad 1 1 num filter 512 name conv4 1 relu4 1 mx sym Activation data conv4 1 act type relu name relu4 1 conv4 2 mx sym Convolution data relu4 1 kernel 3 3 pad 1 1 num filter 512 name conv4 2 relu4 2 mx sym Activation data conv4 2 act type relu name relu4 2 conv4 3 mx sym Convolution data relu4 2 kernel 3 3 pad 1 1 num filter 512 name conv4 3 relu4 3 mx sym Activation data conv4 3 act type relu name relu4 3 pool4 mx sym Pooling data relu4 3 kernel 2 2 stride 2 2 pool type max name pool4 conv5 conv5 1 mx sym Convolution data pool4 kernel 3 3 pad 1 1 num filter 512 name conv5 1 relu5 1 mx sym Activation data conv5 1 act type relu name relu5 1 conv5 2 mx sym Convolution data relu5 1 kernel 3 3 pad 1 1 num filter 512 name conv5 2 relu5 2 mx sym Activation data conv5 2 act type relu name relu5 2 conv5 3 mx sym Convolution data relu5 2 kernel 3 3 pad 1 1 num filter 512 name conv5 3 relu5 3 mx sym Activation data conv5 3 act type relu name relu5 3 pool5 mx sym Pooling data relu5 3 kernel 2 2 stride 2 2 pool type max name pool5 fc6 flat6 mx sym Flatten data pool5 name flat6 fc6 mx sym FullyConnected data flat6 num hidden 4096 name fc6 relu6 mx sym Activation data fc6 act type relu name relu6 drop6 mx sym Dropout data relu6 p 0 5 name drop6 fc7 fc7 mx sym FullyConnected data drop6 num hidden 4096 name fc7 relu7 mx sym Activation data fc7 act type relu name relu7 drop7 mx sym Dropout data relu7 p 0 5 name drop7 fc8 fc8 mx sym FullyConnected data drop7 num hidden 2 name fc8 softmax mx sym Softmax data fc8 name softmax return softmax,,,2017-04-03 11:46:03,2017-07-18 14:03:04
PR,Fix a spelling mistake,,,Harmonicahappy,2017-07-18 08:26:11,2017-07-18 16:29:11
PR,fix custom op and add tutorial,,,piiswrong,2017-07-17 18:54:29,2017-07-18 16:30:24
IS,Cannot install,install packages mxnet Installing package into C Users Jaroslaw Documents R win library 3 2 as lib is unspecified Warning in install packages dependencies DiagrammeR 0 9 0 visNetwork 1 0 3 are not available Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Warning in install packages unable to access index for repository cannot open URL '' Pakiet kt ry jest dost pny jedynie w formie r d owej i mo e wymaga kompilacji C C Fortran mxnet These will not be installed,,thirdwing,2017-07-18 15:47:08,2017-07-18 16:52:32
IS,Windows 10 Pro Install Problems,Am following the procedure to install the GPU version of MXNET for useage in R but I'm not even able to find the setupenv file,,"piiswrong,thirdwing,thirdwing",2016-11-04 01:19:51,2017-07-18 17:28:31
IS,predict function error,Hello I have created multiple feed forward NN is using mx model FeedForward create and then try to predict on a test dataset and am receiving this error Error in External list name CppMethod invoke notvoid address pointer nil NULL value passed as symbol address From my searching somehow the Rcpp package is not finding some dependency possibly However I have not been able to fix Platform x86 64 w64 mingw32 x64 Windows 10 R version 3 3 1 mxnet CPU version Thanks for this great package Chris,,"piiswrong,thirdwing,thirdwing",2016-11-02 15:14:46,2017-07-18 17:33:16
IS,Multiple GPUs training failed,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 LTS and 14 04LTS Compiler Package used Python R Scala Julia MXNet version Or if installed from source Yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 22 38 21 src mxnet dmlc core include dmlc logging h 304 22 38 21 src mxnet mshadow mshadow stream gpu inl h 55 Check failed e cudaSuccess CUDA an illegal memory access was encountered Minimum reproducible example if you are using your own code please provide a short script that reproduces the error run the provided example rcnn with multiple GPUs,,sandeep-krishnamurthy,2017-07-11 02:42:45,2017-07-18 18:06:24
PR,Add h5py support to NDArrayIter,This pull request adds basic h5py support to NDArrayIter Data will be read from the h5 dataset on demand when creating a batch so that large datasets that do not fit in memory are supported Shuffling is also supported by storing a set of shuffled indices,,"leezu,piiswrong,leezu,leezu,leezu,leezu",2017-06-23 02:37:22,2017-07-18 19:41:57
IS,How to get the data in symbol,I think the definition of symbol in mxnet is just like the Variable in pytorrch In pytorch i can use Variable data to visit the original tensor values now i want to do same thint in mxnet Buuut i can not find API which can deal this task So anyone knows it,,Soonhwan-Kwon,2017-07-14 04:21:24,2017-07-19 11:20:23
PR,Fix argmax channel method and add preqs in scala mnist tutorial 7101,,,"kottmann,madjam,kottmann,piiswrong,Ldpe2G",2017-07-19 08:48:08,2017-07-19 15:26:01
PR,Fix a spelling mistake,,,Harmonicahappy,2017-07-19 06:22:11,2017-07-19 16:45:33
PR,example add missing files for speech recognition example,add missing files for speech recognition example 7095,,Soonhwan-Kwon,2017-07-19 02:29:44,2017-07-19 17:27:52
IS,gluon Using mxnet evaluation metrics in gluon,I was wondering if it was already easy to use mxnet metric as the loss function in place of defining my own or using the gluon loss For instance I would prefer to use the mx metric perplexity Currently I am defining my own loss function which calls the perplexity class but this seems like a feature that would be good to have in gluon,,piiswrong,2017-07-19 17:21:21,2017-07-19 17:51:02
IS,gluon no way to express skip connection inside for loop due to the name scope,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler gcc nvcc Package used Python R Scala Julia python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 70a781b62a0dcaf782cf5cf96394b1e87091ed5f If you are using python package please provide Python version and distribution 2 7 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Gluon has Sequential and HybridSequential for recursively adding blocks but it seems no way to add skip connection inside a for loop If we hardcode all the layer and give them names the code will be pretty ugly here is an example based on TensorFlow,,"jingpengw,piiswrong,jingpengw,jingpengw",2017-07-19 15:05:22,2017-07-19 21:00:46
PR,R Image segmentation example and test close 5003,,,thirdwing,2017-07-18 23:26:48,2017-07-20 00:02:33
IS,Problem Training Network for Image segmentation using u net like network MXNET R,I'm trying to build a modest network for image segmentation in medical images for a school project I'm a total beginner by the way at this So I tried to build a network that is structured like the u net here is my code when i run with the gpu device rstudio just end up by crashing without error message although I get one when I make it run from the console in R the images I'm using here to train are from the ISBI Challenge 2012 neuronal images I would like to understand why it is crashing because I do not have a clue for now there is probably something to set more correctly and precisely for the output layer softmax since this a segmentation problem and each pixel has a true label also i got some issues with the dimension of the train x and train y but it seemed to be solved When I run with the cpu and just 1 training epoch i get a model For 2 epochs and more i get this error message Error in exec update arg arrays arg arrays match name skip null executor cc 65 RCheck failed to containsElementNamed names i c str cannot find key NA in the array arg arrays MXNet version 9 4 If you are using R package please provide R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 16 04 1 LTS locale 1 LC CTYPE fr FR UTF 8 LC NUMERIC C LC TIME fr FR UTF 8 4 LC COLLATE fr FR UTF 8 LC MONETARY fr FR UTF 8 LC MESSAGES fr FR UTF 8 7 LC PAPER fr FR UTF 8 LC NAME C LC ADDRESS C 10 LC TELEPHONE C LC MEASUREMENT fr FR UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 jpeg 0 1 8 mxnet 0 9 4 loaded via a namespace and not attached 1 igraph 1 0 1 Rcpp 0 12 9 rstudioapi 0 6 magrittr 1 5 5 munsell 0 4 3 colorspace 1 3 2 R6 2 2 0 brew 1 0 6 9 stringr 1 1 0 plyr 1 8 4 dplyr 0 5 0 visNetwork 1 0 3 13 Rook 1 1 1 tools 3 3 2 grid 3 3 2 gtable 0 2 0 17 DBI 0 5 1 influenceR 0 1 0 DiagrammeR 0 9 0 htmltools 0 3 5 21 lazyeval 0 2 0 digest 0 6 12 assertthat 0 1 tibble 1 2 25 gridExtra 2 2 1 RColorBrewer 1 1 2 ggplot2 2 2 1 codetools 0 2 15 29 htmlwidgets 0 8 viridis 0 3 4 rgexf 0 15 3 stringi 1 1 2 33 scales 0 4 1 XML 3 98 1 5 jsonlite 1 2 Error Message 1 Rstudio crashes with GPU 2 Error in exec update arg arrays arg arrays match name skip null executor cc 65 RCheck failed to containsElementNamed names i c str cannot find key NA in the array arg arrays with CPU MY CODE,,"thirdwing,thirdwing",2017-02-13 14:50:40,2017-07-20 00:02:39
IS,can not import mxnet autograd,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu16 04 Compiler Package used Python R Scala Julia Python MXNet version 0 10 0 gpu when i flow the tutorials2 import mxnet autograd as ag Traceback most recent call last File stdin line 1 in module ImportError No module named autograd,,szha,2017-07-20 03:04:31,2017-07-20 03:50:16
IS,Errors about multi machine training Distribution training,Environment info Operating System Ubuntu 14 04 Compiler g 4 8 Package used Python R Scala Julia Python MXNet version Or if installed from source from source MXNet commit hash git rev parse HEAD 70a781b62a0dcaf782cf5cf96394b1e87091ed5f If you are using python package please provide Python version and distribution 2 7 anaconda2 If you are using R package please provide R sessionInfo Error Message I am sure i input the correct password but return the Permission denied info anyway also the both machines are sshable the user wuxiaomin are not the admin in both machines 2017 07 19 12 00 32 848 INFO rsync home wuxiaomin software mxnet example image classification xxx xxx xxx xxx tmp mxnet bin sh 1 launcher not found Exception in thread Thread 1 Traceback most recent call last File home wuxiaomin anaconda2 lib python2 7 threading py line 801 in bootstrap inner self run File home wuxiaomin anaconda2 lib python2 7 threading py line 754 in run self target self args self kwargs File home wuxiaomin software mxnet tools dmlc core tracker dmlc tracker tracker py line 363 in lambda target lambda subprocess check call self cmd env env shell True args File home wuxiaomin anaconda2 lib python2 7 subprocess py line 186 in check call raise CalledProcessError retcode cmd CalledProcessError Command 'launcher ssh' returned non zero exit status 127 wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password wuxiaomin amax software mxnet example image classification wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password Permission denied publickey password wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password Permission denied publickey password wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password Permission denied please try again wuxiaomin xxx xxx xxx xxx is password Permission denied publickey password Minimum reproducible example python tools launch py s 2 n 2 H hosts sync dst dir tmp mxnet launcher ssh,,,2017-07-18 12:06:45,2017-07-20 04:22:15
PR,added casts and overrides to fix compiler warnings,index t and int comparison warnings are fixed by casting appropriately,,"rahul003,piiswrong,rahul003,piiswrong,rahul003",2017-07-19 17:03:11,2017-07-20 04:35:04
IS,Scala mnist tutorial does not compile,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info MXNet version Latest master verion Minimum reproducible example if you are using your own code please provide a short script that reproduces the error error value argmaxChannel is not a member of object ml dmlc mxnet NDArray INFO val predictedY NDArray argmaxChannel prob,,"kottmann,madjam",2017-07-19 08:32:23,2017-07-20 07:32:11
PR,Fix svm mnist py,Fix doc Remove workaround,,"yanboliang,yanboliang",2017-07-14 03:45:42,2017-07-20 10:51:58
IS,Get logits for pretrained models,Is there any way to get logits i e the activations before the softmax layer from the pretrained models Thanks,,thirdwing,2017-07-19 14:06:07,2017-07-20 15:48:29
PR,Updating dmlc core,,,ptrendx,2017-07-20 00:42:01,2017-07-20 16:53:33
PR,Resolve a problem when import mxnet in python cannot import name gluon,We use 'python setup py install' install mxnet into a anaconda environment There is a problem when import mxnet from import gluon ImportError cannot import name gluon 1 Add 'mxnet gluon' 'mxnet gluon nn' 'mxnet gluon rnn' in python setup py 2 'python setup py install' can copy these packages into right python path,,"solin319,piiswrong",2017-07-20 09:07:14,2017-07-20 17:10:30
PR,R add allow extra params update related documents,I have added the feature extraction as an example in our documents I think more people might need this,,thirdwing,2017-07-20 16:59:03,2017-07-20 21:43:32
IS,fresh installation of mxnet nothing worked,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler gcc Package used Python R Scala Julia python MXNet version mxnet 0 10 0 post2 py2 py3 none manylinux1 x86 64 whl Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 on linux2 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Following mxnet offical tutorials but found almost nothing worked autograd is in mxnet contrib instead of mxnet as listed in tutorial attach grad is visible in API documentation but not available in python environment Looks like documentation and actual code are completely unmatched what is wrong import mxnet as mx import mxnet ndarray as nd import mxnet autograd as ag Traceback most recent call last File stdin line 1 in module ImportError No module named autograd import mxnet contrib autograd as ag x mx nd array 1 2 3 4 x attach grad Traceback most recent call last File stdin line 1 in module AttributeError 'NDArray' object has no attribute 'attach grad' Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it,,piiswrong,2017-07-20 21:30:54,2017-07-20 22:05:58
PR,Adding absolute tolerance to few tests,,,ptrendx,2017-07-20 20:51:13,2017-07-20 22:55:09
PR,New algo for batchnorm for cuDNN 7,,,ptrendx,2017-07-20 18:01:09,2017-07-20 22:55:49
PR,Volta arch support in Makefile and cooperative groups,It does not include fixes for ModernGPU library for CTC operator will do it in later PR,,"ptrendx,piiswrong,piiswrong,ptrendx,ptrendx",2017-07-20 17:53:42,2017-07-20 22:58:20
PR,recursively include modules in setup py,piiswrong,,szha,2017-07-20 17:38:17,2017-07-20 22:59:27
IS,Keras Support MXNET backend for Keras,I have been working on supporting MXNET as a backend for Keras A popular neural networks Python library which currently supports TensorFlow or Theano I am hopeful the endeavor is win win for both projects Keras will benefit from the MXNET s multi device multi node support and MXNET will get broader exposure The task will also test out and most probably enhance MXNET API capabilities for a broader set of audience To this end I have started the process and am able to get low hanging APIs checked off I could say I have 25 of the work done The amount of changes required in Keras is not huge we just need to add one more file on the same lines as of tensorflow backend py I think most of the work would be in figuring out how to map the functionalities to MXNET APIs and implementing the missing ones The backend tests are a good way to go about implementing it and tracking our progress Here is my rough current status as measured in terms of those tests APIs converted test linear operations test shape operations test elementwise operations test nn operations test value manipulation test random normal test random uniform test random binomial APIs I am currently working on test gradient test rnn test rnn no states test conv2d test conv3d test pool2d test pool3d APIs I think might need changes updates to MXNET test function test ctc test ctc decode greedy test ctc decode beam search test one hot test sparse dot test sparse concat test map test foldl test foldr Things that are currently missing but are nice to have Python distutil distribution for MXNET Provide core functionality in NDArray For some of the functionalities I had to use Symbol module First things first I want to know if this is inline with the MXNET community s needs and something which you agree needs to be pursued and is worthwhile If we agree then I can use the issue as a high level task to track and update my progress and also request more info features and help with implementing them,,"shivarajugowda,piiswrong,piiswrong,shivarajugowda,jspisak,shivarajugowda,shivarajugowda,shivarajugowda,shivarajugowda,shivarajugowda,piiswrong,tqchen,tqchen,shivarajugowda,piiswrong,shivarajugowda,piiswrong,shivarajugowda,shivarajugowda,shivarajugowda,shivarajugowda",2016-12-09 17:09:34,2017-07-20 23:20:10
IS,KERAS on MXNET ISSUE,i am getting the following error on KERAS with MXNET background i changed backend in keras json from tensorflow to mxnet i am using keras version of 2 0 6 and mxnet version 0 10 0 image,,,2017-07-21 00:03:55,2017-07-21 00:41:56
IS,cannot build MXNet Package for Perl,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OS X 10 11 6 Compiler Apple LLVM version 7 0 2 clang 700 1 81 Package used Python R Scala Julia Perl MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 897cc55a0ffd53317628bbccadea34b072228937 If you are using python package please provide Python version and distribution n a If you are using R package please provide R sessionInfo n a Error Message Please paste the full error message including stack trace perl Makefile PL INSTALL BASE HOME perl5 3 Error Unable to find 'perl5 swg' mxnet i 3 Error Unable to find 'typemaps i' Warning L lib changed to L Users arussel7 Projects mxnet perl package AI MXNetCAPI lib Generating a Unix style Makefile Writing Makefile for AI MXNetCAPI Writing MYMETA yml and MYMETA json Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error simply following the instructions at I built the mxnet library successfuly but it seems that the swig bindings were not built What have you tried to solve it I looked through the buildfiles to see where this may be an option that was not included in the build or some similar issue but to no avail,,"adamcrussell,sergeykolychev,adamcrussell,sergeykolychev,sergeykolychev,adamcrussell,sergeykolychev,sergeykolychev",2017-07-17 19:58:23,2017-07-21 01:56:04
PR,Remove gluon tutorials from index,,,mli,2017-07-21 02:33:50,2017-07-21 02:37:26
IS,Finetune error according tutorial ValueError Cannot find output that matches name flatten0 output,I want to finetune pretrained inception BN squeezeNet NIN caffenet vgg16 vgg19 models from model zoo which happened this ValueError It is okay for finetuning resnet resneXt models laughing THanks a lot,,"ysh329,ysh329,ysh329,ysh329,tornadomeet,ysh329",2017-05-29 12:03:04,2017-07-21 02:58:09
PR,scala package improve the readability of Spark Mxnet implementation,the basic approach is to disassemble the long method into several shorter ones,,"CodingCat,piiswrong,Ldpe2G,CodingCat",2017-07-20 05:37:01,2017-07-21 03:27:28
PR,add the link thestraightdope mxnet io,,,mli,2017-07-21 03:51:48,2017-07-21 03:57:56
PR,add data partition for libsvm iter,reminisce,,eric-haibin-lin,2017-07-13 16:24:11,2017-07-21 05:23:05
IS,mxnet compile error usr bin ld cannot find lnnpack with NNPACK successfully installed,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System CentOS 7 Compiler gcc 4 8 5 20150623 Package used Python R Scala Julia Python MXNet version Or if installed from source yes used git to get the latest version MXNet commit hash git rev parse HEAD N A If you are using python package please provide Python version and distribution 2 7 5 virtualenv If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace build src c api c api executor o build src c api c predict api o build src c api c api o build src c api c api error o build src executor inplace addto detect pass o build src executor graph executor o build src executor attach op execs pass o build src executor attach op resource pass o build src kvstore kvstore o build src resource o build src initialize o local data1 users shane mxnet ps lite build libps a local data1 users shane mxnet dmlc core libdmlc a local data1 users shane mxnet nnvm lib libnnvm a pthread lm lopenblas fopenmp lrt L local data1 users shane lib lopencv dnn lopencv ml lopencv objdetect lopencv shape lopencv stitching lopencv superres lopencv videostab lopencv calib3d lopencv features2d lopencv highgui lopencv videoio lopencv imgcodecs lopencv video lopencv photo lopencv imgproc lopencv flann lopencv core lnnpack lopencv core lopencv imgproc lopencv imgcodecs local data1 users shane mxnet deps lib libprotobuf lite a local data1 users shane mxnet deps lib libzmq a usr bin ld cannot find lnnpack collect2 error ld returned 1 exit status make lib libmxnet so Error 1 make Waiting for unfinished jobs usr bin ld cannot find lnnpack collect2 error ld returned 1 exit status make bin im2rec Error 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 followed procedures to install NNPACK at no error reported 2 cd mxnet 3 make j8 error reported What have you tried to solve it 1 Without NNPACK enabled Set USE NNPACK 1 in config mk mxnet can be compiled and imported in python All test py files under image classification run successfully OPENCV was installed and enabled 2 looked up on MxNet source code found Makefile under mxnet is related to this lnnpack in opencv section,,,2017-07-21 03:39:13,2017-07-21 06:53:06
IS,scala api docs missing,Both missing Broken links within,,CodingCat,2017-07-20 19:41:02,2017-07-21 07:29:56
IS,How to compile Amalgamation for android,How to compile Amalgamation for android Environment info Operating System Ubuntu 14 04 64bit Desktop NDK vesion android ndk r13b or android ndk r14b Compiler arm linux androideabi clang MXNet version v0 10 0 or v0 9 3 MXNet commit hash master 799ed45 Error Message Library Output jni libmxnet predict so copy jni libmxnet predict so and libc shared so to android project,,"piiswrong,Piyush3dB,eric-haibin-lin",2017-07-21 08:57:21,2017-07-21 08:58:07
IS,Using AdaGrad and AdaDelta with SVM and Liner Regression in Spark Scala,I want to run SVM and Linear Regression in some datasets using Spark Scala However those methods in Spark only run using Stochastic Gradient Descent with no adaptative learning rates I want to test them using AdaGrad and AdaDelta so I would like to know if I there is a way to use the mxnet implementations of these optimizers along with the SVM and Linear Regression methods implemented in Spark with scala For instance I know in Spark it is possible to set the optimizer to be used by SVM so can I just set the optimizer and use one from mxnet library if the answer is yes how should I instantiate the AdaGrad and AdaDelta optimizers,,"Ldpe2G,sethah",2017-01-14 17:04:45,2017-07-21 16:03:38
PR,add save params and load params to Block,,,piiswrong,2017-07-18 23:45:04,2017-07-21 19:39:30
PR,Fix a typo,,,astonzhang,2017-07-21 18:37:40,2017-07-21 19:42:00
PR,Streamlined fp16 examples,Also added fp16 versions of inception v3 inception v4 and resnext,,ptrendx,2017-07-21 19:44:53,2017-07-21 21:12:34
PR,Fix a typo,,,astonzhang,2017-07-21 19:49:34,2017-07-21 21:13:19
PR,Khatri Rao product,Add Khatri Rao product API Fix unittest mk for macOS,,"jli05,JeanKossaifi,JeanKossaifi,JeanKossaifi,jli05,jli05,JeanKossaifi,jli05,JeanKossaifi,JeanKossaifi,JeanKossaifi,JeanKossaifi,jli05,jli05,jli05,JeanKossaifi,JeanKossaifi,JeanKossaifi,JeanKossaifi,JeanKossaifi,JeanKossaifi,jli05,JeanKossaifi,jli05,jli05,jli05,jli05,JeanKossaifi,JeanKossaifi,JeanKossaifi,jli05,jli05,jli05,JeanKossaifi,JeanKossaifi,JeanKossaifi,jli05,piiswrong,mli,JeanKossaifi,jli05,JeanKossaifi,jli05,jli05,jli05,JeanKossaifi,jli05,jli05,jli05,domdivakaruni,jli05,jli05",2017-06-04 23:58:16,2017-07-21 21:18:26
PR,Mx image2,unittest not available yet A record file hosted on dmlc data might be required before writing test cases,,"zhreshold,piiswrong,piiswrong,piiswrong,piiswrong,zhreshold,zhreshold,zhreshold",2017-07-08 03:03:20,2017-07-22 04:37:31
IS,Amalgamation compilation for Android,I try to compile Amalgamation for Android development I got an Error on include path,,,2017-06-30 09:20:43,2017-07-22 11:05:49
IS,Installation link for R package is out of date broken,This is from copying and pasting the Installation code on the R package page,,thirdwing,2017-07-22 17:54:36,2017-07-22 21:43:22
PR,add messages for homebrew support only,show compile time configs print run time env vars print link to MXNet tutorials print brew and pip package dependencies krishnamurthy,,"bhavinthaker,bhavinthaker",2017-07-17 00:05:34,2017-07-23 01:48:08
PR,remove sparse embedding,Remove sparse embedding op since its definition is confusing compared to the existing embedding op User will simply use dot operator instead e g,,eric-haibin-lin,2017-07-22 22:37:24,2017-07-23 03:06:43
IS,6966 breaks metrics,szha 6966 introduced an issue with shapes that was fixed in 54e4614590789204caa6ab7c9e91fcc8392ef0a0 Besides that it also makes my code fail due to the new check lengths equal function,,"leezu,szha,szha",2017-07-14 07:54:32,2017-07-23 13:24:22
PR,Add NVVM is NNSymbolListInputNames to python,Add NVVM is NNSymbolListInputNames to python is symbol py,,"alues,piiswrong,alues",2017-06-04 17:42:23,2017-07-23 15:10:24
PR,remove where from find packages,,,szha,2017-07-23 07:27:54,2017-07-23 23:23:50
IS,where is the clip function in ndarray module defined,Environment info Operating System macOS Compiler clang Package used Python R Scala Julia Python MXNet version 0 10 1 Python version and distribution python 2 7 Error Message The import lines tries to import clip function from ndarray module however I looked everywhere and no clip function is defined in ndarray As I observed the function is implemented in ndarray cc file so I wildly guess it is should be provided by ctypes ndarray py but I can not find any clip function imported from there either I'm wondering what did I miss to import the clip function from ndarray module How should I fix that I'm doing this in macOS did not confirm the issue in other operating systems,,"liangfu,piiswrong,liangfu",2017-07-22 15:12:44,2017-07-24 01:56:00
IS,gluon tutorial Language modeling with RNN dropout during inference,I believe that in the tutorial during inference dropout is still set whereas it should not be or am I missing something,,szha,2017-07-19 23:16:49,2017-07-24 13:13:01
IS,Cant convert caffe model to mxnet,Cant convert caffe model I treid python convert caffe modelzoo py resnet 50 Error Message Minimum reproducible example python convert caffe modelzoo py resnet 50 Environment info Operating System Ubuntu 14 04 Compiler gcc version 4 8 4 Ubuntu 4 8 4 2ubuntu1 14 04 3 Installed from source MXNet commit hash a242e9f4b77bf705ae679de05642fbe88f870a9f Caffe commit hash 4efdf7ee49cffefdd7ea099c00dc5ea327640f04 Python version and distribution Python 2 7 6,,,2017-07-22 15:56:42,2017-07-24 17:19:17
IS,R package problem with predict function array shapes mismatch,Environment info Operating System Windows 10 Package used R MXNet version 0 10 1 given by R I installed the gpu version by following this tutorial one month ago and it was working fine until now R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 8 x64 build 9200 Error Message 18 18 32 d program files x86 jenkins workspace mxnet mxnet dmlc core include dmlc logging h 304 18 18 32 d program files x86 jenkins workspace mxnet mxnet src operator tensor elemwise op common h 33 Check failed assign dattr vec i Incompatible attr in node mul11 at 1 th input expected 89 got 89 10 Error in symbol infer shape list Error in operator mul11 18 18 32 d program files x86 jenkins workspace mxnet mxnet src operator tensor elemwise op common h 33 Check failed assign dattr vec i Incompatible attr in node mul11 at 1 th input expected 89 got 89 10 Minimum reproducible example This example is a multi label regression on a generalized linear model with poisson likelihood we simulate random data p 32 n label 10 n 89 X matrix runif p n 0 1 p n y matrix round rpois n label n 10 n label n model building data mx symbol Variable data label mx symbol Variable 'label' we create a parametrized linear combination of input variables in fc fc mx symbol FullyConnected data num hidden n label name fc As the loss we write the Negative Log Likelihood associated with an exponential link function vecto symb mx symbol MakeLoss data mx symbol exp fc label fc name poisson devices mx gpu 0 model mx model FeedForward create symbol vecto symb ctx devices X X y y num round 5 array layout colmajor learning rate 0 01 optimizer sgd initializer mx init normal 0 03 array batch size 20 p predict model X array layout colmajor What have you tried to solve it The error appears when using the predict function The same code works fine on another machine same Windows 10 same R version which has the classic cpu R install used install packages of mxnet So the error might be an installation problem with the gpu version or it might be related to 113,,"thirdwing,thirdwing,thirdwing",2017-07-03 16:34:23,2017-07-24 19:15:33
PR,remove untested gpu operators,Removed a few gpu registered operators since they are not tested on gpu due to lack of cast storage dns rsp implementation,,"eric-haibin-lin,reminisce",2017-07-23 22:35:16,2017-07-24 20:05:35
PR,bugfixes for linear algebra operators,Fixes for bugs detected when using the linear operators Can we merge them in before the bigger PRs for all teh CUDA support,,"asmushetzel,asmushetzel",2017-07-24 08:59:32,2017-07-24 20:09:17
PR,Sparse,sorry i made a mistake,,solin319,2017-07-24 12:20:01,2017-07-24 20:09:45
PR,scala package improve the readability of Spark Mxnet implementation,the basic approach is to disassemble the long method into several shorter ones,,"CodingCat,CodingCat,Ldpe2G,yzhliu,CodingCat",2017-07-21 03:26:58,2017-07-25 02:25:13
PR,Fix ndarray aux data issue,1 Returns a sparse tensor is data aux data in form of an NDArray which is a deep copy of the original data blob This behavior is same as scipy is csr matrix 2 Changed functions csr and row sparse for generating sparse tensors accordingly and added unit tests haibin lin,,"reminisce,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,reminisce,eric-haibin-lin,reminisce,eric-haibin-lin,eric-haibin-lin,anirudh2290,reminisce,eric-haibin-lin,reminisce",2017-07-19 02:29:04,2017-07-25 02:32:23
PR,Gluon data pipeline,,,piiswrong,2017-07-21 22:36:46,2017-07-25 03:39:51
PR,Fix a spelling mistake,,,Harmonicahappy,2017-07-25 06:51:09,2017-07-25 10:43:23
PR,Fix 2 spelling mistakes,,,Harmonicahappy,2017-07-25 06:39:10,2017-07-25 10:43:51
IS,Tutorial web page about NDArray has some trivial problems,About just beginner is point of view 1 Using print mxnet ndarray in all section This show only information about array is shape and context not array is elements as in the tutorial site It might be helpful to add comments to use print x asnumpy to check the elements 2 Output Error is shown in slicing section slicing Below the Now let is try whiting to a specific element Output error is show I guess Jupyter is cell is type is not in markdown I guess these tutorials are generated by Jupyter in some ways continuous deployment failed img width 821 alt screen shot 2017 07 16 at 13 31 46 src These might be occured by my environment as below Docker image mxnet python latest 0 10 0 Host macOS Sierra python 3 4 3 jupyter 1 0 0 Regards,,,2017-07-16 04:50:49,2017-07-25 15:41:44
PR,Support K dimensional row sparse tensor,overhauled existing sparse operators with rowsparse tensor Added checks for the ones does not support kdim rowsparse tensor Modified elemwise add sparse retain and cast storage to support dim rowsparse tensor removed set storage shape interface from NDArray It is now set automatically every time aux shape kIdx is set,,"eric-haibin-lin,reminisce,reminisce,reminisce,reminisce,cjolivier01,stefanhenneking,stefanhenneking,stefanhenneking,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,stefanhenneking,piiswrong,stefanhenneking,eric-haibin-lin,eric-haibin-lin,reminisce,eric-haibin-lin",2017-07-24 16:41:31,2017-07-25 19:23:05
PR,Improve sparse ndarray error message,improve sparse ndarray error message throw exception on shared memory functions at slice reshape inherited from NDArray add test for transpose and broadcast to,,eric-haibin-lin,2017-07-24 18:53:30,2017-07-25 19:23:36
PR,Set default value for dtype for alexnet,,,ptrendx,2017-07-25 18:41:22,2017-07-25 19:23:52
PR,im2rec py will use id2 to save high bit if given ID is too long,In IRHeader we have id and id2 to save ID And id2 is used to save higher bit But im2rec py not support that Update to support using id2 when given ID cannot be handle by one unsigned int And when user want to get original ID from id and id2 he just need to call im2rec id encode P S In 64 bits OS python using 64 bits for int in 32 bits OS python using 32 bits for int,,"jiayue666,jiayue666,jiayue666",2017-06-05 18:05:40,2017-07-25 20:26:36
PR,fix ndarray setitem,,,piiswrong,2017-07-25 20:27:16,2017-07-26 08:26:40
PR,Fix incorrect reference sym symbol,Fix variable name from sym to local variable symbol,,knjcode,2017-07-26 04:12:53,2017-07-26 08:27:51
PR,Compilation warnings,Final step in a series of PRs towards reducing compilation warnings In this PR two cast warnings in newly introduced code were fixed And secondly added scripts which monitor time for compilation and generate a summary based on counts and different types of warnings A jenkins nightly job has been started based on this script Currently the job runs on my fork but once this script is merged into the main repo I will change the target of the job to run with this repo Reference Merged Merged Discussion ongoing and others Please review and let me know if you have any suggestions to improve,,rahul003,2017-07-25 23:05:30,2017-07-26 08:28:23
PR,fix DotCsrRspRspImpl error message,piiswrong,,stefanhenneking,2017-07-25 18:05:25,2017-07-26 08:29:19
PR,Add post build email to Jenkinsfile,Add post build action to email if build failed This branch will be used to test this Jenkinsfile,,lxn2,2017-07-26 19:08:14,2017-07-26 19:09:25
PR,R RNN update close 6723,,,thirdwing,2017-07-27 00:20:35,2017-07-27 01:12:07
IS,R grad req is hard coded in model training,See L114,,"thirdwing,thirdwing,jeremiedb,thirdwing",2017-06-16 18:35:19,2017-07-27 01:12:15
PR,Add post build email notification fix Windows CPU build,Fixed post build email notification logic and changed Windows' CPU build logic to delete the right workspace it has been failing because it looked for STORAGE TYPE ID TO STR,,lxn2,2017-07-27 01:24:02,2017-07-27 01:27:21
IS,Can I achieve Sigmoid as the output with binary cross entropy as the loss function,I want to use the Sigmoid as the output with binary cross entropy as the loss function are there some ways I can achieve it OR is there custom version of LogisticRegressionOutput layer or SoftmaxOutput layer so that I can follow the examples to write one Thanks,,CNevd,2017-07-06 05:20:37,2017-07-27 08:36:22
IS,How to Save visualized networks in jpg png,How to Save visualized networks in jpg png in python code Environment info Operating System Ubuntu 16 0 4 Compiler Package used Python MXNet version 0 9 3 Or if installed from source 0 9 3 If you are using python package please provide Python version and distribution 2 7 13,,"Ldpe2G,Ldpe2G",2017-07-27 07:06:47,2017-07-27 16:50:48
PR,Jenkins tests fix,1 Fixes mxnet pip test to run in user mode so its files can be deleted automatically by Jenkins 2 Optimizations to CompilationWarnings job,,rahul003,2017-07-27 01:32:19,2017-07-27 17:24:28
PR,GPU implementation of cast storage dense to csr,piiswrong haibin lin Added gpu implementation for cast storage dense to csr unit tests and benchmark Additionally cast storage interface change to accommodate the need of temporary storage in cuda kernels Benchmarked on g2 8xlarge Speedup vs baseline naive parallel GPU implementation 40x Speedup vs current CPU implementation 4x to 8x warp kernel density context m n time ms m n time ms 100 0 gpu 0 512 50000 6 31 512 100000 10 23 80 0 gpu 0 512 50000 6 87 512 100000 11 96 60 0 gpu 0 512 50000 6 25 512 100000 10 67 40 0 gpu 0 512 50000 5 64 512 100000 9 36 20 0 gpu 0 512 50000 4 80 512 100000 8 34 10 0 gpu 0 512 50000 4 29 512 100000 7 63 5 0 gpu 0 512 50000 4 01 512 100000 7 36 2 0 gpu 0 512 50000 4 01 512 100000 7 15 1 0 gpu 0 512 50000 3 79 512 100000 6 93 block kernel density context m n time ms m n time ms 100 0 gpu 0 512 50000 5 20 512 100000 9 61 80 0 gpu 0 512 50000 5 26 512 100000 9 72 60 0 gpu 0 512 50000 5 17 512 100000 9 53 40 0 gpu 0 512 50000 5 11 512 100000 9 30 20 0 gpu 0 512 50000 4 97 512 100000 9 20 10 0 gpu 0 512 50000 4 92 512 100000 9 05 5 0 gpu 0 512 50000 4 95 512 100000 8 96 2 0 gpu 0 512 50000 4 82 512 100000 8 86 1 0 gpu 0 512 50000 4 68 512 100000 8 66,,"stefanhenneking,reminisce,reminisce,reminisce,stefanhenneking,reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,stefanhenneking,stefanhenneking,eric-haibin-lin,reminisce,piiswrong,stefanhenneking",2017-07-17 23:29:07,2017-07-27 17:25:20
PR,Sparse square sum,This PR implements square sum op for row sparse matrices It actually fuses operators square and sum together to avoid computing dense gradient of the operator sum in backward passes The operator square sum is registered as an internal operator since it is a temporary solution After the functionality of fusing operators is finished in the future this operator may become deprecated Supported use cases square sum rsp axis 0 keepdims False True dns output square sum rsp axis 1 keepdims False dns output square sum rsp axis 1 keepdims True rsp output haibin lin,,"reminisce,eric-haibin-lin,reminisce,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,reminisce,cjolivier01,reminisce,cjolivier01",2017-07-26 18:26:27,2017-07-27 17:26:19
PR,fix doc misleading test conv py in how to,tests python gpu test conv py is not available anymore and is refeering to the mnist example mxnet example image classification see 4806 and 7204,,jqueguiner,2017-07-26 19:53:16,2017-07-27 17:34:25
PR,Add new variables to Jenkinsfile,Add err and currentBuild result so references do not throw errors when they have not been defined,,lxn2,2017-07-27 17:55:28,2017-07-27 17:55:35
PR,Modify and Add documentation for mx nd zeros,1 Added arg name for stype 2 Added documentation to ndarray utils py haibin lin,,"anirudh2290,eric-haibin-lin,eric-haibin-lin",2017-07-25 22:55:50,2017-07-27 17:56:36
PR,Fix broken amalgamation by include missing symbol definition in the expansion list,Fix for issue 7180 Recent updates to the ndarray module introduced the autograd runtime whose declarations were expanded by the python amalgamation script but left out the definition The result was an undefined symbol error when loading the shared library The fix for now is to include the definition file in the expansion list,,Piyush3dB,2017-07-27 18:47:44,2017-07-27 18:57:42
PR,add set ctx to gluon parameter parameter dict,this is for allowing moving the parameter data around contexts without loading from checkpoints or re initializing I found it useful in finetuning models when using part of the weights from a pretrained model,,"szha,szha",2017-07-27 07:11:39,2017-07-27 21:35:26
IS,AssertionError when generate rpn detetcion Why does it show this error and how can I fix it,INFO root GENERATE RPN DETECTION voc 2007 trainval gt roidb loaded from home myname DPNs mxnet example rcnn data cache voc 2007 trainval gt roidb pkl prepare roidb Traceback most recent call last File train alternate py line 107 in module args frequent args kv store args work load list File train alternate py line 35 in alternate train test rpn image set year root path devkit path 'model rpn1' rpn epoch ctx 0 File home myname DPNs mxnet example rcnn tools test rpn py line 32 in test rpn imdb boxes generate detections detector test data voc vis vis File home myname DPNs mxnet example rcnn rcnn rpn generate py line 55 in generate detections for databatch in test data File home myname DPNs mxnet example rcnn rcnn loader py line 94 in next provide data self provide data provide label self provide label File usr local lib python2 7 dist packages mxnet io py line 133 in init assert isinstance data list tuple Data must be list of NDArrays AssertionError Data must be list of NDArrays,,precedenceguo,2017-07-22 05:59:39,2017-07-28 02:24:48
IS,TypeError init got an unexpected keyword argument 'multi precision',Operating System Ubuntu 14 04 MXNet version mxnet cu80 0 10 0 post2 Python version and distribution based on anaconda2 CUDA8 0 cudnn v5 1 I reference the install method from mxnet io use pip install mxnet cu80 and successfully installed And I can sucessfully run the simple MXNet code as follow import mxnet as mx a mx nd ones 2 3 mx gpu b a 2 1 b asnumpy array 3 3 3 3 3 3 dtype float32 And then I do these operation git clone recursive home user deep learning mxnet MXNET HOME ' home user deep learning mxnet' But when I run the example I missed the error TypeError init got an unexpected keyword argument 'multi precision' Terminal messages user user ASUS deep learning mxnet example image classification python train mnist py INFO root start with arguments Namespace add stn False batch size 64 disp batches 100 dtype 'float32' gpus None kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 Traceback most recent call last File train mnist py line 79 in module fit fit args sym get mnist iter File home user deep learning mxnet example image classification common fit py line 190 in fit monitor monitor File home user anaconda2 lib python2 7 site packages mxnet module base module py line 465 in fit optimizer params optimizer params File home user anaconda2 lib python2 7 site packages mxnet module module py line 478 in init optimizer optimizer params File home user anaconda2 lib python2 7 site packages mxnet optimizer py line 128 in create optimizer return Optimizer opt registry name lower kwargs File home user anaconda2 lib python2 7 site packages mxnet optimizer py line 328 in init super SGD self init kwargs TypeError init got an unexpected keyword argument 'multi precision' I searched a lot but can not find method to solve this problem How can I fix this error Thanks for all of you to give me some advice,,"rahul003,ysh329",2017-07-27 04:32:01,2017-07-28 03:45:23
IS,Can MXNet implement scale layer is function of caffe How,I want to change my model of caffe to MXNet and there is a Scale layer in my original model I wonder if MXNet can implement this And how I search with Bing but find nothing helpful,,"Harmonicahappy,Harmonicahappy",2017-07-28 06:49:08,2017-07-28 07:42:49
PR,Add note about about Apache migration,,,lxn2,2017-07-28 19:32:37,2017-07-28 19:33:42
PR,Add Keras Installation Link,,,kevinthesun,2017-07-28 20:43:09,2017-07-28 21:57:35
PR,Add post build email notification fix Windows CPU build,This PR adds a post build action to email a notification if any part of the build for master branch only fails The email is currently configured to my Apache email within the Jenkins job Once our master builds look stable we should change that email to the dev list or set up some build mailing list This PR also includes a small change to delete the correct workspace for the Windows CPU build which will help avoid looking for the ' STORAGE TYPE ID TO STR' not found error,,lxn2,2017-07-28 00:01:27,2017-07-28 23:22:31
PR,Use different array comparison function for useful output,test operator gpu test rnn layer has been failing intermittently Increasing rtol will make this test more lenient,,"lxn2,lxn2,szha,lxn2,szha,cjolivier01,lxn2,szha,lxn2",2017-07-26 18:08:30,2017-07-28 23:26:29
PR,R allow users to use other names than label close 7126,,,thirdwing,2017-07-28 06:52:20,2017-07-28 23:57:53
IS,R allow users to use other names than label,Currently R users can use any names for the input see L163 L164 however a label is still required,,thirdwing,2017-07-20 17:01:51,2017-07-28 23:58:02
PR,R lstm bucketing example,,,thirdwing,2017-07-28 19:22:33,2017-07-29 00:42:19
PR,Modify announcement,,,"kevinthesun,nswamy,szha,piiswrong",2017-07-28 21:13:48,2017-07-29 18:59:36
PR,Quiet regression stacktrace pr,Permits unit tests to test for exception generation without the stack trace output appearing in the logs Introduces wouldiscard stderr ' used as in,,"DickJC123,piiswrong",2017-07-22 00:50:43,2017-07-30 01:44:24
PR,Update README md,,,kli-nlpr,2017-07-30 01:18:25,2017-07-30 01:44:47
PR,Correct Python Docs about tensorboard path,The path of tensorboard in MXNet is wrong Not mxnet tensorboard but mxnet contrib tensorboard,,"ysh329,zihaolucky",2017-07-30 04:16:29,2017-07-30 06:18:37
PR,gluon models,piiswrong refactoring for gluon vision model examples,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2017-07-13 07:35:29,2017-07-30 19:03:15
PR,fix pretty print,,,szha,2017-07-30 20:53:24,2017-07-30 21:43:13
PR,Update README md,,,kli-nlpr,2017-07-30 02:06:31,2017-07-31 00:18:16
PR,Logo Update README md,,,domdivakaruni,2017-07-31 00:19:35,2017-07-31 00:26:15
PR,add reset ctx,,,"piiswrong,szha,piiswrong,szha",2017-07-27 18:44:15,2017-07-31 02:55:56
PR,Fixes for gluon RNN WIP,The Recurrent state shape is wrongly described in the documentation Arguably the behaviour of the current documentation is what we actually want but for now the documentation should describe the current behaviour Furthermore it seems as if some more work is needed to fix the rnn layers when layouts different to TNC are used,,"leezu,szha,leezu",2017-07-31 01:48:15,2017-07-31 02:57:27
PR,MXExecutorSimpleBindEx for backward compatability,added MXExecutorSimpleBindEx since perl package is already using MXExecutorSimpleBind,,"eric-haibin-lin,sergeykolychev,reminisce,eric-haibin-lin,eric-haibin-lin,sergeykolychev,eric-haibin-lin,sergeykolychev,eric-haibin-lin,sergeykolychev,sergeykolychev",2017-07-28 01:03:06,2017-07-31 06:06:01
IS,Custom Operator compute forward twice terminate with error,Hi I am new to Mxnet and recently when I implement some customOps and I get the following errors in the forward image However I print the params in forward entry I find that the forward in my customOp seems to be computed twice and combine the params together randomly image I just create the operator once and i have no idea why it can be computed twice My mxnet version is 0 10 0 and my environment is ubantu14 04 my customOp is the following image Please give me a hint if someone has met the same error Thanks for the helping,,piiswrong,2017-07-28 08:24:08,2017-07-31 07:56:13
IS,How to save a learned network in C API,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Win10 Compiler VS2015 Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message I learn my network with c API I wan to use the Predict C API but it requires I save the network in json format How can I do that in c API Has anyone an example Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"lx75249,lx75249",2017-06-16 21:09:40,2017-07-31 14:43:54
IS,im2rec py can not work,Hello I use im2rec py create lst and rec looks work well until my number of images grow to 1 million when i create rec it stop without any error any suggestion I check memory and disk are enough image update error image Thanks for your time Environment info Operating System win7 64bit Package used Python R Scala Julia python 2 7 MXNet version mxnet 0 9 5 py2 7,,kevinthesun,2017-06-26 18:21:38,2017-07-31 15:38:51
PR,Fix a spelling mistake,,,Harmonicahappy,2017-07-31 11:18:05,2017-07-31 16:12:04
PR,Fix gluon zero grad,self grad is a dictionary for i in self grad iterates over keys,,leezu,2017-07-31 08:54:08,2017-07-31 16:14:53
PR,Doc Add the new doc link to to hybrid md,,,mli,2017-07-31 19:38:13,2017-07-31 21:34:43
IS,mxnet R 3 4 installation for CentOS release 6 9 Final,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System CentOS release 6 9 Final Compiler gcc version 4 9 2 20150212 Red Hat 4 9 2 6 GCC Package used Python R Scala Julia R MXNet version 0 10 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide 3 4 R sessionInfo sessionInfo package NULL R version 3 4 0 2017 04 21 Platform x86 64 redhat linux gnu 64 bit Running under CentOS release 6 9 Final Matrix products default BLAS usr lib64 R lib libRblas so LAPACK usr lib64 R lib libRlapack so locale 1 LC CTYPE en US UTF 8 LC NUMERIC C 3 LC TIME en US UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 7 LC PAPER en US UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C 11 LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 drat 0 1 2 compiler 3 4 0 tools 3 4 0 Error Message Please paste the full error message including stack trace Warning message package mxnet is not available for R version 3 4 0 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 From R console I entered the following commands install packages drat repos drat addRepo dmlc install packages mxnet 2 3 What have you tried to solve it 1 Attempted to install from source and received the following error message when running the make rpkg command mkdir p R package inst mkdir p R package inst libs cp rf lib libmxnet so R package inst libs mkdir p R package inst include cp rf include R package inst include cp rf dmlc core include R package inst include cp rf nnvm include R package inst include echo import Rcpp R package NAMESPACE echo import methods R package NAMESPACE R CMD INSTALL R package make 1 Entering directory root mxnet R package src' make 1 Nothing to be done for all' make 1 Leaving directory root mxnet R package src' No man pages found in package mxnet Error package or namespace load failed for mxnet onLoad failed in loadNamespace for 'mxnet' details call dyn load file DLLpath DLLpath error unable to load shared object ' usr lib64 R library mxnet libs libmxnet so' libopenblas so 0 cannot open shared object file No such file or directory Error loading failed Execution halted 2 3,,thirdwing,2017-07-03 12:52:56,2017-07-31 22:21:47
IS,predict model val produces fewer number of samples compared to original sample size,I have noticed a puzzling phenomenon Every time I predict on a rec file the number of samples becomes fewer and fewer Let is say train and val are the rec files generated by im2rec py Suppose the model is trained as follows model mx model FeedForward create symbol new soft X train eval data val I removed this part as it is optional and will use val for validation outside ctx mx gpu 0 eval metric mx metric accuracy num round 1 learning rate 0 05 momentum 0 9 wd 0 00001 kvstore local array batch size 128 epoch end callback mx callback save checkpoint my model batch end callback mx callback log train metric 150 initializer mx init Xavier factor type in magnitude 2 34 optimizer sgd arg params arg params new aux params inception bn aux params probs predict model val The first time I executed this command ncol probs is the correct number of validation size The second time I execute the command ncol probs has 98 less number of samples compared to the correct validation size The third time I execute the command ncol probs has 189 less number of samples compared to the correct validation size I believe ncol probs is size should be the same as the validation size no mater how many times I run the predict command Can you help explain what went wrong here or what I can do to ensure I get the correct number of samples back Thanks,,"thirdwing,thirdwing",2017-07-06 22:39:25,2017-07-31 22:42:46
PR,add getitem to container layers,This is to add getitem to the container layers so that it is easier to locate the child layers along with their weights For example,,"szha,piiswrong,piiswrong",2017-07-31 09:02:28,2017-07-31 22:58:23
PR,R DOC update vignettes make sure all code in vignettes can run without error,,,thirdwing,2017-07-31 22:09:48,2017-08-01 01:09:14
PR,add support for port package manager,add support for port package manager along with brew show compile time configs print run time env vars print link to MXNet tutorials print brew port and pip package dependencies krishnamurthy,,"bhavinthaker,rahul003,bhavinthaker,cjolivier01,bhavinthaker,bhavinthaker",2017-07-23 01:58:09,2017-08-01 01:49:36
PR,Fix a spelling mistake,,,Harmonicahappy,2017-08-01 02:34:07,2017-08-01 03:36:23
PR,Jenkins correctly cleans build when incremental build failed,Fix for 7272,,b0noI,2017-08-01 00:20:33,2017-08-01 03:37:14
PR,Gluon RNN fixes for seqlen 1,In unroll we assume that format sequence returns a list L202,,"leezu,piiswrong,leezu",2017-07-31 03:47:57,2017-08-01 03:40:01
PR,Update cub for CUDA 9,,,ptrendx,2017-07-31 19:27:43,2017-08-01 03:40:15
PR,Update NNVM,,,tqchen,2017-07-28 05:02:27,2017-08-01 04:42:46
PR,fix random sized crop,,,"zhreshold,piiswrong,zhreshold",2017-07-24 03:27:48,2017-08-01 04:51:03
PR,Operator add n for row sparse ndarrays,1 Added add n mxnet ndarray add n operator for row sparse ndarrays 2 Added IdentityComputeEx for dense sparse ndarrays as FComputeEx which is required in the backward pass of add n operator This work may overlap with is but I need this function to make add n backward pass work for row sparse inputs 3 Fixed the bug of not filling zeros for the output of square sum forward FComputeEx function when the input sparse ndarray is not storage initialized in this PR 4 Removed test cast storage ex from test operator gpu py since it has not been completely implemented haibin lin,,"reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,reminisce,reminisce",2017-07-28 22:10:31,2017-08-01 05:03:13
IS,jenkins incubator mxnet pipeline is broken on the master,output amalgamation Running shell script tests ci build ci build sh cpu make C amalgamation USE BLAS openblas MIN 1 WORKSPACE home jenkins jenkins slave workspace amalgamation CI DOCKER EXTRA PARAMS COMMAND make C amalgamation USE BLAS openblas MIN 1 CONTAINER TYPE cpu BUILD TAG jenkins incubator mxnet master 83 NODE NAME mxnet3 DOCKER CONTAINER NAME mx ci cpu PRE COMMAND tests ci build with the same user Building container mx ci cpu Sending build context to Docker daemon 59 39 kB Step 1 9 FROM ubuntu 14 04 4a2820e686c4 Step 2 9 COPY install ubuntu install core sh install Using cache a2648bbfb7e4 Step 3 9 RUN install ubuntu install core sh Using cache bd54bb963df0 Step 4 9 COPY install ubuntu install python sh install Using cache b79f713d5145 Step 5 9 RUN install ubuntu install python sh Using cache 570842ae2af2 Step 6 9 COPY install ubuntu install scala sh install Using cache 6f8e5f2011ba Step 7 9 RUN install ubuntu install scala sh Using cache e2b49cb08c67 Step 8 9 COPY install ubuntu install r sh install Using cache f5f990f7f4ac Step 9 9 RUN install ubuntu install r sh Using cache d795cd130ad6 Successfully built d795cd130ad6 Running 'make C amalgamation USE BLAS openblas MIN 1' inside mx ci cpu Adding group jenkins' GID 1001 Done make Entering directory workspace amalgamation' g std c 11 Wno unknown pragmas Wall DMSHADOW USE CBLAS 0 DDISABLE OPENMP 1 DMSHADOW USE CUDA 0 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DDMLC LOG STACK TRACE 0 DMSHADOW FORCE STREAM DMXNET USE OPENCV 0 DMXNET PREDICT ONLY 1 fPIC o mxnet predict all o c mxnet predict all cc mxnet predict all cc 42 37 fatal error io threaded input split h No such file or directory include io threaded input split h compilation terminated make Leaving directory workspace amalgamation' make mxnet predict all o Error 1,,"b0noI,b0noI",2017-07-31 20:22:59,2017-08-01 16:16:14
IS,Bug in gluon dcgan py example,Running MXNet on EC2 G2 billionXlarge instance Gluon dcgan py example give this error with Python 3 Traceback most recent call last File dcgan py line 203 in module visual 'gout' fake asnumpy name os path join outf 'fake img iter d png' iter File dcgan py line 32 in visual fill buf buff i img X shape 1 3 File dcgan py line 22 in fill buf buf sy sy shape 1 sx sx shape 0 img TypeError slice indices must be integers or None or have an index method For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python2 dcgan py 2 3 What have you tried to solve it 1 whining on Github 2 3,,zackchase,2017-08-01 09:12:01,2017-08-01 17:05:01
PR,fixed dcgan to use integer division so it wo not die horribly in Python3 making users want to die as well,Fixed dcgan example for Python3 compatibility Closes 7285,,"zackchase,zackchase",2017-08-01 09:43:13,2017-08-01 17:05:01
PR,change random sized crop lower bound to 0 08,,,zhreshold,2017-08-01 05:37:07,2017-08-01 17:08:42
PR,fix dataloader length bug,,,,2017-08-01 07:27:37,2017-08-01 17:09:15
PR,Properly check argument types in ndarray save to avoid segfaults,Prior to this change you could do the easy mistake of accidentally passing in a ndarray as the second argument of mx nd save instead of a dict or list This would lead to a segmentation fault Here is the example Basically if data is a ndarray we can still iterate over it This means we iterate over the individual rows val will be a temporary ndarray python object holding a reference to a new NDarray C object in handle The problem is that we keep a reference to val handle right before val and therefore also val handle will be gargabe collected,,"tdomhan,piiswrong,tdomhan,piiswrong,tdomhan,tdomhan,tdomhan",2017-06-30 14:41:56,2017-08-01 18:00:31
PR,Add support for port package manager on macOS,add support for port package manager along with brew show compile time configs print run time env vars print link to MXNet tutorials print brew port and pip package dependencies add user is PATH before the script is PATH remove the test files if the test passes,,"bhavinthaker,rahul003",2017-08-01 01:48:54,2017-08-01 18:09:28
PR,Require Mouse v2 1 0 with explicit v,The ExtUtils MakeMaker version that comes with perl 5 24 0 7 10 01 does not like three part versions without the initial v So it gives me the following message And then installation does not proceed smoothly Adding the v solves this,,sergeykolychev,2017-07-23 21:50:11,2017-08-01 18:10:16
IS,How to use regularization for a feed forward neural network in R using mxnet,Environment info Operating System Windows Package used R Installed using the binaries Which parameter should I use to do regularization L2 for a feed forward neural network Where should I implement this I could find references to the weight decay parameter wd But I am not sure where to include it An example would be much appreciated,,thirdwing,2017-07-20 17:01:56,2017-08-01 18:35:51
PR,GPU implementation of cast storage dense to rsp,piiswrong haibin lin Added gpu implementation for cast storage dense to rsp unit tests and benchmark Benchmarked on g2 8xlarge Speedup vs baseline naive parallel GPU implementation 10x dense 70x sparse Speedup vs current CPU implementation 3x sparse to 10x dense Additional notes haibin lin benchmark here only done for n 100 000 Additional benchmarking as requested in 7081 with real datasets and larger matrices later when dot operator and elemwise sum functionality is implemented on GPU priority haibin lin I added a util function in src operator mxnet op h to get the cuda DeviceProperties to read the warpSize as discussed in 7081 The GPU kernels will need some changes to work for warp sizes other than 32 see TODO GPU block kernel density context m n time ms m n time ms 100 0 gpu 0 512 50000 4 16 512 100000 6 91 80 0 gpu 0 512 50000 3 72 512 100000 5 95 60 0 gpu 0 512 50000 3 38 512 100000 5 36 40 0 gpu 0 512 50000 2 60 512 100000 4 64 20 0 gpu 0 512 50000 2 06 512 100000 3 53 10 0 gpu 0 512 50000 1 77 512 100000 2 87 5 0 gpu 0 512 50000 1 73 512 100000 2 53 2 0 gpu 0 512 50000 1 59 512 100000 2 47 1 0 gpu 0 512 50000 1 53 512 100000 2 38 CPU kernel density context m n time ms m n time ms 100 0 cpu 0 512 50000 35 29 512 100000 74 21 80 0 cpu 0 512 50000 31 05 512 100000 59 23 60 0 cpu 0 512 50000 22 39 512 100000 49 95 40 0 cpu 0 512 50000 18 82 512 100000 35 03 20 0 cpu 0 512 50000 11 84 512 100000 23 66 10 0 cpu 0 512 50000 7 77 512 100000 16 63 5 0 cpu 0 512 50000 6 56 512 100000 16 95 2 0 cpu 0 512 50000 3 92 512 100000 7 33 1 0 cpu 0 512 50000 4 19 512 100000 6 61,,"stefanhenneking,eric-haibin-lin,stefanhenneking,reminisce,reminisce,reminisce,reminisce,stefanhenneking,eric-haibin-lin,stefanhenneking,reminisce,stefanhenneking,eric-haibin-lin,stefanhenneking,reminisce,eric-haibin-lin,reminisce,stefanhenneking,eric-haibin-lin,stefanhenneking,stefanhenneking,eric-haibin-lin",2017-07-27 19:24:47,2017-08-01 21:38:09
IS,NNVM Master Merge CheckList,This is a acting items for Checklist before we merge NNVM branch into master x Remove backward index in gradient pass Verify all the code in example folder run x Verify existing models can be loaded by nnvm branch Fix amalgamation code under nnvm Verify all packages works correctly x Make python setup py install work correctly 1 R 2 Scala 3 Julia Our goal is to bring this before Nov 20th Please edit this to include additional items while being reasonable and specific e g more test cases is not specific add what testcase is specific,,"tqchen,piiswrong,tornadomeet,sxjscience,sxjscience,leopd,Ldpe2G,tornadomeet,tqchen,tqchen,sxjscience,tqchen,yzhliu,kevinthesun,kevinthesun",2016-11-11 01:27:54,2017-08-01 21:45:17
PR,change unnecessarily longer array,simplifies code by removing the possibility of unused variables This confused g earlier as some variables were uninitialized and unused Removes three compiler warnings,,"rahul003,szha,rahul003,szha,rahul003,piiswrong,rahul003,szha,rahul003,szha,rahul003",2017-07-28 18:19:11,2017-08-01 21:57:53
PR,Fix typoes,,,astonzhang,2017-08-01 23:05:39,2017-08-01 23:09:57
PR,fix initialization warning by g,With reference to Fixed the warning now by initializing the array Created a new PR so as to remove references to old way of fixing,,rahul003,2017-08-01 22:11:46,2017-08-01 23:10:18
PR,Change variable name from defered init to deferred init,,,"astonzhang,szha",2017-08-01 23:42:56,2017-08-01 23:50:35
IS,Gluon Minpy and NN,It seems all those three packages are developed to suppport dynamical graph which can provide flexible programming interface as pytorch Will Minpy and NN be discarded And will mxnet continue to complete Gluon without moving to develop a new package I 'm a little confused by these packages,,jermainewang,2017-08-01 09:31:39,2017-08-02 04:45:37
PR,Sparse retain improvement,1 Added one kernel for sparse retain op The kernel is invoked when the input row sparse tensor is actually dense i e input rsp storage shape input rsp shape 2 Moved all sparse retain code to file sparse retain inl h and sparse retain cc for better readability and maintenance haibin lin,,"reminisce,reminisce",2017-08-01 23:42:44,2017-08-02 04:46:20
PR,finetune now includes short hint how to run the script on windows,Time to time this question raised in different places 45449015 so this information has been added to the documentation,,b0noI,2017-08-01 23:26:31,2017-08-02 04:50:27
PR,Update ndarray py,,,piiswrong,2017-08-01 22:54:58,2017-08-02 05:45:56
PR,update sparse ndarray api,bug fix for mx nd zeros stype wouldefault' support mx nd empty stype plan to change SparseNDArray class to BaseSparseNDArray class to indicate it is an abstract class support mx nd array with CSRNDArray RowSparseNDArray as input,,"eric-haibin-lin,reminisce,eric-haibin-lin,eric-haibin-lin",2017-07-30 04:19:38,2017-08-02 06:20:15
IS,Cross entropy cost for SoftmaxOutput as evaluation metric,I am wondering if there really is no implementation of the cross entropy cost function as a evaluation metric when doing multi class classification For the binary case i e logistic regression it is provided I do understand that for training it is not necessary to calculate the cross entropy but still it is a valuable evaluation metric to monitor the training process of a multi class classifier I did some extensive googling but could not find any implementation of the cross entropy cost as a evaluation metric in Python as mentioned above only the binary case is implemented So is there really no implementation of it available,,"hesseltuinhof,hesseltuinhof",2017-08-01 12:38:13,2017-08-02 08:39:28
PR,Fix mis of tabs and spaces,,,larroy,2017-08-02 15:22:41,2017-08-02 15:23:01
PR,Fix mix of tabs and spaces,,,larroy,2017-08-02 15:23:49,2017-08-02 20:39:09
PR,Change variable function names defered deferred,,,astonzhang,2017-08-01 23:51:31,2017-08-02 20:41:33
PR,R RNN bucketing with multiple devices,jeremiedb,,thirdwing,2017-08-02 18:08:34,2017-08-02 21:51:34
PR,gluon lstm crf example,LSTM CRF example on toy data,,"szha,piiswrong,szha",2017-07-30 20:37:24,2017-08-02 21:52:18
PR,add doc for gluon sym nd contrib,,,"szha,piiswrong,piiswrong,piiswrong,szha,szha",2017-08-01 08:06:38,2017-08-02 23:19:42
PR,Fix install error for jupyter,,,alues,2017-08-02 14:32:30,2017-08-03 00:11:08
IS,test gluon rnn test rnn cells hang,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System DL AMI Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 861e929cec8fa8fbab06884d9605debb74cd7217 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 run nosetests 2 7 v tests python unittest it hangs when it comes to test gluon rnn test rnn cells 2 3 What have you tried to solve it 1 run nosetests 2 7 v tests python unittest test gluon rnn py but it successfully passes the test 2 3,,"eric-haibin-lin,szha,szha,piiswrong,szha",2017-08-01 21:46:53,2017-08-03 05:50:56
PR,reduce model zoo test size,this should fix 7293,,"szha,eric-haibin-lin,szha,piiswrong,szha",2017-08-02 21:20:20,2017-08-03 05:50:56
IS,Update arg arrays from trained model params with names matched in Python,Hi I would like to do the equivalent in Python to the below R commands updating arg arrays with names matched any method in Python there Many thanks Trying to translate from R to Python mx exec update arg arrays testExecutor model arg params match name TRUE mx exec update aux arrays testExecutor model aux params match name TRUE mx exec update arg arrays testExecutor list data mx nd array t test x match name TRUE,,,2017-08-03 10:23:28,2017-08-03 15:11:56
PR,add backward is train False and always mode for dropout,,,"piiswrong,piiswrong,piiswrong,piiswrong,leezu",2017-08-02 04:56:40,2017-08-03 17:36:12
PR,Fix module tutorial,,,madjam,2017-08-03 18:20:57,2017-08-03 19:53:07
PR,assert size eq between shared module execs and context,piiswrong,,"formath,piiswrong,formath,sxjscience,formath,sxjscience,formath,formath,sxjscience,formath",2017-07-28 08:14:35,2017-08-03 20:01:45
PR,Add document for BilinearSampler Op,Please review and merge,,"dsqx71,piiswrong,dsqx71",2017-07-26 09:10:08,2017-08-03 20:03:51
PR,Fix bug in symbolic ConvRNN,According to the suggestions in 7264 I made the following changes Remove forget bias in ConvLSTM Remove the hard code about conv layout Add interface for initializer Add conv forward and remove repetitive codes in call function,,"dsqx71,dsqx71",2017-08-01 07:19:12,2017-08-03 20:04:53
PR,fix py3 compatibilities,,,mwbyeon,2017-08-02 09:38:02,2017-08-03 20:05:43
PR,Python CustomOp with parameter failed when call infer shape entry in,operator py 7057,,"tornadomeet,piiswrong",2017-07-16 07:29:56,2017-08-03 20:06:13
PR,Added sparsity functionality with tests,,,"Guneet-Dhillon,piiswrong,piiswrong,Guneet-Dhillon,Guneet-Dhillon,piiswrong,piiswrong,piiswrong,Guneet-Dhillon,Guneet-Dhillon,Guneet-Dhillon,piiswrong,arank,Guneet-Dhillon",2017-07-21 00:41:47,2017-08-03 20:08:15
PR,Add empty commit DO NOT MERGE,DO NOT MERGE,,lxn2,2017-07-28 21:57:58,2017-08-03 20:08:34
PR,improve convert symbol py add support to SUM with coeff,,,"Ldpe2G,Ldpe2G,Ldpe2G",2017-07-20 09:39:50,2017-08-03 20:10:53
PR,add mobilenet,This mobilenet implements article MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications It speeds up training and scoring in ImageNet 2012 with CPU version but not GPU I will try to implemented depth wise conv using cuda kernel,,"qingzhouzhen,qingzhouzhen,qingzhouzhen",2017-07-20 13:45:31,2017-08-03 20:12:29
PR,cpp package add lr scheduler,1 add lr scheduler to optimizer 2 move learning rate and weight decay to optimizer 3 fix examples,,"CNevd,lx75249,lx75249,lx75249,CNevd,lx75249,CNevd,CNevd,lx75249,CNevd",2017-06-30 04:36:44,2017-08-03 20:18:08
PR,Change node labels to match new slave configurations,For our new CI on Apache we configured slaves with more specific names Without changing the node labels in this file to match the configuration builds will fail,,lxn2,2017-07-14 17:30:20,2017-08-03 20:18:29
PR,Change node labels to match new slave configurations,For our new CI on Apache we configured slaves with more specific names Without changing the node labels in this file to match the configuration builds will fail,,lxn2,2017-07-14 17:12:55,2017-08-03 20:18:39
PR,WIP Add apache license header to each file,TODO lists how to deal with the mkl license see R package license Pass CI,,"mli,bhavinthaker,mli,glingyan,thirdwing,piiswrong,piiswrong",2017-07-06 21:25:08,2017-08-03 20:18:59
PR,update RCNN example for BaseModule init params,It seems that a new argument allow extra is added into BaseModule init params which makes the old RCNN example unable to run So I append it into the example,,"linmx0130,piiswrong",2017-06-26 06:15:49,2017-08-03 20:32:42
PR,support convert mtcnn and MobileNet model,support convert mtcnn model and fix MobileNet fix bn index is not initialized issue,,joey2014,2017-07-07 21:54:07,2017-08-03 20:33:41
PR,Fixed visualization code error for bi directional lstms,Refer This is caused because num outputs is decreased over the sequence and it becomes ve for the backward lstm It is not clear why it is decreased as the size estimate is the same for all the time steps Hence I have removed that line the shape is determined by the 0th index key this assumes that the shape remains the same across time steps Also refer to,,gurumurthys,2017-06-12 23:23:59,2017-08-03 20:34:46
PR,fix example rnn Speedometer auto reset False,If the Speedometer resets the eval metric and due to an unlucky number of batches the end of batch is reached immediately after i e there are no batches left anymore the Perplexity will throw an ZeroDivisionError as eval metric num inst 0,,leezu,2017-06-13 03:19:23,2017-08-03 20:35:18
PR,Fixed Python 3 compatibility with sys version info,Updated as suggested by mxnet earlier,,,2017-05-28 03:18:21,2017-08-03 20:36:41
PR,Test macOS installation,Created another pull request for 6231 after we reverted it due to broken docs This PR adds logic to test the python installation guide for MacOS CPU on Travis We test the installation instructions in a clean environment via pip virtualenv and from source There are two tasks that I have added to Travis as shown in the travis yml file 1 installation packaged test Tests instructions for installing MXNet via either 1 virtualenv 2 pip Runs a simple Python sanity test create NDArray Takes 2 minutes 2 installation source test Tests instructions for installing MXNet from source Runs a simple Python sanity test create NDArray Takes 16 minutes Needed to be put in a second task because it needs a clean environment Note 1 I added logic to run installation tasks only if the diff between this branch and the mxnet is master branch contains install md 2 The installation source test task fails in Travis because my changes to install mxnet osx python sh needs to be merged 3 The motivation behind this PR is to test the installation guide rather than the package 4 I have built the docs and manually checked that the docs page looks okay,,"lxn2,piiswrong",2017-06-19 22:17:49,2017-08-03 20:43:38
PR,Test installation of pip pre,Quick fix to change this nightly test to install pip packages using pre release A different task should add coverage for other packages later,,"lxn2,szha,lxn2",2017-08-02 17:46:12,2017-08-03 20:44:55
PR,DOC R update docs from mx symbol MakeLoss close 2922,,,thirdwing,2017-08-03 21:50:12,2017-08-03 22:22:10
IS,custom loss symbol in R Python,I tried to create a custom loss symbol in R or Python I found an example using MakeLoss in Python at I tried to create a network to minimize MSE for linear regression but never work Could anyone please provide an example Thanks,,"precedenceguo,precedenceguo,precedenceguo,precedenceguo,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing",2016-09-24 23:37:52,2017-08-03 22:35:55
IS,R Is the mxnet always minimize the custom loss function,Package used Python R Scala Julia R Before I write my own custom loss function with mx metric custom I checked the I find two functions mx metric accuracy is used as a metric for classification and mx metric rmse is used as a metric for regression So far as I know the model should minimize the loss function like MSE But minimize accuracy indicates a bad model When I test the mx metric accuracy the increasing accuracy of each epoch indicates the mxnet know how to treat the loss function This confuses me when I write my own loss function I do not know when the mxnet will maximum the loss function and when the mxnet will minimize the loss function Could you explain this stuff Thank you,,"thirdwing,thirdwing,thirdwing",2016-12-09 03:39:05,2017-08-03 22:36:40
IS,R session crashing after GPU VRAM reached the maximum,Hello everyone I am trying to task mxnet R package 10 1 with a regression mxnet has been compiled on ubuntu 16 04 using the official tutorial using a titan xp nvidia gpu and accessed with Rstudio server The data is a genomic matrix 300 samples x 4457 features The network is trained until memory reaches the max 12go VRAM on the gpu followed using nvidia smi and then makes R crashes unexpectedly without any error message at all 12go are reached between 50 and 100 epochs I think the mx set seed 1234 is not passed to the gpu for some reason The same model using cpu uses around 8 9 go of RAM and fits normally Here is a code that reproduce the crash on the machine with very quick and very dirty data generation The GPU driver are the last official ones found here The compilation of mxnet from sources did not seem to cause any issue I did not tried to re compile but could it be the problem Any advise would be greatly appreciated I am opening another issue for the mx set seed on gpu Environment info Operating System ubuntu 16 04 Compiler gcc Package used Python R Scala Julia R MXNet version 10 1 Or if installed from source MXNet commit hash git rev parse HEAD R sessionInfo R version 3 4 0 2017 04 21 Platform x86 64 pc linux gnu 64 bit Running under Ubuntu 16 04 2 LTS Matrix products default BLAS usr lib openblas base libblas so 3 LAPACK usr lib libopenblasp r0 2 18 so locale 1 LC CTYPE en US UTF 8 LC NUMERIC C LC TIME en US UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 LC PAPER en US UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C Error Message No error message just crashing and restarting,,"thirdwing,thirdwing,thirdwing",2017-06-22 14:31:39,2017-08-03 23:01:56
IS,Gluon Autograd is train documentation,As far as I understand AutogradRuntime Get IsTraining is used throughout the NDArray API to determine how to set the OpContext argument to the functions with autograd record sets the training attribute of AutogradRuntime to True and resets it when leaving the block Is this correct This should be documented The current Gluon documentation does not state anything about how OpContext is train is determined even though this is very relevant to the BatchNorm and Dropout operators If you confirm my understanding I can add a short note to the Gluon Tutorial If a documentation page for autograd is added it would make sense to also mention it there,,"leezu,leezu,leezu",2017-08-02 12:32:14,2017-08-03 23:06:24
PR,Add KEYS file,,,,2017-07-28 19:53:41,2017-08-03 23:16:59
PR,Gluon toy ssd,A minimal toy gluon ssd example with mnist Train in 10min using single k80,,zhreshold,2017-07-25 18:14:00,2017-08-03 23:48:22
PR,Mobilenet,Modify conv 4 in line 17 of mobilenet py last time I commited the wrong file but all my experiments is done by this one,,qingzhouzhen,2017-08-04 01:06:22,2017-08-04 03:39:53
PR,Scala Make Module Api sync with Python interface,javelinjs help to review,,"Ldpe2G,CodingCat,CodingCat,Ldpe2G,yzhliu,Ldpe2G,madjam,Ldpe2G,Ldpe2G,Ldpe2G",2017-07-29 08:41:46,2017-08-04 15:46:51
PR,fix cpplint,,,szha,2017-08-04 05:29:09,2017-08-04 17:12:32
PR,Add Autograd doc to gluon page,I can not build the docs locally so this is currently untested Fixes 7306 In reply to issuecomment 319790394,,"leezu,piiswrong,leezu",2017-08-03 23:02:28,2017-08-04 17:14:44
PR,Fixes improper deconv workspace alloc,The size of the temp workspace required by deconv Backward should be the max needed by the two kernels employed by this operation namely cudnnConvolutionForward and cudnnConvolutionBackwardFilter Includes some code cleanup to both conv and deconv operators to prevent similar errors from reappearing,,DickJC123,2017-08-03 22:35:15,2017-08-04 17:22:40
PR,Attempting to add Perl interface to Apache CI,,,"sergeykolychev,lxn2,sergeykolychev,lxn2,sergeykolychev,lxn2,sergeykolychev,sergeykolychev,sergeykolychev,eric-haibin-lin,sergeykolychev,lxn2,sergeykolychev,lxn2,sergeykolychev",2017-07-23 19:52:24,2017-08-04 18:57:32
IS,install mxnet on R make rpkg Error package or namespace load failed for 'mxnet',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos 6 4 Compiler shell Package used Python R Scala Julia R 3 4 MXNet version 0 1 0 last Or if installed from source git clone recursive Python version and distribution 2 6 If you are using R package please provide R sessionInfo 3 4 Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 install gcc4 8 5 wget 2 install openblas git clone 3 install opencv git clone 4 in mxnet docment make j nproc have passed What have you tried to solve it make rpkg The compiler does not pass,,"thirdwing,thirdwing,thirdwing",2017-06-28 08:44:31,2017-08-04 21:08:34
IS,Questions about distributed training on multiple machines,Hi I tried to run the mnist example provided in mxnet on a set of ssh able machines I run the program using the following command python tools launch py n 2 H hosts python train mnist py network mlp num epochs 1 kv store dist sync Everything seems work well But I have two concerns 1 How can I get a global view this distributed process The log generated by the workers is similar to what I got if I run the same workload under the single machine setting Even if I set the PS VERBOSE 1 I can only know that all the servers and workers have registered with the scheduler and can talk with each other Take the mnist as an example there are 60000 samples in the training set how can I know these workers are coorperating together to get the work done instead of running seperately FYI here is the log generated by one of the workers when run the distributed training example 2 I found that it is quite convienent to run the workload on a set of cpu machines or a set of gpu machines However I wonder is there a way to run the workload on a mixed of cpu and gpu workers For example there are 5 workers 2 of them have 2 CPUs on each of them others have 3 GPUs on each of them Is that possbile for us to launch the distributed training task on these 5 machines Could you please give a pointer of how to do that Thanks in advance Bo,,"madjam,madjam",2017-07-12 21:37:45,2017-08-04 22:31:12
PR,Build versioning website,Versioning website for apache migration Those two scripts are not directly used to build mxnet docs They are used in apache jenkins job to apply versioning feature to built docs,,"kevinthesun,lxn2,lxn2,lxn2",2017-08-04 21:06:39,2017-08-04 22:41:28
PR,grad add has been replaced with elemwise add,,,"b0noI,piiswrong,b0noI",2017-08-03 23:16:48,2017-08-05 00:04:13
PR,Add DISCLAIMER and lxn2 GPG keys,Adding DISCLAIMER file as required of Apache Incubator requirements Also added my GPG keys for signing the upcoming release,,lxn2,2017-08-05 00:14:50,2017-08-05 00:32:07
PR,Fix gluon bottleneck v2,szha,,kevinthesun,2017-08-04 20:37:34,2017-08-05 05:03:58
PR,R fix mx symbol min close 7219,,,thirdwing,2017-08-04 23:19:50,2017-08-05 06:34:04
IS,R How to include a minimum function in MakeLoss,Hi I am trying under R to use a constant as minimum for instance in a custom MAE loss function to cap it at 1 How do I do that Tried lro2 mx symbol MakeLoss mx symbol min mx symbol abs mx symbol Reshape fc3 shape 0 label 1 0 name lro2 Which returns Error in mx varg symbol min list symbol cc 302 RCheck failed keys i length 0 Non Symbol parameters is only accepted via key value style Thanks,,"thirdwing,thirdwing,thirdwing",2017-07-27 14:35:06,2017-08-05 06:34:10
PR,refactor gluon trainer,,,piiswrong,2017-08-04 19:46:41,2017-08-06 04:28:25
IS,gluon Trainer does not accept Optimizer object only str,The gluon Trainer is supposed to accept either a string name of an Optimizer or an actual Optimizer object However if you try to pass it an Optimizer object it does not handle it correctly expecting a string Modifying the Trainer init optimizer method as follows fixes it although I'm not sure if this is the way it wants to be handled I would also recommend making the optimizer params argument of the Trainer constructor optional since they are not needed if you provide a full Optimizer object,,piiswrong,2017-08-02 15:59:30,2017-08-06 14:46:07
PR,Perl Fix for CI,lxn2 This fixes perl tests jenkins perl run is successful please merge,,sergeykolychev,2017-08-04 23:48:47,2017-08-06 20:28:35
IS,AI MXNet 'make test' failed,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 3 LTS GNU Linux 4 10 0 30 generic x86 64 Compiler gcc version 5 4 0 20160609 Ubuntu 5 4 0 6ubuntu1 16 04 4 Package used Python R Scala Julia perl 5 version 22 subversion 1 v5 22 1 built for x86 64 linux gnu thread multi with AI MXNet 1 0101 AI MXNetCAPI 1 0101 AI NNVMCAPI 1 01 MXNet version 0 10 0 Or if installed from source MXNet commit hash b2360f4ff5cccbcf09a780d8065ecc4a64ce470f If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace AI MXNet 'make test' failed In next test 1 test autograd t 2 test model parallel t I attach detail log files test autograd log txt test model parallel log txt Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Uninstall AI MXNet AI MXNetCAPI AI NNVMCAPI and 'make clean' for MXNet 2 Rebuild MXNet with GoogleTest 3 Reinstall AI NNVMCAPI and AI MXNetCAPI 4 Rebuild AI MXNet Now I'm here,,sergeykolychev,2017-08-06 13:19:15,2017-08-06 20:56:17
IS,Compilation failure of module ctc loss cu,Environment info Operating System Linux Mint 18 2 Ubuntu 16 04 Compiler nvcc NVIDIA R Cuda compiler driver Copyright c 2005 2015 NVIDIA Corporation Built on Tue Aug 11 14 27 32 CDT 2015 Cuda compilation tools release 7 5 V7 5 17 Building from source attempting to cloned from GitHub 2017 July 30 MXNet commit hash git rev parse HEAD 66df7c819d51059f30f9002c479f33324e9c72a5 Error Message usr include c 5 bits stl iterator base types h 154 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator contrib ctc loss inl h 169 here implicit generation of mxnet op CTCLossOp xpu CTCLossOp with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of class mxnet op CTCLossOp xpu with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of mxnet op CTCLossOp xpu CTCLossOp mxnet op CTCLossParam with xpu mxnet gpu src operator contrib ctc loss cu 39 here usr include c 5 bits stl iterator base types h 155 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator contrib ctc loss inl h 169 here implicit generation of mxnet op CTCLossOp xpu CTCLossOp with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of class mxnet op CTCLossOp xpu with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of mxnet op CTCLossOp xpu CTCLossOp mxnet op CTCLossParam with xpu mxnet gpu src operator contrib ctc loss cu 39 here usr include c 5 bits stl iterator base types h 156 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator contrib ctc loss inl h 169 here implicit generation of mxnet op CTCLossOp xpu CTCLossOp with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of class mxnet op CTCLossOp xpu with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of mxnet op CTCLossOp xpu CTCLossOp mxnet op CTCLossParam with xpu mxnet gpu src operator contrib ctc loss cu 39 here usr include c 5 bits stl iterator base types h 157 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator contrib ctc loss inl h 169 here implicit generation of mxnet op CTCLossOp xpu CTCLossOp with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of class mxnet op CTCLossOp xpu with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of mxnet op CTCLossOp xpu CTCLossOp mxnet op CTCLossParam with xpu mxnet gpu src operator contrib ctc loss cu 39 here usr include c 5 bits stl iterator base types h 158 error name followed by must be a class or namespace name detected during instantiation of class std iterator traits Iterator void with Iterator int 163 here instantiation of class std iterator traits Iterator with Iterator int src operator contrib ctc loss inl h 169 here implicit generation of mxnet op CTCLossOp xpu CTCLossOp with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of class mxnet op CTCLossOp xpu with xpu mxnet gpu src operator contrib ctc loss inl h 137 here instantiation of mxnet op CTCLossOp xpu CTCLossOp mxnet op CTCLossParam with xpu mxnet gpu src operator contrib ctc loss cu 39 here 5 errors detected in the compilation of tmp tmpxft 00004619 00000000 16 ctc loss compute 52 cpp1 ii ctc loss cu appears to have the same error All other modules are compiling correctly CPU only version compiles without any problems Minimum reproducible example usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 compute 52 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home james Desktop ML mxnet mshadow I home james Desktop ML mxnet dmlc core include fPIC I home james Desktop ML mxnet nnvm include I home james Desktop ML mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local include opencv I usr local include fopenmp DMXNET USE LAPACK DMSHADOW USE CUDNN 1 D MWAITXINTRIN H INCLUDED D STRICT ANSI I home james Desktop ML mxnet cub DMXNET USE NVRTC 0 M MT build src operator contrib ctc loss gpu o src operator contrib ctc loss cu build src operator contrib ctc loss gpu d Steps to reproduce 1 clone latest mxnet source from git hub 2 install CUDA openBlas OpenCV Cudnn 3 attempt to compile make j3 USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 What have you tried to solve it 1 I have added the D MWAITXINTRIN H INCLUDED D STRICT ANSI arguments because of another compile error I was having and this solved that 2 ctc loss gpu d does not compile with or without these options 3 Have tried starting from scratch with a fresh copy of mxNet and received the same problem Note that the CPU only version compiled just fine,,,2017-07-30 11:16:14,2017-08-07 02:53:14
IS,BatchNorm compute problem,I am new to Mxnet and I am trying to implement a caffe code to Mxnet The training phase is good but the testing phase goes wrong the output from the BatchNorm layers is 1000 times smaller than that from caffe is BN layers my caffe prototxt is the following image it does not have a scale layer so that the gamma 1 and beta 0 so here is my symbol in mxnet image and I set eps 1e 5 use global stat True and fix gamma False and the following is the moving mean from mxnet image this is the mean from caffe image then the vars from mxnet image and the vars from caffe image Both the means and vars are similar in numbers but the mxnet ones are 1000 times smaller I init the weights with the following codes but since other layers' params are fine so the code should be ok image In the train phase the loss can convergence so that the symbols should be ok So could anyone helps many thanks,,,2017-08-01 06:05:38,2017-08-07 05:38:41
PR,fix build,Fix for issue reported in 7354,,"szha,szha",2017-08-07 00:19:15,2017-08-07 06:21:00
IS,I checkout to MXNet commit 62ecb60 then compile mxnet error but when i dont checkout to MXNet commit 62ecb60 compile successfully,First I clone mxnet if I compile it is ok But when i try to checkout to MXNet commit 62ecb60 by git checkout 62ecb60 And I compile again it fails Environment info Operating System CentOS release 6 5 Package used Python R Scala Julia python Python version and distribution python2 7 Error Message In file included from src operator custom ndarray op cc 7 0 src operator custom ndarray op inl h virtual bool mxnet op NDArrayOpProp InferShape std vector nnvm TShape std vector nnvm TShape std vector nnvm TShape const src operator custom ndarray op inl h 116 36 std vector unsigned int push back nnvm dim t shapes push back iter data src operator custom ndarray op inl h 116 36 In file included from data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 vector 64 0 from data1 NLPRMNT fujun tianhj test mxnet mxnet dmlc core include dmlc logging h 13 from src operator custom ndarray op inl h 10 from src operator custom ndarray op cc 7 data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 void std vector Tp Alloc push back const value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back const value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int const data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 void std vector Tp Alloc push back std vector Tp Alloc value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int In file included from src operator custom custom cc 7 0 src operator custom custom inl h virtual bool mxnet op CustomOpProp InferShape std vector nnvm TShape std vector nnvm TShape std vector nnvm TShape const src operator custom custom inl h 190 36 std vector unsigned int push back nnvm dim t shapes push back iter data src operator custom custom inl h 190 36 In file included from data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 vector 64 0 from data1 NLPRMNT fujun tianhj test mxnet mxnet dmlc core include dmlc logging h 13 from src operator custom custom inl h 10 from src operator custom custom cc 7 data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 void std vector Tp Alloc push back const value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back const value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int const data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 void std vector Tp Alloc push back std vector Tp Alloc value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int In file included from src operator custom native op cc 7 0 src operator custom native op inl h virtual bool mxnet op NativeOpProp InferShape std vector nnvm TShape std vector nnvm TShape std vector nnvm TShape const src operator custom native op inl h 204 36 std vector unsigned int push back nnvm dim t shapes push back iter data src operator custom native op inl h 204 36 In file included from data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 vector 64 0 from data1 NLPRMNT fujun tianhj test mxnet mxnet dmlc core include dmlc logging h 13 from src operator custom native op inl h 10 from src operator custom native op cc 7 data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 void std vector Tp Alloc push back const value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back const value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int const data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 void std vector Tp Alloc push back std vector Tp Alloc value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int src operator custom custom cc virtual mxnet Operator mxnet op CustomOpProp CreateOperatorEx mxnet Context std vector nnvm TShape std vector int const src operator custom custom cc 166 34 std vector unsigned int push back nnvm dim t shapes push back iter data src operator custom custom cc 166 34 In file included from data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 vector 64 0 from data1 NLPRMNT fujun tianhj test mxnet mxnet dmlc core include dmlc logging h 13 from src operator custom custom inl h 10 from src operator custom custom cc 7 data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 void std vector Tp Alloc push back const value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back const value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 901 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int const data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 void std vector Tp Alloc push back std vector Tp Alloc value type with Tp unsigned int Alloc std allocator unsigned int std vector Tp Alloc value type unsigned int push back value type x data1 NLPRMNT fujun local gcc 4 8 4 include c 4 8 4 bits stl vector h 919 7 no known conversion for argument 1 from nnvm dim t aka long int to unsigned int In file included from src operator custom native op cc 7 0 src operator custom native op inl h In instantiation of void mxnet op NativeOp xpu SyncVec const std vector mxnet TBlob const string mshadow Stream Device int with xpu mshadow cpu std string std basic string char src operator custom native op cc 27 1 required from here src operator custom native op inl h 146 66 const dim t aka const long int mxnet index t aka unsigned int const cast shapes push back const cast index t vec i shape data make build src operator custom native op o 1 make make build src operator custom custom o 1 make build src operator custom ndarray op o 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-08-07 04:24:25,2017-08-07 08:12:03
PR,Update Install script,,,"alues,alues,piiswrong,alues,alues,alues",2017-08-02 14:38:38,2017-08-07 10:50:41
IS,io cc 54 Data and label shape in consistent,Hi I am creating a neural network with R and MXnet the case is that when I work with a single neuron output it work well but when I put in tain y two fields and I put the output layer two neurons measure the error Io cc 54 Data and label shape in consistent I do not quite understand why The network that works without problems is train x DataNeurona train ind length DataNeurona 1 train y DataNeurona train ind length DataNeurona 1 test x DataNeurona c 1 200 train ind length DataNeurona 1 test y DataNeurona c 1 200 train ind length DataNeurona 1 data mx symbol Variable data fc1 mx symbol FullyConnected data name fc1 num hidden 40 weight w act1 mx symbol Activation fc1 name sigmoid1 act type sigmoid fc2 mx symbol FullyConnected act1 name fc2 num hidden 50 act2 mx symbol Activation fc2 name tanh1 act type sigmoid fc4 mx symbol FullyConnected act2 name fc4 num hidden 1 lro mx symbol LinearRegressionOutput data fc4 grad scale 1 mx set seed 0 model mx model FeedForward create symbol lro X train x y train y ctx mx cpu num round 25 array batch size NumBatch learning rate 0 05 momentum 0 9 eval metric mx metric mse And I only change this to take two exit fields train x DataNeurona train ind c length DataNeurona 1 length DataNeurona 1 1 train y DataNeurona train ind c length DataNeurona 1 length DataNeurona 1 1 And into the model num hidden 0 fc4 mx symbol FullyConnected act2 name fc4 num hidden 2 lro mx symbol LinearRegressionOutput data fc4 grad scale 1 Thank you,,"thirdwing,thirdwing,thirdwing",2017-08-04 17:18:48,2017-08-07 13:08:12
PR,Clarify definition of cross entropy metric in the documentation,Fix for 7290 Updated the misleading definition of the cross entropy metric in python mxnet metric py in order to represent multi class classification,,"hesseltuinhof,piiswrong",2017-08-01 15:30:20,2017-08-07 14:31:56
PR,Fix data tutorial,,,kevinthesun,2017-08-04 00:27:27,2017-08-07 17:08:11
PR,Revive CI by disabling caffe converter check temporarily,piiswrong Temporarily disable test check for vgg16 and resnet to let all CI going until we can fix this The problems are described in 7368,,"zhreshold,szha,zhreshold",2017-08-07 18:56:44,2017-08-07 20:43:21
PR,Small fix for versioning doc build,,,kevinthesun,2017-08-07 20:19:20,2017-08-07 21:15:33
PR,Seed numpy random,This test fails intermittently We will seed this so it gives us more predictability but will still need to do a deep dive asap,,lxn2,2017-08-07 22:05:54,2017-08-08 00:07:45
IS,API missing mx nd stack mx sym stack,Hi there I found mx nd stack description on web documents mxnet ndarray stack however in latest mxnet this API no longer exists is it removed,,"Lyken17,piiswrong,Lyken17,Lyken17",2017-08-06 21:37:11,2017-08-08 02:10:09
PR,Update custom cc,,,piiswrong,2017-08-07 23:39:04,2017-08-08 02:41:26
IS,R Character RNN program question,Package used Python R Scala Julia R MXNet version 0 10 0 This is not an issue of mxnet but my problem of trying to understand the basic Character RNN program The program in sets the following parameter values batch size 32 seq len 32 num hidden 16 num embed 16 num lstm layer 1 My question is what are the dimensions of the variables embed and wordvec in lines 53 and 55 of the following program As a general question in R how to check the internal variable values dimensions in functions such as lstm unroll,,thirdwing,2017-07-13 03:54:41,2017-08-08 02:54:22
IS,strange phenomenon 1080Ti is slower than maxwell gtx titan x How to analysis the results of Profiler,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution I use 1080Ti and maxwell titan x to train the same network with the same mxnet but 1080Ti is strangely slower I use the profiler to analyze the results as following figures One iteration cycle of 1080ti takes 1 343s One iteration cycle of maxwell titan x takes 1 005s In the figures we can see the forward and backward time of 1080ti is much smaller than maxwell titan x But for 1080ti there is a large unknown time consumption after each syncCopyGpu2Cpu operation the green block Do you know what is unknown time doing in the mxnet Why the 1080ti is slower than maxwell Titan x Thank you Evironments both cuda 8 0 cudnnv6 0 mxnet Version 0 10 0 Release I have recompiled the mxnet for 1080ti cpu e5 2620 ubuntu 16 04 for maxwell titan x cpu i7 4790k ubuntu 14 04 1080ti profiler 1080ti profiler2 maxwell gtx titan x profiler max well titan x,,,2017-08-08 06:15:27,2017-08-08 09:54:23
PR,Do not Docker cache git clones,This implements a work around to help Docker cache a GitHub repo properly Caching will now occur if and only if there are no new commits present,,"KellenSunderland,piiswrong",2017-07-25 13:18:03,2017-08-08 12:17:05
IS,Usage of Tensorboard in Distributed MXNet,Hi all I tried to use Tensorboard to visualize my model training process In the single node training mode the usage of Tensorboard is straightforward Thing is different when it comes to the distributed training mode Suppose I have 2 servers and 4 workers in my cluster how can I use Tensorboard to track the overall training process Basically I can imagine there will be 4 different set of log files locate in each worker and I need to use 4 separate Tensorboard processes to visualize the whole process After some research I found the following question on StackOverflow which said that in TensorFlow only one of the workers need to write the log I wonder what is the by design usage of Tensorboard in Distributed MXNet My main concern of writing summary on one of the worker is whether the log from a single worker can be a good representative to the overall learning process Thanks a lot for your work to make the Tensorboard on MXNet come true I wonder do you have any idea of my question Thanks in advance Bo,,"zihaolucky,zihaolucky",2017-08-04 22:27:12,2017-08-08 16:45:08
PR,Docs for GAN,Based on the notebook I made here,,yash1,2017-08-08 04:16:41,2017-08-08 17:27:22
PR,fix 7382 fixes how the batch size is obtained in RNN forward pass,,,peschn,2017-08-08 11:35:21,2017-08-08 17:30:04
IS,gluon nn Conv2D initialization int stride not handled correctly,When creating 'gluon nn Conv2D' passing the stride with integer value it does not automatically convert to tuple and cause 'Floating Point Exception' for example will cause error when forwarding,,"zhanghang1989,szha,zhanghang1989,zhanghang1989,Jerryzcn,szha",2017-08-08 03:50:03,2017-08-08 18:57:30
PR,RowSparse pull push,Opening this PR for interface review Added row sparse pull python API to kvstore and c api which requires providing row id for each row sparse value to pull The row id s are sorted in backend to contain only unique elements The old pull API will not accept pulling a value stored in row sparse storage Added kRowSparsePushPull type to kvstore server When received data request of this type keys are encoded as key row id1 row id2 row idn Added temporary support for SparseNDArray inplace add mul div sub by making an extra copy in frontend explicitly See regression example using row sparse pull in regression example py Note this PR contains duplicated code in PR 6765 TODOs for next PR row ids aggregation for multi device row sparse pull in dist kvstore lazy initialization for large weights,,"eric-haibin-lin,szha,reminisce,eric-haibin-lin",2017-07-12 23:49:30,2017-08-08 21:12:13
PR,decouple record train and add state readers,piiswrong changes include decouple training and testing add training record state getters to allow model codes to change behavior based on the states,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,szha,zackchase,szha,piiswrong,piiswrong,piiswrong,szha",2017-08-07 00:03:54,2017-08-08 22:13:29
PR,Update conv layers py,,,piiswrong,2017-08-08 18:54:14,2017-08-08 22:20:21
PR,Add license header,Added ASF license header to all files except for submodules R package not apache2 src operator mkl licensed to intel src operator contrib ctc include licensed to nvidia Also add a CI job to test if new added files have a proper header,,"mli,piiswrong,mli,lxn2",2017-08-08 05:25:00,2017-08-08 23:36:24
PR,Clarify definition of cross entropy metric in the documentation clean up PR 7291,Clean pull request for 7291 messed it up there Fixed the pylint error trailing whitespace,,hesseltuinhof,2017-08-07 14:31:01,2017-08-09 00:25:49
IS,How could I define function for GPU only,I need to define a function in h file for GPU version only then implenent it in cu file declaration in h the problem is I am force to declare the same function for CPU version and implement it or it will compile failed I just need the function for GPU How could I get ride of this problem,,qingzhouzhen,2017-08-08 11:49:02,2017-08-09 00:38:43
PR,update for cudnn7 for rnn changes,This is to reflect a change in cudnnSetRNNDescriptor in cudnn 7 and should enable building with cudnn7,,"szha,szha",2017-08-09 03:39:10,2017-08-09 04:41:20
PR,Update CONTRIBUTORS md,Because my patches have been merged in 6908 and 6927,,kkk669,2017-08-09 02:11:18,2017-08-09 05:30:59
PR,R im2rec in R close 7273,,,thirdwing,2017-08-08 21:24:01,2017-08-09 06:44:59
IS,Cannot find custom operator type BoxAnnotatorOHEM,Hi I am running MXNet in the latest Docker image with GPU versions I have copied the custom defined operator BoxAnnotatorOHEM from to my own project When I run my faster RCNN model an error occured 07 55 12 mxnet dmlc core include dmlc logging h 304 07 55 12 src operator custom custom inl h 128 Check failed registry find param op type registry end Cannot find custom operator type BoxAnnotatorOHEM There exists the operator register process in the BoxAnnotatorOHEM file as follows 62 operator register 'BoxAnnotatorOHEM' 63 class BoxAnnotatorOHEMProp mx operator CustomOpProp 64 def init self num classes roi per img cfg 65 super BoxAnnotatorOHEMProp self init need top grad False 66 self num classes int num classes 67 self roi per img int roi per img 68 self cfg cPickle loads cfg I wonder what have I missed,,,2017-08-09 08:10:46,2017-08-09 08:16:15
PR,Remove inline keyword from methods we want to generate code for,When linking to a larger c project we want to generate code for all the methods in the hpp files the inline keyword prevents this and causes undefined symbols The compiler normally uses it is own heuristics for inlining code so this is unlikely to have any impact on the code generated,,"piiswrong,lx75249",2017-08-09 17:14:23,2017-08-09 17:47:36
PR,Extending the GPU dot operator,Ready for review merge haibin lin Adding GPU implementations of dot csr rsp dns dot csr T rsp1 rsp2 dot csr T dns rsp dot csr rsp dns Table 1 shows a sample of the dot csr rsp benchmark results on g2 8xlarge Speedup vs baseline naive parallel GPU implementation 5x to 200x Speedup vs current CPU implementation 0 3x to 10x More detailed benchmark results including breakdown per kernel invoke and performance analysis on request dot csr T rsp1 rsp2 no benchmark results added at this point baseline functionality only dot csr T dns rsp Table 2 shows a sample of the dot csr T dns benchmark results on g2 8xlarge Speedup vs baseline naive parallel GPU implementation 10x dense to 30x sparse Speedup vs current CPU implementation 0 04x dense to 4x sparse More detailed benchmark results including breakdown per kernel invoke and performance analysis on request Table 1 GPU performance DotCsrRspDnsImpl csr rsp dns csr density rsp density context m k n time ms m k n time ms 64 0 64 0 gpu 0 512 50000 64 60 35 512 100000 128 247 68 64 0 32 0 gpu 0 512 50000 64 55 38 512 100000 128 216 04 64 0 16 0 gpu 0 512 50000 64 52 15 512 100000 128 207 04 64 0 8 0 gpu 0 512 50000 64 50 75 512 100000 128 201 06 64 0 4 0 gpu 0 512 50000 64 50 03 512 100000 128 197 60 64 0 2 0 gpu 0 512 50000 64 49 49 512 100000 128 195 32 64 0 1 0 gpu 0 512 50000 64 49 44 512 100000 128 193 02 64 0 64 0 gpu 0 512 50000 64 60 35 512 100000 128 247 68 32 0 64 0 gpu 0 512 50000 64 31 59 512 100000 128 131 06 16 0 64 0 gpu 0 512 50000 64 16 26 512 100000 128 68 71 8 0 64 0 gpu 0 512 50000 64 8 34 512 100000 128 35 43 4 0 64 0 gpu 0 512 50000 64 4 39 512 100000 128 18 18 2 0 64 0 gpu 0 512 50000 64 2 27 512 100000 128 9 29 1 0 64 0 gpu 0 512 50000 64 1 24 512 100000 128 4 88 Table 2 GPU performance DotCsrDnsRspImpl csr T dns rsp lhs density rhs density context m k n time ms 64 0 100 0 gpu 0 512 50000 64 1907 24 32 0 100 0 gpu 0 512 50000 64 934 09 16 0 100 0 gpu 0 512 50000 64 455 44 8 0 100 0 gpu 0 512 50000 64 216 59 4 0 100 0 gpu 0 512 50000 64 88 69 2 0 100 0 gpu 0 512 50000 64 34 48 1 0 100 0 gpu 0 512 50000 64 14 02 0 5 100 0 gpu 0 512 50000 64 6 19 0 25 100 0 gpu 0 512 50000 64 3 62,,"stefanhenneking,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,stefanhenneking,stefanhenneking,anirudh2290,eric-haibin-lin,stefanhenneking,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,stefanhenneking,stefanhenneking,stefanhenneking,stefanhenneking,eric-haibin-lin,stefanhenneking,reminisce,reminisce,reminisce,reminisce,stefanhenneking,reminisce,reminisce,reminisce,reminisce,eric-haibin-lin,stefanhenneking,stefanhenneking,stefanhenneking,stefanhenneking,stefanhenneking,stefanhenneking,reminisce,stefanhenneking,stefanhenneking",2017-07-28 00:28:35,2017-08-09 18:34:39
PR,Drafted documentation for autograd,It is a little verbose and I can pare down but given the bare state of the page I think it is a big improvement over the status quo I would also like to add a simplest examples for with train mode and with predict mode Probably the very most convincing would be just running an NDArrray through a dropout layer Pinging and,,"zackchase,szha,szha,szha,fhieber,fhieber,fhieber,zackchase,szha",2017-08-09 08:28:42,2017-08-09 20:07:58
PR,fix 7368 Caffe converter test fails causing CI to halt for all PRs,,,"joey2014,piiswrong,joey2014,piiswrong,joey2014,szha",2017-08-08 07:10:09,2017-08-09 20:19:22
PR,Perl Bugfixes and sync with python symbolic code,1 Fixes for ImageIter 2 Convolutional RNN 3 Improved Visualization 4 PearsonCorrelation metric 5 Fixed tests,,"sergeykolychev,sergeykolychev",2017-08-07 00:15:08,2017-08-09 20:20:05
IS,ubuntu mxnet install issue,Hi I have been trying to install mxnet 0 0 7 targ gz For and R client on ubuntu 16 04 but the following error message is returned installing to home stef R x86 64 pc linux gnu library 3 3 mxnet libs R demo inst preparing package for lazy loading help installing help indices building package indices installing vignettes testing if installed package can be loaded Error in usr lib R bin exec R' free invalid pointer 0x0000000003e0f488 Thanks,,"thirdwing,thirdwing,thirdwing",2016-10-17 18:50:15,2017-08-09 22:42:26
IS,R version CUDA failure on Amazon ec2 ubuntu14 04,Environment info Operating System Amazon ec2 ubuntu 14 04LTS Compiler gcc Package used Python R Scala Julia R MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo 3 3 3 RC I am trying to install MXNet R version on Amazon Web Service EC2 ubuntu 14 04LTS by following the instruction First I downloaded CUDA 8toolkit from nvidia I want to make sure I modified the config mk file 'before' I actaully compile by 'bash install mxnet ubuntu r sh' command Changed enviornment variables as many ways as possible Repeated above steps at least 7 times My final goal is to run a code which contains mxnet lenet by batch file R CMD BATCH R I would be very appreciated if someone can actually solve my problem,,"thirdwing,thirdwing,thirdwing",2017-04-10 07:51:45,2017-08-09 22:43:38
IS,building R mxnet from source R session get blocked after run 'make rpkg',I compiled the mxnet so and then I want to build the R package as well After I run make rpkg in the root folder the session just stopped at testing if installed package can be loaded it hangs here Any idea on this Environment info Operating System SUSE enterprise 11 Compiler gcc g Package used Python R Scala Julia R MXNet version installed from source MXNet commit hash git rev parse HEAD 11fe46683ef629ac094e5dbd37ca1dfdd4454568 R sessionInfo R version 3 2 0 2015 04 16 Platform x86 64 unknown linux gnu 64 bit locale 1 LC CTYPE en US UTF 8 LC NUMERIC C 3 LC TIME en US UTF 8 LC COLLATE en US UTF 8 5 LC MONETARY en US UTF 8 LC MESSAGES en US UTF 8 7 LC PAPER en US UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C 11 LC MEASUREMENT en US UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base Error Message after I run make rpkg There is no error the R session just hanged at R demo inst preparing package for lazy loading help installing help indices building package indices installing vignettes testing if installed package can be loaded it hangs here What have you tried to solve it 1 reinstall devtools 2 downgrade roxygen2 into 5 0 1,,"thirdwing,thirdwing",2017-05-05 20:43:45,2017-08-09 23:16:24
IS,R CMD INSTALL mxnet 0 7 tar gz cannot run successfully,R CMD INSTALL mxnet 0 7 tar gz it returns in the tail R demo inst preparing package for lazy loading help installing help indices building package indices installing vignettes testing if installed package can be loaded then it stops for a long time Is it downloading something when testing But when I require mxnet in another R terminal it returns back library mxnet caught segfault address 0x20 cause 'memory not mapped' Traceback 1 Call Module functions names xp 2 Module module mustStart TRUE where env 3 doTryCatch return expr name parentenv handler 4 tryCatchOne expr names parentenv handlers 1L 5 tryCatchList expr classes parentenv handlers 6 tryCatch Module module mustStart TRUE where env error function e e 7 loadModule mxnet TRUE 8 fun libname pkgname 9 doTryCatch return expr name parentenv handler 10 tryCatchOne expr names parentenv handlers 1L 11 tryCatchList expr classes parentenv handlers 12 tryCatch fun libname pkgname error identity 13 runHook onLoad env package lib package 14 loadNamespace package lib loc 15 doTryCatch return expr name parentenv handler 16 tryCatchOne expr names parentenv handlers 1L 17 tryCatchList expr classes parentenv handlers 18 tryCatch expr error function e call conditionCall e if is null call if identical call 1L quote doTryCatch call sys call 4L dcall deparse call 1L prefix paste Error in dcall LONG 75L msg conditionMessage e sm strsplit msg n 1L w 14L nchar dcall type w nchar sm 1L type w if is na w w 14L nchar dcall type b nchar sm 1L type b if w LONG prefix paste0 prefix n else prefix Error msg paste0 prefix conditionMessage e n Internal seterrmessage msg 1L if silent identical getOption show error messages TRUE cat msg file stderr Internal printDeferredWarnings invisible structure msg class try error condition e 19 try attr package LibPath which lib loc ns loadNamespace package lib loc env attachNamespace ns pos pos deps 20 library mxnet Possible actions 1 abort with core dump if enabled 2 normal R exit 3 exit R without saving workspace 4 exit R saving workspace Selection clearly mxnet has not been installed successfully How can I fix this problem BTW i have installed python version,,"thirdwing,thirdwing,miguelgfierro,thirdwing,thirdwing",2016-10-06 06:10:22,2017-08-09 23:16:37
PR,Tensorcore conv deconv support,Enables TensorCore in convolution and deconvolution operators for users with NVIDIA Volta GPU cuda9 cudnn7 On by default this can be disabled through an environment variable as in export MXNET CUDA ALLOW TENSOR CORE 0,,"DickJC123,piiswrong,piiswrong,DickJC123,piiswrong,piiswrong,DickJC123",2017-08-05 01:30:06,2017-08-09 23:54:25
PR,remove dmlc mxnet logo from readme,,,"domdivakaruni,szha",2017-07-31 00:33:22,2017-08-10 00:16:23
IS,Not enough information to get shape,Hello I am implementing a neural network in MXNetR I attempted to customize my loss function to compute the correlation between my output vector and the targeting vector Below is my code Below is my code I can compile with this version but my model runs very slowly and the code is apparently not very readable I wonder if there is any way to implement get around the error and implement the first version as I described above,,"thirdwing,thirdwing,thirdwing,thirdwing",2017-06-09 12:03:58,2017-08-10 00:44:19
PR,Fix two typos in autograd md,Fixing two typos that snuck into this doc Thanks for the careful eyes,,zackchase,2017-08-10 01:05:49,2017-08-10 03:15:19
PR,R MISC update Makefile Jenkinsfile use mx ctx default in R test,,,thirdwing,2017-08-09 21:04:21,2017-08-10 15:19:28
IS,MxnetR chunk wise neural nets,Dear all I just now started using the mxnet R package in Windows I have a pretty big dataset which I need to process in chunks due to memory limits Therefore I split my dataset and wanted to run a feed forward neural net on chunk 1 and use the parameters weights and bias for chunk 2 which in turn can be used for chunk 3 and so on However despite some examples I cannot figure out how to get that to work My model looks like that But now I am unsure how to use the weights and bias for the next batch of mx model FeedForward create Any hint would be greatly appreciated Thank you Best Laura,,"thirdwing,thirdwing",2017-08-07 08:46:23,2017-08-10 15:31:32
PR,Hotfix mx image documents,Fix wrong doc descriptions in image package,,zhreshold,2017-08-10 00:34:20,2017-08-10 16:54:43
PR,add verification to gluon dataset,piiswrong,,"szha,piiswrong,piiswrong,szha,piiswrong,piiswrong,szha,piiswrong",2017-08-03 04:05:55,2017-08-10 17:20:40
PR,add Sequential compatibility to rnn layers,,,"szha,piiswrong,piiswrong,szha,piiswrong,piiswrong,szha,szha",2017-08-06 05:47:30,2017-08-10 17:24:04
PR,Add disclaimer and download link,Merge after this PR,,"kevinthesun,lxn2,lxn2",2017-08-09 22:58:47,2017-08-10 20:58:20
PR,Initial commit of an MXNet converter,Working with and on MXNet converter,,"srikris,pracheer",2017-08-10 06:35:26,2017-08-10 22:22:08
IS,OP Input gamma mutated in Batch Norm,In BatchNorm operator gamma is not listed as aux data in backend but actually mutated when fix gamma is True For sparse storage fallback when a dense executors see a mutable sparse input it will generate a dense copy execute fcompute and convert the mutated dense copy back to source NDArray For BatchNorm gamma is not listed as mutable inputs so storage fallback will not convert this input back to sparse NDArray and fail We should use temp storage when fix gamma True instead of changing input Need to fix it for 7082,,eric-haibin-lin,2017-08-05 01:15:14,2017-08-11 06:22:12
IS,OSError home dir mxnet mxnet python mxnet lib libmxnet so undefined symbol cblas ddot,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System RHEL 6 9 2 6 32 696 el6 x86 64 Compiler gcc GCC 5 3 0 Package used Python R Scala Julia Python MXNet version Or if installed from source git clone recursive Installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 10 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I have installed mxnet at this dir mxnet mxnet PYTHONPATH is set to mxnet mxnet python A OpenBLAS used 1 git clone git 2 cd OpenBLAS 3 make TARGET SANDYBRIDGE our processor is Intel R Xeon R CPU E5 2680 v3 2 50GHz which is haswell but it was giving this error with make command kernel x86 64 dtrmm kernel 4x8 haswell c 77 Error no such instruction vpe rmpd 0x1b ymm3 ymm3' kernel x86 64 dtrmm kernel 4x8 haswell c 78 Error no such instruction vpe rmpd 0xb1 ymm2 ymm2' So I changed TARGET SANDYBRIDE 4 Installed OpenBLAS with make PREFIX dir install B 1 Installed mxnet with USE BLAS openblas and USE OPENCV 0 and specifying path of OpenBLAS in mxnet make config mk file 2 make j4 Successful as it generated lib libmxnet a and libmxnet so 3 export PYTHONPATH mxnet mxnet python 4 Ran python in interactive mode as c244728 lx host1 module load gcc 5 3 0 c244728 lx host1 python Python 2 7 10 default Jun 6 2015 14 08 03 GCC 4 4 7 20120313 Red Hat 4 4 7 4 on linux2 Type help copyright credits or license for more information import mxnet as mx Traceback most recent call last File stdin line 1 in module File home c244728 lx mxnet mxnet python mxnet init py line 7 in module from base import MXNetError File home c244728 lx mxnet mxnet python mxnet base py line 69 in module LIB load lib File home c244728 lx mxnet mxnet python mxnet base py line 61 in load lib lib ctypes CDLL lib path 0 ctypes RTLD LOCAL File lrlhps apps python python 2 7 10 lib python2 7 ctypes init py line 365 in init self handle dlopen self name mode OSError home c244728 lx mxnet mxnet python mxnet lib libmxnet so undefined symbol cblas ddot Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Also ran mxnet example as c244728 lx host1 mxnet python example image classification train mnist py Traceback most recent call last File example image classification train mnist py line 8 in module from common import find mxnet fit File home c244728 lx mxnet mxnet example image classification common find mxnet py line 3 in module import mxnet as mx File home c244728 lx mxnet mxnet python mxnet init py line 7 in module from base import MXNetError File home c244728 lx mxnet mxnet python mxnet base py line 69 in module LIB load lib File home c244728 lx mxnet mxnet python mxnet base py line 61 in load lib lib ctypes CDLL lib path 0 ctypes RTLD LOCAL File lrlhps apps python python 2 7 10 lib python2 7 ctypes init py line 365 in init self handle dlopen self name mode OSError home c244728 lx mxnet mxnet python mxnet lib libmxnet so undefined symbol cblas ddot Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-08-09 10:29:46,2017-08-11 12:30:04
PR,Enable Apache Rat add more license headers,This PR enables Apache Rat to run as part of the sanity check It also includes the rat excludes file which will make Rat ignore files folders that match those names Also ran Rat to add the remaining license headers to files missed,,"lxn2,lxn2,lxn2,piiswrong,lxn2,lxn2,lxn2,lxn2,mli",2017-08-10 20:46:29,2017-08-11 14:57:10
PR,Add autograd function,,,piiswrong,2017-08-10 00:33:59,2017-08-11 20:26:49
PR,Add more license files,Added more extension handlers to license header py Made it verbose so we keep track of what we are whitelisting skipping Added license headers to files with newly added extension headers,,"lxn2,nswamy,nswamy,nswamy,piiswrong,mli,lxn2",2017-08-11 15:09:54,2017-08-11 21:12:47
PR,fix consistency of cpu gpu in stn,calc of cpu and gpu is not consistent before this fix this fix is ref to,,"tornadomeet,piiswrong,tornadomeet,piiswrong,tornadomeet",2017-08-08 00:22:59,2017-08-12 00:08:37
PR,MXNet Apple CoreML converter Review only Do not merge,This tool helps convert MXNet models into Apple CoreML format which can then be run on Apple devices This PR is currently for review only since we are in the process of figuring out the licensing issues with Apple We got the initial code from them and they have handed over the maintenance and development of this tool to MXNet community from now on This has been tested against MXNet 0 10 0 as well as against the current master branch Example of a sample command to convert squeeze net model python mxnet coreml converter py model prefix isqueezenet v1 1' epoch 0 input shape ' data 3 224 224 ' output file squeezenet v11 mlmodel This tools is currently able to convert Inception Resnet VGG Squeezenet models Support for more vision models as well as nlp models will be added later Unit tests have been added which test 1 each layer individually 2 some of the standard models from mxnet zoo against random data as well as on a subset of imagenet data In all the tests we compare the accuracy of MXNet vs that of CoreML using the converted model,,"pracheer,madjam,madjam,madjam,jiajiechen,zhreshold,madjam,szha,pracheer",2017-08-04 17:43:05,2017-08-12 00:49:24
PR,Prepare for v0 11 0 release,This PR prepares for v0 11 0 release Bumps up version number Adds updates to README NEWS,,"lxn2,sandeep-krishnamurthy,nswamy,nswamy,nswamy,srochel",2017-08-11 17:19:15,2017-08-12 01:02:37
PR,MXNet Apple CoreML converter,This tool helps convert MXNet models into Apple CoreML format which can then be run on Apple devices This has been tested against MXNet 0 10 0 as well as against the current master branch Example of a sample command to convert squeeze net model python mxnet coreml converter py model prefix isqueezenet v1 1' epoch 0 input shape ' data 3 227 227 ' mode classifier pre processing arguments ' image input names data ' class labels classLabels txt output file squeezenetv11 mlmodel This tools is currently able to convert Inception Resnet VGG Squeezenet NiN models Support for more vision models as well as nlp models will be added later Unit tests have been added which test 1 each layer individually 2 some of the standard models from mxnet zoo against random data as well as on a subset of imagenet data In all the tests we compare the accuracy of MXNet vs that of CoreML using the converted model,,"pracheer,nswamy",2017-08-12 00:53:30,2017-08-12 01:18:00
PR,Website fix,1 Remove dmlc copyright 2 Add link to CoreML converter 3 Modify some logics in building versioning website,,"kevinthesun,nswamy",2017-08-11 21:52:57,2017-08-12 06:58:18
PR,cuda support for linalg functions restructuring of linalg interfaces,Linear algebra operators are now restructured to following interfaces and cuda support has been added c lapack api h Header to integrate lapack functions on CPU linalg h Tensor based interface to call linear algebra operators on gpu cpu in a unified way la op cc la op cu Operators that expose these functions to a user The whole implementation of cpu gpu handling is in linalg impl h It turned out that there are various specialties when dealing with cuBlas cuSolver for the different functionalities so decided to hide this in a separate file and not clutter the interface linalg h with all these details We are planning to add more functions in the future and other people are planning this too So hopefully we can ensure that this structure is used as much as possible for new additional linear algebra functionalities This PR requires PR 274 for mshadow as prerequisite The CUDA functions still have to be tested Am in the process of doing so but would like to send the changes already now for review,,"asmushetzel,jli05,piiswrong,asmushetzel,asmushetzel,piiswrong,asmushetzel,piiswrong,asmushetzel,DickJC123,asmushetzel,asmushetzel",2017-07-21 09:29:22,2017-08-12 19:12:44
PR,Small doc cleanups,This patch cleans up several places where a double space at the end of a line resulted in no spaces being rendered It also moves the location of an HTML comment before a code block because the code block markdown did not render previously I verified the changes produce the desired result by building the docs locally,,sethah,2017-08-12 20:01:26,2017-08-12 22:15:11
PR,scala package clean up pom remove duplicate definition,,,"CodingCat,terrytangyuan,yzhliu",2017-08-12 22:48:15,2017-08-12 23:00:28
PR,broken link in readme,,,tornadomeet,2017-08-13 02:04:49,2017-08-13 03:48:31
PR,R vignette update,,,thirdwing,2017-08-12 00:24:55,2017-08-13 21:11:13
PR,gluon bce loss,,,"szha,piiswrong,szha,piiswrong,piiswrong,piiswrong,piiswrong,szha,szha,piiswrong,szha",2017-08-02 08:05:41,2017-08-14 00:01:14
IS,train mnist py failed TypeError init got an unexpected keyword argument 'multi precision',Release 0 10 0 is okay However Current master branch git clone recursive occurs this problem as below running in the official docker then I cloned from github com and run this train mnist py,,"ysh329,ysh329,ptrendx,ysh329",2017-08-11 02:34:15,2017-08-14 02:27:39
PR,Updating CoreML readme file,Updated the structure fixed the grammar typos Calling out issues with InceptionV3 model Removed references to label names parameter but keeping the code as it is due to time constraints,,pracheer,2017-08-14 20:51:51,2017-08-14 21:29:40
PR,add nswamy code signing key,lxn2,,nswamy,2017-08-14 21:00:19,2017-08-14 21:43:16
PR,New code signing key README file changes,Cherry picking from nswamy and pracheer is changes,,lxn2,2017-08-14 22:12:34,2017-08-14 22:14:03
PR,Change RC version in NEWS,,,lxn2,2017-08-14 23:43:14,2017-08-14 23:43:27
PR,Fix toc,,,kevinthesun,2017-08-14 22:44:56,2017-08-15 00:17:59
PR,Fix apache link,,,kevinthesun,2017-08-14 23:44:44,2017-08-15 00:21:10
PR,Update Jenkinsfile,added make clean before make docs,,"srochel,piiswrong,mli",2017-08-14 23:35:58,2017-08-15 03:11:36
IS,time series delay output mxnet,Question Having a rnn to predict a point in time based in indirect inputs This is not the 'normal' time series prediction in which the future is predicted on previous values but for which a certain curve property is predicted on a bunch of indirectly related features When training the first S let is say 50 outputs will not be accurate at all During training I will have to provide at least 51 inputs and hence there will be 51 outputs as well 1 output per input Let is call this number T I would only like to use only the last outputs T 50 for the parameter weights estimation during training omitting the first S How can I do achieve that in mxnet Environment info This is not really relevant but well I do not mind providing it Operating System,,"jeremiedb,jeremiedb",2017-08-05 18:28:52,2017-08-15 03:44:26
IS,Why are resnet is RELU and BN set before CONV,Why are resnet is RELU and BN set before CONV In bn1 mx sym BatchNorm data data fix gamma False eps 2e 5 momentum bn mom name name ' bn1' act1 mx sym Activation data bn1 act type arelu' name name ' relu1' conv1 mx sym Convolution data act1 num filter int num filter 0 25 kernel 1 1 stride 1 1 pad 0 0 no bias True workspace workspace name name ' conv1' bn1 and act1 are set before conv1 But resnext and resnet v1 are normal In conv1 mx sym Convolution data data num filter int num filter 0 5 kernel 1 1 stride 1 1 pad 0 0 no bias True workspace workspace name name ' conv1' bn1 mx sym BatchNorm data conv1 fix gamma False eps 2e 5 momentum bn mom name name ' bn1' act1 mx sym Activation data bn1 act type arelu' name name ' relu1' conv1 mx sym Convolution data data num filter int num filter 0 25 kernel 1 1 stride stride pad 0 0 no bias True workspace workspace name name ' conv1' bn1 mx sym BatchNorm data conv1 fix gamma False eps 2e 5 momentum bn mom name name ' bn1' act1 mx sym Activation data bn1 act type arelu' name name ' relu1',,,2017-08-15 07:27:34,2017-08-15 08:44:27
IS,Training error when using cifar100,I train the network given in with cifar100 But here comes a training error like this file Users Administrator Desktop QQ E6 88 AA E5 9B BE20170815173949 png What is the problem aileli,,,2017-08-15 09:56:11,2017-08-15 10:00:51
PR,R package RNN refactor,Related to 5488 this is to provide a more flexible and efficient framework to run RNN within the R package This pull does brake the current RNN API better to renamed files function to maintain legacy support Built around mx symbol RNN Support bucketing and masking through mx io bucket iter R Support multi device training single instance inference Only support seq to one at the moment but inference on one to one should be added shortly Requires to run with CUDA until mx symbol RNN gets a CPU support End to end tutorial can be seen here Will be finalized in a blog post when one to one inference suport is completed,,"jeremiedb,thirdwing,jeremiedb",2017-08-15 06:47:27,2017-08-15 14:25:17
PR,Tensorcore fullyconnected support,Adds TensorCore algo support to the FullyConnected operator for users with NVIDIA Volta cuda9 cudnn7 On by default this can be disabled through an environment variable as in export MXNET CUDA ALLOW TENSOR CORE 0 Applies to float16 I O instances only,,"DickJC123,piiswrong,piiswrong",2017-08-11 03:17:46,2017-08-15 16:56:21
PR,fix autograd memory cost,,,piiswrong,2017-08-11 20:36:26,2017-08-15 17:16:24
PR,fix autograd memory cost,,,piiswrong,2017-08-15 17:16:43,2017-08-15 17:17:18
PR,Fix autograd memory,,,piiswrong,2017-08-15 17:31:20,2017-08-15 17:32:04
PR,Fix more broken links,kevinthesun,,sandeep-krishnamurthy,2017-08-15 17:51:40,2017-08-15 18:01:23
PR,fix autograd memory cost,,,piiswrong,2017-08-15 17:18:22,2017-08-15 19:24:36
PR,Add resnet50 v2 resnet101 V2 and resnet152 v2 gluon pre trained model,szha,,"kevinthesun,zhreshold",2017-08-10 21:45:11,2017-08-16 02:01:37
PR,add gluon resnet18 v2 resnet34 v2 models,resnet18v2 validation accuracy 0 696827 top k accuracy 5 0 888473 resnet34v2 validation accuracy 0 732103 top k accuracy 5 0 910415,,zhreshold,2017-08-15 20:25:15,2017-08-16 02:22:07
IS,no get mnist in test utils,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler GCC 4 9 Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 Conda If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace AttributeError 'module' object has no attribute 'get mnist' Minimum reproducible example I followed the instruction I checked the test utils py It does not provide these function Maybe the document is out of date,,,2017-05-15 22:15:45,2017-08-16 16:51:33
PR,Fixes scaling issue identified in 7455,mli,,"madjam,szha",2017-08-16 02:00:43,2017-08-16 17:31:05
PR,Fix more links,,,kevinthesun,2017-08-15 21:03:41,2017-08-16 17:31:34
PR,add depthwise convolution is gpu version optimization,As the cudnn is not optimized for depthwise convolution we optimized the gpu version of depthwise 2D convolution The training effect is as follows cudnn version mobilenet training in imagenet Hardware TITAN X Pascal Intel R Xeon R CPU E5 2620 v4 2 10GHz 16 128GMem Software cuda8 0 cudnn5 1 As described above we get about 3 4 times speed compared the cudnn version About the test we have compared the result in every depthwise layer with the conv version,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,solin319,solin319,solin319,piiswrong,solin319,chinakook,piiswrong,piiswrong,piiswrong,terrychenism,chinakook",2017-08-09 05:08:09,2017-08-16 17:32:15
PR,fix a formula typo in doc,,,"piiswrong,tqchen,piiswrong",2017-08-11 20:52:36,2017-08-16 17:37:48
PR,op h was changed to operator h without updating includes,,,"dtmoodie,piiswrong,lx75249,dtmoodie,lx75249,dtmoodie,lx75249,dtmoodie",2017-08-16 15:33:04,2017-08-16 19:22:16
IS,cpp package does not build with makefile build,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler GCC 5 4 MXNet commit hash git rev parse HEAD 75ee5976bc1a7ec24746d764b529d0ae8e6e9583 Error Message Please paste the full error message including stack trace cpp package example mlp cpu cpp In function 'mxnet cpp Symbol mlp const std vector int ' cpp package example mlp cpu cpp 44 16 error 'FullyConnected' was not declared in this scope layers i cpp package example mlp cpu cpp 45 61 error 'ActivationActType' has not been declared outputs i i layers size 1 fc Activation fc ActivationActType kRelu cpp package example mlp cpu cpp 48 45 error 'SoftmaxOutput' was not declared in this scope return SoftmaxOutput outputs back label cpp package example test score cpp In function 'mxnet cpp Symbol mlp const std vector int ' cpp package example test score cpp 44 16 error 'FullyConnected' was not declared in this scope layers i cpp package example test score cpp 45 60 error 'ActivationActType' has not been declared outputs i i layers size 1 fc Activation fc ActivationActType kRelu cpp package example test score cpp 48 45 error 'SoftmaxOutput' was not declared in this scope return SoftmaxOutput outputs back label cpp package example mlp gpu cpp In function 'mxnet cpp Symbol mlp const std vector int ' cpp package example mlp gpu cpp 44 16 error 'FullyConnected' was not declared in this scope layers i cpp package example mlp gpu cpp 45 61 error 'ActivationActType' has not been declared outputs i i layers size 1 fc Activation fc ActivationActType kRelu cpp package example mlp gpu cpp 48 45 error 'SoftmaxOutput' was not declared in this scope return SoftmaxOutput outputs back label What I have tried Have not been building the cpp package,,"dtmoodie,lx75249,dtmoodie,dtmoodie",2017-08-16 19:21:59,2017-08-16 19:51:40
PR,Gpu access,,,"dtmoodie,piiswrong,piiswrong,piiswrong,dtmoodie,piiswrong,dtmoodie",2017-05-30 13:59:39,2017-08-16 19:52:06
IS,Distributed training is slow,Environment info Operating System Ubuntu 16 4 Compiler gcc 5 4 Package used Python R Scala Julia Python MXNet version Last code Or if installed from source installed from source MXNet commit hash git rev parse HEAD 1a3faa If you are using python package please provide Python version and distribution Python 2 7 13 Anaconda custom 64 bit I tried to train image classification model using two servers with infiniband cards But the speed is a little slow just like using one server I used the code of example image classifaction when training on one server the training command is It seems that using distributed training with 2 servers the speed is only a little better than standalone training 600x2 samples sec VS 1000 samples sec I tried to test the IB bandwidth using iperf it can get 24 0 Gbits sec using 1 thread so I think the IB is bandwidth is not the bottleneck Does anyone can give any suggestion about distributed training using MxNet,,"piiswrong,szha,szha,solin319,szha,ptrendx,szha,madjam,mli,mli",2017-08-14 14:51:41,2017-08-16 20:26:29
PR,remove WaitToRead in dist kvstore,Try to resolve 7455 This patch is based on is solution I did a simple test on two P2 8xlarge with tools bandwidth Observed 20 30 performance improvement,,mli,2017-08-16 06:49:26,2017-08-16 20:26:29
PR,merge from V0 11 0,,,piiswrong,2017-08-16 20:36:53,2017-08-16 20:37:41
PR,update to rc2,,,nswamy,2017-08-16 22:09:50,2017-08-16 22:12:47
PR,Change git clone to specific tag for installation,,,"kevinthesun,piiswrong,kevinthesun",2017-08-16 23:40:10,2017-08-17 01:11:02
PR,Create vgg 16 vgg 19 and fp16 version in vgg,We create vgg 16 vgg 19 and fp16 version in vgg Parameters 'num layers' and wouldtype' can control which model will be selected The model architecture were came from VGG website www robots ox ac uk vgg research very deep,,solin319,2017-07-29 07:57:46,2017-08-17 08:38:18
IS,Is there any plan to refactor the legacy Ops into nnvm,Right now primitive Tensor operations are directly registered with nnvm but the legacy layer operations go through a more convoluted forward backward wrapper to interact with nnvm and thus the graph executor After 6928 Is there any thought to refactor the legacy ops into the nnvm tensor primitive ops for easier graph optimization,,piiswrong,2017-08-02 20:40:51,2017-08-17 19:08:42
PR,Fix description of argument parser,Change argument parser is description from train cifar10 to train imagenet 1k,,peiyunh,2017-08-17 05:42:56,2017-08-17 19:43:47
IS,how to correctly have multiple inputs with Perl API,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OS X 10 11 6 Compiler gcc Package used Python R Scala Julia Perl MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 897cc55a0ffd53317628bbccadea34b072228937 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error this code my data age mx symbol Variable data age my fc0 mx symbol FullyConnected data data age name fc0 num hidden 1 my data hours mx symbol Variable data hours my fc1 mx symbol FullyConnected data data hours name fc1 num hidden 1 my data years mx symbol Variable data years my fc2 mx symbol FullyConnected data data years name fc2 num hidden 1 my composed input mx symbol Concat data age data hours data years my fc3 mx symbol FullyConnected data composed input name fc3 num hidden 1 my softmax mx symbol MAERegressionOutput data fc3 name softmax my module mx mod Module symbol softmax gives the following error You created Module with Module data names data but input with name wouldata' is not found in symbol list arguments Did you mean one of data age data hours data years softmax label at Users arussell perl5 lib perl5 AI MXNet Module Base pm line 46 AI MXNet Module Base check input names AI MXNet Module HASH 0x7fec8583efd8 AI MXNet Symbol HASH 0x7fec83a943d8 ARRAY 0x7fec842f7458 data 1 called at Users arussell perl5 lib perl5 AI MXNet Module pm line 223 AI MXNet Module BUILD AI MXNet Module HASH 0x7fec8583efd8 HASH 0x7fec85812830 called at Users arussell perl5 lib perl5 AI MXNet Module pm line 229 AI MXNet Module Module AI MXNet Module symbol AI MXNet Symbol HASH 0x7fec83a943d8 called at I have multiple data inputs each with their own iterator and I assumed based on the documentation that something along these lines was the way in which to handle this situation but this does no seem to work How does one combine multiple inputs with the Perl API Did I have the right idea and just get the API calls wrong,,"adamcrussell,sergeykolychev,adamcrussell,sergeykolychev,sergeykolychev,adamcrussell,adamcrussell,sergeykolychev,adamcrussell",2017-08-16 20:45:31,2017-08-17 20:04:51
PR,fixes broken compilation by including tensor blob,This PR in mshadow removed a header file which causes mxnet compilation to break This header file needs to be included from this file to replace it Discussed with the author of that PR With this inclusion mxnet compiles fine with the latest commits of mshadow So you could go ahead and merge the latest commits of mshadow as well along with this,,"rahul003,reminisce,rahul003,rahul003,ZihengJiang,piiswrong,rahul003",2017-08-03 02:13:08,2017-08-18 00:01:06
IS,build error expected at end of input,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 4 Compiler G 5 4 Package used Python R Scala Julia Python MXNet version Or if installed from source source MXNet commit hash git rev parse HEAD 1286809a1fc76c0b808b988084fc0950300f40d4 If you are using python package please provide Python version and distribution 3 5 2 Error Message In file included from src operator random operator common h 37 0 from src operator random sample multinomial op h 31 from src operator random sample multinomial op cc 24 src operator random common cuda utils h 95 43 error mxnet common cuda CusolverGetErrorString declared as an inline variable inline const char CusolverGetErrorString cusolverStatus t error src operator random common cuda utils h 95 43 error cusolverStatus t was not declared in this scope src operator random common cuda utils h 95 67 error expected or before token inline const char CusolverGetErrorString cusolverStatus t error src operator random sample multinomial op cc 111 1 error expected at end of input namespace mxnet src operator random sample multinomial op cc 111 1 error expected at end of input src operator random sample multinomial op cc 111 1 error expected at end of input Makefile 275 recipe for target 'build src operator random sample multinomial op o' failed make build src operator random sample multinomial op o Error 1 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 make j What have you tried to solve it N A,,"piiswrong,ptrendx,szha,asmushetzel",2017-08-17 05:16:31,2017-08-18 00:13:06
PR,Tensorcore fullyconnected support2,Consider this an alternative approach to getting TensorCore working with FullyConnected It is far simpler than my first PR for this new functionality If anything this is my proof that one can invoke TensorCore algos through manipulation of the cublas handle along with the existing dot function is use of Hgemm and SgemmEx This PR also shows the type of per instance handle manipulations that are necessary since blindly setting the handle globally to enable TensorCore will have the unfortunate side effect of introducing fp16 casts on the inputs of fp32 I O gemms Bottom line I would not expect you to accept this PR without a discussion I have begun studying the new linear algebra code with the idea of producing an enable TensorCore PR for this new approach I notice the new LA code does not support fp16 I O gemms yet and the solution there will not fit the mold of the existing function templates Also what is the plan for switching over MXNET is use of dot to use the new functions,,"DickJC123,piiswrong,DickJC123,piiswrong,DickJC123",2017-08-14 01:28:16,2017-08-18 01:49:08
PR,Fixed Makefile so a null CUDA ARCH is treated like an unset one,Users can override the Makefile is setting of CUDA ARCH with their own value but then they may think they can revert back to the default setting by leaving a line such as CUDA ARCH in their custom config mk A defined but null CUDA ARCH variable caused the platform to compile for all NVIDIA GPU architectures including unsupported ones like Fermi This PR changes the behavior so that a null CUDA ARCH is treated the same as an undefined one the platform is compiled to the list of supported GPU architectures in the Makefile,,DickJC123,2017-08-18 01:45:33,2017-08-18 04:14:18
PR,Changed FullyConnected to use new linalg gemm plus TensorCore if fp16 I O,GEMMs within the FullyConnected operator switched from using mshadow dot to the new linalg gemm After a trial run this can be the model for removing all uses of dot within MXNet Added a specialization linalg gemm gpu half t that includes use of TensorCore algos by default Users can disable TensorCore on Volta by setting the environment variable MXNET CUDA ALLOW TENSOR CORE 0,,"DickJC123,piiswrong,piiswrong,piiswrong,DickJC123,DickJC123,DickJC123,piiswrong,piiswrong,piiswrong,DickJC123,piiswrong,DickJC123,piiswrong,DickJC123,piiswrong",2017-08-17 04:05:19,2017-08-18 04:16:52
PR,Modify pip install to specific tag,,,kevinthesun,2017-08-18 00:06:10,2017-08-18 04:20:30
PR,scala package spark fix example script,I think it has been broken for a while and this is one of the fixes which I am bringing to spark module,,"CodingCat,yzhliu,CodingCat,CodingCat",2017-08-10 04:52:09,2017-08-18 09:27:32
PR,V0 11 0,,,szha,2017-08-18 07:35:41,2017-08-18 16:41:07
IS,Gluon Resnet152 v1 batch size 32 does not work quite well,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Amazon Linux Compiler Package used Python R Scala Julia Python MXNet version Master Or if installed from source MXNet commit hash git rev parse HEAD 1a617fadc3cf612f964722e09571e1b80ad16f68 If you are using python package please provide Python version and distribution Python 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Using resnet152 v1 model 2 python image classification py dataset dummy gpus 8 epochs 10 benchmark mode imperative model resnet152 v1 log interval 1 This command uses default batch size of 32 and the results are not comparable as compared to symbolic approach Here are the results with symbolic imperative and hybrid approach after running for 10 epochs img width 981 alt screen shot 2017 08 14 at 2 32 53 pm src,,"Roshrini,szha,Roshrini,szha",2017-08-14 21:33:41,2017-08-18 16:50:50
IS,Quick question about LSTM parameters,I am using LSTM from mxnet and was able to get the parameters of the LSTM block I have a question about the biases According to the equation below taken from here there is a single bias f defined to get f t But mxnet is LSTM parameters contain two biases for this equation 1 i2h f bias and 2 h2h f bias Is b f here simply i2h f bias h2h f bias Or there is some other relation screen shot 2017 08 15 at 3 13 07 pm I am also seeing that i2h f bias and h2h f bias are same sometimes Thank You,,"szha,szha,szha",2017-08-15 22:20:18,2017-08-18 20:28:55
IS,Feature Request save symbol as well in gluon,I start to use gluon but I found it cannot save the model into the symbol file Is it possible to implement this function,,"piiswrong,szha,szha,szha",2017-08-16 23:08:31,2017-08-18 21:06:36
IS,example for custom iterator not working,I am following the description and example for creating a custom iterator as described here The following code produces a ValueError mod fit data iter num epoch 5 ValueError Shape of labels 0 does not match shape of predictions 1 My questions Can anyone reproduce this problem Does anyone know a solution I am using jupyter on a mac with everything freshly installed including python I have also tested on python directly using Python 3 6 1 Anaconda custom x86 64 default May 11 2017 13 04 09 GCC 4 2 1 Compatible Apple LLVM 6 0 clang 600 0 57 on darwin Code import mxnet as mx import os import subprocess import numpy as np import matplotlib pyplot as plt import tarfile import warnings warnings filterwarnings ignore category DeprecationWarning import numpy as np data np random rand 100 3 label np random randint 0 10 100 data iter mx io NDArrayIter data data label label batch size 30 for batch in data iter print batch data batch label batch pad NDArray 30x3 0 NDArray 30 0 0 NDArray 30x3 0 NDArray 30 0 0 NDArray 30x3 0 NDArray 30 0 0 NDArray 30x3 0 NDArray 30 0 20 lets save data into a csv file first and try reading it back np savetxt wouldata csv' data delimiter ' ' data iter mx io CSVIter data csv wouldata csv' data shape 3 batch size 30 for batch in data iter print batch data batch pad NDArray 30x3 0 0 NDArray 30x3 0 0 NDArray 30x3 0 0 NDArray 30x3 0 20 class SimpleIter mx io DataIter def init self data names data shapes data gen label names label shapes label gen num batches 10 self provide data zip data names data shapes self provide label zip label names label shapes self num batches num batches self data gen data gen self label gen label gen self cur batch 0 def iter self return self def reset self self cur batch 0 def next self return self next def provide data self return self provide data def provide label self return self provide label def next self if self cur batch self num batches self cur batch 1 data mx nd array g d 1 for d g in zip self provide data self data gen label mx nd array g d 1 for d g in zip self provide label self label gen return mx io DataBatch data label else raise StopIteration import mxnet as mx num classes 10 net mx sym Variable wouldata' net mx sym FullyConnected data net name 'fc1' num hidden 64 net mx sym Activation data net name arelu1' act type relu net mx sym FullyConnected data net name 'fc2' num hidden num classes net mx sym SoftmaxOutput data net name isoftmax' print net list arguments print net list outputs wouldata' 'fc1 weight' 'fc1 bias' 'fc2 weight' 'fc2 bias' isoftmax label' isoftmax output' import logging logging basicConfig level logging INFO n 32 data iter SimpleIter wouldata' n 100 lambda s np random uniform 1 1 s isoftmax label' n lambda s np random randint 0 num classes s mod mx mod Module symbol net mod fit data iter num epoch 5 Error ValueError Traceback most recent call last ipython input 57 6ceb7dd11508 in module 9 10 mod mx mod Module symbol net 11 mod fit data iter num epoch 5 Users bernd anaconda lib python3 6 site packages mxnet module base module py in fit self train data eval data eval metric epoch end callback batch end callback kvstore optimizer optimizer params eval end callback eval batch end callback initializer arg params aux params allow missing force rebind force init begin epoch num epoch validation metric monitor 493 end of batch True 494 495 self update metric eval metric data batch label 496 497 if monitor is not None Users bernd anaconda lib python3 6 site packages mxnet module module py in update metric self eval metric labels 678 Typically data batch label 679 680 self exec group update metric eval metric labels 681 682 def sync params from devices self Users bernd anaconda lib python3 6 site packages mxnet module executor group py in update metric self eval metric labels 561 labels OrderedDict zip self label names labels slice 562 preds OrderedDict zip self output names texec outputs 563 eval metric update dict labels preds 564 565 def bind ith exec self i data shapes label shapes shared group Users bernd anaconda lib python3 6 site packages mxnet metric py in update dict self label pred 89 label label values 90 91 self update label pred 92 93 def update self labels preds Users bernd anaconda lib python3 6 site packages mxnet metric py in update self labels preds 369 Predicted values 370 371 check label shapes labels preds 372 373 for label pred label in zip labels preds Users bernd anaconda lib python3 6 site packages mxnet metric py in check label shapes labels preds shape 22 if label shape pred shape 23 raise ValueError Shape of labels does not match shape of 24 predictions format label shape pred shape 25 26 ValueError Shape of labels 0 does not match shape of predictions 1,,,2017-08-18 09:43:05,2017-08-20 10:46:21
IS,Network with shared layer,I try to make 2 networks with shared layer something like that But I want to be able to train each branch with a different data iterator So I can not use the method used here for triplet loss Why I need this structure I have two data base with the same kind of input but different results And I hope this kind of structure can reduce global model memory size from this comment issuecomment 320524117 I try to use shared module arguments in mx mod bind I was able to use shared module for a full clone But when I try to add specialized part with this code I got this error after first fit ValueError Find name fc3 2 bias that is not in the arguments,,,2017-08-19 18:41:07,2017-08-20 12:22:18
IS,Change Network parameters in Gluon API,For the example let say I want to have a dropout probability that change over iteration In NDArray API I can do it like that from gluon tutoriel,,szha,2017-08-19 21:46:10,2017-08-20 12:35:34
PR,Fix a bug in SequentialRNNCell reset,SequentialRNNCell reset should invoke reset in all layer cells This is an issue in L379 It is wrong if someone use SequentialRNNCell call to unroll the network as the way in BaseRNNCell unroll Although SequentialRNNCell unroll is correct now because it uses unroll in all layer cells cell unroll will reset at the beginning,,"ZiyueHuang,szha,szha,ZiyueHuang,ZiyueHuang,ZiyueHuang,szha,ZiyueHuang,szha,ZiyueHuang",2017-08-14 07:49:35,2017-08-20 21:43:02
IS,cmake DUSE DIST KVSTORE 1 ERROR,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04lts Compiler gcc 4 8 4 Package used Python R Scala Julia python MXNet version 0 11 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace mxnet cmake DUSE DIST KVSTORE 1 Performing Test SUPPORT CXX11 Performing Test SUPPORT CXX11 Success Performing Test SUPPORT CXX0X Performing Test SUPPORT CXX0X Success Performing Test SUPPORT MSSE2 Performing Test SUPPORT MSSE2 Success Could NOT find MKL missing MKL INCLUDE DIR MKLML GNU LIBRARY MKL not found Could NOT find MKL missing MKL INCLUDE DIR MKLML GNU LIBRARY Found OpenBLAS libraries usr lib libopenblas so Found OpenBLAS include usr include Looking for pthread h Looking for pthread h found Looking for pthread create Looking for pthread create not found Looking for pthread create in pthreads Looking for pthread create in pthreads not found Looking for pthread create in pthread Looking for pthread create in pthread found Found Threads TRUE CUDA detected 8 0 Found cuDNN include usr local cuda include library usr local cuda lib64 libcudnn so Added CUDA NVCC flags for sm 61 OpenCV LIBS opencv core opencv highgui opencv imgproc OpenCV found usr share OpenCV Try OpenMP C flag fopenmp Performing Test OpenMP FLAG DETECTED Performing Test OpenMP FLAG DETECTED Success Try OpenMP CXX flag fopenmp Performing Test OpenMP FLAG DETECTED Performing Test OpenMP FLAG DETECTED Success Found OpenMP fopenmp Found cuDNN include usr local cuda include library usr local cuda lib64 libcudnn so You have called ADD LIBRARY for library mxnet without any source files This typically indicates a problem with your CMakeLists txt file Found ZMQ usr lib x86 64 linux gnu libzmq so Found Protobuf usr lib x86 64 linux gnu libprotobuf so Found PROTOBUF Compiler usr bin protoc CMake Error at CMakeLists txt 438 target link libraries The debug argument must be followed by a library Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 cmake DUSE DIST KVSTORE 1 2 3 What have you tried to solve it 1 2 3,,rahul003,2017-08-18 09:33:59,2017-08-21 00:55:21
IS,Need Python bindings for Depthwise 2d Convolution,do we need to implement in python,,"domdivakaruni,piiswrong",2017-08-21 01:19:31,2017-08-21 04:04:34
IS,too large launch parameter MapReduceKeepDim1,error message this will happen when use mx sym L2Normalization and one dim of the shape is bigger than 65536 how to fix it some similar issue which not solved thanks,,"tornadomeet,tornadomeet",2017-08-18 16:28:27,2017-08-21 08:54:37
IS,mx sym argsort cannot sort array with large tensor,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler GCC4 4 7 Package used Python R Scala Julia python MXNet version 0 11 0 MXNet commit hash 568b5a2d3e701768ff6f270238e5edccc2f35ff1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce 1 The provided code will give the cnt 0 which should be 0 because the argsort should return all the integer from 0 to 2047 2 If the result 4 0 i has been changed to result 3 0 i the result will be correct again What have you tried to solve it 1 If the size of coord data is 10000 2048 and the axis in argsort changed to 1 then result 8000 will cover from 0 to 2047 as desired but vis int result 9000 i will be incorrect again I guess it may because 8000 4 2048 9000 2 By several tests it seems that the argsort function can only deal with the first 2 24 element,,"piiswrong,sxjscience,sxjscience,sxjscience,sxjscience",2017-08-17 08:32:27,2017-08-21 18:00:55
PR,Fix argsort Update MShadow,Fix We use the sort in CUB now,,"sxjscience,sxjscience",2017-08-21 08:07:08,2017-08-21 18:00:55
PR,Temporarily disable MIN 1 and add clean in Amalgamation to fix test,There are currently two problems in Amalgamation 1 linalg introduces dependency on cblas but the MIN 1 mode of amalgamation the most restricted mode does not include cblas I turned of MIN 1 for now Please reenable it after linalg situation is resolved 2 The amalgamatiom makefile does not handle dependencies correctly thus mxnet predict all cc is not regenerated when mxnet source files changes I added clean every time to fix it for now,,"piiswrong,asmushetzel,piiswrong,asmushetzel,asmushetzel,asmushetzel,asmushetzel",2017-08-21 18:29:35,2017-08-21 18:44:52
PR,Move usage of persistent BN to cuDNN 7 0 3,,,ptrendx,2017-08-21 19:33:54,2017-08-21 23:15:55
PR,0 11 1,piiswrong,,"szha,domdivakaruni",2017-08-21 19:15:12,2017-08-21 23:17:26
PR,Fix optimizer parms in fit py Do not repeatedly call slow prepare mkl sh script,,,cjolivier01,2017-08-21 21:39:08,2017-08-21 23:19:08
PR,remove MXNet License from rcnn license,remove MXNet License from rcnn license,,mbaijal,2017-08-22 01:36:39,2017-08-22 01:38:20
IS,Enable dropout in testing,Currently dropout is only enabled in training process with parameters rescaled I like to have dropout in testing i e while making predictions how could this be implemented In this case the rescaling in testing should be aligned with that in the training phase Can I simply set is train True for forward pass in testing A similar problem has been solved in keras Update I find a same question here 3821 which is however for R Does anybody know a solution for Python,,piiswrong,2017-07-26 05:39:22,2017-08-22 03:51:58
IS,mx sym L2Normalization has bug when shpae 0 is bigger than 65536 and multi gpus,when first shape of L2Normalization data 65536 and using mulit gpus then it will give errors like this single gpu training is ok image we can reproduce as follows change code form to run mxnet train mnist py examples from then when running with single gpu training is ok but when trained when more than one gpu it will broken thanks,,"tornadomeet,tornadomeet",2017-08-21 09:15:07,2017-08-22 06:14:19
IS,link in mxnet io mismatch,The content in this page is not matching with this Could it be possible to add document of 'mx name prefix' into python API page It is not included yet while hiding in this,,"Godricly,mli,Godricly",2017-04-13 03:23:00,2017-08-22 07:22:32
IS,mx ndarray random with native int type support,Hi I notice that current mx ndarray only support float type Is there any plan to create a native one for int type,,"Godricly,Godricly",2017-04-24 11:10:10,2017-08-22 07:22:43
PR,Fix optimizer parms in fit py Do not repeatedly call slow prepare mk,l sh script 7545 Only call MKL script once Fix 'momentum' and 'multi precision' optimizer args fix working,,cjolivier01,2017-08-21 23:49:53,2017-08-22 16:27:25
IS,Feature Request cuddn v7 Grouped Convolutions,The current implementation of num groups in mxnet is fairly slow and I was wondering since Cudnn v7 now supports grouped convolutions what is the timeline progress of the implementation,,"samster25,chinakook",2017-08-21 23:34:18,2017-08-22 19:16:05
PR,Sparse Tensor request for reviews,Following please find a summary of changes made for sparse tensor feature on CPU Note this PR contains duplicate code in PR 7015 Frontend Interfaces NDArray Class Hierarchy NDArrayBase root NDArray SparseNDArray RowSparseNDArray CSRNDArray Sparse Storage Types Overview Two storage formats are supported row sparse and csr row sparse L378 tensors typically used for sparse gradient updates similar to tf IndexedSlice csr L331 tensors with the standard csr storage format Compressed sparse row 28CSR 2C CRS or Yale format 29 Functionality Overview operators with sparse inputs A list of operators in this PR include Arithmetics elemwise add row sparse dot csr row sparse dot csr dense Conversion cast storage csr dense cast storage row sparse dense Indexing sparse retain row sparse L232 slice csr Creation zeros row sparse zeros csr iterators and IO utils functions NDArrayIter with CSRNDArray LibsvmIter which reads files in libsvm format and produces CSRNDArray load save RowSparseNDArray CSRNDArray kvstore and optimizer with sparse gradient updates sgd update row sparse and sgd mom update row sparse will update rows of the weight where the gradient of the row contains nonzeros kv push with row sparse gradient which only sends over the nonzero gradients over the network kv row sparse pull with row ids which only pulls back a RowSparseNDArray with specified row id over the network unsupported functionality There is a list of basic functionality supported in NDArray but is either not supported or inefficient in SparseNDArray get set an element in CSRNDArray RowSparseNDArray by index not supported reshape in CSRNDArray RowSparseNDArray not supported slice at in CSRNDArray RowSparseNDArray not supported inplace add sub mul div mod broadcast to in CSRNDArray RowSparseNDArray not efficient Please refer to the TODO list for a list of ongoing planned operators Backend Interfaces NDArray NDArray uses storage type to determine what storage format to use to represent a tensor When binding a graph InferStorageType pass will be applied to infer graph entries with unknown storage type The pass is implemented in MXNet instead of NNVM infer type and infer shape passes are both moved from NNVM to MXNet plan memory pass For node entries of non default storage type they are annotated with kDynamicStorageID memory are neither requested nor released No memory sharing optimization is applied memory allocation and sharing For node entries of kDefaultStorage i e dense storage memory allocation and sharing is done as usual For node entries of non default storage type memory allocation is delayed to runtime No memory sharing is applied to these entries even when shared exec is provided executor dispatch and storage fallback For each node in graph if it involves non default storage in either its inputs or outputs FComputeExExecutor will be created if FComputeEx attribute is registered If FComputeEx is not registered an FComputeExcutor FStatefulComputeExcutor will be created inputs with non default storage will be converted to NDArrays with default i e dense storage the outputs will be converted back to non default storage if necessary The converted NDArrays are temporary and will be released when there is no reference to them kvstore Sparse gradient update requires a parameter to be initialized as row sparse format in kvstore If an optimizer is registered with FComputeEx then the update may only involve the rows with non zero gradients Gradients are sent in row sparse format to reduce the stress on network bandwidth For distributed training kRowSparsePushPull mode is introduced so that row ids of the row sparse gradients are encoded as keys by kvstore workers When the kvstore server receives the message it constructs a row sparse gradient NDArray based on the row ids decoded from it and applies the optimizer to the weight The memory for parameters are pre allocated on parameter servers instead of storing weight row by row TODO list Operators elemwise binary unary operators row sparse done to be merged in another PR negative square abs sqrt round ceil floor fix elemwise add elemwise sub elemwise div elemwise mul maximum minimum mul scalar elemwise binary unary operators csr ongoing negative square broadcast mul mul scalar inplace broadcast add broadcast sub mul scalar operators broadcast operators csr broadcast operators row sparse elemwise operators csr sync copy from with scipy sparse csr matrix input asscipy csr kvstore and optimizer lazy initialization postpone parameter initialization to kvstore server storing row sparse weight by rows to support weights of 2 billion rows This avoid pre allocating memory for row sparse weight and reduces memory consumption at kvstore servers other optimizers for sparse gradient update e g adam List of important files NDArray related files ndarray h c api h ndarray cc common utils h Executor related files c api executor h attach op exec pass cc graph attr types h op attr types h executor h graph executor h graph executor cc infer graph attr pass cc inplace addto detect pass cc Kvstore related files comm h kvstore dist h kvstore dist server h kvstore dist local h pending PR 7015,,"eric-haibin-lin,piiswrong,eric-haibin-lin,ZihengJiang,ZihengJiang,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,formath,CNevd,eric-haibin-lin,eric-haibin-lin,CNevd,eric-haibin-lin,reminisce,CNevd,eric-haibin-lin,CNevd,eric-haibin-lin,jermainewang,eric-haibin-lin,jermainewang,reminisce,jermainewang,tqchen,eric-haibin-lin,piiswrong,jermainewang,eric-haibin-lin,eric-haibin-lin,piiswrong,gautamkmr",2017-07-18 00:03:32,2017-08-22 21:56:33
PR,nightly build stochastically choose optimizer,Nightly test to validate optimizer is not broken Also gfixed build problem with USE KVSTORE enabled when using cmake I have validated that all of these converge beyond the test accuracy krishnamurthy,,"cjolivier01,madjam,cjolivier01,madjam,cjolivier01,nswamy,cjolivier01,cjolivier01",2017-08-22 16:27:28,2017-08-22 23:04:03
PR,add flatten option to fc,piiswrong,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2017-08-22 00:16:00,2017-08-23 00:03:08
PR,Updating the LICENSE and NOTICE Files,nswamy dev mxnet apache org mbaijal amazon com,,"mbaijal,nswamy",2017-08-22 23:43:04,2017-08-23 00:04:17
PR,Update License and Notice Files,,,"nswamy,nswamy",2017-08-23 00:15:57,2017-08-23 00:17:17
PR,update NEWS README,mbaijal,,nswamy,2017-08-23 00:39:02,2017-08-23 00:40:45
IS,As keras use mxnet as backend I can not use the GPU to train,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu16 04 Compiler Package used Python R Scala Julia Python2 7 MXNet version mxnet 0 11 0rc2 Or if installed from source mxnet 0 11 0rc2 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 22 01 04 G mxnet cpp mxnet20170819 mxnet src storage storage cc 109 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA no CUDA capable device is detected Minimum reproducible example if you are using your own code please provide a short script that reproduces the error keras examples mnist mlp py model Sequential model add Dense 512 input shape 784 model add Activation arelu' model add Dropout 0 2 model add Dense 512 model add Activation arelu' model add Dropout 0 2 model add Dense 10 model add Activation isoftmax' model summary gpu list gpu 0 model compile loss 'categorical crossentropy' optimizer SGD context gpu list Remove this line we use cpu to train metrics 'accuracy' Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I run the sample named mnist mlp py When I use cpu it works When I try it with GPU it does not work 2 I write some codes to test the gpu function in mxnet it works The codes are like these import mxnet as mx a mx nd ones 2 3 mx gpu 3 What have you tried to solve it 1 I tried to compile the source codes Then I tried again I got the same errors My cuda version is 8 0 In gpu mode tensorflow and mxnet are ok But when keras use mxnet as backend I can not use the GPU to train,,"domdivakaruni,sandeep-krishnamurthy",2017-08-19 14:19:31,2017-08-23 02:29:45
PR,add resnet50 v2 pretrained,validation accuracy 0 748306 top k accuracy 5 0 921317 verified,,"zhreshold,piiswrong,zhreshold,piiswrong,zhreshold,szha,zhreshold,szha",2017-08-22 23:46:00,2017-08-23 02:57:33
IS,Segmentation fault during ab load test,Environment info Operating System Ubuntu 16 04 2 LTS on AWS p2 xlarge Compiler gcc version 5 4 0 Package used Python R Scala Julia Python MXNet version Or if installed from source 0 10 1 MXNet commit hash git rev parse HEAD 3ceb6d2f91121d5ffa5b81f435e8bcfcc1a75792 Python version and distribution Python 2 7 12 pip 9 0 1 from usr local lib python2 7 dist packages python 2 7 What am I trying to achieve I wanted to speed up predictions on a bunch of urls in order to scale So I decided to preload the model I setup a server using rpyc package that preloads the model I have client py that connects to the server and predicts the result All of this is working fine but when I 'load test using apache bench' it gives me the following error Error Message I tried 1 gdb to understand the error 2 htop to see how its performing Finally I would try loading the model normally instead of pre loading Let me know what can be done Thanks,,,2017-06-28 09:16:42,2017-08-23 04:39:12
PR,Non maximum suppression op,Add a general box non maximum suppression op,,"zhreshold,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,zhreshold,zhreshold,piiswrong,piiswrong,zhreshold",2017-07-14 19:34:48,2017-08-23 18:24:45
PR,Minor change fixed typo,Minor change suggested by,,stefanhenneking,2017-08-23 18:38:17,2017-08-23 19:10:51
PR,modify parameters counting of FC and CONV,Now I am using computer of my company which restrict to push large mount of code to internet so I open a new pull request and I will close the previous one,,"qingzhouzhen,piiswrong,qingzhouzhen",2017-08-23 01:34:56,2017-08-23 20:33:57
PR,FP16 I O conv deconv to use pseudo fp16 ignoring MSHADOW USE PASCAL,Eliminates all references to MSHADOW USE PASCAL and dictates fp32 math for fp16 IO Convolution and Deconvolution operators i e pseudo fp16 consistent with the approach now taken with gemms in FullyConnected This setting plays to the strength of the Volta Tensor Core algos which are enabled by default,,DickJC123,2017-08-19 00:42:55,2017-08-23 21:15:56
PR,Set dev id in streams also update mshadow,This sets the stream is dev id field properly so that in the future we can have low level routines like linalg gemm stream s make GPU arch specific choices based on the stream This change exposes an issue with an old version of mshadow so we bundle this with an mshadow update,,DickJC123,2017-08-18 22:45:55,2017-08-23 21:41:18
PR,I found a bug modify counting parameters of FullyConnected layer,I found a bug on mx viz print summary net data 32 3 192 192 I thing paremeter of FullyConnected goes wrong for example the output of flatten layer length is 13824 and feeds it to a fullyconnect num hidden is 4096 the paremter should be 13824 4096 4096 56627200 but currently it cound as 13824 4096 13824 56636928 below show my test the first is currently version the second row is the shape of out put data the third row id the paremeters of the layter before modify pool5 Pooling 384x6x6 0 bsrlast relu flatten Flatten 13824 0 pool5 fc6 fc FullyConnected 4096 56636928 flatten after modify pool5 Pooling 384x6x6 0 bsrlast relu flatten Flatten 13824 0 pool5 fc6 fc FullyConnected 4096 56627200 flatten is added by me I donot know why they huddled toghter if I donot add,,"qingzhouzhen,zhreshold,qingzhouzhen,piiswrong,qingzhouzhen",2017-08-19 11:19:46,2017-08-24 00:23:35
IS,Gluon Data augmentation,How to augment the data in the data loading for an array dataset like CIFAR in gluon One way I can think is using image record Is there any other way to augment the data in the iterator,,,2017-08-23 14:55:14,2017-08-24 04:52:59
IS,conflict mxnet version hint,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia Python MXNet version 0 11 0 Or if installed from source yep If you are using python package please provide Python version and distribution anaconda2 4 4 0 Error Message Please paste the full error message including stack trace After finishing compiling mxnet 0 11 0 from source I use the command cd python pip install e to install mxnet egg for python The output is Installing collected packages mxnet Running setup py develop for mxnet Successfully installed mxnet 0 10 1 while the return sting of the command mxnet version is '0 11 0' Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 git clone recursive 2 make 3 cd python pip install e,,szha,2017-08-21 10:16:07,2017-08-24 07:47:33
IS,Paradox VRAM demand as a function of batch size Low batch size high VRAM demand,Dear community I'm running mxnet on the environment as mentioned below with the following hardware 32 GB RAM 8 x NVIDIA 1080 Ti 8 GB VRAM 12 CPUs MXnet is utlized to train a stacked denoising autoencoder reconstruction of an array with dimension about 62 000 x 100 I was looking for a batch size with minimum VRAM demand Therefore I measured the VRAM demand per single Card for a given combination of number of GPUs and array batch size The results are shown in the attached table Columns indicate array batch size and rows indicate number of GPUs Each entry indicates the GPU memory usage per Card paradox vram batchsize As one can see if array batch size is below or above a particular value VRAM demand rises Given an array batch size of 12 computation crashes due to cuda memory allocation error red style Given a array batch size of 128 2 and 4 GPUS or 256 6 results in a minimum VRAM demand of about 3GB per card green style Entries without measurements are empty My question is Why does VRAM hunger rise below a particular array batch size value Environment info Operating System REDHAT 7 3 Package used Python R Scala Julia R MXNet version 0 1 0 1 R sessionInfo R version 3 4 0 2017 04 21 Platform x86 64 redhat linux gnu 64 bit Running under Red Hat Enterprise Linux Server 7 3 Maipo Matrix products default BLAS LAPACK usr lib64 R lib libRblas so locale 1 LC CTYPE en GB UTF 8 LC NUMERIC C LC TIME en GB UTF 8 LC COLLATE en GB UTF 8 LC MONETARY en GB UTF 8 LC MESSAGES en GB UTF 8 LC PAPER en GB UTF 8 8 LC NAME C LC ADDRESS C LC TELEPHONE C LC MEASUREMENT en GB UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 compiler 3 4 0 tools 3 4 0,,ptrendx,2017-08-15 11:16:41,2017-08-24 10:02:55
IS,error LNK2019 class nnvm Graph cdecl nnvm ApplyPasses,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System win10 Compiler vs2013 x64 debug Package used Python R Scala Julia cpp package MXNet version mxnet cpu apache mxnet src 0 11 rc1 incubating Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message 1 error LNK2019 class nnvm Graph cdecl nnvm ApplyPasses class nnvm Graph class std vector class std basic string char struct std char traits char class std allocator char class std allocator class std basic string char struct std char traits char class std allocator char const ApplyPasses nnvm YA AVGraph 1 V21 AEBV vector V basic string DU char traits D std V allocator D 2 std V allocator V basic string DU char traits D std V allocator D 2 std 2 std Z class nnvm Graph cdecl nnvm ApplyPass class nnvm Graph class std basic string char struct std char traits char class std allocator char const ApplyPass nnvm YA AVGraph 1 V21 AEBV basic string DU char traits D std V allocator D 2 std Z D YISA PR MXNet cpp master windows vs MxnetTestApp lenet with mxdataiter obj MxnetTestApp Minimum reproducible example nnvm Symbol sym nnvm Graph g g attrs json std make shared nnvm any symbol json str sym outputs nnvm ApplyPass g LoadLegacyJSON outputs Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 add all lib openblas dmlc mxnet opencvlib but still error 2 3,,chinakook,2017-08-22 10:09:30,2017-08-24 11:15:56
PR,Update MShadow,The MShadow side has fixed the bug in the reduce kernel and has enhanced the speed of batch gemm,,"sxjscience,sxjscience,szha,sxjscience,sxjscience",2017-08-22 06:30:01,2017-08-24 12:26:05
IS,tutorial for ImageDetIter,Is there any more detailed tutorial about ImageDetIter Its documentation is almost identical to ImageIter,,"Godricly,Godricly",2017-08-22 07:41:04,2017-08-24 16:05:17
PR,fix for amalgamation build with MIN 1,Fixing the first issue listed in PR 7541,,"asmushetzel,piiswrong,asmushetzel,piiswrong,asmushetzel,piiswrong,mli,DickJC123,piiswrong,DickJC123,eric-haibin-lin,rahul003",2017-08-24 15:03:42,2017-08-24 17:06:02
PR,Fix import error of broadcast max min mod in ndarray py,1 Added import broadcast minimum broadcast maximum broadcast mod in ndarray py 2 Added unit tests for mx nd functions defined in ndarray py maximum minimu etc 3 Reformatted test operator py by surrounding top level functions with two blank lines so that PyCharm would not complain non conforming coding style Ref blank lines haibin lin,,"reminisce,eric-haibin-lin,reminisce,piiswrong,reminisce,szha,reminisce,szha,eric-haibin-lin",2017-08-23 07:33:44,2017-08-24 17:29:49
PR,add ctx to begin state in rnn layer,,,szha,2017-08-23 18:33:22,2017-08-24 17:43:53
PR,contrib ctc interface changes cudnn7 CTC and gluon CTC,This change is to make current contrib CTC compatible with the cudnn7 CTC interface and to add CTC loss layer for gluon,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,szha,piiswrong,szha",2017-08-13 03:00:38,2017-08-24 19:02:42
PR,Fix symbol load json,Fix 6419,,"lx75249,lx75249,piiswrong,lx75249,lx75249,piiswrong,lx75249,piiswrong,lx75249",2017-05-24 11:12:08,2017-08-24 19:03:24
PR,Add flag to hide automatic logging in mod fit,Would something like this be of interest This is useful if your own callback for batch end and eval end reset the metrics Without this these metrics would always print out NaNs The alternative is to not reset the metrics this leads to misleading values for batch end metrics for others you may wish to change the formatting of the log for easy parsing and do not want repeats in your log,,"jmerkow,jmerkow,piiswrong",2017-05-25 14:34:11,2017-08-24 19:05:43
PR,Fixed Python 3 compatibility with sys version info,Decode the bytes object to produce a string required by Python 3,,"mli,piiswrong",2017-05-28 03:23:09,2017-08-24 19:06:43
PR,WIP Scala rnn interface,pls ignore the version change,,"yzhliu,piiswrong,yzhliu,piiswrong",2017-05-31 16:14:52,2017-08-24 19:09:59
PR,Added gen data py and modified README md for bi lstm sort example,Bi lstm sort example did not have gen data py file which was required to generate data See I used the code from and added it to repo,,"gurumurthys,rahul003",2017-06-02 22:23:21,2017-08-24 19:11:18
PR,Provide instructions to install a particular release,,,"indhub,mli,indhub,mli,indhub,madjam,mli,indhub",2017-06-05 19:53:14,2017-08-24 19:11:36
PR,Fix RCNN multi gpu bucketing warning which may cause OOM error,There is two bug indeed 1 When extra is zero shuffle will not be executed loader py L153 L153 L154 2 Different aspect ratio images may in the same bucket when len horz inds self batch size 0 and len vert inds self batch size 0 loader py L145 L156 L145 L156 In order to avoid this happening I just pad some images that has the same aspect ratio Warning message as below,,"Zehaos,piiswrong,precedenceguo",2017-07-08 14:33:02,2017-08-24 19:14:27
PR,Fix MNIST tutorial Rebind before inference,Changes in master broke the MNIST tutorial and the tutorial now throws the following error This commit modifies the tutorial to rebind module before inference,,"indhub,piiswrong",2017-07-10 07:52:22,2017-08-24 19:15:09
PR,Changed next method to use the seq size attribute,Changed next method to use the seq size attribute and not the global variable seq size,,"adamcrussell,sergeykolychev",2017-08-18 15:08:05,2017-08-24 20:21:10
PR,Relaxing condition in slice,,,"ptrendx,ptrendx",2017-08-15 22:22:23,2017-08-24 20:44:19
PR,Fixing loss function code in tutorial,The function softmax cross entropy loss is no longer available in gluon loss Making the necessary update to the tutorial code Also record in one part of the tutorial is not preceded by autograd resulting in an undefined error adding the reference to autograd,,"szha,szha",2017-08-23 18:55:07,2017-08-24 20:51:26
PR,Add MXNet MKL pip install,,,kevinthesun,2017-08-24 21:16:36,2017-08-24 22:00:57
PR,add license to new file resolving jenkins build issues,This PR added a new file which did not have a header This caused all builds to fail sanity check Example of a build is here Excerpt from that Used this tool to add a header,,rahul003,2017-08-24 21:45:08,2017-08-24 22:08:40
PR,Update note engine md,small edits,,"szha,piiswrong",2017-08-04 05:15:16,2017-08-24 22:17:08
PR,fix print format in im2rec py,,,"mwbyeon,piiswrong,mwbyeon,piiswrong",2017-08-05 02:41:45,2017-08-24 22:18:37
PR,nightly build test mnist training and optimizer 7559,Only call MKL script once Fix 'momentum' and 'multi precision' optimizer args fix cmake build for active kvstore stochastic choice of optimizer for mnist training Run all three optimizers Add just lenet test,,"cjolivier01,cjolivier01",2017-08-22 23:06:59,2017-08-24 22:43:22
PR,update description in train imagenet py,,,wangg12,2017-07-26 08:54:52,2017-08-24 22:57:48
PR,Added early stop call back class,This is an updated version of the pull request 6304 It adds a new class for handling early stopping while training the neural networks The class supports early stopping and save checkpoints when the early stopping happened,,"piiswrong,piiswrong",2017-07-11 09:00:40,2017-08-24 22:59:18
PR,Add installation instruction for Fedora 7107,,,"kottmann,kottmann,madjam,kottmann,sandeep-krishnamurthy,kottmann,sandeep-krishnamurthy,kottmann,piiswrong",2017-07-20 14:23:52,2017-08-24 23:03:54
PR,make MXDataIter work without indices,indices are optional custom cpp iterators providing data batches without indices should work while using MXDataIter Testing python tests python unittest test io py 10 58 26 src io iter mnist cc 91 MNISTIter load 60000 images shuffle 1 shape 100 784,,saswatac,2017-08-14 14:59:44,2017-08-24 23:22:26
IS,how to construct contrib operator,auto rois mxnet cpp Operator contrib Proposal SetInput cls prob rpn cls prob reshape SetInput bbox pred rpn bbox pred SetInput im info im info SetParam feature stride rpn feat stride SetParam scales 8 16 32 SetParam ratios 0 5 1 2 SetParam rpn pre nms top n test rpn pre nms top n SetParam rpn post nms top n test rpn post nms top n SetParam threshold test nms thresh SetParam rpn min size test rpn min size CreateSymbol rois is this right way to create Proposal operator but why segment falut in this code,,,2017-08-24 04:04:29,2017-08-25 00:05:47
PR,add std rgba to normalize options,,,"zhreshold,szha,zhreshold,piiswrong,zhreshold,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,zhreshold,piiswrong,zhreshold",2017-08-05 00:25:18,2017-08-25 00:14:04
PR,Expand linalg gemm use,This substitutes linalg gemm for the direct use of mshadow dot However for the case where a cpu gemm is needed but where MSHADOW USE CBLAS 0 the linalg gemm implementation still reverts to calling mshadow dot Adds MIN 1 back to Jenkinsfile Relates to recent work of and,,"DickJC123,DickJC123",2017-08-25 01:18:06,2017-08-25 01:49:59
IS,define a new Parametrized symbol layer and how to use bind init set learning rate it,I have been learning to define Parametrized layer of which parameters will be leared during training time I start by defining a fc layer with reference to 1 2 3 The new parameterized layer definition seems OK but i dont know how to use it Eg how to bind the symbol and how to set myFC parameters learning rate Here is my code What have you tried to solve it 1 a b c net infer shape data 5 1 28 28 custom0 w 10 200 custom0 b 10 infer shape operation is ok Can anyone show me how to use bind init set learning rate a new parameterizd layer Thanks,,,2017-08-13 07:39:17,2017-08-25 02:04:28
PR,NAG also has 'momentum' optimizer args,,,knjcode,2017-08-25 01:33:15,2017-08-25 04:10:43
PR,Convert dot to linalg gemm,This substitutes linalg gemm for the direct use of mshadow dot However for the case where a cpu gemm is needed but where MSHADOW USE CBLAS 0 the linalg gemm implementation still reverts to calling mshadow dot Adds MIN 1 back to Jenkinsfile Relates to recent work of and Resubmission on top of trunk,,"DickJC123,chinakook,DickJC123,asmushetzel,rahul003",2017-08-25 01:48:47,2017-08-25 04:14:01
IS,run time error after compile on ubuntu16 cuda8 cudnn6,Trying to run on ubunut 16 cuda 8 cudnn 6 compiled ok but when I run I get the following seems to be looking for cudnn 5,,,2017-08-23 00:47:58,2017-08-25 04:32:02
IS,create a new layer with mx operator CustomOpProp in mxnet,Hi I wanna create a layer perturbation that generate a noisy tensor N loc 1 scale 1 to perturb input tensor Input The output is N Input But some error about asynchronous engine operation occured Below is the some information Any hits will be appreciated Environment info Operating System Ubuntu 16 04 CPU Compiler gcc 5 4 0 Package used Python R Scala Julia Python 2 7 MXNet version 0 7 0 Error Message runfile ' home xzqjack mxnet example numpy ops permutation layer py' wdir ' home xzqjack mxnet example numpy ops' 22 26 08 src operator softmax output inl h 329 Softmax symbol is renamed to SoftmaxOutput This API will be deprecated in Dec 2015 22 26 09 src io iter mnist cc 91 MNISTIter load 60000 images shuffle 1 shape 100 784 22 26 10 src io iter mnist cc 91 MNISTIter load 10000 images shuffle 1 shape 100 784 INFO root Start training with cpu 0 'Error in CustomOp forward ' object of type 'long' has no len 22 26 10 home xzqjack mxnet dmlc core include dmlc logging h 235 22 26 10 src operator custom cc 79 Check failed op info forward ptrs size ptrs data tags data reqs data ctx is train op info p forward 22 26 10 home xzqjack mxnet dmlc core include dmlc logging h 235 22 26 10 src engine threaded engine h 306 22 26 10 src operator custom cc 79 Check failed op info forward ptrs size ptrs data tags data reqs data ctx is train op info p forward An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 22 26 10 src engine threaded engine h 306 22 26 10 src operator custom cc 79 Check failed op info forward ptrs size ptrs data tags data reqs data ctx is train op info p forward An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Minimum reproducible example Steps to reproduce 1 The code is modified from mxnet example numpy ops custom softmax py If you want run you should download mnist dataset first What have you tried to solve it I created the new layer by referencing mxnet example numpy ops custom softmax py and some other example codes,,"kevinthesun,kevinthesun",2016-12-12 14:48:25,2017-08-25 04:40:12
PR,Update linalg impl h,DickJC123,,piiswrong,2017-08-25 05:15:05,2017-08-25 05:28:49
IS,is 1x1 in convolution operator is not really used,I found that is 1x1 in convolution inl h is not really used MXNet just create the is 1x1 but not use it to skip the im2col function call In contrast the is 1x1 in Caffe is really used to skip im2col function call to accelerate the 1x1 convolution It is very important in some cases such as the Google is Mobilenet,,"chinakook,piiswrong,reminisce,chinakook,reminisce,chinakook,chinakook",2017-08-24 13:57:15,2017-08-25 08:11:03
IS,error C2768 linalg gemm Illegal use of explicit template arguments,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows Compiler Visual studio 2015 update 3 MKL WITHOUT CUDA Package used Python R Scala Julia non MXNet version 0 11 1 Or if installed from source master MXNet commit hash git rev parse HEAD bc468b0 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error Codes in line 357 to 376 of linalg impl h is wrong and cause VS show error C2768 What have you tried to solve it 1 2 3,,"chinakook,chinakook,piiswrong,chinakook",2017-08-25 06:28:09,2017-08-25 08:27:06
IS,MXNetR How to corrupt input layer,I have asked a similar but not the same question in issue 7490 The difference is The intention is not to dropout input units rather corrupt the input by setting values to zero I do not know how to implement it in a net using symbolic language How can one implement the corrupting procedure into the net corrupting procedure p 0 25 set seed 1 x mx nd array matrix runif 25 1 20 nrow 5 mask mx nd uniform 0 1 dim x mask mx nd broadcast lesser mask mx nd array 1 p original input x,,,2017-08-24 12:10:35,2017-08-25 12:01:56
PR,Profiler memleak,Memory leak found with Intel Inspector corrected Validated all unit tests,,cjolivier01,2017-08-25 16:36:34,2017-08-25 16:36:55
PR,fix linalg impl,,,yajiedesign,2017-08-25 06:41:04,2017-08-25 16:59:05
IS,gluon rnn bug,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 2 LTS Compiler Package used Python R Scala Julia python MXNet version mxnet cu80 0 11 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message From the document I know I can run the rnn or lstm layer only feeding in input do not need to feed in hidden state I can run it in CPU but I can not run it in GPU maybe there is a bug This is my example code MXNetError Traceback most recent call last ipython input 7 da2f5a81429d in module 2 layer initialize ctx mx gpu 3 input mx nd random uniform shape 5 3 10 ctx mx gpu 4 output layer input 5 print output shape home sherlock anaconda3 envs mx lib python3 6 site packages mxnet gluon block py in call self args 266 def call self args 267 Calls forward Only accepts positional arguments 268 return self forward args 269 270 def forward self args home sherlock anaconda3 envs mx lib python3 6 site packages mxnet gluon rnn rnn layer py in forward self inputs states 186 self i2h weight i finish deferred init 187 if inputs context device type 'gpu' 188 out self forward gpu inputs states 189 else 190 out self forward cpu inputs states home sherlock anaconda3 envs mx lib python3 6 site packages mxnet gluon rnn rnn layer py in forward gpu self inputs states 218 rnn ndarray RNN inputs params states state size self hidden size 219 num layers self num layers bidirectional self dir 2 220 p self dropout state outputs True mode self mode 221 222 if self mode 'lstm' home sherlock anaconda3 envs mx lib python3 6 site packages mxnet ndarray py in RNN data parameters state state cell state size num layers bidirectional mode p state outputs out name kwargs home sherlock anaconda3 envs mx lib python3 6 site packages mxnet ctypes ndarray py in imperative invoke handle ndargs keys vals out 87 ctypes c int len keys 88 c array ctypes c char p c str key for key in keys 89 c array ctypes c char p c str str val for val in vals 90 91 if original output is not None home sherlock anaconda3 envs mx lib python3 6 site packages mxnet base py in check call ret 127 128 if ret 0 129 raise MXNetError py str LIB MXGetLastError 130 131 if sys version info 0 3 MXNetError 14 22 00 src c api c api ndarray cc 128 Check failed ndinputs i ctx dev mask ctx dev mask 1 vs 2 All inputs must live on the same context But the first argument is on gpu 0 while the 3 th argument is on cpu 0 Stack trace returned 10 entries bt 0 home sherlock anaconda3 envs mx lib python3 6 site packages mxnet libmxnet so 0x1d57cc 0x7f819f9eb7cc bt 1 home sherlock anaconda3 envs mx lib python3 6 site packages mxnet libmxnet so 0xdec2e1 0x7f81a06022e1 bt 2 home sherlock anaconda3 envs mx lib python3 6 site packages mxnet libmxnet so 0xdf2af1 0x7f81a0608af1 bt 3 home sherlock anaconda3 envs mx lib python3 6 site packages mxnet libmxnet so MXImperativeInvoke 0x254 0x7f81a06097a4 bt 4 home sherlock anaconda3 envs mx lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ffi call unix64 0x4c 0x7f81d7cda5a0 bt 5 home sherlock anaconda3 envs mx lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ffi call 0x1f5 0x7f81d7cd9d45 bt 6 home sherlock anaconda3 envs mx lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so ctypes callproc 0x3dc 0x7f81d7cd188c bt 7 home sherlock anaconda3 envs mx lib python3 6 lib dynload ctypes cpython 36m x86 64 linux gnu so 0x9df3 0x7f81d7cc9df3 bt 8 home sherlock anaconda3 envs mx bin lib libpython3 6m so 1 0 PyObject FastCallDict 0x9e 0x7f81de8efaae bt 9 home sherlock anaconda3 envs mx bin lib libpython3 6m so 1 0 0x1482bb 0x7f81de9cc2bb I think the reason is the default hidden state in on cpu and if I put the layer and input on GPU the hidden state is not on GPU Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-08-24 06:24:09,2017-08-25 17:12:35
IS,OSError libcudart so 8 0 cannot open shared object,Paths are correct the library libcudart so 8 0 is present it compiles without error Environment vars are set correctly for cuda 8 with cudnn 6 running on ubuntu 16 04 Can not figure out what is happening to cause this error Is this a cuda issue and nivida issue tensorflow issue any help please,,,2017-08-25 05:44:48,2017-08-25 19:40:18
PR,shujon,Adversarial Variational AutoEncoder implementation,,,2017-08-25 20:52:42,2017-08-25 21:08:06
PR,set build status to success only after job ends,Earlier code marks status as success initially So any new PR shows jenkins status as success if we see the check mark on github On opening the full build status we see that builds have not even started or are running If something fails variable changes to failure then So even without this merge a red mark on github indicates that build has failed correctly That behavior is unchanged,,rahul003,2017-08-26 02:50:41,2017-08-26 03:20:56
PR,Fix build status of a image classification test,On a nightly build of this test the final build status is shown as false Example failed build is here The final status is marked as FAILED because juLog checks for the regex match of argument error 'Fail' matches the output because of an opencv driver warning More information about the driver warning is here Changing this regex helps us overcome issues when this driver issue is not fixed in the environment Also installs bc which is needed by sh2ju sh Example successful build with the changes in this PR is here Adding the driver resolution command to this test is not a good idea because it entirely disables the driver on the instance,,rahul003,2017-08-26 03:06:49,2017-08-26 03:21:52
PR,entire codebase build with mshadow use clas 0,The following file is the result of solving build issues for the entire codebase that arise after setting MSHADOW USE CBLAS 0 in mshadow make mshadow mk Problems were often that cpu versions of the linalg interface were missing at final link time and this PR supplies stubs that log a fatal message that the routine is unimplemented In addition I reworked the linalg gemm cpu DType specialization to be more consistent with the rest of the file a ' define' that is instantiated with float and double types This clearly supplies a full function specialization avoiding the partial function specialization error reported against an earlier version of the file by Also I personally find the existing implementation of a routine that is templated on as confusing Again this PR offers a simple approach consistent with the rest of the file,,"DickJC123,asmushetzel,DickJC123",2017-08-25 23:36:07,2017-08-26 04:27:35
PR,Update README md,,,kli-nlpr,2017-08-26 07:09:56,2017-08-26 07:22:09
PR,unit test for csv iter doc update for libsvmiter,Add the missing unit test for csv iterator Found memory for inst index was allocated with new but freed with delete which may cause undefined behavior in test io py updated doc for libsvm iter Doc Preview mxnet io LibSVMIter,,eric-haibin-lin,2017-08-25 21:59:31,2017-08-26 18:37:10
PR,gpu access of ndarray,,,dtmoodie,2017-08-16 20:06:07,2017-08-26 18:51:35
PR,refactor cudnn algo reg to no use string,,,piiswrong,2017-08-22 19:21:49,2017-08-27 00:51:31
PR,Update io md,,,kli-nlpr,2017-08-27 01:08:14,2017-08-27 05:17:16
PR,fix tests,,,"szha,szha",2017-08-27 00:04:27,2017-08-27 07:14:29
PR,build explicitly install JDK8,I had a PR failed on calling a Java 8 API I think the major reason is that in the OS where Jenkins runs the default jdk points to JDK7 and it is time to move on to JDK8,,"CodingCat,piiswrong,CodingCat,piiswrong,yzhliu,yzhliu,CodingCat,yzhliu,piiswrong,CodingCat,CodingCat,yzhliu",2017-08-23 13:40:36,2017-08-27 08:12:23
IS,pvanet Incompatible input shape why,I implemented the pvanet as the article pvanet firstly I pretrained it with Imagenet and got a model then got 66x40x512 as the output as told in the article I put the firstly 128 channels as the input of RPN the whole 512 channels as input of the faster R CNN but I got incompatible input shape as below INFO root Called with argument Namespace begin epoch 0 dataset 'PascalVOC' dataset path wouldata VOCdevkit' end epoch 10 frequent 20 gpus '0' image set '2007 trainval' kvstore wouldevice' lr 0 001 lr step '7' network 'pva' no flip False no shuffle False prefix 'model e2e' pretrained 'model pvanet' pretrained epoch 0 resume False root path wouldata' work load list None INFO root 'ANCHOR RATIOS' 0 5 1 2 'ANCHOR SCALES' 8 16 32 'FIXED PARAMS' 'conv1' 'conv2' 'FIXED PARAMS SHARED' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' 'IMAGE STRIDE' 0 'NUM ANCHORS' 9 'NUM CLASSES' 21 'PIXEL MEANS' array 103 939 116 779 123 68 'RCNN FEAT STRIDE' 16 'RPN FEAT STRIDE' 16 'SCALES' 1056 640 'TEST' 'BATCH IMAGES' 1 'CXX PROPOSAL' True 'HAS RPN' False 'NMS' 0 3 'PROPOSAL MIN SIZE' 16 'PROPOSAL NMS THRESH' 0 7 'PROPOSAL POST NMS TOP N' 2000 'PROPOSAL PRE NMS TOP N' 20000 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'ASPECT GROUPING' True 'BATCH IMAGES' 1 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' True 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'CXX PROPOSAL' True 'END2END' True 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 4 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 12000 'RPN PRE NMS TOP N' 12000 INFO root voc 2007 trainval num images 5011 INFO root voc 2007 trainval gt roidb loaded from data cache voc 2007 trainval gt roidb pkl INFO root voc 2007 trainval append flipped images to roidb INFO root load data filtered 0 roidb entries 10022 10022 INFO root providing maximum shape wouldata' 1 3 1056 640 'gt boxes' 1 100 5 'label' 1 23760 'bbox target' 1 36 66 40 'bbox weight' 1 36 66 40 INFO root output shape 'bbox loss reshape output' 1L 128L 84L 'blockgrad0 output' 1L 128L 'cls prob reshape output' 1L 128L 21L 'rpn bbox loss output' 1L 36L 30L 40L 'rpn cls prob output' 1L 2L 270L 40L INFO root lr 0 001000 lr epoch diff 7 lr iters 70154 20 36 13 src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable 20 36 22 src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable infer shape error Arguments 20 36 25 home dlmxnet hanqing incubator mxnet dmlc core include dmlc logging h 308 20 36 25 src operator concat inl h 166 Check failed shape assign in shape i dshape Incompatible input shape expected 1 0 27 40 got 1 384 28 40 x data 1 3 426 640 Stack trace returned 10 entries bt 0 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fdea9c701e9 bt 1 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNK5mxnet2op10ConcatProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x892 0x7fdeaabad082 bt 2 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x13f7008 0x7fdeaa8e7008 bt 3 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276e3c1 0x7fdeabc5e3c1 bt 4 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276fbb2 0x7fdeabc5fbb2 bt 5 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x27704d6 0x7fdeabc604d6 bt 6 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7fdeabc7c141 bt 7 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fdeaac215de bt 8 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x24e 0x7fdeaac242de bt 9 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so MXSymbolInferShape 0x14b6 0x7fdeaac1cce6 Traceback most recent call last File home dlmxnet hanqing incubator mxnet example rcnn train end2end py line 185 in module main File home dlmxnet hanqing incubator mxnet example rcnn train end2end py line 182 in main lr args lr lr step args lr step File home dlmxnet hanqing incubator mxnet example rcnn train end2end py line 144 in train net arg params arg params aux params aux params begin epoch begin epoch num epoch end epoch File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet module base module py line 499 in fit next data batch next data iter File home dlmxnet hanqing incubator mxnet example rcnn rcnn core loader py line 303 in next self get batch File home dlmxnet hanqing incubator mxnet example rcnn rcnn core loader py line 370 in get batch feat shape self feat sym infer shape data shape File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 961 in infer shape res self infer shape impl False args kwargs File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 1090 in infer shape impl ctypes byref complete File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet base py line 102 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator concat 20 36 25 src operator concat inl h 166 Check failed shape assign in shape i dshape Incompatible input shape expected 1 0 27 40 got 1 384 28 40 Stack trace returned 10 entries bt 0 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fdea9c701e9 bt 1 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNK5mxnet2op10ConcatProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x892 0x7fdeaabad082 bt 2 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x13f7008 0x7fdeaa8e7008 bt 3 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276e3c1 0x7fdeabc5e3c1 bt 4 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276fbb2 0x7fdeabc5fbb2 bt 5 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x27704d6 0x7fdeabc604d6 bt 6 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7fdeabc7c141 bt 7 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fdeaac215de bt 8 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x24e 0x7fdeaac242de bt 9 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so MXSymbolInferShape 0x14b6 0x7fdeaac1cce6 Process finished with exit code 1 the special two line above I have outlined it anyone can help me,,"qingzhouzhen,reminisce,qingzhouzhen",2017-08-24 11:52:48,2017-08-28 01:57:16
IS,Add Depthwise Deconvolution support,crazy cat Hi cat You have make the convolution faster when the num filter num group depthwise convolution I have a question for that the mx sym Deconvolution is the opposite operator of convolution Can it benefit from the num filter num group This trick is sometimes used in the FCN semantic segmentation to upsample features from lower layers and keep the deconvolution weights less,,chinakook,2017-08-18 05:42:12,2017-08-28 05:57:37
IS,pvanet infer shape error Arguments,1 I read the pvanet article 2 Firstly I implemented the pretrain network for details see my github pvanet pretrain py for the pvanet pretrain network I follow the author who implement it whth Caffe author is pvanet pretrain file 3 Then I train my pvanet pretrain network with Image I got a amazing training speed about 700 samples second with 2 K80 in a machine but a poor Validation accuracy just 0 638871 Validation top 5 accuracy 0 852400 4 As the article I implemented the pvanet it is a little different with pvanet pretrain network then I use the code of RPN and faster rcnn This is my symbol pvanet file I sliced the first 128 channels of the shared Conv then feeded it to RPN network In this part RPN adn faster rcnn network have some different with the author is implement indetail see the implement of author is For RPN and faster rcnn I think it is better to use the examples of Mxnet 5 To feded pvanet to faster rcnn I make these change 1 add symbol pvanet py as 4 infered 2 modify code in config py just like resnet 101 Then I got error it said src operator concat inl h 166 Check failed shape assign in shape i dshape Incompatible input shape expected 1 0 38 63 got 1 384 38 64 infer shape error Arguments data 1 3 600 1000 bellow show my whole error log dlmxnet localhost rcnn bash script pvanet alter voc07 sh 0 INFO root Called with argument Namespace dataset 'PascalVOC' dataset path wouldata VOCdevkit' frequent 20 gpus '0' image set '2007 trainval' kvstore wouldevice' network 'pvanet' no flip False no shuffle False pretrained 'model pvanet' pretrained epoch 0 rcnn epoch 8 rcnn lr 0 001 rcnn lr step '6' resume False root path wouldata' rpn epoch 8 rpn lr 0 001 rpn lr step '6' work load list None INFO root TRAIN RPN WITH IMAGENET INIT INFO root 'ANCHOR RATIOS' 0 5 1 2 'ANCHOR SCALES' 8 16 32 'FIXED PARAMS' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' wouldeconv' 'concat' 'convf' 'FIXED PARAMS SHARED' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' wouldeconv' 'concat' 'convf' 'IMAGE STRIDE' 0 'NUM ANCHORS' 9 'NUM CLASSES' 21 'PIXEL MEANS' array 103 939 116 779 123 68 'RCNN FEAT STRIDE' 16 'RPN FEAT STRIDE' 16 'SCALES' 600 1000 'TEST' 'BATCH IMAGES' 1 'CXX PROPOSAL' True 'HAS RPN' False 'NMS' 0 3 'PROPOSAL MIN SIZE' 16 'PROPOSAL NMS THRESH' 0 7 'PROPOSAL POST NMS TOP N' 2000 'PROPOSAL PRE NMS TOP N' 20000 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'ASPECT GROUPING' True 'BATCH IMAGES' 1 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' False 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'CXX PROPOSAL' True 'END2END' False 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 7 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 2000 'RPN PRE NMS TOP N' 12000 INFO root voc 2007 trainval num images 5011 INFO root voc 2007 trainval gt roidb loaded from data cache voc 2007 trainval gt roidb pkl INFO root voc 2007 trainval append flipped images to roidb INFO root load data filtered 0 roidb entries 10022 10022 20 37 07 home dlmxnet hanqing incubator mxnet dmlc core include dmlc logging h 308 20 37 07 src operator concat inl h 166 Check failed shape assign in shape i dshape Incompatible input shape expected 1 0 38 63 got 1 384 38 64 Stack trace returned 10 entries bt 0 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fcbaa28a1e9 bt 1 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNK5mxnet2op10ConcatProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x892 0x7fcbab1c7082 bt 2 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x13f7008 0x7fcbaaf01008 bt 3 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276e3c1 0x7fcbac2783c1 bt 4 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276fbb2 0x7fcbac279bb2 bt 5 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x27704d6 0x7fcbac27a4d6 bt 6 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7fcbac296141 bt 7 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fcbab23b5de bt 8 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x24e 0x7fcbab23e2de bt 9 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so MXSymbolInferShape 0x14b6 0x7fcbab236ce6 infer shape error Arguments data 1 3 600 1000 Traceback most recent call last File train alternate py line 121 in module main File train alternate py line 118 in main args rcnn epoch args rcnn lr args rcnn lr step File train alternate py line 40 in alternate train train shared False lr rpn lr lr step rpn lr step File home dlmxnet hanqing incubator mxnet example rcnn rcnn tools train rpn py line 66 in train rpn max data shape max label shape train data infer shape max data shape File home dlmxnet hanqing incubator mxnet example rcnn rcnn core loader py line 346 in infer shape feat shape self feat sym infer shape max shapes File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 961 in infer shape res self infer shape impl False args kwargs File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 1090 in infer shape impl ctypes byref complete File home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet base py line 102 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator concat 20 37 07 src operator concat inl h 166 Check failed shape assign in shape i dshape Incompatible input shape expected 1 0 38 63 got 1 384 38 64 Stack trace returned 10 entries bt 0 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fcbaa28a1e9 bt 1 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZNK5mxnet2op10ConcatProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x892 0x7fcbab1c7082 bt 2 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x13f7008 0x7fcbaaf01008 bt 3 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276e3c1 0x7fcbac2783c1 bt 4 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x276fbb2 0x7fcbac279bb2 bt 5 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so 0x27704d6 0x7fcbac27a4d6 bt 6 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7fcbac296141 bt 7 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fcbab23b5de bt 8 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x24e 0x7fcbab23e2de bt 9 home dlmxnet anaconda2 envs hanqing lib python2 7 site packages mxnet 0 10 1 py2 7 egg mxnet libmxnet so MXSymbolInferShape 0x14b6 0x7fcbab236ce6 I tried end2end train alternate train make pvanet as default like vgg but got a shape error problem somebody help me,,"qingzhouzhen,reminisce,qingzhouzhen,qingzhouzhen",2017-08-26 13:17:18,2017-08-28 08:16:11
IS,nd array changes specific values,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler GCC4 4 7 Package used Python R Scala Julia python MXNet version 0 11 0 MXNet commit hash e05129774e76206fe890b511c346953107b05fce Minimum reproducible example,,,2017-08-28 10:03:31,2017-08-28 10:53:34
IS,is num group implement mobilenet,Is argumnet num group in mx sym Convolution has implemented the article MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications I have a demo used num group in my repository I added this symbol to emamples and train it use ImageNet train speed is about 56 samples second like this INFO root Epoch 0 Batch 20 Speed 56 48 samples sec accuracy 0 000744 INFO root Epoch 0 Batch 40 Speed 56 53 samples sec accuracy 0 000781 INFO root Epoch 0 Batch 60 Speed 56 53 samples sec accuracy 0 000000 INFO root Epoch 0 Batch 80 Speed 56 55 samples sec accuracy 0 001563 INFO root Epoch 0 Batch 100 Speed 56 55 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 120 Speed 56 55 samples sec accuracy 0 002734 INFO root Epoch 0 Batch 140 Speed 56 53 samples sec accuracy 0 001563 INFO root Epoch 0 Batch 160 Speed 56 53 samples sec accuracy 0 002734 INFO root Epoch 0 Batch 180 Speed 56 54 samples sec accuracy 0 001172 INFO root Epoch 0 Batch 200 Speed 56 50 samples sec accuracy 0 002734 INFO root Epoch 0 Batch 220 Speed 56 53 samples sec accuracy 0 001953 INFO root Epoch 0 Batch 240 Speed 56 55 samples sec accuracy 0 003516 INFO root Epoch 0 Batch 260 Speed 56 54 samples sec accuracy 0 001953 INFO root Epoch 0 Batch 280 Speed 56 55 samples sec accuracy 0 005078 INFO root Epoch 0 Batch 300 Speed 56 54 samples sec accuracy 0 004297 And in another version I do not use num group I set it to 1 to do fully converlution like this train speed is about 93 samples second like this INFO root Epoch 0 Batch 20 Speed 92 89 samples sec accuracy 0 002604 INFO root Epoch 0 Batch 40 Speed 93 21 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 60 Speed 93 19 samples sec accuracy 0 003516 INFO root Epoch 0 Batch 80 Speed 93 09 samples sec accuracy 0 000781 INFO root Epoch 0 Batch 100 Speed 93 11 samples sec accuracy 0 001953 INFO root Epoch 0 Batch 120 Speed 93 16 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 140 Speed 93 16 samples sec accuracy 0 001563 INFO root Epoch 0 Batch 160 Speed 93 20 samples sec accuracy 0 003906 INFO root Epoch 0 Batch 180 Speed 93 35 samples sec accuracy 0 003125 INFO root Epoch 0 Batch 200 Speed 93 74 samples sec accuracy 0 001953 INFO root Epoch 0 Batch 220 Speed 93 65 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 240 Speed 93 72 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 260 Speed 93 62 samples sec accuracy 0 002734 INFO root Epoch 0 Batch 280 Speed 93 59 samples sec accuracy 0 003125 INFO root Epoch 0 Batch 300 Speed 93 51 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 320 Speed 93 52 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 340 Speed 93 59 samples sec accuracy 0 003906 INFO root Epoch 0 Batch 360 Speed 93 72 samples sec accuracy 0 001953 INFO root Epoch 0 Batch 380 Speed 93 47 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 400 Speed 93 45 samples sec accuracy 0 003516 INFO root Epoch 0 Batch 420 Speed 93 59 samples sec accuracy 0 002344 INFO root Epoch 0 Batch 440 Speed 93 65 samples sec accuracy 0 004297 INFO root Epoch 0 Batch 460 Speed 93 70 samples sec accuracy 0 004687 INFO root Epoch 0 Batch 480 Speed 93 59 samples sec accuracy 0 005859 INFO root Epoch 0 Batch 500 Speed 93 73 samples sec accuracy 0 006250 INFO root Epoch 0 Batch 520 Speed 93 67 samples sec accuracy 0 003906 INFO root Epoch 0 Batch 540 Speed 93 48 samples sec accuracy 0 006250 INFO root Epoch 0 Batch 560 Speed 93 68 samples sec accuracy 0 003906 INFO root Epoch 0 Batch 580 Speed 93 65 samples sec accuracy 0 005859 INFO root Epoch 0 Batch 600 Speed 93 67 samples sec accuracy 0 003125 INFO root Epoch 0 Batch 620 Speed 93 67 samples sec accuracy 0 006641 INFO root Epoch 0 Batch 640 Speed 93 67 samples sec accuracy 0 008594 INFO root Epoch 0 Batch 660 Speed 93 73 samples sec accuracy 0 007812 INFO root Epoch 0 Batch 680 Speed 93 84 samples sec accuracy 0 008594 INFO root Epoch 0 Batch 700 Speed 93 74 samples sec accuracy 0 005859 INFO root Epoch 0 Batch 720 Speed 93 65 samples sec accuracy 0 006641 I think I am wrong mobilenet is to speed up train speed but my demo have slowed down it,,"qingzhouzhen,qingzhouzhen",2017-07-16 12:46:38,2017-08-28 12:15:25
PR,Add script to build doc files for all versions,Script to build all versions selected Can be used for website recovering if apache jenkins automate building system generates unexpected result,,kevinthesun,2017-08-27 08:01:28,2017-08-28 18:00:15
PR,add fashion mnist and move mnists to s3,mli,,"szha,piiswrong,piiswrong,szha",2017-08-27 06:08:59,2017-08-28 18:05:51
PR,retry logic for jenkins tests,Lot of tests on build fail intermittently Some of them throw issues like lack of memory or cuda memory possibly because of the state the instance is at the time of the run And there are some possible issues with some tests Add retry logic for tests so such tests can be tried again This will especially help till the build environment stabilizes Update Added retry for deploy as well after build failed when trying to fetch a deb file 404 The server now does not respond with 404,,rahul003,2017-08-28 17:18:55,2017-08-28 18:36:46
PR,add doc for dataset,piiswrong,,szha,2017-08-28 18:50:50,2017-08-28 19:06:19
PR,Change apache package URL to https,,,kevinthesun,2017-08-25 21:13:29,2017-08-28 19:06:39
IS,MXNetR how can i get the shape dimension of a certain symbol,Given data as a symbol I want to add noise This is given data mx symbol Variable data How can I do this noise mx symbol random normal shape To do this data noisy data noise I have found but did not helped 2763 how can i get the shape of a certain symbol 1090 How to get layer shapes I have tried 1 data mx symbol random normal shape mx symbol infer shape data Error in symbol infer shape list symbol cc 165 RCheck failed HasName kwargs Need to pass parameters in key value style 2 data mx symbol random normal shape dim data Error in mx varg symbol random normal list base h 271 RCheck failed TYPEOF val INTSXP TYPEOF val REALSXP Only accept integer vectors or simple types 3 data mx symbol random normal shape data infer shape Error in mx varg symbol random normal list Cannot convert object to an environment type closure target ENVSXP 4 data mx symbol random normal shape data infer shape Error in data infer shape could not find valid method 5 data mx symbol random normal shape mx symbol infer shape symbol data data mx symbol zeros like data Error in symbol infer shape list Not compatible with requested type type S4 target integer,,jeremiedb,2017-08-28 11:20:58,2017-08-28 20:11:47
PR,Updating install doc to remove xcode prerequisite where not necessary,,,lupesko,2017-08-25 07:35:31,2017-08-28 20:52:40
PR,Pip installer for CoreML Converter mxnet to coreml,The new way to install the converter pip install mxnet to coreml Current version is 0 1 0 and runs only on MacOS and python 2 7 Once inside the directory pip package user needs to run python setup py bdist wheel twine upload dist Once uploaded it will look like this Also updated the README for converter to reflect this Unit tests continue to pass Also converted the squeezenet model after installing from fresh using the pip install,,"pracheer,pracheer",2017-08-25 23:21:24,2017-08-29 00:22:32
PR,Parallelize windows unit tests of python 2 and 3 in jenkins,Parallelize windows unit tests of python 2 and 3 in jenkins pipeline for both CPU and GPU krishnamurthy,,rahul003,2017-08-29 00:16:16,2017-08-29 00:31:46
PR,Fixed website insecure asset load,Updated insecure asset logo to be loaded securely from origin Added the asset Removed dead HTML code,,"lupesko,kevinthesun",2017-08-29 01:33:07,2017-08-29 03:08:51
PR,skip failing test temporarily,This gpu test is failing once in 3 times and causing many builds to fail Disabling the test till it gets fixed Issue regarding the test is here krishnamurthy,,rahul003,2017-08-29 00:46:37,2017-08-29 03:09:42
PR,lower really high threshold to fix test failure,One test failed because the accuracy was 0 989 instead of 0 99 0 98 accuracy is good enough,,rahul003,2017-08-29 01:35:08,2017-08-29 03:10:09
PR,Doc updates for install and doc generation,Fixed a typo and out of date git url in the doc is readme Updated install prerequisites to include XCODE only in the build from source section on Mac,,lupesko,2017-08-29 00:42:00,2017-08-29 03:10:43
PR,convenience fluent methods for sym nd,,,"szha,piiswrong,szha,piiswrong,piiswrong,reminisce,reminisce,szha,szha,eric-haibin-lin,szha",2017-08-23 23:20:40,2017-08-29 03:12:10
PR,rm unused variables,tqchen,,formath,2017-08-14 08:47:07,2017-08-29 08:20:15
PR,Refactor random linalg contrib namespaces,What are included 1 Added contrib py linalg py and random py under the folders python mxnet ndarray and python mxnet symbol for registering ops of those categories 2 Consolidated op registration functions init ndarray module L174 and init symbol module L206 into a single one init op module refactor random linalg contrib namespaces expand 1 diff bdc88d735b5304ea6d327d5c0144b6efR381 in python mxnet base py In the future if there are more submodules added under mxnet ndarray and mxnet symbol we just need to add the prefix name in OP NAME PREFIX LIST diff bdc88d735b5304ea6d327d5c0144b6efR366 without modifying the op registration function itself 3 Changed to register all regular backend ops not any kind of contrib linalg random or sparse under ndarray op py and symbol op py for clearer op ownership management Imported all registered op in ndarray init py and symbol init py Thank for discussion and his PR 4 Changed to use the new namespace ops in all unit tests and examples 5 Changed stype comparison diff 966da4762e41c3d42821d8b2f82416a4R836 from string to int for internal functions suggested by The ops under the newly added namespaces haibin lin,,"reminisce,piiswrong,reminisce,piiswrong,piiswrong,piiswrong,reminisce,piiswrong,reminisce,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,piiswrong,piiswrong,piiswrong,reminisce,szha,szha,szha,reminisce,szha,reminisce,reminisce,reminisce",2017-08-25 01:50:01,2017-08-29 17:34:57
PR,fix gluon fasionmnist dataset,,,"szha,piiswrong,piiswrong",2017-08-29 20:01:15,2017-08-29 20:47:54
PR,Parallelize Python 2 and 3 unit test cases in Jenkins CI,Parallelly run Python 2 and 3 unit tests This will save around 45 minutes of build time in Jenkins,,"sandeep-krishnamurthy,rahul003,sandeep-krishnamurthy,rahul003",2017-08-29 21:47:12,2017-08-29 22:07:09
PR,Change namespace and make logging functionality changes,Namespace changes according to issuecomment 324176627 and logging changes for sparse end2end py haibin lin,,"anirudh2290,eric-haibin-lin,reminisce,anirudh2290,anirudh2290,anirudh2290",2017-08-26 01:34:24,2017-08-29 22:36:14
PR,update mklml and mkl mac support,needs dmlc mshadow 290 to work,,"szha,piiswrong,piiswrong,szha,szha,mli",2017-08-24 04:55:17,2017-08-30 00:14:02
IS,Slicing mx nd array with postive start and neg end alone same axis,Current mx nd array does not support slicing with positive start end negative end alone same axis while numpy can do it,,"Godricly,Godricly",2017-08-25 02:54:08,2017-08-30 02:34:37
PR,scala package spark Resources running PS role server should be explicit to Spark,another PR to facilitate the further work on integrating with Spark The current implementation starts PS role server release the cluster resources as if no one is using that and then start PS role worker To make the integration more seamless we should make the resources used by PS role server explicit to Spark Question to why we choose to start PS in a child process instead of wrap it with a Spark task,,"CodingCat,yzhliu,yzhliu,CodingCat,yzhliu,yzhliu,CodingCat,CodingCat,CodingCat,mbaijal,CodingCat,mbaijal",2017-08-23 03:46:48,2017-08-30 03:29:53
PR,Remove python function negative for rendering ndarray api in doc,The function negative defined in ndarray py has the same name as the backend op resulting in failure of rendering ndarray api page It is deleted in this PR since it is not used anywhere This should fix the documentation display error introduced by this PR Thank for debugging the js code to find the out the root cause and haibin lin for verifying the markdown files haibin lin See this link for the documentation of this PR,,reminisce,2017-08-29 21:33:06,2017-08-30 05:11:36
PR,CSRNDArray from to scipy csr matrix fix rand shape nd,Added preliminary inefficient support so that user can construct CSRNDArray from scipy csr matrix convert CSRNDArray to scipy csr matrix throw exception when setters for indices data indptr are called Also fixed rand shape nd so that it returns tuple consistent w rand shape 2d TODO in next PR move copy logic to cpp backend to reduce copy blocking,,"eric-haibin-lin,piiswrong,reminisce,reminisce,eric-haibin-lin,eric-haibin-lin,anirudh2290,reminisce,anirudh2290,eric-haibin-lin,anirudh2290,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-08-28 05:50:09,2017-08-30 05:12:05
PR,fix ctc on softmax grad and req option,,,"szha,piiswrong,piiswrong,szha,szha,piiswrong,piiswrong",2017-08-29 23:18:48,2017-08-30 17:28:17
PR,ndarray hpp needs to include op h,Compile issues due to Operator undefined otherwise,,"dtmoodie,piiswrong,dtmoodie,lx75249",2017-08-16 19:53:01,2017-08-30 17:39:25
PR,Add string interface to updater to make it consistent with kvstore,example,,"eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin",2017-08-24 00:33:13,2017-08-30 18:02:01
PR,1x1 convolution acceleration,GEMM directly without im2col or col2im in 1x1 convolution stride 1 pad 0 The 1x1 convolution is used very common in modern CNN networks such as Googlenet Inception Resnet Mobilenet etc,,"chinakook,reminisce,reminisce,chinakook,chinakook,piiswrong,chinakook,reminisce,piiswrong",2017-08-25 08:05:15,2017-08-30 18:18:48
PR,Update ndarray py,Godricly,,"piiswrong,Godricly,Godricly,zhreshold",2017-08-25 06:12:32,2017-08-30 18:35:04
PR,RCNN remove self implemented speedometer,Because mx callback Speedometer has parameter auto rest now its behavior is the same with the self implemented Speedometer when auto rest false,,"ZiyueHuang,ZiyueHuang",2017-08-11 15:39:32,2017-08-30 18:38:16
PR,Add grad req parameter to Block that is passed to ParameterDict get,This allows specifying the grad req for Blocks that is then used when creating Parameters,,"leezu,piiswrong,leezu,piiswrong,leezu,piiswrong,leezu,piiswrong,piiswrong,leezu,leezu,leezu,leezu,piiswrong,piiswrong",2017-07-31 06:52:54,2017-08-30 18:39:50
PR,R update tutorial link close 6536,,,"thirdwing,piiswrong,thirdwing,thirdwing,piiswrong,piiswrong,kevinthesun",2017-08-14 21:22:51,2017-08-30 18:49:28
IS,R Documentation hyperlinks at all lead to something other than R,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"thirdwing,thirdwing,thirdwing",2017-06-02 05:03:26,2017-08-30 18:49:37
PR,add variational autoencoder example,I have implemented a basic variational autoencoder model as introduced from the original paper Auto Encoding Variational Bayes and demonstrated the usage on the mnist hand written digit dataset,,"piiswrong,szha,madjam,sandeep-krishnamurthy",2017-07-25 17:25:34,2017-08-30 19:57:52
PR,Empty commit DO NOT MERGE,Empty commit to test Apache CI DO NOT MERGE,,lxn2,2017-07-18 15:18:14,2017-08-30 19:58:45
PR,Add missing license header,,,"madjam,madjam",2017-08-30 21:15:58,2017-08-30 22:13:16
PR,add 1 indexing to ndarray,,,piiswrong,2017-08-30 18:34:56,2017-08-30 22:43:01
PR,R fix CI test close 7669,sandeep krishnamurthy,,thirdwing,2017-08-30 23:44:52,2017-08-31 02:42:06
IS,Jenkins CI Broken R unit test test img seg,thirdwing One of the R unit tests is failing in the Jenkins CI Looks like the issue is due to the installation of package failing Master builds are failing Can you please take a look at the issue Build Failure Log Link Error Output Failed 1 Error UNET test img seg R 93 trying to use CRAN without setting a mirror 1 install packages new packages at R package tests testthat test img seg R 93 2 grep file contriburl 3 contrib url repos type 4 stop trying to use CRAN without setting a mirror DONE Error Test failures Execution halted make rpkgtest Error 1 script returned exit code 2,,"sandeep-krishnamurthy,thirdwing",2017-08-30 21:05:02,2017-08-31 04:25:41
PR,sign compare warnings,These changes were introduced in with some other changes I'm splitting the PR for two reasons 1 These changes are simpler can be merged while we wait on more information regarding the template 2 That PR is now old and master has changed a lot,,rahul003,2017-08-31 01:46:09,2017-08-31 05:19:59
PR,Add more gpu tests and fix nested try catch in Imperative Invoke,added all tests in test ndarray to test gpu fixed nested try catch in NDArray so that None is returned when exception happens,,"eric-haibin-lin,mli,eric-haibin-lin",2017-08-31 00:46:45,2017-08-31 06:12:07
PR,update log epsilons to 1e 12,,,szha,2017-08-30 21:10:34,2017-08-31 06:13:13
PR,Update docs README link and folder name,I noticed a few small typos in the README and the docs and wanted to help to fix them I'm not sure how contributing changes to projects like this normally works so please let me know if I should change anything to do with my branching forking etc Thanks,,"alexandraj777,piiswrong,alexandraj777,piiswrong,alexandraj777,alexandraj777,piiswrong,alexandraj777,alexandraj777",2017-08-30 22:11:11,2017-08-31 06:14:52
IS,Python CustomOp with parameter failed when call infer shape entry in operator py,the operator can be found here L156 When build the graph with this operator the weight symbol is not provided Calling infer shape entry failed as tensor types i 1 in this line L651 If the weight symbol is provided when using the operator tensor types i 0 and wo not throw a KeyError,,luoyetx,2017-07-15 03:31:37,2017-08-31 07:16:43
PR,R update Makefile close 7675,,,"thirdwing,madjam,thirdwing,sandeep-krishnamurthy,thirdwing,cjolivier01,thirdwing",2017-08-31 16:58:08,2017-08-31 19:47:14
PR,Fix shape inference bug,This PR fixes a shape inference bug in function InferAttr Consider the following network Given a data shape if the shape forward inference starts from weight there would be several node entries with unknown shapes for abs op and sum op in the first pass The weight shape is only known after fc op is shape inference is done When the backward shape inference starts it assumes that all forward node entries' shapes are known However this is not true in this case and therefore leads to CHECK EQ failure The problem is fixed by skipping CHECK EQ for empty shapes in the backward shape inference haibin lin,,"reminisce,tqchen,reminisce",2017-08-31 06:42:11,2017-08-31 21:11:15
PR,Unit test patch,,,astonzhang,2017-09-01 00:12:31,2017-09-01 00:13:23
PR,Unit test patch,,,astonzhang,2017-09-01 00:33:55,2017-09-01 00:34:21
PR,Fix unit test for test loss py,,,astonzhang,2017-09-01 00:41:43,2017-09-01 03:12:59
PR,WIP support autograd with sparse grad,piiswrong,,eric-haibin-lin,2017-09-01 00:52:14,2017-09-01 03:14:47
PR,R fix Jenkins R testing,Please do not merge too quick I need some time to confirm I did not break anything,,thirdwing,2017-09-01 00:46:35,2017-09-01 03:15:43
PR,Adding opencv python so import cv2 in the tutorial will actually work,Small fix to make the predict image tutorial work,,lupesko,2017-09-01 06:24:20,2017-09-01 17:07:15
PR,Fix memory leak in profiler,Automatically delete DevStat object at shutdown,,"cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01",2017-08-25 16:47:24,2017-09-01 18:58:06
PR,forbid setattr type changes and enable block replacement,this change includes 1 prevent attributes whose names do not start with ' ' from changing types 2 properly replace children if both existing value and new value are blocks,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,szha,piiswrong",2017-08-30 17:51:46,2017-09-02 03:34:37
IS,what is the meaning of dtype parameter in get symbol function,Here is the code I do not understand what this code does I read an image from opencv python an it prints uint8 Do i have to specify the dtype parameter when calling get symbol function ''' def get symbol num classes 1000 dtype 'float32' kwargs data mx sym Variable name data if dtype 'float32' data mx sym identity data data name 'id' else if dtype 'float16' data mx sym Cast data data dtype np float16 ''',,Godricly,2017-09-01 10:46:31,2017-09-02 04:20:20
PR,scala package fix the bug in parent section of pom files,Sorry for introducing this bug several weeks ago Actually Maven cannot parse any variable in parent section I have changed those project version back and validate in my local maven working fine now,,"CodingCat,yzhliu,yzhliu,CodingCat,CodingCat",2017-09-01 02:59:35,2017-09-02 15:44:35
PR,Use a big ndarray in gluon data vision,use nd array data instead of a list of ndarray to accelerate the time,,"mli,piiswrong,piiswrong,mli",2017-08-30 00:01:12,2017-09-02 23:10:48
PR,Add dtype to 7661,another thing i noticed is that now we can write before PR 7661 it is not doable because data in transform will be a list,,mli,2017-09-02 23:21:22,2017-09-02 23:36:27
PR,Fixing a broken link in a tutorial,zackchase FYI,,lupesko,2017-09-03 08:10:49,2017-09-03 18:26:12
PR,WIP Quantization 8bit Quantization and GPU Support,This PR Implements the 8bit quantization algorithm and GPU kernels for it We have achieved good inference accuracy but still work on the performance,,ZihengJiang,2017-09-03 21:12:48,2017-09-03 21:29:33
PR,Fix build error C2872 on VS2015,I find the latest version will raise a build error in VS 2015 The error Code is C2872 which reports that the cpu is ambiguous either mxnet cpu or mshadow cpu This PR should fix the build,,sxjscience,2017-09-03 11:28:38,2017-09-04 03:54:53
IS,How to do downsampling by a factor of n in mxnet,How to do downsampling by a factor of n in mxnet Kindly suggest,,,2017-09-04 08:19:20,2017-09-04 10:10:33
PR,Fix Error,,,"alues,piiswrong,alues,alues",2017-09-04 13:34:00,2017-09-04 20:17:28
PR,gluon save load optimizer states,piiswrong,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,szha,piiswrong,szha,ptrendx,szha,ptrendx,szha,szha,ptrendx,szha,piiswrong,szha",2017-08-16 19:44:43,2017-09-04 22:31:57
PR,Implementation of cbrt and rcbrt operators,As suggested in 3201 rcbrt was not mentioned in the above issue but we have rsqrt and rcbrt is present in other libs one two three v vs 120 aspx so I figured it is worth implementing,,"apaleyes,piiswrong,piiswrong,apaleyes,apaleyes",2017-07-14 09:35:28,2017-09-04 23:20:32
PR,update caffe model converter for ssd models only,Not compatible with general converter,,"zhreshold,piiswrong,zhreshold,piiswrong",2017-07-18 00:46:06,2017-09-04 23:21:56
PR,Fix for hang in depthwise convolution for CUDA 9,,,"ptrendx,rahul003,ptrendx",2017-08-31 14:42:11,2017-09-04 23:24:01
PR,Isolated benchmarking support,Allows for benchmarking only IO or compute costs using io only and compute only flags and benchmarking communication costs using communication only haibin lin,,"anirudh2290,szha,anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290",2017-08-30 01:25:05,2017-09-04 23:27:12
PR,Fix for UnicodeDecodeError on Python 3 x,Adding 'rb' fixes UnicodeDecodeError 'utf 8' codec can not decode byte 0xff in position 0 invalid start byte on Python 3 x,,jaewoosong,2017-09-05 02:00:08,2017-09-05 04:23:13
IS,MXNetError This layer requires uniform type Expected 3 v s given 0 at weight,when i used gluon package i met the error following MXNetError Traceback most recent call last home wangy test multi cnn mxnet py in module 113 114 115 train 400 116 117 t1 time time home wangy test multi cnn mxnet py in train batch size 93 data data as in context ctx 94 label label as in context ctx 95 output net data 96 loss bce output label 97 home wangy tools anaconda2 lib python2 7 site packages mxnet gluon block pyc in call self args 266 def call self args 267 Calls forward Only accepts positional arguments 268 return self forward args 269 270 def forward self args home wangy tools anaconda2 lib python2 7 site packages mxnet gluon nn basic layers pyc in forward self x 44 def forward self x 45 for block in self children 46 x block x 47 return x 48 home wangy tools anaconda2 lib python2 7 site packages mxnet gluon block pyc in call self args 266 def call self args 267 Calls forward Only accepts positional arguments 268 return self forward args 269 270 def forward self args home wangy tools anaconda2 lib python2 7 site packages mxnet gluon block pyc in forward self x args 408 i finish deferred init 409 params i j data ctx for i j in self reg params items 410 return self hybrid forward ndarray x args params 411 412 assert isinstance x Symbol home wangy tools anaconda2 lib python2 7 site packages mxnet gluon nn conv layers pyc in hybrid forward self F x weight bias 125 act getattr F self op name x weight name 'fwd' self kwargs 126 else 127 act getattr F self op name x weight bias name 'fwd' self kwargs 128 if self act is not None 129 act self act act home wangy tools anaconda2 lib python2 7 site packages mxnet ndarray pyc in Convolution data weight bias kernel stride dilate pad num filter num group workspace no bias cudnn tune cudnn off layout out name kwargs home wangy tools anaconda2 lib python2 7 site packages mxnet ctypes ndarray pyc in imperative invoke handle ndargs keys vals out 87 ctypes c int len keys 88 c array ctypes c char p c str key for key in keys 89 c array ctypes c char p c str str val for val in vals 90 91 if original output is not None home wangy tools anaconda2 lib python2 7 site packages mxnet base pyc in check call ret 127 128 if ret 0 129 raise MXNetError py str LIB MXGetLastError 130 131 if sys version info 0 3 MXNetError 11 28 32 src operator convolution inl h 550 Check failed in type i dtype 0 vs 3 This layer requires uniform type Expected 3 v s given 0 at weight Following is part of my code def cnn model net gluon nn Sequential net add gluon nn Conv1D channels 700 kernel size 8 strides 1 activation arelu' net add gluon nn MaxPool1D pool size 20 net add gluon nn Dropout 0 2 net add gluon nn Flatten net add gluon nn Dense 5000 activation arelu' net add gluon nn Dropout 0 2 net add gluon nn Dense 919 activation isigmoid' return net Loading data print Loading training data f1 h5py File ' data train 100 hdf5' data1 f1 isequence code' data1 data1 swapaxes 1 2 label1 f1 'label' t dataset gluon data ArrayDataset data1 label1 train data mx gluon data DataLoader t dataset batch size shuffle False traning and validing net cnn model ctx mx gpu net collect params initialize mx init Xavier magnitude 2 24 ctx ctx bce mx gluon loss SigmoidBinaryCrossEntropyLoss trainer mx gluon Trainer net collect params isgd' 'learning rate' 1 'wd' 1e 6 print Start training epochs 15 smoothing constant 01 for e in range epochs for i data label in enumerate train data data data as in context ctx label label as in context ctx output net data,,"szha,szha",2017-09-05 03:37:37,2017-09-05 04:40:36
PR,Use memcpy instead of assigning each individual element,,,,2017-09-05 10:11:23,2017-09-05 10:48:41
PR,Use memcpy instead of assigning each individual element,,,,2017-09-05 11:04:29,2017-09-05 15:33:08
PR,display type string instead of type code in error message,,,szha,2017-09-05 05:31:35,2017-09-05 17:09:31
PR,update submoules with android fixes,,,"larroy,piiswrong,larroy,larroy,larroy,piiswrong,larroy,larroy,piiswrong,larroy,piiswrong,larroy",2017-08-10 10:24:04,2017-09-05 17:26:24
PR,Update optimizer py,,,"szha,piiswrong,szha,piiswrong,piiswrong,szha,szha,szha,ptrendx,szha",2017-09-05 07:54:22,2017-09-05 18:34:58
PR,Fix bug in rpn py,logger level loggin INFO by default which can harm the performance when trained using default setting,,,2017-09-01 05:39:25,2017-09-05 18:37:18
PR,Clean amalgamation ws and skip failing test,madjam,,"sandeep-krishnamurthy,szha,madjam,szha,astonzhang,madjam,astonzhang,sandeep-krishnamurthy",2017-08-31 01:29:16,2017-09-05 19:36:31
PR,Use memcpy instead of assigning each individual element,,,,2017-09-06 00:37:51,2017-09-06 00:38:47
PR,Use memcopy of assigning each individual element,,,,2017-09-06 00:58:57,2017-09-06 01:03:13
IS,My jupyter notebook uses old mxnet version 0 9 5 whereas pycharm debugger uses 0 11 1,Recently i deleted mxnet version 0 9 5 and upgraded to 0 11 1 version Still my jupyter notebook points to old version0 9 5 rather than new one 0 11 1 Kindly help Chowkamlee,,szha,2017-09-05 08:56:32,2017-09-06 03:22:18
PR,modify syntax error,a syntax error,,"qingzhouzhen,piiswrong,ZiyueHuang",2017-09-06 01:52:12,2017-09-06 05:11:24
IS,The right way to submit job on Yarn,I can not find any yarn example so try myself I submit a job of image classification in example via this command tar xf is executed in dmlc core tracker dmlc tracker launcher py So my question is 1 Is this the right way to submit a yarn job 2 What is the problem of tar xf Thanks,,"formath,formath",2017-09-05 10:23:36,2017-09-06 05:22:36
PR,Gluon trainer updates add learning rate and lr scheduler properties and add setter for learning rate,,,"astonzhang,piiswrong,piiswrong,piiswrong,szha,szha,astonzhang,astonzhang,astonzhang,astonzhang,szha,szha,astonzhang,astonzhang,astonzhang,piiswrong,astonzhang,piiswrong,astonzhang,astonzhang,szha,astonzhang,astonzhang",2017-08-29 22:20:42,2017-09-06 06:01:54
IS,Unable to install MXNet with GPU support for python using pip,I am trying to install MXNet for Python using pip having GPU support I am following the instructions mentioned in this documentation My OS is Ubuntu 16 04 After I install MXNet I run the validation code mentioned in documentation I get the error as stated below However if I change the context to CPU the code runs properly Please can somebody tell me how to install MXNet having GPU support Thanks in advance,,"Prasad9,szha,Prasad9,szha",2017-09-05 12:08:10,2017-09-06 06:33:06
IS,Element wise maximum operations using mx symbol,I had a feature map of dim 12 800 1280 dimension No of channels 12 height 800 width 1280 I want to perform element wise maximum operation on all the channels Kindly suggest the mxnet symbol operator and its sample example,,"szha,szha",2017-09-06 06:11:17,2017-09-06 07:45:33
PR,Use memcopy instead of assigning each individual element,,,,2017-09-06 02:12:20,2017-09-06 07:51:08
PR,Use memcopy instead of assigning each individual element,,,,2017-09-06 08:06:11,2017-09-06 08:41:24
PR,Use memcopy instead of assigning each individual element,,,,2017-09-06 08:51:26,2017-09-06 09:15:14
IS,Looking for a C alternative for symbol get internals in Python API,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System win10 Compiler VS2015 Package used Python R Scala Julia c api MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message I would like to change a loaded existing network In is explained how to do this using the Python API I work with the C API and look for a replacement of the code line all layers symbol get internals In the C API I can not find a similar function A code snippet reflecting the Python functionality in C would be very much appreciated Minimum reproducible example Steps to reproduce 1 2 3 What have you tried to solve it 1 2 3,,,2017-07-31 14:50:03,2017-09-06 10:36:58
PR,Use memcopy instead of assigning each individual element,,,,2017-09-06 09:21:16,2017-09-06 11:11:49
PR,Merge pull request 1 from apache master,update,,,2017-09-06 13:57:54,2017-09-06 13:58:15
PR,Fix R test problem,,,"cjolivier01,thirdwing",2017-09-05 21:06:55,2017-09-06 15:22:12
PR,fix caffe model convert leakyrelu,I tried to convert an trained caffe model to mxnet but the predict result is wrong Finally I found what cause the error I had some activation define in prototxt such as in fact according to caffe document Given an input value x The ReLU layer computes the output as x if x 0 and negative slope x if x 0 so this is LeakyRelu not relu but the convert tools treat this as relu so error happen I fixed this error and the result is now the same with caffe,,"vsooda,piiswrong,vsooda,piiswrong,vsooda,piiswrong",2017-09-01 07:53:53,2017-09-06 17:43:14
PR,CoremlBugFixes Input variable name can be something other than data,making pad and stride optional Earlier we were not providing data names argument while creating the module which meant that the input data variable name was assumed to be data This is fixed Also added a unit test for it due to which utils load model had to be refactored The second bug was we missed assuming pad and stride parameters for convolutional layers are optional arguments Added a unit test for this too Also tested with mnist model from the tutorial page by changing the input variable name to something other than data,,"pracheer,jiajiechen,pracheer,domdivakaruni",2017-09-05 19:53:42,2017-09-06 18:23:04
PR,Update doc for sparse related APIs,update NDArrayIter API doc with CSRNDArray inputs update document for kv init kv push add test for 7676 preview mxnet optimizer SGD,,"eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-08-31 22:14:10,2017-09-06 18:59:55
PR,iOS sample app for the Core ML converter tool,pracheer Request to add the iOS sample app for the Core ML converter tool,,piiswrong,2017-09-06 07:06:04,2017-09-06 19:24:52
PR,Changing each task timeout from 60minutes to 120minutes,madjam Increased the max time from 60 mins to 120 minutes for each task in the Jenkinsfile since some build jobs fail due to a timeout Moving forward The plan is to analyze the exact time it takes for each task in the Jenkins Job and modify these timeouts accordingly,,"mbaijal,gautamkmr",2017-09-06 19:20:37,2017-09-06 19:50:20
PR,New operators linalg syrk linalg gelqf,Works for CPU only for now We will supply the GPU versions next Added dtype parameters to unit test support code so we can run numerical tests in float64,,"mseeger,asmushetzel,asmushetzel,asmushetzel,asmushetzel,mseeger,mseeger,mseeger,asmushetzel,piiswrong,mseeger,eric-haibin-lin,mseeger,mseeger,piiswrong,mseeger,mseeger,asmushetzel,mseeger,asmushetzel,asmushetzel,mseeger",2017-09-05 12:54:22,2017-09-06 20:55:26
PR,multinode dist sync fails with mklml,multinode dist sync fails with mklml this patch uses utility function in tensor blob to handle it correctly,,ashokei,2017-09-06 19:12:29,2017-09-06 21:19:20
PR,fix doc,,,zhreshold,2017-09-06 22:15:16,2017-09-06 22:18:23
IS,Jenkins CI Broken R Unit test during build,thirdwing R GPU unit tests are failing more frequently on the master branch Can you please take a look at this Example failed CI build,,"sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,thirdwing,cjolivier01",2017-08-31 00:11:20,2017-09-06 22:23:33
PR,Fix Jenkins R gpu compilation failure,xlib backend c 34 74 fatal error X11 Intrinsic h No such file or directory include X11 Intrinsic h Xlib h Xutil h Xresource h compilation terminated make 1 xlib backend o Error 1 make 1 Leaving directory tmp RtmpgjTXsi R INSTALLbad63cf92901 Cairo src' ERROR compilation failed for package 'Cairo' Fixing by installing necessary dependencies libxt dev,,yuruofeifei,2017-09-06 22:12:51,2017-09-06 22:27:08
IS,cuDNN performance,Hi I have run LeNet model with CUDA 8 0 and cuDNN enabled and later disabled The results shows a 5 10 better performance time with cuDNN enabled Is this result typical or the performance should be higher Thank you,,ptrendx,2017-09-07 11:07:06,2017-09-07 13:26:23
PR,Tiny update of ndarray md,Sorry as pointed out a little too late on my recent request I forgot this tiny update,,"mseeger,mseeger,eric-haibin-lin",2017-09-07 07:42:37,2017-09-07 15:22:32
PR,Providing explicit null pointer for the provided arg stypes,eric haibin lin Instead of introducing a special case to C code to work around the bug in perl api it is better to fix the original bug This makes this pull unnecessary I believe,,"sergeykolychev,cjolivier01,sergeykolychev,cjolivier01",2017-09-07 15:40:29,2017-09-07 17:18:04
PR,check num provided arg types and nullptr in simple bind api for perl,In the SimpleBind CAPI it only checks if provided arg stypes nullptr to know whether any stype is passed in In Perl however the default value for the pointer is not nullptr but some garbage value See the code generated for Perl where arg19 and arg20 are the value for the pointers 7577 failed Perl test because of this I did not check if there are more cases like this where CAPI assumes the default value of a pointer to be nullptr,,"eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,eric-haibin-lin,cjolivier01,eric-haibin-lin,cjolivier01,sergeykolychev,eric-haibin-lin,cjolivier01,eric-haibin-lin",2017-09-07 01:04:35,2017-09-07 17:18:16
PR,Dist kvstore bug fix,rename Pull Push RowSparsePull to PxxxImpl and make them virtual so that KvstoreDist Push str key will eventually call KvstoreDist PushImpl instead of KvstoreLocal PushImpl Previously it always calls functions in KvstoreLocal parallel memcpy for row sparse pull on dist server Improves row sparse pull performance by 30,,eric-haibin-lin,2017-09-06 06:37:35,2017-09-07 17:30:27
PR,rm duplicated and unused code,piiswrong,,formath,2017-09-06 07:45:50,2017-09-07 17:30:46
PR,Fix Moderngpu usages in MXNet for CUDA 9,,,"ptrendx,piiswrong,ptrendx",2017-09-07 08:22:51,2017-09-07 18:31:26
PR,Website redesign,Website UI upgrade Merge after web1 web2 krishnamurthy,,"kevinthesun,sandeep-krishnamurthy,piiswrong,piiswrong,szha,kevinthesun,kevinthesun",2017-09-07 04:29:55,2017-09-07 19:37:36
PR,Remove warnings about not used variable when compiling with CUDA 8,,,ptrendx,2017-09-06 09:33:33,2017-09-07 19:50:47
PR,fix empty prefix case for namescope,piiswrong,,szha,2017-09-07 18:35:12,2017-09-07 20:09:32
PR,Python symbol ndarray api re org,preview 999 api python Summary add one more level to NDArray API and Symbol API Now there are separate pages for sparse NDArray Symbol add separate pages for random linear algebra contrib operators updated the docs for a few sparse operators,,"eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,asmushetzel",2017-09-03 06:36:47,2017-09-07 20:45:22
PR,Fix build server test running out of GPU memory CUDNN allocation failure,,,"cjolivier01,sandeep-krishnamurthy,cjolivier01,cjolivier01",2017-09-07 19:15:34,2017-09-07 21:57:51
PR,The order of hue composed transformation,was incorrect According to The final composed transform should be T hsv T rgb T H T S T V T YIQ translating to t np dot np dot self ityiq bt self tyiq T With this correction you can also change hue and saturation at the same time as in For example su alpha s np cos alpha h np pi sw alpha s np sin alpha h np pi bt np array alpha v 0 0 0 0 0 0 su sw 0 0 sw su t np dot np dot self ityiq bt self tyiq T src nd dot src nd array t return src,,"piiswrong,zhreshold",2017-09-06 01:56:17,2017-09-08 00:15:37
IS,How can I build a conv block using other conv is weights,Hi I m trying reimplement from torch7 to mxnet Since I am new to mxnet and I encounter several prombles I could not find solution in documents The torch code are as follows besides I am comfusing about how to add get Q tmp since I have to implement addtable in hybrid forward function but Q tmp need the results Thank you,,szha,2017-09-07 03:33:05,2017-09-08 02:12:04
IS,Feature request mobilenet,I am intending to implement mobilenet MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications Any suggestions,,"qingzhouzhen,qingzhouzhen",2017-07-20 13:55:17,2017-09-08 02:35:48
IS,how can i do a image mean substraction operation in symbol,hi new to mxnet i wonder how can i do a image mean substraction operation in symbol using existing layer any suggestions would be greatly apprecitate thanks ahead,,"szha,szha",2017-09-08 01:46:28,2017-09-08 03:31:48
IS,mx nd save function can not be used to save into S3 bucket with periods in bucket name,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler gcc Package used Python R Scala Julia Python MXNet version 0 7 Or if installed from source Yes MXNet commit hash git rev parse HEAD 487c22a50541686cc3fd207ad4656dbd2f9fa969 If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message MXNetError Traceback most recent call last ipython input 23 133db28d92b2 in module 1 mx nd save s3 a b home ubuntu local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet ndarray pyc in save fname data 1082 mx uint len handles 1083 c array NDArrayHandle handles 1084 keys 1085 1086 def imdecode str img clip rect 0 0 0 0 out None index 0 channels 3 mean None home ubuntu local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 if sys version info 0 3 MXNetError 03 32 57 src io s3 filesys cc 682 Check failed num retry max error retry maximum retry time reached Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Create a new Jyputer notebook run the following Python make sure that the bucket name of S3 has period in it a mx nd zeros 100 200 b a 1 mx nd save s3 a b Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Start Jupyter 2 Create a notebook with the sample code above 3 Run it and then will fail What have you tried to solve it 1 Tried another S3 bucket without period in the bucket name It worked 2 In the document of S3 found the following description When using virtual hosted style buckets with SSL the SSL wild card certificate only matches buckets that do not contain periods To work around this use HTTP or write your own certificate verification logic 3 Looked into the source code dmlc core src io s3 filesys cc 682 found that the source code is using virtual host style url to access S3 as the following code shows surl https path host s3 amazonaws com ' ' RemoveBeginSlash path name args This virtual host style method will fail because of SSL issue when the target bucket has period in the bucket name,,"DamonDeng,piiswrong,DamonDeng",2016-12-16 03:48:52,2017-09-08 05:13:31
IS,NDArray saved in file with GPU context can not be loaded by NDArray in CPU only environment,NDArray saved in file with GPU context can not be loaded by NDArray in CPU only environment If GPU context was saved together with the data in NDArray saved file it can not be loaded back as NDArray in CPU only environment In the source code of src ndarray ndarray cc we can find the following code in NDArray load dmlc Stream strm method if strm Read load data dptr nread nread return false if ctx dev mask cpu kDevMask this std move temp return true else this temp Copy ctx return true The loading code will check the Context of reading file if the Context in that file is GPU it will call NDArray Copy method which will finally call CUDA lib to copy data from GPU to CPU While in CPU only environment CUDA is not enabled so the operation will cause GPU not enabled fatal error The issue can be fixed by simply adding if MXNET USE CUDA as the following code But I am not sure whether it is a good coding style matching MXNet please comment If it is OK one Pull Request will be created for this if strm Read load data dptr nread nread return false if ctx dev mask cpu kDevMask this std move temp return true else if MXNET USE CUDA this temp Copy ctx return true else this std move temp return true endif,,"DamonDeng,piiswrong",2017-01-09 00:40:11,2017-09-08 05:14:01
PR,Call vcvars for x64 before build on GPU,,,"cjolivier01,piiswrong,cjolivier01,piiswrong",2017-09-07 18:04:19,2017-09-08 06:37:23
PR,Update doc README,,,"kevinthesun,kevinthesun",2017-09-07 21:41:31,2017-09-08 06:38:42
PR,Scala fix,Adjust test threashold which causes a test failure about 1 of the time Also fix a signed unsigned comparison warning,,"cjolivier01,piiswrong,szha,cjolivier01,cjolivier01,yzhliu,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,rahul003,cjolivier01,piiswrong",2017-09-06 23:15:34,2017-09-08 06:40:26
IS,ImageRecordIter max random contrast and max random illumination reasonable value range,mxnet io ImageRecordIter max random contrast and max random illumination reasonable value range 0 1 0 255,,,2017-09-08 13:52:49,2017-09-08 14:06:06
PR,benchmark py modified,stop old processes fixed added option for doing gluon vision models benchmarks,,"Roshrini,Roshrini",2017-08-24 22:09:37,2017-09-08 18:35:48
PR,Add fail flag for tutorial test,,,kevinthesun,2017-09-08 22:49:53,2017-09-08 23:43:51
PR,Perl bugfixes for distributed training using tools launch py,I converted example image classification train mnist py to Perl and then worked my way through what needed to change to get the instructions from how to multi devices html to work with much help from,,"tlby,sergeykolychev",2017-09-08 01:36:00,2017-09-08 23:54:08
PR,Scala add BucketingModule,javelinjs pls help to review thanks The train log of the LstmBucketing example,,"Ldpe2G,yzhliu,yzhliu,yzhliu,Ldpe2G",2017-09-03 14:48:10,2017-09-09 06:00:20
PR,Add back get started index page,,,kevinthesun,2017-09-09 09:06:26,2017-09-09 22:20:54
PR,fix linalg impl uninitialized error,fix build error related to issue,,tornadomeet,2017-09-09 01:50:52,2017-09-09 22:23:44
PR,correct file path for API reference auto index,Preview This PR fixes the missing drop down list for API reference caused by the wrong path of the js file Also moved ImageRecordDataset from data to data vision section,,eric-haibin-lin,2017-09-09 01:03:55,2017-09-09 22:25:15
PR,Disabling the test CSVIter for now,nswamy,,"gautamkmr,gautamkmr",2017-09-09 22:18:44,2017-09-09 22:25:31
PR,update dataset to use acceleration,piiswrong ref,,szha,2017-09-09 07:45:07,2017-09-10 03:58:14
IS,newest mxnet build failed with linalg impl uninitialized error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler 4 8 4 Package used Python R Scala Julia MXNet version 0 11 1 Or if installed from source MXNet commit hash git rev parse HEAD 3f742d254846fbc14dbaa459191403330425ef83 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace only build with gpu will make this error build failed,,"tornadomeet,piiswrong,tornadomeet,tornadomeet",2017-09-08 13:21:50,2017-09-10 06:22:12
PR,modify gluon image classification for imagenet,make gluon example able to train imagenet improve symbolic add std option to recordio 2 cc recordio cc,,"zhreshold,piiswrong,piiswrong,piiswrong,piiswrong,zhreshold,piiswrong,piiswrong,zhreshold,piiswrong,piiswrong,szha,zhreshold",2017-08-26 01:22:50,2017-09-10 07:03:05
PR,multinode dist sync fails with mklml,multinode dist sync fails with mklml this patch uses utility function in tensor blob to handle it correctly,,"ashokei,piiswrong,piiswrong,piiswrong",2017-09-05 18:33:40,2017-09-10 07:03:34
PR,Use Github Code Owners Feature,Current owners are Could you help turn on the require review from owner option,,piiswrong,2017-08-30 21:11:46,2017-09-10 07:14:05
PR,Use GNUInstallDirs and improve CMake handling of shared static mxnet library,This PR does the same as dmlc dmlc core 257 but for mxnet It uses GNUInstallDirs cmake for standard but overridable installation locations for different types of files Additionally it uses plain CMake install instead of custom targets that copy files around In addition it prevents leaking mxnet static and the flags Wl whole archive TARGET FILE mxnet static Wl no whole archive into INTERFACE LINK LIBRARIES of mxnet by marking them as private It also sets the output name of mxnet static to mxnet to let CMake handle the output name rather than symlinking to mxnet static a,,"de-vri-es,cjolivier01,cjolivier01,de-vri-es,de-vri-es,cjolivier01,de-vri-es,de-vri-es,piiswrong,cjolivier01,de-vri-es",2017-05-28 13:18:11,2017-09-10 07:15:21
PR,update core,,,szha,2017-09-08 17:12:54,2017-09-10 07:17:01
PR,Change gluon link,,,"kevinthesun,piiswrong,kevinthesun,piiswrong,piiswrong,kevinthesun",2017-09-01 00:27:23,2017-09-10 07:17:26
PR,Update vgg py in example classification,Reference to the program in gluon model zoo vision vgg py I update the vgg py in example classification 1 The new vgg include vgg 11 13 16 19 Parameter 'num layers' means the number of layers for the variant of densenet 2 Parameter batch norm controls weather to use batch normalization 3 Parameter wouldtype' describe the data precision during the network execution,,"solin319,piiswrong,piiswrong,piiswrong,solin319,solin319,piiswrong",2017-08-22 08:07:30,2017-09-10 07:26:08
PR,Minor update correct badge in README,,,"JeanKossaifi,szha",2017-09-05 15:19:35,2017-09-10 07:27:22
PR,fix windows setup md and add install md,,,"yajiedesign,zhreshold,piiswrong",2017-09-06 01:51:01,2017-09-10 07:28:59
PR,Disabling the test CSVIter for now,nswamy,,"gautamkmr,cjolivier01",2017-09-09 22:29:58,2017-09-10 08:19:04
PR,Added C package option to osx mk file,Added C package option to the osx mk file for easier make configuration on Mac OS,,,2017-09-10 11:05:16,2017-09-10 16:46:26
PR,Adversarial VAE,,,"tornadomeet,piiswrong,piiswrong,piiswrong,piiswrong",2017-08-25 21:06:55,2017-09-10 16:47:47
PR,Add sparse memory benchmark script,eric haibin lin Ran this with valgrind and massif to obtain the memory benchmarking results documented here,,"anirudh2290,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290",2017-09-08 00:17:24,2017-09-10 16:48:20
PR,Add learning rate properties to gluon trainer and optimizer,,,"astonzhang,piiswrong,piiswrong,astonzhang,piiswrong,astonzhang",2017-09-06 06:01:21,2017-09-10 16:49:38
PR,Remove duplicate pylint,,,astonzhang,2017-09-10 20:02:30,2017-09-10 22:31:46
PR,Add example of manipulating learning rates in gluon,,,astonzhang,2017-09-10 20:23:11,2017-09-11 02:11:41
PR,Refactor API Table of Contents,Refactor API folder to split API into multiple files No index md is required for each API folder It is similar to the structure of TOC in Tutorials apitoc apitoc1 haibin lin,,kevinthesun,2017-09-11 05:10:44,2017-09-11 05:21:16
PR,add option for segfault logger,,,"szha,piiswrong,szha",2017-09-10 23:00:09,2017-09-11 05:22:20
PR,fix condition,,,zhreshold,2017-09-11 07:12:16,2017-09-11 17:01:19
PR,change contrib CTC to zero indexed in log space sequence mask for grad,,,"szha,piiswrong,piiswrong,szha,szha,szha,szha,piiswrong,szha,piiswrong,piiswrong,szha,szha,piiswrong",2017-09-04 23:23:58,2017-09-11 17:05:09
PR,Nightly test Compilation warnings only print if there are warnings,Now that we are at that stage without warnings on gcc4 x and gcc5 x only print list of warnings if there are warnings,,rahul003,2017-09-11 18:05:22,2017-09-11 18:17:02
PR,Resolve more compile warnings,All sign compare warnings Warnings in fIrst two files showed up on GPU and the last file is a test file which had sign compare warnings,,"rahul003,piiswrong,piiswrong,rahul003,piiswrong,rahul003,rahul003,piiswrong,rahul003,tornadomeet,rahul003,rahul003,rahul003,rahul003,tornadomeet,rahul003,tornadomeet,rahul003,rahul003",2017-08-11 01:37:17,2017-09-11 18:46:27
PR,Disabling the test CSVIter for now,This test causing random failure while running on windows Disabling it for now till we fix it An git hub issue has been created to track it,,"gautamkmr,eric-haibin-lin,rahul003,rahul003,gautamkmr",2017-09-10 08:25:39,2017-09-11 22:12:57
IS,This code may cause some error,L246 I think line 246 should be something like this coverages self calculate areas intersects object areas valid objects Because if valid objects len object areas an error will occur File usr local lib python2 7 dist packages mxnet image detection py line 246 in check satisfy constraints coverages self calculate areas intersects object areas ValueError operands could not be broadcast together with shapes,,zhreshold,2017-09-11 06:32:10,2017-09-12 05:51:27
IS,Create cublas handle failed,Environment info Operating System Ubuntu cuda8 0 MXNet commit hash git rev parse HEAD e05129774e76206fe890b511c346953107b05fce python2 7 Error Message Please paste the full error message including stack trace incubator mxnet mshadow mshadow stream gpu inl h 115 Check failed err CUBLAS STATUS SUCCESS 1 vs 0 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error,,Godricly,2017-08-30 10:19:28,2017-09-12 08:38:35
IS,ndarray Slice with step,Current slicing still does not support slicing step Is there any plan to support one,,Godricly,2017-08-31 03:29:21,2017-09-12 08:38:39
IS,doc and code mismatch,code L724 and doc image iterators mismatches The condition omits the n and k in raw when checking raw data shape The example in doc shows that k and n are not considered as header,,Godricly,2017-09-01 06:37:49,2017-09-12 08:38:47
IS,disable part of graph in some iterations,Is there any solution to disable part of execution graph during some iterations I'm trying to implement detection based on RCNN with some GAN stuff I prefer not to cut RCNN into pieces Thanks,,Godricly,2017-09-08 03:28:03,2017-09-12 08:38:53
IS,proper masking approach,What is the proper approach to set part of input into zero Here gan pool is a 128 256 7 7 shape feature map and difficult is a 128 dim label with values 1 0 1 Here I want to set those feature maps one with label into zero The broadcast seems not support this kind of case,,"Godricly,Godricly",2017-09-12 08:38:26,2017-09-12 08:56:27
PR,adding ranking metrics precision recall at position K,The file takes in data in dictionary format for both predictions and ground truth Ranking problems are very common in several areas of ML and specifically computing metrics at a certain position is common in many applications Usually for recommender systems we do not want to compute the overall precision but only say precision 10 because only 10 items might be shown to the user It makes sense to optimize your model that gets the top entries right sacrificing overall precision Similar arguments hold for recall coverage etc Having such a standard metric available in MXNet will let more people in the ML community make use of it and increase it is adoptation,,"eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,szha,szha,eric-haibin-lin,szha,szha,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-08-08 22:04:54,2017-09-12 17:32:47
PR,Optimizing the build,1 Move the MSVC specific code under condition 2 Created a private ephemeral location so that build size can be reduced by utilizing that location,,"gautamkmr,gautamkmr",2017-09-07 23:39:34,2017-09-12 17:44:27
IS,OSError libcuda so 1 cannot open shared object file No such file or directory,I exactly followed the instruction to install the GPU version in the server however I am unable to install it successfully Environment info Operating System CentOS release 6 9 Final Compiler gcc version 4 8 1 GCC Package used Python R Scala Julia Python MXNet version mxnet cu80 Or if installed from source No MXNet commit hash git rev parse HEAD NA If you are using python package please provide Python version and distribution Python 3 5 2 Anaconda 4 1 1 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Traceback most recent call last File stdin line 1 in module File home panwei wuxx0845 local lib python3 5 site packages mxnet init py line 7 in module from base import MXNetError File home panwei wuxx0845 local lib python3 5 site packages mxnet base py line 52 in module LIB load lib File home panwei wuxx0845 local lib python3 5 site packages mxnet base py line 44 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File panfs roc msisoft anaconda anaconda3 4 1 1 lib python3 5 ctypes init py line 347 in init self handle dlopen self name mode OSError libcuda so 1 cannot open shared object file No such file or directory Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I followed the steps as described on the website What have you tried to solve it 1 I tried to fix the path including PATH USE CUDA PATH LD LIBRARY PATH Here is the information echo PATH panfs roc msisoft anaconda anaconda3 4 1 1 bin panfs roc msisoft openmpi el6 1 7 2 gnu 4 8 1 bin panfs roc msisoft gcc 4 8 1 bin panfs roc msisoft opencv 3 1 0 bin panfs roc msisoft ffmpeg 2 6 2 bin panfs roc msisoft java jdk1 8 0 45 bin panfs roc msisoft vtk 7 0 0 bin panfs roc msisoft gcc 6 1 0 bin panfs roc msisoft cuda 8 0 bin usr local bin usr local sbin usr lib64 qt 3 3 bin adm suacct bin opt suacct bin opt moab bin usr local bin bin usr bin usr local sbin usr sbin sbin opt ibutils bin panfs roc msisoft cuda sdk 8 0 NVIDIA CUDA 8 0 Samples bin x86 64 linux release echo USE CUDA PATH panfs roc msisoft cuda 8 0 echo LD LIBRARY PATH panfs roc msisoft cuda 8 0 panfs roc msisoft cuda 8 0 lib64 stubs panfs roc msisoft openmpi el6 1 7 2 gnu 4 8 1 lib panfs roc msisoft gcc 4 8 1 lib64 panfs roc msisoft opencv 3 1 0 lib panfs roc msisoft ffmpeg 2 6 2 lib panfs roc msisoft java jdk1 8 0 45 jre lib amd64 server panfs roc msisoft vtk 7 0 0 lib panfs roc msisoft vtk 7 0 0 lib vtk 7 0 panfs roc msisoft isl 0 16 1 gcc6 1 0 lib panfs roc msisoft mpc 1 0 3 gcc6 1 0 lib panfs roc msisoft mpfr 3 1 4 p1 gcc6 1 0 lib panfs roc msisoft gmp 6 1 0 gcc6 1 0 lib soft intel x86 64 2013 composer xe 2013 composer xe 2013 5 192 compiler lib intel64 soft intel x86 64 2013 composer xe 2013 composer xe 2013 5 192 mkl lib intel64 panfs roc msisoft cudnn 5 1 lib64 panfs roc msisoft cuda 8 0 lib64 panfs roc msisoft cuda 8 0 lib panfs roc msisoft gcc 6 1 0 lib64 Since I am using the server I just load cuda and related packages module load cuda 8 0 module load cuda sdk 8 0 module load cudnn 5 1 module load mkl module load opencv 3 1 0 gcc6 1 0 module load ompi gnu module load python3 3 5 2 anaconda4 1 1 I am not sure what is wrong with my procedures I really want to install mxnet and use it Thanks for your help in advance,,"szha,szha,szha,szha,mli,szha",2017-06-22 23:10:06,2017-09-12 18:19:14
PR,Adding developer keys for sandeep,nswamy,,"sandeep-krishnamurthy,sandeep-krishnamurthy,nswamy",2017-08-15 18:16:34,2017-09-12 18:24:34
PR,Added a git clean to the Jenkinsfile,gautamkmr Adding git clean because some builds are picking changes from otherbranches Let is merge only after the PR build succeeds,,"mbaijal,gautamkmr",2017-09-12 20:02:35,2017-09-12 20:08:22
PR,Test ci please ignore,Testing PR integration hooks,,larroy,2017-09-12 21:21:24,2017-09-12 21:45:29
PR,Update Jenkinsfile,gautamkmr,,"nswamy,gautamkmr",2017-09-13 01:32:22,2017-09-13 01:40:44
PR,add pip dependency package use,,,yajiedesign,2017-09-13 01:21:31,2017-09-13 01:43:43
IS,Cannot run mnist example after building from source,Hi guys I am a newbie on mxnet and have encountered issues when deploying mxnet on my machine The detailed description is provided as follows Any suggestion would be appreciated Thanks in advance Environment info Operating System Ubuntu 16 04 LTS Compiler g 5 3 1 Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source YES MXNet commit hash git rev parse HEAD a5edbf94094581ee27157eae4f2113115a3994e7 If you are using python package please provide Python version and distribution 2 7 11 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 When building MXNet from source I encountered the issue then I commented out these lines L374 L376 to make sure the build passed And I tested python binding according to the official instruction validate mxnet installation and it is OK 2 But I cannot successfully import get mnist from mx test utils thus I copied the very function and its dependencies ie download and some py modules Then the dataset is successfully downloaded but received the above error messages 3,,,2017-09-11 08:09:57,2017-09-13 02:55:32
IS,Python stops and throws segmentation fault error,Environment info Operating System Ubuntu 16 04 Compiler GCC 4 9 3 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 5e5f19a Python version and distribution 2 7 12 GCC 5 4 0 20160609 Error Message Segmentation fault core dumped Steps to reproduce 1 compile mxnet according to the official tutorial 2 go to python directory and input sudo python setup py install 3 run python c 'import mxnet',,,2017-02-25 09:41:59,2017-09-13 02:58:04
IS,Weird align issue when training on datasets other than mnist,Environment info Operating System Ubuntu 16 04 Compiler GCC 4 9 4 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 5e5f19a Python version and distribution 2 7 12 GCC 5 4 0 20160609 Error Message Segmentation fault core dumped Steps to reproduce 1 compile mxnet according to the official tutorial 2 go to python directory and input sudo python setup py install 3 run python train cifar10 py as stated in the title we can sucessfully train model on mnist by running train mnist py then the error message INFO root start with arguments Namespace batch size 128 benchmark 0 data nthreads 4 data train wouldata cifar10 train rec' data val wouldata cifar10 val rec' disp batches 20 dtype 'float32' gpus None image shape '3 28 28' kv store wouldevice' load epoch None 18 05 28 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 train rec use 4 threads for decoding 18 05 28 opt share1 xxx Engine mxnet dmlc core include dmlc logging h 300 18 05 28 src io input split base cc 23 Check failed files i size align bytes 0 file do not align by 4 bytes Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f2a7a136f0c bt 1 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc2io14InputSplitBase4InitEPNS0 10FileSystemEPKcm 0x1ad 0x7f2a7adbec5d bt 2 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc10InputSplit6CreateEPKcjjS2 0xe2e 0x7f2a7ad8eefe bt 3 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io19ImageRecordIOParserIfE4InitERKSt6vectorISt4pairISsSsESaIS5 EE 0x76a 0x7f2a7a9fceda bt 4 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io15ImageRecordIterIfE4InitERKSt6vectorISt4pairISsSsESaIS5 EE 0x8a 0x7f2a7a9fd3ba bt 5 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io18ImageNormalizeIter4InitERKSt6vectorISt4pairISsSsESaIS4 EE 0x88 0x7f2a7a9f9508 bt 6 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io11BatchLoader4InitERKSt6vectorISt4pairISsSsESaIS4 EE 0x48f 0x7f2a7a9cdc9f bt 7 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io14PrefetcherIter4InitERKSt6vectorISt4pairISsSsESaIS4 EE 0x1fc 0x7f2a7a9ce5fc bt 8 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so MXDataIterCreateIter 0x2c8 0x7f2a7ad477d8 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f2ad4936e40 Traceback most recent call last File train cifar10 py line 53 in module fit fit args sym data get rec iter File opt share1 xxx Engine mxnet example image classification common fit py line 105 in fit train val data loader args kv File opt share1 xxx Engine mxnet example image classification common data py line 134 in get rec iter part index rank File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet io py line 667 in creator ctypes byref iter handle File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 18 05 28 src io input split base cc 23 Check failed files i size align bytes 0 file do not align by 4 bytes Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f2a7a136f0c bt 1 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc2io14InputSplitBase4InitEPNS0 10FileSystemEPKcm 0x1ad 0x7f2a7adbec5d bt 2 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc10InputSplit6CreateEPKcjjS2 0xe2e 0x7f2a7ad8eefe bt 3 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io19ImageRecordIOParserIfE4InitERKSt6vectorISt4pairISsSsESaIS5 EE 0x76a 0x7f2a7a9fceda bt 4 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io15ImageRecordIterIfE4InitERKSt6vectorISt4pairISsSsESaIS5 EE 0x8a 0x7f2a7a9fd3ba bt 5 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io18ImageNormalizeIter4InitERKSt6vectorISt4pairISsSsESaIS4 EE 0x88 0x7f2a7a9f9508 bt 6 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io11BatchLoader4InitERKSt6vectorISt4pairISsSsESaIS4 EE 0x48f 0x7f2a7a9cdc9f bt 7 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2io14PrefetcherIter4InitERKSt6vectorISt4pairISsSsESaIS4 EE 0x1fc 0x7f2a7a9ce5fc bt 8 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so MXDataIterCreateIter 0x2c8 0x7f2a7ad477d8 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f2ad4936e40,,,2017-02-25 10:09:25,2017-09-13 03:35:22
IS,run example ssd with memonger error,I try to run example ssd reset50 with me monger but error simple add me monger code afer net get symbol train net data shape 1 num classes num classes nms thresh nms thresh force suppress force suppress nms topk nms topk import memonger net memonger search plan net and force to use me monger def resnet units num stages filter list num classes image shape bottle neck True bn mom 0 9 workspace 256 memonger True and I run ssd without memonger is ok Environment info mxnet version is 0 11 1 centreos 7 Error Message infer shape error Arguments Traceback most recent call last File train py line 150 in module voc07 metric args use voc07 metric File home kevin incubator mxnet example ssd train train net py line 202 in train net net memonger search plan net File home kevin mxnet memonger memonger py line 141 in search plan sym make mirror plan sym threshold threshold plan info info kwargs File home kevin mxnet memonger memonger py line 62 in make mirror plan out shapes internals infer shape kwargs File home kevin incubator mxnet example ssd tools python mxnet symbol symbol py line 962 in infer shape res self infer shape impl False args kwargs File home kevin incubator mxnet example ssd tools python mxnet symbol symbol py line 1091 in infer shape impl ctypes byref complete File home kevin incubator mxnet example ssd tools python mxnet base py line 143 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator bn data 12 40 25 src operator batch norm inl h 238 Check failed channelAxis dshape ndim 1 vs 0 Channel a is out of range 1,,junranhe,2017-09-11 04:59:04,2017-09-13 03:49:50
IS,confused about update on kvstrore in gluon trainer step,if self kvstore self kvstore push i param list grad priority i if self update on kvstore self kvstore pull i param list data priority i continue else self kvstore pull i param list grad priority i if the kvstore is in wouldevice' mode is it just use the all reduced gradients to replaece the weights,,eric-haibin-lin,2017-09-12 10:29:11,2017-09-13 08:49:12
PR,Enabling persistent BN in cuDNN 7 0 2,,,ptrendx,2017-09-13 07:13:20,2017-09-13 19:07:09
PR,Sparse operators for unary and binary elemwise NDArray operators,Pre reviewed here Most F OP computes replaced with Kernel Launch Does not include reducing operators Supported operators forward and backward passes row sparse csr CPU GPU Sparse Unit Test relu Y Y Y Y sigmoid Y Y Y copy Y Y Y implicit BlockGrad Y Y Y implicit make loss Y Y Y implicit Cast Y Y Y implicit cast storage Y Y Y implicit negative Y Y Y Y abs Y Y Y sign Y Y Y round Y Y Y rint Y Y Y ceil Y Y Y floor Y Y Y fix Y Y Y square Y Y Y Y sqrt Y Y Y rsqrt Y Y Y exp Y Y log Y Y log10 Y Y Y log2 Y Y Y sin Y Y Y log1p Y Y Y expm1 Y Y Y cos Y Y Y tan Y Y Y arcsin Y Y Y arccos Y Y Y arctan Y Y Y degrees Y Y Y radians Y Y Y sinh Y Y Y cosh Y Y Y tanh Y Y Y arcsinh Y Y Y arccosh Y Y Y arctanh Y Y Y gamma Y Y Y gammaln Y Y Y elemwise add Y Y Y elemwise sub Y Y Y elemwise mul Y Y Y elemwise div Y Y Y power Y Y maximum Y Y Y minimum Y Y Y hypot Y Y Y hypot scalar Y Y Y mul scalar Y Y Y Y,,"cjolivier01,cjolivier01,eric-haibin-lin,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,eric-haibin-lin,eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,eric-haibin-lin,piiswrong,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,szha,eric-haibin-lin",2017-08-23 14:59:01,2017-09-13 19:34:48
PR,Many loss functions,Added a plethora of loss functions probably more to come to make losses a winning thing In particular added many classical scalar loss functions more max margin losses and some losses for the dual Ali Slivey estimation,,"smolix,szha,smolix",2017-08-25 02:35:46,2017-09-13 19:52:08
IS,Failure to install R GPU version package,Hi unable to install R GPU package install seems to be ok but when loading it does not find libmxnet dll the dll is where its supposed to be Environment info Operating System Windows 10 Pro Compiler 14393 Package used R 3 4 1 R sessionInfo R version 3 4 1 2017 06 30 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 8 x64 build 9200 Matrix products default locale 1 LC COLLATE Spanish Spain 1252 LC CTYPE Spanish Spain 1252 LC MONETARY Spanish Spain 1252 LC NUMERIC C LC TIME Spanish Spain 1252 attached base packages 1 stats graphics grDevices utils datasets methods base loaded via a namespace and not attached 1 Rcpp 0 12 12 compiler 3 4 1 RColorBrewer 1 1 2 influenceR 0 1 0 plyr 1 8 4 bindr 0 1 viridis 0 4 0 tools 3 4 1 digest 0 6 12 10 jsonlite 1 5 viridisLite 0 2 0 tibble 1 3 3 gtable 0 2 0 rgexf 0 15 3 pkgconfig 2 0 1 rlang 0 1 1 igraph 1 1 2 rstudioapi 0 6 19 bindrcpp 0 2 gridExtra 2 2 1 downloader 0 4 DiagrammeR 0 9 2 dplyr 0 7 2 stringr 1 2 0 htmlwidgets 0 9 hms 0 3 grid 3 4 1 28 glue 1 1 1 R6 2 2 2 Rook 1 1 1 XML 3 98 1 9 readr 1 1 1 purrr 0 2 3 tidyr 0 6 3 ggplot2 2 2 1 magrittr 1 5 37 scales 0 5 0 htmltools 0 3 6 assertthat 0 2 0 colorspace 1 3 2 brew 1 0 6 stringi 1 1 5 visNetwork 2 0 1 lazyeval 0 2 0 munsell 0 4 3 Error Message Error package or namespace load failed for mxnet onLoad failed in loadNamespace for 'mxnet' details call inDL x as logical local as logical now error unable to load shared object 'C Users Miguel Documents R win library 3 4 mxnet libs x64 libmxnet dll' LoadLibrary failure No se puede encontrar el m dulo especificado Steps to reproduce Just running the install packages code with last GPU version 1 cran getOption repos cran dmlc options repos cran install packages mxnet 2 library mxnet What have you tried to solve it 1 Checked that libmxnet dll is where its supposed to be 2 Confirmed that CPU version works fine 3 Confirm CUDA is in path 4 Try multiple times restart etc 5 Tried building with Thirdwing is instructions failure for other reasons error asked there in comments,,,2017-09-11 09:10:34,2017-09-13 19:54:43
PR,Fix the CMAKE RUNTIME OUTPUT DIRECTORY path,cjolivier01,,"gautamkmr,gautamkmr",2017-09-13 21:42:16,2017-09-13 21:44:41
PR,Fix the CMAKE RUNTIME OUTPUT DIRECTORY path,,,gautamkmr,2017-09-13 22:28:46,2017-09-13 22:37:11
PR,copy op minor fix,fix wrong FInferStorageType attr of copy,,eric-haibin-lin,2017-09-13 23:38:33,2017-09-14 00:16:34
PR,Adjust elemwise xxx naming for gpu,Also re added storage type inference to copy,,"cjolivier01,eric-haibin-lin,cjolivier01",2017-09-13 23:52:51,2017-09-14 01:13:09
PR,Testing empty commit,,,gautamkmr,2017-09-13 22:31:25,2017-09-14 05:19:22
PR,Testing the commit,,,gautamkmr,2017-09-13 22:33:29,2017-09-14 05:19:27
PR,Fix CMakeLists txt for unit tests variable was not expanded,,,cjolivier01,2017-09-13 21:31:56,2017-09-14 05:20:10
PR,Adding installation step for libgfortron,rahul003,,gautamkmr,2017-09-14 00:00:46,2017-09-14 19:12:18
PR,Fix missing of CUDA device on non GPU host,The issue that no CUDA capable device is detected occurs when calling MxNet from Java Scala on a host without GPU The user scenario is to run MxNet model on CPU only host with Context input as cpu 0 The default GPU unit shares the same index of zero with the default CPU unit By default the MXNET USE CUDA is enabled and the logic will proceed to the line of CUDA CALL The calling of cudaSetDevice exits with exception since there is GPU found for index of zero The proposal here is to enable the CUDA CALL only when the device type of Context is not CPU The whole project with code changes proposed here has been built successfully in local host After that the running of MxNet model was successful in CPU only host for evaluation,,"piiswrong,Ldpe2G,Ldpe2G,Ldpe2G,yzhliu",2017-09-11 22:43:49,2017-09-15 02:41:10
PR,Fix Symbol Index,,,kevinthesun,2017-09-14 18:41:44,2017-09-15 06:57:07
IS,accuracy metric result does not match the result from confusion matrix,I trained a multi class classification model using LSTM and softmax output to classify time series data I have about 17000 labeled time series data for 12 different classes For model performance evaluation I used the mxnet metric Accuracy and got an 70 accuracy However when I evaluated the performance with sklean confusion matrix the accuracy is only about 10 Would anyone know why the MXNet metric class would return such drastically different result,,eric-haibin-lin,2017-09-15 00:58:06,2017-09-15 13:17:23
IS,Problems with CNN model training and prediction for text classification,Hi I was trying to train a text classification model and consistently faced with the following 2 problems Appreciate any guidance on this 1 Whenever a dropout layer is added the kernel would crash when the Module fit is called If I remove the dropout the model trains successfully 2 When trying to make new prediction with the trained model always receive the following error Check failed shape 0 end 1 vs 50 Slice end index out of range For prediction I have tried various input data shape combinations e g 50 500 1 500 and none seemed to work Please see below for the network definition code batch size 50 input x mx sym Variable wouldata' input y mx sym Variable isoftmax label' vocab size 10000 sentence size 500 num embed 300 embed layer mx sym Embedding data input x input dim vocab size output dim num embed name 'vocab embed' conv input mx sym Reshape data embed layer target shape batch size 1 sentence size num embed filter list 3 4 5 num filter 100 pooled outputs for i filter size in enumerate filter list convi mx sym Convolution data conv input kernel filter size num embed num filter num filter relui mx sym Activation data convi act type arelu' pooli mx sym Pooling data relui pool type 'max' kernel sentence size filter size 1 1 stride 1 1 pooled outputs append pooli combine all pooled outputs total filters num filter len filter list concat mx sym Concat pooled outputs dim 1 h pool mx sym Reshape data concat target shape batch size total filters dropout layer dropout 0 0 if dropout 0 0 h drop mx sym Dropout data h pool p dropout else h drop h pool fully connected layer num label 2 cls weight mx sym Variable 'cls weight' cls bias mx sym Variable 'cls bias' fc mx sym FullyConnected data h drop weight cls weight bias cls bias num hidden num label softmax output sm mx sym SoftmaxOutput data fc label input y name isoftmax',,"zihaolucky,formath",2017-06-22 06:00:36,2017-09-15 13:18:15
PR,Elementwise Sum add n for rowsparse on GPU,eric haibin lin thanks for resolving conflicts See original PR code review,,"stefanhenneking,eric-haibin-lin",2017-09-10 19:01:38,2017-09-15 18:22:16
PR,sparse add ftrl optimizer for sparse,eric haibin lin,,"CNevd,eric-haibin-lin,CNevd,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-09-04 02:51:10,2017-09-15 18:25:13
IS,The Meaning of parameters of mxnet gluon Embedding function,The doc of mxnet gloun Embedding function have the following states Input shape 2D tensor with shape N M Output shape 3D tensor with shape N M output dim So What is the meaning of N M Does it means the shape of inputs say batch size vocab size And another one is what is the implementation strategy of this function Does it just a one or two layer neural network,,"szha,szha",2017-09-14 02:12:37,2017-09-16 04:22:38
IS,Dividing input changes BatchNorm output,I was trying to experiment with ResNet 152 model downloaded from Model Zoo For some reason first batch normalization vary it is output if I divide or apply any other elementwise operation to it is input Environment info Operating System ArchLinux Linux archshad 4 12 10 1 ARCH 1 SMP PREEMPT Wed Aug 30 12 18 42 CEST 2017 x86 64 GNU Linux Compiler gcc GCC 7 2 0 gcc 5 GCC 5 4 0 Cuda compilation tools release 8 0 V8 0 61 Package used Python R Scala Julia Python MXNet version 0 11 0 behavior was the same on 0 10 0 Python version Python 3 6 2 Python 2 7 13 Minimum reproducible example I wrote script for checking outputs of model on test image Steps to reproduce Run code from gist with some colored image saved resnet 152 model What have you tried to solve it Changing library versions python version,,,2017-09-15 17:02:02,2017-09-16 08:06:04
PR,WIP CSRNDArray Tutorial,Note the behavior described for Sparse Operators and Storage Type Inference section requires and storage inference refactoring This should not be merged before 7577,,"eric-haibin-lin,rahul003,rahul003,anirudh2290,anirudh2290,cjolivier01,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-08-29 21:03:30,2017-09-16 23:19:47
PR,WIP RowSparseNDArray tutorial,Preview 801 tutorials sparse rowsparse html Note the behavior described for Sparse Operators and Storage Type Inference section requires 7577 and storage inference refactoring This should not be merged before 7577 TODO update index md,,"eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-09-02 05:52:43,2017-09-16 23:19:59
PR,WIP Tutorial Training with Sparse Symbols,Do not merge some part of the tutorial requires storage type refactoring is merged first TODO update tutorial index md,,"eric-haibin-lin,eric-haibin-lin",2017-09-08 20:30:13,2017-09-16 23:20:12
IS,ViBe,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-09-16 12:55:26,2017-09-17 00:30:05
IS,ImportError for mxnet cannot import name libinfo,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows10 Compiler cl nvcc vs2013 Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 Anaconda 4 1 1 64 bit If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python setup py install 2 import mxnet 3 What have you tried to solve it 1 I tried to run a edited setup py file 4753 but still get error 2 3,,szha,2017-01-24 11:49:32,2017-09-17 01:33:05
IS,What about adding mxnet to pip,What about adding mxnet to pip so that we can easily install mxnet,,"piiswrong,szha",2017-02-13 05:41:11,2017-09-17 01:33:50
IS,Build from source with GPU supported ended with cannot find lopencv imgcodecs,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler g MXNet commit hash 2e4be17a014bb5315c18a8fc974fd0c7754081be Error Message Hi I tried to build from source with GPU support on Ubuntu but got following error message usr bin ld cannot find lopencv imgcodecs What have you tried to solve it I googled and found that lopencv imgcodecs is available from opencv 3 0 but apt get install libopencv dev will only install open 2 4 9 could this be the issue,,"piiswrong,reminisce,eric-haibin-lin,szha",2017-03-20 23:17:47,2017-09-17 01:34:29
IS,Problem about bi rnn,I have got this error If I turn bidirectional False there will be no error I do not know why System Ubuntu 14 04 5 LTS GNU Linux 4 4 0 45 generic x86 64 Python 2 7 MxNet 0 9 4,,"piiswrong,szha",2017-04-01 03:38:04,2017-09-17 01:34:49
IS,Question about sparsity regularization and IdentityAttachKLSparseReg,Hi I would like to apply sparsity regularization to outputs of ReLU layers I notice there is a layer called IdentityAttachKLSparseReg However its documentation states that it applies a sparse regularization to the output of a sigmoid activation function Then is it correct to apply IdentityAttachKLSparseReg to outputs of ReLU functions Could anyone provide the mathematical formula of this layer If this layer does not work for outputs of ReLU functions how can I apply sparsity regularization correctly Thank you very much,,"szha,eric-haibin-lin",2017-04-07 08:27:01,2017-09-17 01:35:00
IS,pip installation does not check libcudnn version,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu desktop 16 04 2 Compiler Not used Package used Python R Scala Julia Python MXNet version version pip install mxnet cu80 0 9 3a3 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 112 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace import mxnet as mx Traceback most recent call last File stdin line 1 in module File usr local lib python2 7 dist packages mxnet init py line 7 in module from base import MXNetError File usr local lib python2 7 dist packages mxnet base py line 43 in module LIB load lib File usr local lib python2 7 dist packages mxnet base py line 35 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File usr lib python2 7 ctypes init py line 362 in init self handle dlopen self name mode OSError libcudnn so 5 cannot open shared object file No such file or directory Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I want to try libcudnn 6 but found both pip installations of tensorflow and mxnet requires libcudnn so 5 to import I download and install libcudnn 6 from 2 pip install mxnet cu80 and it is silently installed successfully 3 import mxnet as mx and it prints error OSError libcudnn so 5 cannot open shared object file What have you tried to solve it 1 force pip install to check the depended libcudnn version 2 let pip install to select correct pre compiled mxnet binaries to install 3,,"ysh329,ysh329,szha,ysh329,szha,ysh329,szha,szha,szha,szha",2017-04-14 03:45:56,2017-09-17 01:35:14
IS,updating mxnet pip package,The current mxnet version in pip is not in sync with github master branch and some of the examples are not working because mxnet pip package does not have contrib package yet Please update the pip package with what is latest in master branch Here are repro steps,,"szha,szha",2017-04-21 20:36:11,2017-09-17 01:37:45
IS,Install the scripts in the tools package,MXNet has provides many utility scripts in the tools folder These scripts are commonly used e g tools im2rec py These should packaged and installed when MXNet installed using either PIP or built from source and available in the default PYTHONPATH,,"nswamy,szha",2017-05-11 11:39:25,2017-09-17 01:38:22
IS,How can I export the embedding layer is parameter from a trained word2vec model,I plan to share this parameter to other model,,"szha,zihaolucky,szha",2017-06-23 03:09:43,2017-09-17 01:38:38
IS,A question about ColorJitterAug,I am trying to do do some colour jitter using ColorJitterAug in image py I saw in the code that the function returns a RandomOrderAug object I am wondering how we put augmentation on real images Does it work like cj ColorJitterAug x x x cj image If yes does it simply calculate each pixel in a for loop or use some efficient way internally Thanks,,szha,2017-06-21 13:56:34,2017-09-17 01:38:44
IS,How to do a sobel operation in symbol layer,there is a symbol layer its size is 1 1 300 300 How to use it to do a sobel operation I kown that we need to use symbol broadcast add broadcast sub But how to select the specif area Symbol can be used as numpy Symbol 1 W 1 H Symbol 0 W 1 0 H 1 this is right,,szha,2017-08-10 02:45:07,2017-09-17 01:39:04
IS,GRU not working with layout 'NTC',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 2 LTS Package used Python R Scala Julia Python MXNet version mxnet cu80 0 10 1b20170803 If you are using python package please provide Python version and distribution python 2 7 13 used with anaconda 4 2 23 Error Message Please paste the full error message including stack trace What have you tried to solve it The only thing that solves it is changing the layout and the data accordingly to 'TNC',,"peschn,szha,szha",2017-08-08 07:37:50,2017-09-17 01:39:22
IS,undefined symbol dnnBatchNormalizationCreateBackward v2 F32,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler g 5 4 0 Package used Python R Scala Julia Python MXNet version Or if installed from source YES MXNet commit hash git rev parse HEAD 3ceb6d2f91121d5ffa5b81f435e8bcfcc1a75792 If you are using python package please provide Python version and distribution Python 2 7 13 Anaconda custom 64 bit Error Message Please paste the full error message including stack trace 3 launch python and type import mxnet as mx error occur What have you tried to solve it 1 the generated libmxnet so does not contain the dnnBatchNormalizationCreateBackward v2 F32 function as shown by nm gC libmxnet so,,"szha,szha,szha",2017-06-22 03:16:01,2017-09-17 01:40:43
IS,import mxnet errors,Description If I use pip which is provided by OS is not the version through get pip py script to install mxnet there are some errors occur when I run import mxnet as mx Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 4 Package used Python R Scala Julia Python MXNet version 0 10 0 post2 MXNet commit hash git rev parse HEAD Python version and distribution python 2 7 Error Message OSError Traceback most recent call last ipython input 1 cb88d04c42ec in module 1 import mxnet as mx usr local lib python2 7 dist packages mxnet init py in module 5 6 from context import Context current context cpu gpu 7 from base import MXNetError 8 from import base 9 from import contrib usr local lib python2 7 dist packages mxnet base py in module 50 version libinfo version 51 library instance of mxnet 52 LIB load lib 53 54 type definitions usr local lib python2 7 dist packages mxnet base py in load lib 42 Load libary by searching possible path 43 lib path libinfo find lib path 44 lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL 45 DMatrix functions 46 lib MXGetLastError restype ctypes c char p usr lib python2 7 ctypes init pyc in init self name mode handle use errno use last error 363 364 if handle is None 365 self handle dlopen self name mode 366 else 367 self handle handle OSError usr local lib python2 7 dist packages mxnet libmxnet so invalid ELF header Minimum reproducible example I do not use wget sudo python get pip py and I directly use the pip which provided by OS The pip version is pip 1 5 4 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 I try to uninstall my pip and use the pip which gained through get pip py After that mxnet is running normally 2 3,,"szha,szha",2017-08-07 07:26:11,2017-09-17 01:43:35
IS,Project Ideas Provide additional prebuilt Windows binaries,Hi I'm using both Windows 32 bit and 64 bit and Linux Operating Systems UBuntu and Debian to run MXNet I have found out that the installation steps for Linux Operating Systems are generally straight forward and easy to automate Whereas for Windows it is quite a challenge to build MXNet and the decision I have made was to install prebuilt 64 bit binaries Currently I will need to switch to 32 bit Windows and the prebuilt binaries are not available for 32 bit Looking at the number of build issues reported in the Issues like I do not see how this trend would be reduced The existing working model for new MXNet release is 1 Besides using prebuilt binaries whenever there is a new version both MXNet developers and users are expected to build from scratch 2 MXNet user will have to invest time and efforts into debugging any compilation failure due to the new version 3 It is time consuming for MXNet users to go through this hassle debug report and wait for the official support from this forum One improvement opportunity I would like to propose is for MXNet to provide more prebuilt binaries where 1 Only core MXNet developers will require to build from the source code 2 MXNet released official prebuilt binaries once for all supported major platforms once 3 User will only have to pick up the necessary prebuilt binary targeting their platform 4 Issue found is consistent across MXNet users and chances are the fix will be introduced in the coming prebuilt binaries To clarify I am suggesting MXNet to release official prebuilt binaries including Windows 32 bit as well It would be great if there are other prebuilt binaries for major Linux platforms as well RPM DEB etc Compiler Package used with version and distribution Python 2 7 MXNet version 0 10 0 MXNet commit hash git rev parse HEAD I'm not using git for Windows,,"szha,szha",2017-07-16 03:35:26,2017-09-17 01:45:57
IS,Distributed KeyValue Store broken,Hi All Are the new KVStore API changes applied to distributed KVStore Looks like the new API using strings instead of ints for keys is added only to the KVStore local,,"szha,piiswrong,eric-haibin-lin",2017-06-28 18:46:12,2017-09-17 01:46:13
IS,Periodic loss value,The cross entropy loss for classification task is somehow periodic during iteration and the period is exactly an epoch of iterations of data set The loss is lowest at the end of every epoch and increases rapidly relatively at the beginning of the next epoch This phenomenon appears in different networks training on different data sets in my experiments so I think there might be something wrong with my training procedure rather than the model or data sets Some potential mistake I probably made the use of metric the use of dataIter the use of optimizer anyone have encountered similar problem here are my codes,,"szha,szha",2017-06-24 09:19:03,2017-09-17 01:46:41
IS,can not find op h when I try to use example in cpp package,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler g 4 8 Package used Python R Scala Julia C MXNet version trunk Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 compile test sample in cpp package 2 op h file can not found 3 In file included from include mxnet cpp MxNetCpp h 16 0 from mlp cpp 8 include mxnet cpp optimizer hpp 19 26 fatal error mxnet cpp op h No such file or directory What have you tried to solve it 1 vim a new file named op h just edit it include op h,,"szha,szha",2017-06-23 08:37:13,2017-09-17 01:47:21
IS,pre trained resnext model does not work,I downloaded the pre trained resnext 50 model from and I processed the imagenet1k val data by python CUR DIR tools im2rec py resize 256 quality 90 num thread 16 imagenet1k val val But when I execute this script in the terminal python score py gpus 0 1 2 3 model imagenet1k resnext 50 data val data imagenet1k val rec data nthreads 16 print INFO root Finished with 438 658564 images per second INFO root 'accuracy' 7 992327365728901e 05 INFO root 'top k accuracy 5' 0 0021379475703324807 The above print is obvious wrong How this problem come out I try other pre trained models such as resnet 50 they do not have the above problem,,"zihaolucky,szha,winstywang,terrychenism,terrychenism,Jerryzcn,terrychenism,terrychenism,szha,hubenjm",2017-06-13 10:26:55,2017-09-17 01:47:52
IS,pip install error No matching distribution found for mxnet cu80,I install mxnet cu80 on CentOS7 Power Linux which has been installed cuda,,"ysh329,szha,ysh329,szha,ysh329,szha,szha,ysh329,szha,ysh329,szha,szha,szha,szha,szha,szha,szha,szha,szha",2017-04-28 10:01:33,2017-09-17 01:50:41
IS,PROPOSAL Sparse Tensor Support,Start a thread to gather inputs comments suggestions Frontend Data Structures Classes There are two potential ways to represent sparse ndarrays either create a new class or add a type to the existing ndarray class We need to figure out which one is more desirable 1 Two python classes NDArray Normal NDArray with data stored in dense format SparseNDArray NDArray with data stored in sparse format Execution Flow Imperative Execution For imperative execution in MXImperativeInvoke if at least one of input NDArray is sparse try to use FComputeNDArray If no such operator is registered but input tensor is sparse call to dense on all inputs Symbolic Execution Weights and gradients initialization Weights are initialized in dense format gradients are initialized with specified dense sparse types Bind Symbols The memory for sparse tensors cannot be planned ahead of time and is allocated during op execution For a graph with sparse tensor operations we need to infer if any node in the graph takes sparse input and try to share memory at execution time There are two ways to infer the chunk type 1 Add an InferChunkType pass which infers the chunk types based on input chunk types Ctx information will help infer mkl chunks 2 Compose InferChunkType InferShape InferType into a single InferNDProperty pass which performs all three Type shape and chunk type are wrapped into a struct Additional NDArray constructor with this struct will be added Dispatch and Execution Extend FComputeExecutor to execute FComputeNDArray besides FCompute If the input chunk type is sparse while FCompute is expecting dense input sparse data will be converted to dense format when OpExecutor Run is invoked Since sparse tensors request memory at execution time we should use a memory pool for operators to allocate reallocate memory for its output buffer KVStore KVStore would take a sparse ndarray and perform the update We can simply implement SGD op with the FComputeNDArray interface For communication keys should be the row ids for the parameters,,"eric-haibin-lin,piiswrong,szha",2017-03-20 18:08:43,2017-09-17 01:51:32
IS,Installation instructions need update,Following instructions from MXNet can not be built on Ubuntu 16 04 with the default OpenCV version 2 4 9 because MXNet 0 9 5 requires at least OpenCV 3 x due to the reference to lopencv imgcodecs In addition OpenCV3 1 is not compatible with CUDA8 so OpenCV3 2 is required Installing OpenCV3 2 dev following instructions from resolves the issue For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler gcc 5 4 0 MXNet version 0 9 5,,"piiswrong,szha,piiswrong,szha,szha,sandeep-krishnamurthy,szha,sandeep-krishnamurthy,szha",2017-05-17 15:37:15,2017-09-17 01:55:04
IS,upgrdae to pre release version of mxnet,i want to upgrade to the pre release package of mxnet Can i do this through pip i tried pip with pre but no luck is there another way to upgrade a library to the pre release version i am using mac,,"SCP-173-cool,szha,szha",2017-05-20 09:37:32,2017-09-17 01:56:07
IS,using python interface some doubts for c api,like following python api mxnet symbol Activation data None act type Null name None attr None out None kwargs in activation cc file i can only find those two parameters data act type by following code add argument data NDArray or Symbol Input array to activation function add arguments ActivationParam FIELDS but other parameters can not be found how python knows those parameters where those parameter are defined,,"szha,ZiyueHuang,szha",2017-08-20 14:25:14,2017-09-17 01:57:53
IS,Issue with Loss function,szha test ce loss is failing too frequently leading to failures in master CI builds Based on the conversation it looks like Aston is working on the fix In the meantime as part of stabilizing CI builds and avoid other issues getting in unnoticed I am skipping this test This issue should help to track in the fix and re enabling the test ce loss,,"sandeep-krishnamurthy,szha,astonzhang,sandeep-krishnamurthy,astonzhang",2017-08-31 01:22:14,2017-09-17 01:59:44
IS,Fixed Embedding Layer on Gluon,Is there a way to declare a Layer Block to be constant and not be updated during training The only way I see it as of now is to create my own layer and not add embedding matrix to the ParameterDict,,"rravu3,szha,szha,piiswrong",2017-09-01 00:38:53,2017-09-17 02:00:17
IS,Accuracy does not increase when using distributed kvstore,Accuracy does not increase when using distributed kvstore The accuracy fluctuate around 0 1 Can you help to inform the guys related to the distributed training Environment info Operating System Ubuntu 16 04 3 LTS Compiler gcc Ubuntu 5 4 0 6ubuntu1 16 04 4 5 4 0 20160609 Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD d8e92df0fd80f074188b9c231e6bddcdf4cced46 If you are using python package please provide Python version and distribution Python 2 7 12 If you are using R package please provide R sessionInfo Error Message When using non distributed version mxnet gpu test cx2 vision gpu 1 data cx2 virtualenv mxnet gpu test src incubator mxnet example im age classification python train mnist py INFO root start with arguments Namespace add stn False batch size 64 disp batches 100 dtype 'floa t32' gpus None kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' mo del prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 6000 0 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Epoch 0 Batch 100 Speed 1682 44 samples sec accuracy 0 768719 INFO root Epoch 0 Batch 200 Speed 1621 89 samples sec accuracy 0 905000 INFO root Epoch 0 Batch 300 Speed 1730 96 samples sec accuracy 0 929375 INFO root Epoch 0 Batch 400 Speed 1817 02 samples sec accuracy 0 937344 INFO root Epoch 0 Batch 500 Speed 2072 11 samples sec accuracy 0 940156 INFO root Epoch 0 Batch 600 Speed 2183 72 samples sec accuracy 0 948750 INFO root Epoch 0 Batch 700 Speed 1955 86 samples sec accuracy 0 954688 INFO root Epoch 0 Batch 800 Speed 1764 03 samples sec accuracy 0 951875 INFO root Epoch 0 Batch 900 Speed 2086 64 samples sec accuracy 0 956562 INFO root Epoch 0 Train accuracy 0 959882 INFO root Epoch 0 Time cost 34 679 INFO root Epoch 0 Validation accuracy 0 950936 INFO root Epoch 1 Batch 100 Speed 2131 35 samples sec accuracy 0 955755 INFO root Epoch 1 Batch 200 Speed 2316 70 samples sec accuracy 0 959219 INFO root Epoch 1 Batch 300 Speed 2100 69 samples sec accuracy 0 965313 When using distributed version with single process mxnet gpu test cx2 vision gpu 1 data cx2 virtualenv mxnet gpu test src incubator mxnet example im age classification python tools launch py n 1 launcher ssh H hosts python train mnist py kv store dist sync INFO root start with arguments Namespace add stn False batch size 64 disp batches 100 dtype 'floa t32' gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 6 0000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Epoch 0 Batch 100 Speed 1570 26 samples sec accuracy 0 097308 INFO root Epoch 0 Batch 200 Speed 1684 40 samples sec accuracy 0 101719 INFO root Epoch 0 Batch 300 Speed 1967 76 samples sec accuracy 0 094531 INFO root Epoch 0 Batch 400 Speed 1448 44 samples sec accuracy 0 100469 INFO root Epoch 0 Batch 500 Speed 1653 76 samples sec accuracy 0 093125 INFO root Epoch 0 Batch 600 Speed 1659 77 samples sec accuracy 0 100937 INFO root Epoch 0 Batch 700 Speed 1776 14 samples sec accuracy 0 100625 INFO root Epoch 0 Batch 800 Speed 1707 80 samples sec accuracy 0 098437 INFO root Epoch 0 Batch 900 Speed 1968 17 samples sec accuracy 0 100781 INFO root Epoch 0 Train accuracy 0 098395 INFO root Epoch 0 Time cost 35 276 INFO root Epoch 0 Validation accuracy 0 098029 INFO root Epoch 1 Batch 100 Speed 1615 71 samples sec accuracy 0 097772 INFO root Epoch 1 Batch 200 Speed 1660 07 samples sec accuracy 0 101719 INFO root Epoch 1 Batch 300 Speed 1997 55 samples sec accuracy 0 094531 When using distributed version with 2 processes mxnet gpu test cx2 vision gpu 1 data cx2 virtualenv mxnet gpu test src incubator mxnet example im age classification python tools launch py n 2 launcher ssh H hosts python train mnist py kv store dist sync INFO root start with arguments Namespace add stn False batch size 64 disp batches 100 dtype 'floa t32' gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 6 0000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root start with arguments Namespace add stn False batch size 64 disp batches 100 dtype 'floa t32' gpus None kv store wouldist sync' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 6 0000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 INFO root Epoch 0 Batch 100 Speed 865 23 samples sec accuracy 0 099783 INFO root Epoch 0 Batch 100 Speed 755 68 samples sec accuracy 0 099319 INFO root Epoch 0 Batch 200 Speed 886 68 samples sec accuracy 0 101250 INFO root Epoch 0 Batch 200 Speed 775 73 samples sec accuracy 0 100937 INFO root Epoch 0 Batch 300 Speed 995 09 samples sec accuracy 0 097812 INFO root Epoch 0 Batch 300 Speed 955 45 samples sec accuracy 0 093281 INFO root Epoch 0 Batch 400 Speed 942 12 samples sec accuracy 0 097187 INFO root Epoch 0 Batch 400 Speed 927 82 samples sec accuracy 0 099062 INFO root Epoch 0 Batch 500 Speed 1142 68 samples sec accuracy 0 095000 INFO root Epoch 0 Batch 500 Speed 1207 60 samples sec accuracy 0 098750 INFO root Epoch 0 Batch 600 Speed 1057 95 samples sec accuracy 0 095156 INFO root Epoch 0 Batch 600 Speed 1058 24 samples sec accuracy 0 098281 INFO root Epoch 0 Batch 700 Speed 1047 78 samples sec accuracy 0 102656 INFO root Epoch 0 Batch 700 Speed 974 27 samples sec accuracy 0 099219 Minimum reproducible example Using the original example train mnist Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Compile make j5 USE OPENCV 0 USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 USE DIST KVSTORE 1 2 hosts 127 0 0 1 3 non distributed python train mnist py 4 distributed with single process python tools launch py n 1 launcher ssh H hosts python train mnist py kv store dist sync 5 distributed with 2 processes python tools launch py n 2 launcher ssh H hosts python train mnist py kv store dist sync What have you tried to solve it Not yet However the version 0 11 0 rc3 does not have this issue,,"sxjscience,sxjscience,piiswrong,sxjscience,szha",2017-09-04 18:35:25,2017-09-17 02:00:52
IS,Where is the mxnet ndarray RNN function is implementation code,I want to see below function is python code Does anyone know where is it mxnet ndarray RNN data None parameters None state None state cell None state size Null num layers Null bidirectional Null mode Null p Null state outputs Null out None name None kwargs The output of this function is NDArray or list of NDArrays which notes on mxnet is official web set I want to know which form the list of NDArrays constructed when the mode is 'lstm' Is it in the form of 'h h c ',,"szha,szha,szha,szha,szha,szha,szha",2017-09-13 02:35:05,2017-09-17 02:01:41
IS,threaded engine error when running distributed training program,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System I have no idea how to fix it give me some idea,,"szha,szha,szha",2017-09-11 01:28:22,2017-09-17 02:03:06
IS,label width does not work as expected,I made all images with 256 boxes but label width 350 still not work and suggests me to set over 1500 would like to know if this needs to be considered with batch size as well or GPU number For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version 0 11 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-09-15 02:16:32,2017-09-17 02:08:50
IS,DOC Stale documentations and doc versions,Many sections in the documentation are either 1 stale e g operators in mxnet or 2 provide deprecated broken examples For 1 I suggest that we maintain versioned documentations with release tags Even a simple last updated timestamp would help For 2 we need broader testing coverage 5321,,eric-haibin-lin,2017-03-18 06:52:32,2017-09-17 03:04:23
IS,PROPOSAL SparseNDArray Python API,Overview End to end design overview available at This issue is opened to gather comments regarding priority of sparse operators to support Data Types We focus on two sparse formats csr and row sparse Both symbolic and imperative execution will be supported Implementation on CPU has higher priority than GPU Implicit Sparse Dense Format Conversion Since not all operators are re implemented with sparse input sometimes a sparse tensor is converted to other format if needed Specifically If an operator does not handle sparse input then sparse inputs are converted to dense ones automatically on runtime By default an operator refuses to handle inputs with multiple sparsity formats In the future we might consider convert them to a same format chosen by the operator Warnings will be thrown so that users are aware of such conversion Python API Array Creation Routines mx sparse nd csr mat dense mat ctx None dtype None Creates a CSR matrix from a dense matrix mx sparse nd csr mat data indices indptr shape ctx None dtype None Creates a CSR matrix with standard input mx sparse nd sum mx sparse nd add mx sparse nd subtract mx sparse nd multiply mx sparse nd divide mx sparse nd negative Symbol mx symbol Variable Variable with attr isparse type' will be considered as sparse input mx symbol Embedding Embedding layer with sparse input mx symbol dot mx symbol sum mx symbol broadcast add mx symbol broadcast sub mx symbol broadcast mul mx symbol broadcast div mx symbol negative Array Manipulation Routines SparseNDArray transpose Permute the dimensions of an array Miscellaneous Functions SparseNDArray check format Check whether the matrix format is valid Serialization mx sparse nd load mx sparse nd save KVStore and Distributed Training User is not expected to change kv store code to have sparse gradient updates Once the engine detects sparse gradients their parameters are updated using sparse gradient accordingly Any comment on introducing a new SparseNDArray front end class,,"eric-haibin-lin,eric-haibin-lin,tqchen",2017-04-06 03:06:02,2017-09-17 03:04:41
IS,Worker threads fail to join,The code snippet below hangs It prints done but does not exit program MXNet version 0f6b6b3d98d06dbbc7748086b05b149277773839 Env AWS Ubuntu DL AMI,,"eric-haibin-lin,cjolivier01",2017-04-06 23:55:02,2017-09-17 03:04:59
IS,Missing gen data py in bi lstm sort example,xlvector the gen data py file mentioned in README is missing in the bi lstm sort example,,"eric-haibin-lin,gurumurthys",2017-03-01 22:38:39,2017-09-17 03:06:16
PR,Small improvement to IndexTensorToVector,This fixes a bug I encountered with the python grad tester numeric grad whilst writing tests for SequenceLast op In this eps is added and subtracted from all input arrays For sequence layers which have the lengths of the sequences this causes problems 1 0 0005 0 9995 and static cast int 0 9995 is 0 which causes seg faults I simply changed static cast to std round in IndexTensorToVector which is much more robust to small numerical differences and which allows my tests to pass will add in another PR with these new tests once this is merged It might still be good to fix numeric grad so that it has an option to ignore arrays for which no grad can be calculated,,"sbodenstein,sbodenstein,piiswrong",2017-09-16 22:36:08,2017-09-17 18:18:46
PR,Remove unnecessary sequence layer size restriction,The sequence layers SequenceLast SequenceReverse and SequenceMask unnecessarily banned rank 2 inputs ie sequences of scalars This lifts that restriction Note there are no tests for SequenceLast due to failures caused by this 7919,,sbodenstein,2017-09-17 10:31:33,2017-09-17 18:19:15
PR,add warning to global norm clip,,,"szha,piiswrong",2017-09-14 20:27:08,2017-09-17 18:21:36
PR,add mobilenet to gluon model zoo,zhreshold,,"szha,zhreshold,zhreshold,piiswrong,zhreshold,zhreshold,piiswrong,zhreshold",2017-09-13 06:47:03,2017-09-17 18:27:58
PR,cuda support for new linear algebra operators,Add cuda support for syrk gelqf linear algebra operators which was missing so far Also fixes an issue in linalg sumlogdiag that can lead to NAN values generated during backward pass,,"asmushetzel,piiswrong,asmushetzel,asmushetzel,asmushetzel,asmushetzel",2017-09-13 21:41:09,2017-09-18 04:57:01
PR,add advanced indexing,,,"piiswrong,yajiedesign,piiswrong,szha",2017-09-15 22:03:24,2017-09-18 05:02:29
PR,executor not expose its inner variables to symbol,,,formath,2017-09-18 08:06:50,2017-09-18 10:45:33
PR,Increase the tolerance,piiswrong,,"gautamkmr,szha,cjolivier01,cjolivier01,gautamkmr,cjolivier01,gautamkmr,cjolivier01,gautamkmr,gautamkmr,gautamkmr",2017-09-17 23:39:12,2017-09-18 18:31:41
PR,Gluon InstanceNorm and ReflectancePadding,InstanceNorm and ReflectancePadding are important for generative models and style transfer,,"zhanghang1989,szha,szha,zhanghang1989,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,zhanghang1989,szha,piiswrong,zhanghang1989,piiswrong,zhanghang1989,szha,zhanghang1989,zhanghang1989",2017-08-23 03:12:23,2017-09-18 19:49:07
IS,Resize image to fixed size,I would like to resize image to a fixed size Currently I am able to find method like mxnet image resize short mxnet image resize short which resizes image maintaining the aspect ratio but I would like to resize image removing the aspect ratio Like I wish to resize image from 700 1000 to 224 224 Is there any method which does in MXNet Thanks in advance for the help,,"Prasad9,ZiyueHuang,Prasad9,ZiyueHuang,Prasad9",2017-09-17 17:58:00,2017-09-19 09:42:40
IS,get stuck training on multiple machines error,I try to run mxnet on 2 machines use command under the example image classification directory export DMLC INTERFACE interface name export DMLC PS ROOT PORT 12121 export DMLC PS ROOT URI '11 11 3 6' python tools launch py n 2 H hosts sync dst dir tmp mxnet python train mnist py network lenet kv store dist sync the contents of hosts file 11 11 3 10 11 11 3 6 and when I ran htop i did find it run on both two nodes but it only print 2017 09 17 07 46 42 379 INFO rsync home uniquesc rdma incubator mxnet example image classification 11 11 3 10 tmp mxnet 2017 09 17 07 46 43 837 INFO rsync home uniquesc rdma incubator mxnet example image classification 11 11 3 6 tmp mxnet and did not download the data of train set just stuck without going on I compile mxnet with the commad make j10 USE DIST KVSTORE 1 by adding some imformation to print in the souce code of mxnet and ran tcpdump in both machines I just found that they keep sending packets to each other again and again the tcpdump print like this 19 37 21 818446 IP node336 ssh node332 40246 Flags P seq 541692 5418 88 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 196 19 37 21 818477 IP node332 40246 node336 ssh Flags ack 541692 win 1366 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818493 IP node336 ssh node332 40246 Flags P seq 541888 5424 80 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 592 19 37 21 818526 IP node332 40246 node336 ssh Flags ack 541888 win 1365 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818557 IP node336 ssh node332 40246 Flags P seq 542480 5426 60 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 180 19 37 21 818574 IP node332 40246 node336 ssh Flags ack 542480 win 1361 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818584 IP node336 ssh node332 40246 Flags P seq 542660 542 56 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 196 19 37 21 818605 IP node336 ssh node332 40246 Flags P seq 542856 543 36 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 180 19 37 21 818623 IP node332 40246 node336 ssh Flags ack 542660 win 1360 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818632 IP node336 ssh node332 40246 Flags P seq 543036 543 12 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 376 19 37 21 818634 IP node332 40246 node336 ssh Flags ack 542856 win 1359 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818678 IP node332 40246 node336 ssh Flags ack 543036 win 1358 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818682 IP node336 ssh node332 40246 Flags P seq 543412 543 08 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 196 19 37 21 818683 IP node332 40246 node336 ssh Flags ack 543412 win 1356 options nop nop TS val 2856082090 ecr 2856082226 length 0 19 37 21 818702 IP node336 ssh node332 40246 Flags P seq 543608 543 04 ack 289 win 325 options nop nop TS val 2856082226 ecr 2856082090 l ength 196 how can I solve the problem,,,2017-09-17 12:15:31,2017-09-19 14:02:17
IS,How to load params with LoadToMap c api and set device contect to GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 Compiler VS2015 Package used Python R Scala Julia MXNet version 0 905 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message The loaded NDArrays in the map from LoadToMap do not have the device context 'GPU' Minimum reproducible example I use the following code to read params from file m params map NDArray LoadToMap fileNamePars m args map clear m auxs map clear int pos 0 for auto it m params map begin it m params map end it const string key it first NDArray val it second if pos key find arg string npos m args map key substr 4 val if pos key find aux string npos m auxs map key substr 4 val Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,,2017-09-12 20:33:03,2017-09-19 18:41:16
PR,fix batchnorm test failure,channelAxis can be negative as line 1369 shows us,,"rahul003,reminisce,rahul003,reminisce",2017-09-14 02:26:42,2017-09-19 18:50:21
PR,Perl reduce buffer copies in NDArray PDL conversions,A bit of profiling and research lead me to try the performance test suggested in 2739 for Perl The performance test code gave me GB s 0 851562 on master after looking into the code a bit this patch brings the performance test to GB s 8 35156 due to reduced buffer copying overhead this example API Creating a piddle manually from Perl from the PDL docs uses the same pattern to load directly into the piddle without an intermediate buffer so I think it is safe,,"tlby,sergeykolychev,tlby,sergeykolychev,sergeykolychev",2017-09-08 20:21:16,2017-09-19 23:24:45
PR,Update README md,,,szha,2017-09-18 22:35:46,2017-09-19 23:28:02
PR,Use nn interface for the activation operator,This commit only contains the basic implementation of the activation operator It has been tested on CPU and GPU But it does not contain the cuDNN version of activation yet This pull request is mainly for testing,,"zheng-da,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,zheng-da",2017-09-19 20:15:53,2017-09-20 01:01:06
PR,executor not expose its inner variables to symbol,,,formath,2017-09-18 11:01:29,2017-09-20 05:01:07
PR,simplify CTC forward after namespace reorganize,,,szha,2017-09-18 17:48:12,2017-09-20 07:14:36
PR,executor not expose its inner variables to symbol,,,formath,2017-09-20 05:22:38,2017-09-20 09:36:10
IS,can not perform operations between mx sym var and mx nd array,Environment info Operating System Windows 10 Package used Python R Scala Julia Python 3 6 1 in Anaconda 4 4 0 MXNet version 0 11 Error Message I want to use low level API of mxnet But I found that there is no operation works between mx sym var and mx nd array For example I write two class firstly,,kevinthesun,2017-09-18 03:36:11,2017-09-20 15:25:16
PR,R package RNN refactor,New to pull with corrected submodules Related to 5488 this is to provide a more flexible and efficient framework to run RNN within the R package This pull does brake the current RNN API better to renamed files function to maintain legacy support Built around mx symbol RNN Support bucketing and masking through mx io bucket iter R Support multi device training single instance inference Only support seq to one at the moment but inference on one to one should be added shortly Requires to run with CUDA until mx symbol RNN gets a CPU support End to end tutorial can be seen here Will be finalized in a blog post when one to one inference suport is completed,,"jeremiedb,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,thirdwing,jeremiedb,jeremiedb,piiswrong,thirdwing,jeremiedb,thirdwing,jeremiedb,thirdwing,thirdwing,thirdwing,jeremiedb",2017-08-15 14:24:28,2017-09-20 17:07:43
PR,DO NOT MERGE Just a test of adding properties to JenkinsFile,Please DO NOT MERGE,,"mbaijal,mbaijal",2017-09-19 01:43:11,2017-09-20 18:02:11
PR,Fix computation of num inputs for Python API create operator entry,This solves the following problem in MXNet v 0 11 x not present in v 0 10 0 If a user creates a CustomOperator in Python that requires no arguments but has at least one output the create operator entry L754 callback from C receives a wrong num input argument and then fails with the following error but runs fine with this PR We have a need for such operators in our Sockeye project and would like to update to MXNet v 0 11 0 Hence it would be great one of the core maintainers could take a look at this and confirm that this change does not break anything else or if there is a better way of handling custom operators without input arguments It would also be great to backport this to the v 0 11 0 release for us to update to the stable version,,"fhieber,fhieber,piiswrong",2017-09-20 14:57:52,2017-09-20 19:55:20
PR,Default shape and context attributes for operators,This PR resolves the issues reported by as the following 1 Imperative random operators such as mx nd random uniform results in segmentation fault when used without user input shapes That is because the shape attribute defaults to an empty shape if it is not provided by users But in Numpy such statement np random uniform can run through and prints a single value In order to keep consistent with Numpy is behavior we need to default shape 1 if it is not provided by users for random operators through either shape or out attribute imperatively For symbolic random operators default shape is still an empty one since it might need to be inferred from other symbolic variables 2 For the imperative operators with ctx as one of its attributes it should default to the context current context instead of mx cpu 0 if users did not specify through either ctx or out attribute so that the following use case can be supported haibin lin,,"reminisce,piiswrong,piiswrong,mli,reminisce,zhreshold,reminisce",2017-09-13 05:06:39,2017-09-20 19:55:51
PR,Increase the tolerance,piiswrong,,gautamkmr,2017-09-18 18:36:50,2017-09-20 20:04:10
PR,Add Kaggle w gluon pipeline and k fold cross validation,Kagglers might use gluon,,astonzhang,2017-09-16 07:40:22,2017-09-20 20:06:07
PR,update doc icon,,,szha,2017-09-20 20:47:27,2017-09-20 20:59:07
PR,executor not expose its inner variables to symbol,,,"formath,piiswrong,formath,piiswrong",2017-09-20 10:58:01,2017-09-21 04:49:01
IS,Perl AI MXNet,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System CentOS 7 3 10 0 693 2 2 el7 x86 64 1 SMP Compiler gcc version 4 8 5 20150623 Red Hat 4 8 5 16 GCC perl 5 16 3 Package used Python R Scala Julia mxnet perl package AI MXNetCAPI MXNet version Or if installed from source git branch 0 11 0 MXNet commit hash git rev parse HEAD a5edbf94094581ee27157eae4f2113115a3994e7 Error Message Please paste the full error message including stack trace usr bin perl MExtUtils Command MM e 'cp nonempty' MXNetCAPI bs blib arch auto AI MXNetCAPI MXNetCAPI bs 644 gcc c I include mxnet D REENTRANT D GNU SOURCE fno strict aliasing pipe fstack protector I usr local include D LARGEFILE SOURCE D FILE OFFSET BITS 64 O2 g pipe Wall Wp D FORTIFY SOURCE 2 fexceptions fstack protector strong param ssp buffer size 4 grecord gcc switches m64 mtune generic DVERSION 1 0102 DXS VERSION 1 0102 fPIC I usr lib64 perl5 CORE mxnet wrap cxx mxnet wrap cxx In function void wrap NDArrayCreate PerlInterpreter CV mxnet wrap cxx 4194 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap NDArrayCreateEx PerlInterpreter CV mxnet wrap cxx 4299 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap NDArraySave PerlInterpreter CV mxnet wrap cxx 4502 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 4530 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx In function void wrap NDArrayLoad PerlInterpreter CV mxnet wrap cxx 4612 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 i mxnet wrap cxx 4629 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg4 i mxnet wrap cxx In function void wrap NDArrayReshape PerlInterpreter CV mxnet wrap cxx 4994 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap NDArrayGetShape PerlInterpreter CV mxnet wrap cxx 5070 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 i mxnet wrap cxx In function void wrap ListFunctions PerlInterpreter CV mxnet wrap cxx 5422 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg1 i mxnet wrap cxx In function void wrap FuncGetInfo PerlInterpreter CV mxnet wrap cxx 5533 28 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg4 i mxnet wrap cxx In function void wrap FuncInvoke PerlInterpreter CV mxnet wrap cxx 5676 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 5702 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 5727 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap FuncInvokeEx PerlInterpreter CV mxnet wrap cxx 5809 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 5835 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 5860 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap ImperativeInvoke PerlInterpreter CV mxnet wrap cxx 5998 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6026 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6085 39 error av top index was not declared in this scope if av top index AV SvRV ST 3 1 mxnet wrap cxx 6083 12 warning unused variable svs Wunused variable SV svs mxnet wrap cxx 6105 39 error av top index was not declared in this scope if av top index AV SvRV ST 3 1 mxnet wrap cxx 6123 39 error av top index was not declared in this scope if av top index AV SvRV ST 3 1 mxnet wrap cxx In function void wrap AutogradMarkVariables PerlInterpreter CV mxnet wrap cxx 6213 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6240 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6265 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap AutogradComputeGradient PerlInterpreter CV mxnet wrap cxx 6340 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap AutogradBackward PerlInterpreter CV mxnet wrap cxx 6407 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6435 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap CreateCachedOp PerlInterpreter CV mxnet wrap cxx 6489 20 warning unused variable temp20 Wunused variable CachedOpHandle temp20 mxnet wrap cxx In function void wrap InvokeCachedOp PerlInterpreter CV mxnet wrap cxx 6594 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6622 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 6645 39 error av top index was not declared in this scope if av top index AV SvRV ST 3 1 mxnet wrap cxx 6643 12 warning unused variable svs Wunused variable SV svs mxnet wrap cxx 6665 39 error av top index was not declared in this scope if av top index AV SvRV ST 3 1 mxnet wrap cxx 6678 39 error av top index was not declared in this scope if av top index AV SvRV ST 3 1 mxnet wrap cxx In function void wrap ListAllOpNames PerlInterpreter CV mxnet wrap cxx 6714 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg1 i mxnet wrap cxx In function void wrap SymbolListAtomicSymbolCreators PerlInterpreter CV mxnet wrap cxx 6758 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg1 i mxnet wrap cxx In function void wrap SymbolGetAtomicSymbolInfo PerlInterpreter CV mxnet wrap cxx 6872 28 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg4 i mxnet wrap cxx In function void wrap SymbolCreateGroup PerlInterpreter CV mxnet wrap cxx 7063 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap SymbolListAttr PerlInterpreter CV mxnet wrap cxx 7588 31 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 2 i mxnet wrap cxx In function void wrap SymbolListAttrShallow PerlInterpreter CV mxnet wrap cxx 7642 31 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 2 i mxnet wrap cxx In function void wrap SymbolListArguments PerlInterpreter CV mxnet wrap cxx 7696 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 i mxnet wrap cxx In function void wrap SymbolListOutputs PerlInterpreter CV mxnet wrap cxx 7750 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 i mxnet wrap cxx In function void wrap SymbolListAuxiliaryStates PerlInterpreter CV mxnet wrap cxx 7937 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 i mxnet wrap cxx In function void wrap SymbolCompose PerlInterpreter CV mxnet wrap cxx 8004 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 8029 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap SymbolGrad PerlInterpreter CV mxnet wrap cxx 8115 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx In function void wrap SymbolInferShape PerlInterpreter CV mxnet wrap cxx 8237 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 8261 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 8285 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 8308 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg6 i mxnet wrap cxx 8330 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg9 i mxnet wrap cxx 8352 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg12 i mxnet wrap cxx In function void wrap SymbolInferShapePartial PerlInterpreter CV mxnet wrap cxx 8487 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 8511 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 8535 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 8558 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg6 i mxnet wrap cxx 8580 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg9 i mxnet wrap cxx 8602 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg12 i mxnet wrap cxx In function void wrap SymbolInferType PerlInterpreter CV mxnet wrap cxx 8727 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 8751 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 8774 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg5 i mxnet wrap cxx 8789 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg7 i mxnet wrap cxx 8804 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg9 i mxnet wrap cxx In function void wrap ExecutorBackward PerlInterpreter CV mxnet wrap cxx 8993 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap ExecutorOutputs PerlInterpreter CV mxnet wrap cxx 9063 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg2 i mxnet wrap cxx In function void wrap ExecutorBind PerlInterpreter CV mxnet wrap cxx 9146 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9174 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9201 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9231 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap ExecutorBindX PerlInterpreter CV mxnet wrap cxx 9369 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 9393 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9418 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9449 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9477 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9504 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9534 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap ExecutorBindEX PerlInterpreter CV mxnet wrap cxx 9695 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 9719 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9744 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9775 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9803 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9830 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 9860 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap ExecutorSimpleBind PerlInterpreter CV mxnet wrap cxx 10070 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 10094 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 10119 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 10150 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 10175 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 10205 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 10229 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 10253 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 10283 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 10307 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 10338 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 10359 11 warning unused variable hash len Wunused variable int hash len mxnet wrap cxx 10424 30 warning comparison between signed and unsigned integer expressions Wsign compare for int i 0 i arg25 i mxnet wrap cxx 10441 30 warning comparison between signed and unsigned integer expressions Wsign compare for int i 0 i arg28 i mxnet wrap cxx In function void wrap ListDataIters PerlInterpreter CV mxnet wrap cxx 10623 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg1 i mxnet wrap cxx In function void wrap DataIterGetIterInfo PerlInterpreter CV mxnet wrap cxx 10784 28 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg4 i mxnet wrap cxx In function void wrap DataIterGetIndex PerlInterpreter CV mxnet wrap cxx 10982 26 warning comparison between signed and unsigned integer expressions Wsign compare for i 0 i arg3 i mxnet wrap cxx In function void wrap KVStoreInitEx PerlInterpreter CV mxnet wrap cxx 11264 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 11289 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap KVStorePushEx PerlInterpreter CV mxnet wrap cxx 11372 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 11397 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap KVStorePullEx PerlInterpreter CV mxnet wrap cxx 11487 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 11512 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx In function void wrap RtcCreate PerlInterpreter CV mxnet wrap cxx 12573 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 12598 32 error av top index was not declared in this scope len av top index tempav 1 mxnet wrap cxx 12623 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 12651 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 12536 15 warning unused variable temp90 Wunused variable RtcHandle temp90 mxnet wrap cxx In function void wrap RtcPush PerlInterpreter CV mxnet wrap cxx 12786 35 error av top index was not declared in this scope av len av top index tempav 1 mxnet wrap cxx 12814 35 error av top index was not declared in this scope av len av top index tempav 1 In file included from mxnet wrap cxx 745 0 mxnet wrap cxx In function void boot AI MXNetCAPI PerlInterpreter CV usr lib64 perl5 CORE XSUB h 164 20 warning unused variable items Wunused variable define dITEMS I32 items I32 SP MARK usr lib64 perl5 CORE XSUB h 172 16 note in expansion of macro dITEMS dSP dAXMARK dITEMS mxnet wrap cxx 13641 3 note in expansion of macro dXSARGS dXSARGS make mxnet wrap o Error 1 Steps to reproduce Just make AI MXNetCAPI as here What have you tried to solve it Rechecked installed dependencies,,sergeykolychev,2017-09-20 11:50:41,2017-09-21 06:22:22
PR,gluon more informative error message for deferred init,,,szha,2017-09-21 05:45:51,2017-09-21 18:43:29
PR,Fix computation of num inputs for Python API create operator entry fo,r custom operators with 0 arguments This is the same change as in 7967 but against v0 11 0 branch Will this be built to Pypi package index automatically,,"fhieber,szha,fhieber,fhieber,szha,fhieber,szha,fhieber,tdomhan,mbaijal,tdomhan",2017-09-21 06:42:49,2017-09-21 18:45:00
PR,Changed How To String to FAQ and added Discuss Tab to the Navigation bar,Provide a Discuss Navigation tabbed link on the top navigation bar Changed How To to FAQ,,"thinksanky,kevinthesun,kevinthesun,thinksanky,kevinthesun,kevinthesun",2017-09-19 23:15:09,2017-09-21 19:15:12
PR,Faq discuss branch,Changed Discuss URL to,,thinksanky,2017-09-21 20:05:07,2017-09-22 01:22:56
IS,CTC ERROR WITH CUDA ILLEGAL MEMORY ACCESS ERROR,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu14 04 Compiler install by pip Package used Python R Scala Julia python MXNet version 0 11 Error Message Please paste the full error message including stack trace Thank you very mush if any one can solve my problem,,"piiswrong,szha,szha,szha,szha,szha,szha",2017-09-02 13:52:22,2017-09-22 02:21:56
PR,suppress warnings for sparse tests,The warnings are generated by numpy when dividing by zeros taking the log of zeros Added a filter to suppress these,,eric-haibin-lin,2017-09-21 00:05:51,2017-09-22 04:28:14
PR,account for batch padding when updating metrics,Fixes executor group update metric to take account of batch padding If a batch has padding only the non padded data is used to update the metric A unit test is included which demonstrates the fix,,"piiswrong,jmerkow,jmerkow,piiswrong,fhieber,piiswrong,jmerkow",2017-09-19 22:35:21,2017-09-22 04:53:28
IS,I do not see any seq len was defined in the example of lstm,Was it inputed though the bucket key in the BucketSentenceIter,,"szha,szha",2017-09-22 02:56:01,2017-09-22 05:26:55
PR,Update vision py,fix typo 7986,,"szha,sergeykolychev",2017-09-22 03:24:06,2017-09-22 17:52:02
PR,Second order gradient and Subgraph execution,,,"piiswrong,eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,szha,szha,szha,piiswrong,tqchen,piiswrong,tqchen,eric-haibin-lin,eric-haibin-lin,piiswrong,jermainewang,piiswrong,jermainewang,sergeykolychev,piiswrong",2017-09-01 03:47:39,2017-09-22 22:06:50
PR,sequential multiple add hybrid warning,,,"szha,piiswrong,szha,szha,piiswrong,mli,szha,szha",2017-09-13 08:42:57,2017-09-22 22:26:44
PR,add math fluent functions,add fluent methods for math functions,,"szha,piiswrong,szha",2017-09-22 05:29:12,2017-09-22 22:31:17
PR,gluon conv rnns,sxjscience,,"szha,sxjscience,sxjscience,szha,piiswrong,szha,piiswrong,piiswrong,piiswrong,szha,szha,piiswrong,szha,piiswrong,sxjscience,sxjscience,szha,szha,szha,dsqx71,szha,szha,szha,szha,piiswrong,adstraw,szha,mbaijal,szha,mbaijal,szha",2017-07-31 08:55:31,2017-09-22 22:35:05
PR,allow users to specify repo url for downloadable contents,,,"szha,piiswrong,szha,szha,piiswrong,piiswrong,piiswrong,szha,szha,piiswrong,szha,szha,piiswrong,piiswrong,szha,piiswrong,szha,mli,szha,piiswrong,szha",2017-09-14 01:09:57,2017-09-22 22:39:32
IS,mx metric EvalMetric bug,My ClsMetric can run tomorrow but cannot run on the new version of today My last layer is a group of five predictions net mx symbol Group mx sym BlockGrad pred cls name 'test' mx sym BlockGrad clsmap name 'clsmap' mx sym BlockGrad smooth l1 name 'l1' det loss huber loss CODE,,"chinakook,piiswrong,chinakook",2017-09-23 05:42:01,2017-09-23 07:11:15
IS,Gradient accumulation of several sample,I search to compute a triplet ranking loss To achieve this I need to accumulate gradient through several example and update weight with the resulting gradient In chainer we can easily do that because backward function accumulate gradient by default We need to clear the gradients first because the backward method accumulates gradients instead of overwriting the previous values Is there a way to do this with Mxnet Gluon and the autograd API That similar to gradient accumulation inside a batch,,"ZiyueHuang,ZiyueHuang,ZiyueHuang,ZiyueHuang",2017-09-20 11:34:33,2017-09-23 13:34:40
PR,Update executor group py,,,piiswrong,2017-09-23 05:55:47,2017-09-23 18:36:34
PR,Update conv rnn cell py,,,szha,2017-09-23 05:53:15,2017-09-23 18:58:21
IS,I found it so slow on train compared with paddle or tensorflow,,,,2017-01-19 06:39:28,2017-09-23 19:02:44
IS,who can give me a docker container with a ssh sever so i can train my model with distributed computation,,,,2017-01-24 08:23:00,2017-09-23 19:02:54
IS,Is ndarray api designed for users construct networks to predict and symbol for training,,,"szha,szha,szha",2017-09-23 01:26:05,2017-09-23 19:25:44
PR,Fix faq url branch,Fixed the broken URLs for FAQ,,thinksanky,2017-09-24 01:06:22,2017-09-24 04:45:09
PR,fix elemwise sum test script,sxjscience I was going to fix it in 7947 but since that PR is not ready I am making this separate PR to fix it I have no idea why the CI did not catch it The root cause of the problem is that 7577 changed identity attr like rhs op implementation and did not check storage type inside the operator These problems will be fixed in 7947 soon,,eric-haibin-lin,2017-09-23 22:43:00,2017-09-24 04:46:37
PR,user friendly image loading,,,"zhreshold,piiswrong,piiswrong,piiswrong",2017-09-21 02:36:22,2017-09-24 04:55:11
PR,Revert Many loss functions 7605,This reverts commit 9d56db66e2e94a8a3d9bf020b9682e91e7baf203 revert before names comments tests are fixed,,"piiswrong,szha,szha,piiswrong,szha,piiswrong,szha,piiswrong,thatindiandude,szha,piiswrong",2017-09-24 05:00:29,2017-09-24 06:32:26
PR,add Loss suffix to losses,,,"szha,piiswrong,szha",2017-09-23 22:12:46,2017-09-24 06:32:42
PR,WIP doc update,This is to add update reorganize doc for Gluon and for x 7875 mobilenet api doc x 7264 conv rnn doc and new contrib package x 7067 var dropout doc x 7992 new fluent methods x missed doc in 7910 x missed doc in 7403 x dataset doc x model zoo doc x model zoo doc fix mx gluon model zoo vision instead of mx gluon models 7023 x move the legacy rnn under symbol since it only supports symbol x loss doc fix for 7918 fix broken links array creation routines in NDArray API x executor doc page,,"szha,eric-haibin-lin",2017-09-16 07:38:36,2017-09-24 06:32:56
PR,Add an argument to omit row sparse push for end to end benchmarking,eric haibin lin,,"anirudh2290,szha,szha,anirudh2290,anirudh2290,anirudh2290",2017-09-08 00:05:08,2017-09-24 06:53:28
IS,Ask MXNet NDArray vs MXNet Minpy,I'm not sure to understand the goal difference between both API As I understand there are both an imperative way to do Deep learning but Minpy use an Numpy like interface and NDArray use a more MXNet like interface,,,2017-07-21 06:35:55,2017-09-24 17:28:47
IS,mxnet predict so read the symbol json error,Environment info Operating System Ubuntu 16 04 Compiler arm linux androideabi clang arch arm api 21 i met a problem when i tried to use the latest mxnet predcit so in android the latest predict so can not load the symbol rightly i upload my predict so and the model json in my attachments symbol json zip the mxnet predict so can not upload successfully so i upload to BaiduYun any help is appreciate,,"szha,arank,arank",2017-09-15 08:52:26,2017-09-25 02:21:02
PR,fix cached op,eric haibin lin,,piiswrong,2017-09-25 18:36:47,2017-09-25 21:48:32
PR,Revert account for batch padding when updating metrics 7949,PR 7949 has introduced a number of problems and the logic looks wrong on multiple devices Reverting for now,,piiswrong,2017-09-25 18:52:39,2017-09-25 23:19:40
PR,Fix Issue 7751 MXNet 0 11 0 Release Feedback NEWS md and CONTRIBUTORS md,nswamy krishnamurthy For details check github issue here or Internal SIM MX 412 CONTRIBUTORS md I have changed the name from 'DMLC MXNet' to ' Apache MXNet incubating ' Should it be ' apache incubator mxnet ' instead in keeping with the original format NEWS md This will have to change with each release,,mbaijal,2017-09-26 01:30:11,2017-09-26 03:44:54
PR,Adam optimizer consistent with paper,,,"formath,piiswrong,formath,sergeykolychev,formath,piiswrong,ZiyueHuang,formath,sxjscience,formath,sxjscience,formath,sxjscience,piiswrong,formath",2017-09-19 09:09:13,2017-09-26 04:27:11
PR,MXCallbackList is a struct,C compiler complains about MXCallbackList This PR fix this tiny error,,"pierric,piiswrong",2017-09-25 14:19:53,2017-09-26 04:31:45
PR,fix bgr channel error,when load color image the cv mat is bgr b channel is first then g and r,,junluan,2017-09-26 05:14:48,2017-09-26 05:45:10
PR,Issue 7748 MXNet 0 11 0 Release Feedback NOTICE file,gautamkmr As discussed in this issue,,mbaijal,2017-09-26 19:04:55,2017-09-26 19:08:36
PR,Add cuda rtc module,yzhang87,,"piiswrong,piiswrong,sergeykolychev,piiswrong,sergeykolychev,sergeykolychev,sergeykolychev,piiswrong,sergeykolychev,piiswrong,sergeykolychev,gautamkmr,sergeykolychev",2017-09-25 05:17:14,2017-09-26 19:10:27
PR,Issue 7748 Update the Copyright years in NOTICE file,gautamkmr Fixing Issue 7748 As discussed on slack channel builds,,mbaijal,2017-09-26 20:10:14,2017-09-26 20:39:04
PR,Random refactor,reminisce,,"piiswrong,asmushetzel,reminisce,piiswrong",2017-09-18 22:31:26,2017-09-26 20:39:24
PR,Fix a mistake in example ssd demo py,If the default arguments are used prefix will be ssd resnet50 5 This seems a mistake The parsed data shape should be used but args data shape is used instead So I fixed it,,"kkk669,piiswrong,zhreshold",2017-09-25 07:46:20,2017-09-26 20:42:25
PR,fix example,fixed multi task learning example which was not running removed multiple copies of mnist iterator and moved away from wget unzip for portability,,"szha,zhreshold,zhreshold,szha,szha,szha",2017-09-23 20:45:11,2017-09-26 22:18:35
IS,The Mulit task learning example can not run,AttributeError 'Multi Accuracy' object has no attribute 'num' Then I change L79 to,,"kli-nlpr,szha,szha,szha",2017-09-22 06:19:48,2017-09-26 22:19:56
PR,Kill running PR builds when a new build is triggered for the same PR,nswamy krishnamurthy This is Part 1 of a series of changes to the current MXNet PR build Process This change might lead to some issues on Apache Jenkins specially with requiring some script approvals as indicated here Security Plugin The code for this method has been picked as is from this stackoverflow post Expected Behavior 1 I have a added a new stage to our builds called 'Purge' other name suggestions are welcome 2 This stage runs on a 'mxnetlinux' slave at the moment This means it joins the queue waiting for an idle slave before it can kill an existing PR build If we want to avoid this it can be changed to be scheduled to the master 3 The method called 'abortPreviousRunningBuilds' simply scans all build numbers for the current PR and checks their status If it is not the current build but is still running the status of the build is changed to Aborted Testing Done I have reproduced a multibranch pipeline job on our internal jenkins similar to apache Tests have only been done on a feature branch and PRs The master branch should work similarly I have tested that 1 When a new branch is created the build goes through fine 2 If a new build is triggered on a branch say using 'Build Now' the purge stage works as expected This behavior might need to be changed specially for the master branch 3 When a new PR is created the build works fine 4 Any updates to the PR terminates the old build and a new one is created screen shot 2017 09 25 at 3 53 11 pm screen shot 2017 09 25 at 3 52 16 pm,,"mbaijal,nswamy,nswamy,mbaijal,mbaijal,gautamkmr,nswamy,nswamy,mbaijal,mbaijal,nswamy,mbaijal,mbaijal,mbaijal",2017-09-25 22:57:03,2017-09-26 23:10:57
IS,Float64 support for symbolic dot operator,The symbolic dot operator cannot handle float64 with the error message 15 52 00 src operator tensor matrix op inl h 349 Check failed outputs 0 type flag kFloat32 1 vs 0 dot only support 32 bit float so far Matrix multiplication is such a common functionality Is there any plans to add the support soon A simple example to recreate the situation,,"piiswrong,eric-haibin-lin",2017-04-11 15:55:53,2017-09-26 23:15:26
IS,Failing to build scalapkg with USE DIST KVSTORE 1 and USE OPENCV 1 on Amazon Linux RHEL,Environment info Operating System Amazon Linux ami a4c7edb2 Package used Python R Scala Julia Scala MXNet version 0 11 Error Message What have you tried to solve it The error is too generic and has to do with some out of bound index in a string If I build mxnet without the USE DIST KVSTORE 1 flag however building scalapkg succeeded without any issues,,"yzhliu,cjolivier01",2017-09-22 20:25:08,2017-09-27 02:34:46
IS,Prediction fails with reused module,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler GCC 5 4 0 Package used Python R Scala Julia Python MXNet version 0 10 1 Or if installed from source MXNet commit hash git rev parse HEAD 202de02cd If you are using python package please provide Python version and distribution Python 2 7 12 If you are using R package please provide R sessionInfo Error Message No error message The program hangs at the self mod predict it line on the second time execute is called Recreating the module every time will not cause it to hang but should be unnecessary Calling it in a loop over a collection of separate images will not cause it to hang Minimum reproducible example if you are using your own code please provide a short script that reproduces the error What have you tried to solve it Recreating the entire module object every time solves it but introduces about a second of computation time that should be unnecessary I have narrowed it down to a call to mod predict mod forward or mod bind force reset True,,,2017-07-12 10:49:53,2017-09-27 06:11:46
IS,I need to add a name prefix in resnet py,I found the symbol is name in resnet py are fixed for example conv0,,,2017-09-27 02:33:43,2017-09-27 07:01:03
PR,Fix bug in Parameter initialize method,Fix bug custom initialize function does not work Example class MyInit in,,,2017-09-27 09:45:02,2017-09-27 09:50:13
PR,Problem in acc metric,In 7561 label and pred label were convert to int32 in NDArray This may slowdown the speed in train I use train imagenet py in example to train some models in 4 gpus in single machine the batch size is 128 The speed was shown as below,,"solin319,mli",2017-09-22 13:37:37,2017-09-27 16:57:57
PR,fix symbolblock,,,"piiswrong,szha,piiswrong,szha,piiswrong",2017-09-27 01:18:29,2017-09-27 17:05:27
PR,Operator linalg syevd Symmetric eigendecomposition,There are some issues with float32 I would like to see what the test suite is doing If problems persist I will deactivate the new operator for float32,,"mseeger,piiswrong,mseeger,mseeger,mseeger,mseeger,asmushetzel,mseeger,mseeger,piiswrong,mseeger,eric-haibin-lin,asmushetzel,mseeger,asmushetzel,mseeger,asmushetzel",2017-09-20 14:24:43,2017-09-27 18:34:43
PR,add cnn highway network architecture for Chinese text classification example,Add new example based character level cnn for chinese text classification I have implemented the Highway Networks architecture The final train model is CNN Highway Network structure and this version can achieve a best dev accuracy of 94 75 with the Chinese corpus,,"szha,piiswrong",2017-09-25 17:06:08,2017-09-27 18:39:08
PR,Disable the test test batchnorm training,piiswrong haibin lin krishnamurthy,,"gautamkmr,piiswrong,gautamkmr,gautamkmr",2017-09-26 19:44:38,2017-09-27 20:23:57
PR,throw err message for wrong order of load param init optimizer in module,7847,,"eric-haibin-lin,piiswrong,szha,piiswrong",2017-09-27 05:44:55,2017-09-27 20:24:00
IS,MNIST data loader requires Python resources package to be installed,MNIST data loader fails unless resources package is manually installed Should resources package be a dependency installed along with mxnet Environment info Operating System Mac OS 10 12 6 Compiler Package used Python R Scala Julia Python 3 6 1 GCC 4 2 1 Compatible Apple LLVM 8 1 0 clang 802 0 42 MXNet version 0 11 0 Error Message Running code in Straight Dope book mx gluon data DataLoader fails to download MNIST data set unless python package resources is manually installed See,,"simoncorstonoliver,szha,szha",2017-09-20 23:30:37,2017-09-27 20:31:13
IS,Errors related to malloc and free,Hi I encounter these errors when training a network Error in usr bin python' malloc memory corruption fast 0x0000000001755880 Error in usr bin python' free invalid pointer 0x000000000171ec30 I am using the latest version of mxnet from engine branch Similar errors occur when I use mxnet from master branch Could anyone help Thank you very much Unfortunately I cannot get Python stack trace But C stack trace is available 0 0x00007ffff782dc37 in GI raise sig sig entry 6 at nptl sysdeps unix sysv linux raise c 56 1 0x00007ffff7831028 in GI abort at abort c 89 2 0x00007ffff786a2a4 in libc message do abort do abort entry 1 fmt fmt entry 0x7ffff79786b0 Error in s' s 0x s 32 1916 at sysdeps posix libc fatal c 175 3 0x00007ffff7874ff7 in malloc printerr action optimized out str 0x7ffff7978a50 malloc memory corruption fast ptr optimized out at malloc c 4996 4 0x00007ffff7877cf4 in int malloc av 0x7fff00000020 bytes 24 at malloc c 3359 5 0x00007ffff78796c0 in GI libc malloc bytes 24 at malloc c 2891 6 0x00007fffddad6dad in operator new unsigned long from usr lib x86 64 linux gnu libstdc so 6 7 0x00007fffebcab89d in std Function base Base manager mxnet NDArray Chunk Chunk lambda mxnet RunContext 2 M manager std Any data std Function base Base manager mxnet NDArray Chunk Chunk lambda mxnet RunContext 2 const std Manager operation from home gaiyu developping mxnet engine python mxnet lib libmxnet so 8 0x00007fffebcf715f in std Function base Base manager mxnet engine ThreadedEngine DeleteVariable std function void mxnet RunContext mxnet Context mxnet engine Var lambda mxnet RunContext 1 M manager std Any data std Function base Base manager mxnet engine ThreadedEngine DeleteVariable std function void mxnet RunContext mxnet Context mxnet engine Var lambda mxnet RunContext 1 const std Manager operation from home gaiyu developping mxnet engine python mxnet lib libmxnet so 9 0x00007fffebcaca74 in std function void mxnet RunContext function std function void mxnet RunContext const 16 1916 from home gaiyu developping mxnet engine python mxnet lib libmxnet so 10 0x00007fffebcf73b1 in mxnet engine ThreadedEngine DeleteVariable std function void mxnet RunContext mxnet Context mxnet engine Var from home gaiyu developping mxnet engine python mxnet lib libmxnet so 11 0x00007fffebcaba6d in std Sp counted ptr inplace mxnet NDArray Chunk std allocator mxnet NDArray Chunk gnu cxx Lock policy 2 M dispose from home gaiyu developping mxnet engine python mxnet lib libmxnet so 12 0x00007fffebcad78e in std vector mxnet NDArray std allocator mxnet NDArray vector from home gaiyu developping mxnet engine python mxnet lib libmxnet so 13 0x00007fffeb4c8eca in std Sp counted base gnu cxx Lock policy 2 M release from home gaiyu developping mxnet engine python mxnet lib libmxnet so 14 0x00007fffebd114b8 in std Function base Base manager mxnet exec GraphExecutor InitCachedOps lambda mxnet RunContext mxnet engine CallbackOnComplete 3 M manager std Any data std Function base Base manager mxnet exec GraphExecutor InitCachedOps lambda mxnet RunContext mxnet engine CallbackOnComplete 3 const std Manager operation from home gaiyu developping mxnet engine python mxnet lib libmxnet so 15 0x00007fffebcf82d6 in std Function handler void mxnet RunContext mxnet engine ThreadedEngine DeleteOperator mxnet engine Opr lambda mxnet RunContext 1 M invoke std Any data const mxnet RunContext from home gaiyu developping mxnet engine python mxnet lib libmxnet so 16 0x00007fffebcab693 in operator args 0 this optimized out at usr include c 4 8 functional 2471 17 operator on complete ctx closure optimized out at include mxnet engine h 213 18 std Function handler void mxnet RunContext mxnet engine CallbackOnComplete mxnet Engine PushSync std function void mxnet RunContext mxnet Context std vector mxnet engine Var std allocator mxnet engine Var const std vector mxnet engine Var std allocator mxnet engine Var const mxnet FnProperty int char const lambda mxnet RunContext mxnet engine CallbackOnComplete 1 M invoke std Any data const mxnet RunContext mxnet engine CallbackOnComplete functor args 0 args 1 at usr include c 4 8 functional 2071 19 0x00007fffebcfe06c in mxnet engine ThreadedEngine ExecuteOprBlock mxnet RunContext mxnet engine OprBlock from home gaiyu developping mxnet engine python mxnet lib libmxnet so 20 0x00007fffebd0097e in std Function handler void mxnet engine ThreadedEnginePerDevice PushToExecute mxnet engine OprBlock bool lambda 1 operator const lambda 1 M invoke std Any data const from home gaiyu developping mxnet engine python mxnet lib libmxnet so 21 0x00007fffddb29a60 in from usr lib x86 64 linux gnu libstdc so 6 22 0x00007ffff7bc4184 in start thread arg 0x7fff433fd700 at pthread create c 312 23 0x00007ffff78f137d in clone at sysdeps unix sysv linux x86 64 clone S 111,,"piiswrong,sifmelcara,sifmelcara,sifmelcara,sifmelcara,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,sifmelcara,sifmelcara,eric-haibin-lin,eric-haibin-lin,sifmelcara,eric-haibin-lin,sifmelcara,eric-haibin-lin,sifmelcara,eric-haibin-lin,sifmelcara,sifmelcara,sifmelcara,eric-haibin-lin",2017-04-07 14:48:25,2017-09-27 20:31:44
IS,SequenceLast of LSTM throws error,After using a LSTM one may just want the last layer throws an error How to get around this,,"szha,szha,szha,ZiyueHuang,szha",2017-09-16 14:20:57,2017-09-27 20:47:11
IS,Why library size so large,I found the libmxnet so is still very large about 80MB after I modified the Makefile options Then I checked the object files in build dir and got this default The operator dir occupies the most part of the total build size And some gpu o generated by nvcc is much larger Uploading gpu JPG,,"chinakook,chinakook,szha,szha,szha",2017-09-13 02:01:55,2017-09-27 20:48:29
IS,Segmentation fault for gluon ndarray random,Environment info Operating System Ubuntu 16 04 Compiler GCC 5 4 0 20160609 on linux Package used Python R Scala Julia Python MXNet version v0 11 Python version and distribution 3 5 2 Error Message Please paste the full error message including stack trace Segmentation fault core dumped Minimum reproducible example if you are using your own code please provide a short script that reproduces the error This also likely works for any random without mandatory args tested with uniform and exponential Do not think this is intended behavior,,"szha,szha,reminisce,szha",2017-09-14 13:13:01,2017-09-27 20:49:30
IS,How can I use UpSampling or add n function in gluon,I want to use mx sym UpSampling and mx symbol add n to build a network in gluon How can I use it The net I build are as follows,,"szha,szha",2017-09-06 02:44:41,2017-09-27 20:50:34
IS,Improve example ssd,Up to now there are several issues with example ssd I'm posting here to track the progress in improving this example in nnvm branch x Make sure the current example does converge This is confirmed Add test score py to allow automatic check for future commits Write a new lr scheduler because initial gradients are not stable since the current model vgg16 has no batchNorm layer Replace data loading augmentation functions with mx image after some experiments I found this is more important than packing images into sequential file this will make training faster with many gpus Support rec files as input Make caffemodel converter available Any suggestion is very welcome I will keep this updated,,"zhreshold,piiswrong,zhreshold,howard0su,zhreshold,howard0su,zhreshold,piiswrong,howard0su,precedenceguo,thatindiandude,zhreshold,piiswrong,zhreshold,piiswrong,andreaolgiati,zhreshold,piiswrong,zhreshold,andreaolgiati,zhreshold",2016-12-13 18:08:11,2017-09-27 20:52:08
IS,Caffe converter test fails causing CI to halt for all PRs,Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python tools caffe converter test converter py 2 3 What have you tried to solve it Tried to debug the metric update process figured out for vgg 16 and resnet the network prediction output is wrong we have the last conv output mixed to the final prediction Thus causing the mismatched shape with label,,"zhreshold,joey2014",2017-08-07 18:53:15,2017-09-27 20:52:36
IS,Webpage rendering error on IE 11 showing blank page,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows server 2016 Base ami 27a58d5c IE version 11 1593 14393 Error Message Internet Explorer can not render mxnet io webpage Showing blank page with scroll bar Debug tool shows Minimum reproducible example or Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"zhreshold,piiswrong,kevinthesun",2017-09-01 22:18:23,2017-09-27 20:53:23
IS,404 not found when install mxnet in R in mac,cran getOption repos cran dmlc options repos cran install packages mxnet Installing package into mnt home fl R x86 64 pc linux gnu library 3 4 as lib is unspecified trying URL '' Warning in install packages cannot open URL '' HTTP status was '404 Not Found' Error in download file url destfile method mode wb cannot open URL '' Warning in install packages download of package mxnet failed,,"szha,szha,szha,szha",2017-09-21 21:26:59,2017-09-27 21:03:53
PR,More sparse related docs,Preview at 800 api python ndarray sparse html,,"eric-haibin-lin,reminisce,eric-haibin-lin,szha,eric-haibin-lin,eric-haibin-lin",2017-09-16 01:00:07,2017-09-27 21:27:21
PR,more python3 compatibility fix,Some python3 compatibility fix,,zhreshold,2017-09-27 22:50:51,2017-09-27 23:07:04
PR,update nnvm,,,zhreshold,2017-09-21 04:34:31,2017-09-27 23:23:23
PR,rename,zhreshold,,szha,2017-09-27 06:44:31,2017-09-27 23:39:42
IS,BUG elewise binary op cc add alias in sub mul div,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 15 10 Compiler gcc 4 9 x Package used Python R Scala Julia Python MXNet version 0 11 1 Or if installed from source MXNet commit hash git rev parse HEAD e81a3a88abaa2c4847602dfb2864e05adc5a5f56 If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example It raises error when ops such as sub mul div call add alias Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Compile from source 2 python setup py install 3 import mxnet What have you tried to solve it After I delete them in elemwise binary op basic cc it works well 1 delete add alias sub 2 delete add alias mul 3 delete add alias div,,piiswrong,2017-09-17 08:37:43,2017-09-28 02:01:40
PR,Fixed a bug of example ssd demo py,Replaced keyword arg force nms to force suppress to fix an inconsistent param error of MultiBoxDetection op,,"xioryu,zhreshold,xioryu",2017-09-27 11:05:45,2017-09-28 02:21:41
IS,Language Model Benchmark can not reproduce the same results as Tensorflow with the same parameters,using example I'm trying to benchmark the performance of Language Model task compared with Tensorflow on PTB dataset Using the same parameters setting I can not produce a same result Thus I modified the parameters e g optimization algorithm is Adam batch size 64 lr 0 01 wd 0 0001 which can only produce a near results on Validation PPL 134 TF can get 121 While I try to use the medium L232 setting in MXNet the result was even worse Can anyone figure out the reason,,"sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,zihaolucky,sxjscience,zihaolucky,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,sxjscience,eric-haibin-lin",2017-04-17 03:20:07,2017-09-28 03:43:07
IS,Performance differs between 0 9 4 and 0 9 5,Hi I found that the latest mxnet 0 9 5 I cloned yesterday performs very differently from 0 9 4 The same code generates very different results with random seed fixed I am doing an NMT task for 0 9 4 20170325 mxnet x64 vc14 gpu 7z the perplexity drops fast and after 10 epoches the perp is still about 40 So I infer that something went wrong between the two versions,,"sxjscience,mli,mli,mli,sxjscience",2017-04-13 04:34:31,2017-09-28 03:46:07
IS,Guided BackPropagation for visualization,I am trying to develop a minimalist simple to use and effective visualization program for visualizing what the deep network learns on the lines of the work presented in the paper and named as Guided Backpropagation I am aiming to make a notebook example and submit a PR for the same These are the rough steps I need 1 Forward pass an image to a network keep the internal activations and weights 2 Choose a neuron n i in some higher layer L for which we want to visualize the pattern in the image that gave rise to it 3 Obtain the gradient of n i w r t the image with Guided ReLU during backpropagation explained in the paper The issue I have now is first how to compute the gradient of one neuron w r t the image I can for example easily do it in Theano Keras Tensorflow by picking that one neuron and computing the grad w r t the image Is it also possible in MxNet Feedforward Module class I know that I can do it in MxNet with hand defined symbols and binding but does the same applies to the Feedforward Module as well Secondly is it possible to have the normal ReLU during forward propagation and Guided ReLU during backpropagation for the same image Remember I will be using a network that is trained using ReLU and it is only during the visualization that I would need the Guided ReLU that too only during back propagation and not during forward propagation Possible solution 1 I write my own Guided ReLU and convert all the network is ReLU activation to Guided ReLU through network surgery I am not quite sure how to proceed with this approach can someone please provide some pointers a simple example of network surgery I am familiar with adding removing layers with different activations,,"sxjscience,sxjscience",2017-04-13 02:11:11,2017-09-28 03:50:38
IS,RMSProp not converging,Hi I updated mxnet last friday March 17th and I saw there have been major changes in the RMSProp implementation I have set the parameters centered True gamma1 0 95 to the values they had before the update but I'm not able to get the same network I was using to converge as it did before the update Any ideas on what might be the reason,,"piiswrong,sxjscience",2017-03-21 17:58:44,2017-09-28 03:51:11
IS,Gradient array contains NAN values,When I run my code with single GPU the gradient array does not contain any nan values However if I use more than one GPU some gradient arrays contain nan values Is this normal If not how can I fix this problem,,"sxjscience,sxjscience",2017-03-14 13:20:01,2017-09-28 03:52:40
IS,some question about implementing gradient normalization clip,i implement gradient normalization clip as followed in file optimizer py is it right,,"sxjscience,sxjscience,sxjscience,sxjscience",2017-02-19 15:22:58,2017-09-28 03:53:35
IS,its convergency is so poor though the same optimizer when i use MKL2017,,,"sxjscience,sxjscience",2017-03-06 05:17:26,2017-09-28 03:54:34
IS,bugs label grad for loss operator,currently mxnet did't handle label grad in loss operator this will cause random problems when label is generated by a subnet Maybe set the label grad to zero is a good idea to avoid this problem,,"sxjscience,sxjscience,sxjscience",2017-02-08 03:05:28,2017-09-28 03:55:39
IS,Bug if do backward on symbol mean,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler gcc4 8 4 Package used Python R Scala Julia Python MXNet version Master Latest Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Traceback most recent call last File test py line 20 in module mod backward mx nd array 1 0 dtype np float32 File home lisabug virtualenvs nnvm local lib python2 7 site packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet module module py line 487 in backward self exec group backward out grads out grads File home lisabug virtualenvs nnvm local lib python2 7 site packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet module executor group py line 437 in backward end islice stop File mxnet cython ndarray pyx line 167 in ndarray make ndarray function generic ndarray function mxnet cython ndarray cpp 3602 File mxnet cython base pyi line 36 in ndarray CALL mxnet cython ndarray cpp 1731 mxnet base MXNetError 16 15 17 src operator tensor matrix op inl h 1028 Check failed end axis size end 0 Invalid begin end get begin 0 end 3 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"sxjscience,sxjscience",2017-02-07 08:24:50,2017-09-28 03:57:05
IS,Is it possible to do broadcast concat,Suppose I have a variable A which has the shape of 1 3 4 4 and a variable B with 1 3 1 1 I have to concatenate them along axis 1 and get C 1 6 4 4 Since the current Concat symbol requires that B should be something like 1 4 4 I am wondering if there is a way I could do broadcast concat,,"sxjscience,piiswrong",2017-02-06 09:44:08,2017-09-28 03:57:33
IS,odd behavior of ndarray,Is this expected or by design import numpy as np import mxnet as mx x np array 0 1 2 3 4 5 6 7 8 9 y mx nd array 0 1 2 3 4 5 6 7 8 9 x 0 0 shape 0 y 0 0 shape 10,,"howard0su,piiswrong,sxjscience",2016-12-28 07:39:29,2017-09-28 04:02:04
IS,Problem installing mxnet on Ubuntu 16 04,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler gcc 4 9 3 Package used Python R Scala Julia Python MXNet version 0 9 1 MXNet commit hash git rev parse HEAD 9f9c135bbb8853351c927caf5c53e5a9524f156f If you are using python package please provide Python version and distribution Python 2 7 12 Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce I did exactly what the guide at mxnet io said here and it seems that previous steps worked out just fine And when I execute bash install mxnet ubuntu python sh the above problems will occur after make j in the script is excuted What have you tried to solve it I have tried make clean make but same problem occurs,,"piiswrong,sxjscience,sxjscience,jingpengw,jingpengw",2017-01-07 19:16:38,2017-09-28 04:02:18
IS,lstm python seems not correct in backward,piiswrong If you look at lstm python implementation in example the backward seems is wrong for dCt calculation Especially dCt should rather than dht dot 1 exp tanCt But I do not know how to correct it in python implementation Any idea,,zhenlinluo,2016-12-02 00:03:55,2017-09-28 04:03:42
IS,output is wrong when set state outputs True for RNN layer,piiswrong and others I change example rnn rnn cell demo py to get 3 ouputs The changes as below rnn mx sym RNN data embed tm state size num hidden num layers num lstm layer mode 'lstm' name 'LSTM' The following params can be omitted provided we do not need to apply the workarounds mentioned above state rnn h init state cell rnn c init parameters rnn params state outputs True hidden mx sym Reshape data rnn 0 shape 1 num hidden However it report the error as below mxnet mxnet cuda dmlc core include dmlc logging h 235 19 58 58 src symbol static graph cc 488 Check failed it grad map end bad graph Cannot find node LSTM is 1 th output Can someone help to check what is wrong in graph,,"zhenlinluo,piiswrong,sxjscience,zhenlinluo,sxjscience,piiswrong,sxjscience,zhenlinluo,sxjscience,zhenlinluo,zhenlinluo,sxjscience,sxjscience",2016-11-29 06:09:37,2017-09-28 04:08:32
IS,Test break for some ops if bound max error instead of mean error,Some of the tests in test operators will break when you change mx test utils reldiff to this,,"piiswrong,zhreshold,piiswrong,zhreshold,zhreshold,piiswrong,precedenceguo,sxjscience,zhreshold,piiswrong,zhreshold,sxjscience,piiswrong,sxjscience,zhreshold,precedenceguo",2016-11-27 02:38:34,2017-09-28 04:09:28
IS,Mxnet Seq2Seq task cost huge memory,Recently I use Mxnet for seq2seq task such as Machine Traslation however I find mxnet will cost huge memory even twice as that of Theano I find that the memory cost very strange Eg The softmax layer is 1024 15w I set minibatch 1 Decoder seq len 40 it will cost 11G memory If i set softmax layer 512 15w minibatch 1 Decoder seq len 40 it cost 8G memory if i set softmax layer 256 15w minibatch 1 Decoder seq len 40 it cost 6G memory Again if i tune the minibatch as softmax layer 512 15w minibatch 16 Decoder seq len 40 it cost 9 5G memory while minibatch 1 it cost 8G memory In the meanwhile if I use theano the softmax layer is 1024 15w minibatch 32 Decoder seq len 40 it onlyt cost 4 5G memory Not only for machine translation I have tried lots of other seq2seq tasks mxnet always cost huge memory As far as i know so far mxnet example has not give a seq2seq example yet while seq2seq task are so popular for deeplearning I am really looking foward to hearing advices from guys using mxnet for seq2seq task,,"piiswrong,sxjscience,sxjscience,sxjscience",2016-11-19 08:28:21,2017-09-28 04:11:50
IS,Multiple ResourceRequest kTempSpace in a single operator,It seems that we cannot use multiple ResourceRequest kTempSpace in a single operator However in some cases we have to create temporary spaces with different data types like int and real t Is it possible to revise the current implementation to achieve that,,"sxjscience,piiswrong,tqchen,tqchen,sxjscience,sxjscience",2016-09-08 11:56:51,2017-09-28 04:19:06
IS,I found my mxnet starts very slow takes about 5 minutes before the gpus begin to run,when starting it stops here for about 5 minutes,,"piiswrong,sxjscience",2016-09-07 01:14:53,2017-09-28 04:19:34
IS,Code freeze halt code merges,A pending item for our release is adding license headers to all source files We first need to stabilize the CI before we can merge the license headers since it is a very big change To avoid introducing any breaking changes until the CI is stable please do not merge a PR unless it is a bug fix,,"lxn2,mli,lxn2",2017-08-07 17:04:36,2017-09-28 04:46:22
IS,This is a test issue,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"szha,szha",2017-09-28 04:32:33,2017-09-28 04:50:48
PR,Perl do not merge debugging ci failure of perl gpu tests,,,sergeykolychev,2017-09-27 05:26:51,2017-09-28 04:59:13
PR,Perl do not merge Gpu lib check,,,sergeykolychev,2017-09-28 05:01:48,2017-09-28 05:04:31
IS,is there a regularization term in the neural network is default loss function,,,"kevinthesun,kevinthesun",2017-05-19 11:59:20,2017-09-28 05:39:34
IS,New Operator Guide,After 3245 get merged in we can start editing a new operator registration guide Let us start by pasting and collaboratively edit markdown format to this issue before we move it to the docs,,"tqchen,tqchen,tqchen,tornadomeet,flyers,tqchen,flyers",2016-09-07 21:14:22,2017-09-28 05:51:12
IS,Have any data augmentation guide or examples,Confused by docs Thanks a lot,,"ysh329,piiswrong,ysh329",2017-05-30 13:41:54,2017-09-28 06:12:32
IS,What is the differences among ImageNet ImageNet 11k and ImageNet 11k 365 ch,I did not find any intro about ImageNet ImageNet 11k and ImageNet 11k 365 ch in imagenet website I saw there are 3 folders in Index of models Have anyone can introduce the differences among these folders Thanks a lot,,ysh329,2017-05-28 04:10:34,2017-09-28 06:12:50
IS,batch norm cannot support unrolled rnn input data,For unrolled rnn lstm we unroll the input sequence When i tried to use batch norm for input data after embed layer i fount it impossible for all characters in a sequnce with just one batch norm first batch norm calc mean and var in forward function every invoking just store in output and used in backward function but in unrolled lstm for one sequence with length m it may invoke forward function m times and we only get the last invoke mean and var which store in output so we loss some mean and var second moving mean and moving var are two ndarrays which we can not assign when the batch norm operator defined the two ndarrays are defined when the batch norm op created which is not support copy for sequence with m length in rnn lstm there may m times aux states for batch norm Is there any good way to use batch norm in unrolled rnn lstm,,phunterlau,2016-08-23 03:39:23,2017-09-28 06:13:03
IS,GRU model have the wrong result,i downloads the gru py and run the gru bucketing py the terminal show 56150 Summary of dataset bucket of len 50 55830 samples 4439 Summary of dataset bucket of len 50 4416 samples 12 12 24 home xuqian bosonnlp mxnet mxnet dmlc core include dmlc logging h 235 12 12 24 src symbol symbol cc 155 Symbol InferShapeKeyword argument name data 49 not found Candidate arguments 0 l1 init h 1 l0 init h 2 data 3 embed weight 4 l0 i2h gates weight i did not change the code just set the buckets 129 and run the code,,phunterlau,2016-08-23 04:13:43,2017-09-28 06:13:04
IS,The random sample about char rnn ipynb,Hi I'm confiused about the 'random sample' parametes in char rnn ipynb which brother can explain to me esteem it a favor,,phunterlau,2016-08-23 09:06:38,2017-09-28 06:13:05
IS,why not the item 'quality' of 'encode params cv2 IMWRITE PNG COMPRESSION quality ' is from 0 to 9,When I read the module python mxnet recordio py and notice that the item 'quality' of 'encode params cv2 IMWRITE PNG COMPRESSION quality ' is from 0 to 9 in function pack img I am wondering Would you like to explain to me Thank you,,"piiswrong,phunterlau",2016-08-24 04:10:16,2017-09-28 06:13:09
IS,How to print data of NDArray in c,in mxnet src kvstore kvstore dist server h how to print the NDArray of store line 227 thx std unordered map int NDArray store,,phunterlau,2016-08-24 07:49:26,2017-09-28 06:13:10
IS,Identity mapping symbol in mxnet,Just want to dump some inner result in the computation graph For example In this example I want to dump the image after conv layer Because mxnet will only store the information about args therefore I tried to manually add an argument to make the result stored by mxnet Any idea on how shall I get this identity mapping symbol,,"piiswrong,piiswrong,phunterlau",2016-08-23 09:39:55,2017-09-28 06:13:13
IS,mxnet predict code running on gpu,I would like to predict test data using predict function as below devs mx cpu if args gpus is None else mx gpu int i for i in args gpus split ' ' and it will report Traceback most recent call last File predict py line 40 in module model mx model FeedForward ctx devs TypeError init takes at least 2 arguments 2 given Exception in thread Thread 4 how can i fix it,,phunterlau,2016-08-24 13:03:00,2017-09-28 06:13:14
IS,batch padding bug in ml dmlc mxnet spark io LabeledPointIter,if sample size batch size 0 the last batch generated by next is still initialized with NDArray empty using original batch size and set pad batch size real size However the mxnet training process seems to ignore the pad field of DataBatch The whole batch is taken into calculation which may produce NaN result in my linear regression model I find that the uninitialized part of the last batch contains randomly huge numbers like xxxxxE10 Using NDArray zeros to initialize the batch or just discarding the last batch can fix in my case And is it possible to use different batch size for different batch in the DataIter object,,phunterlau,2016-08-25 03:58:04,2017-09-28 06:13:16
IS,Finetune Error Segment Fault core dumped,I fine tune pretrained resnet 200 on myself data I referred tutorial codes from docs about finetune,,"ysh329,piiswrong,ysh329,ysh329,ysh329,ysh329",2017-05-20 03:52:58,2017-09-28 06:13:20
IS,Does anyone have pretrained inception resnet v2 model,I did not find it on mxnet model zoo sob Thanks a lot laughing,,"ysh329,ysh329,ysh329,ysh329",2017-05-07 15:14:34,2017-09-28 06:13:49
IS,Have pretrained DeepID MXNet model,Recently I'm making a serires benchmark of inference not trainning using MXNet I found caffe DeepID model but there is some problem when converting to MXNet model Besides I found a MXNet DeepID lacking of README and some key files Sigh Last quetion Can MXNet convert tensorflow model to MXNet model,,"ysh329,ysh329",2017-03-18 04:53:38,2017-09-28 06:14:27
IS,Can tensorflow model convert to MXNet model,,,"ysh329,ysh329",2017-03-18 06:57:23,2017-09-28 06:15:18
IS,import MXNet Error undefined symbol gotoblas,After I install MXNet I tried to import Hower it output below message,,"ysh329,ysh329,ysh329",2017-03-19 09:21:29,2017-09-28 06:15:52
IS,Does MXNet have local convolution layer,Recently I'm to train a network teacher defined but I found my teacher defined many a local convolution layer So I have to write this symbol of network 1 Does MXNet have this local convolution layer I found nothing when searching MXNet docs about local and convolution 2 Can somebody briefly explain this local convolution layer Thanks a lot 3 Can this local convolution layer be replaced by FullyConnectLayer,,"ysh329,ysh329",2017-04-17 09:23:48,2017-09-28 06:17:18
IS,Can mxnet use lmdb data,Now caffe Torch and Keras all can use lmdb data Can mxnet use lmdb data Thank you,,"piiswrong,phunterlau",2016-08-25 01:53:05,2017-09-28 06:17:44
IS,DMLC REGISTER PARAMETER,windows mxnet alexnet DMLC REGISTER PARAMETER main debug image,,phunterlau,2016-08-26 07:29:44,2017-09-28 06:17:45
IS,Prediction using the model of the best echo,If the best validation score occurs in epoch 25 num epoch 50 was set how can I do a prediction using the model at that epoch,,phunterlau,2016-08-26 09:38:18,2017-09-28 06:17:52
IS,Python custom layer is extremely slow,It is from mx operator CustomOp as implemented in rcnn example There is about one and a half sec gap between finish init of the python operator class and its beginning to do forward operation What it is doing during this time Are there some suggestions to squeeze the time,,"precedenceguo,piiswrong,precedenceguo,phunterlau",2016-08-25 08:46:20,2017-09-28 06:17:56
IS,Problem about binary image classification with 'LogisticRegressionOutput',I use 'LogisticRegressionOutput' in image binary classification and get the error information MXNetError 09 50 29 D chhong mxnet src symbol symbol cc 121 Symbol InferShapeKeyword argument name softmax label not found what is wrong part of my code as following ''' def inception nhidden grad scale data data mx symbol Variable name data stage 2 in3a InceptionFactoryA data 64 64 64 64 96 avg 32 '3a' in3b InceptionFactoryA in3a 64 64 96 64 96 avg 64 '3b' in3c InceptionFactoryB in3b 128 160 64 96 '3c' stage 3 in4a InceptionFactoryA in3c 224 64 96 96 128 avg 128 '4a' in4b InceptionFactoryA in4a 192 96 128 96 128 avg 128 '4b' in4c InceptionFactoryA in4b 160 128 160 128 160 avg 128 '4c' in4d InceptionFactoryA in4c 96 128 192 160 192 avg 128 '4d' in4e InceptionFactoryB in4d 128 192 192 256 '4e' stage 4 in5a InceptionFactoryA in4e 352 192 320 160 224 avg 128 '5a' in5b InceptionFactoryA in5a 352 192 320 192 224 max 128 '5b' global avg pooling avg mx symbol Pooling data in5b kernel 7 7 stride 1 1 name global pool pool type 'avg' linear classifier flatten mx symbol Flatten data avg name 'flatten' fc1 mx symbol FullyConnected data flatten num hidden nhidden name 'fc' logicr inception 100 1 0 above is to get the network batch size 1 train dataiter mx io ImageRecordIter shuffle True path imgrec d test train1 rec rand crop True rand mirror True data shape 3 200 200 batch size batch size prefetch buffer 4 preprocess threads 2 test dataiter mx io ImageRecordIter shuffle True path imgrec d test valid1 rec rand crop True rand mirror True data shape 3 200 200 batch size batch size prefetch buffer 4 preprocess threads 2 above is to get the input dataiter from REC files num epoch 5 model prefix d test cifar 100 model mx mod Module symbol logicr data names wouldata' label names 'label' model bind data shapes train dataiter provide data label shapes train dataiter provide label model fit train dataiter eval data test dataiter eval metric accuracy num epoch num epoch optimizer params 'learning rate' 0 01 'momentum' 0 9 batch end callback mx callback Speedometer batch size 200 epoch end callback mx callback do checkpoint model prefix ''' the file name is temp py in Spider and when run it then get the following error information ''' runfile 'C Users ipmlogin spyder2 temp py' wdir 'C Users ipmlogin spyder2' Traceback most recent call last File ipython input 15 d4396a36fbc3 line 1 in module runfile 'C Users ipmlogin spyder2 temp py' wdir 'C Users ipmlogin spyder2' File C Users ipmlogin Anaconda2 lib site packages spyderlib widgets externalshell sitecustomize py line 714 in runfile execfile filename namespace File C Users ipmlogin Anaconda2 lib site packages spyderlib widgets externalshell sitecustomize py line 74 in execfile exec compile scripttext filename 'exec' glob loc File C Users ipmlogin spyder2 temp py line 97 in module model bind data shapes train dataiter provide data label shapes train dataiter provide label File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet module module py line 252 in bind shared group logger self logger File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet module executor group py line 95 in init self bind exec data shapes label shapes shared group File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet module executor group py line 123 in bind exec self execs append self bind ith exec i data shapes label shapes shared group File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet module executor group py line 321 in bind ith exec arg shapes aux shapes self symbol infer shape input shapes File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet symbol py line 412 in infer shape return self infer shape impl False args kwargs File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet symbol py line 472 in infer shape impl ctypes byref complete File C Users ipmlogin Anaconda2 lib site packages mxnet 0 5 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError MXNetError 09 50 29 D chhong mxnet src symbol symbol cc 121 Symbol InferShapeKeyword argument name softmax label not found Candidate arguments 0 data 1 conv 3a 1x1 weight 2 conv 3a 1x1 bias 265 fc weight 266 fc bias 267 label ''' obey the advice in Weird LogisticRegressionOutput Bug Issue 1975 dmlc mxnet I change the value of num hidden with logicr inception 1 1 0 and the error information is the same what is wrong thanks for your help very much,,phunterlau,2016-08-25 02:41:36,2017-09-28 06:17:57
IS,How does decay factor work in Adam,In optimizer py The initialization for Optimizer class is def init self rescale grad 1 param idx2name None wd 0 clip gradient None learning rate 0 01 lr scheduler None sym None begin num update 0 The initialization for Adamclass is def init self learning rate 0 001 beta1 0 9 beta2 0 999 epsilon 1e 8 decay factor 1 1e 8 kwargs super Adam self init learning rate learning rate kwargs self beta1 beta1 self beta2 beta2 self epsilon epsilon self decay factor decay factor I find nowhere using decay factor And instead of using decay factor Adam update use wd to decay updating weight I get confused here Do I misunderstand or the decay factor should renamed as wd,,phunterlau,2016-08-29 02:25:52,2017-09-28 06:17:59
IS,multiple gpus runs slower than one gpu,I have tried toy ctc and rnn demo when I use multiple 2 gpus the two demos all runs slower than use one gpu could any give some explantion,,phunterlau,2016-08-25 03:57:30,2017-09-28 06:18:00
IS,Python docs reference compile,The python docs reference a function called compile As far as I can tell that function does not exist anymore Is this bind nowadays,,"vchuravy,piiswrong,vchuravy,pluskid,phunterlau",2016-08-27 08:07:53,2017-09-28 06:18:01
IS,A problem for prediction with trained model,Hi all After I run the train cifar10 py in the mxnet example I predict the train and test datasets with trained model and the accuracy of prediction for train and test are all 100 is there anyone help me to find the problem my code like this import find mxnet import mxnet as mx import logging import argparse import os sys import train model import numpy as np prefix 'cifar test' iteration 20 model load mx model FeedForward load prefix iteration ctx mx gpu data shape 3 28 28 val mx io ImageRecordIter path imgrec 'test rec mean img 'test bin rand crop False rand mirror False data shape data shape batch size 1 prob data pred label model load predict val return data True,,"zihaolucky,phunterlau",2016-08-24 01:45:55,2017-09-28 06:18:03
IS,A question about async engine,This is related to 1452 which is already closed Since I am still confused about this async engine may I ask you one more question Like you said before executor outputs 0 copyto mx cpu asnumpy will force the engine to sync So if I have two parts in my network which means two executors e1 and e2 The whole training process can be described as So is this the correct way of doing this Since I used too many copyto mx cpu to force the sync would this cause any slow down If this is non optimal what is the best way to implement this Thank you very much,,phunterlau,2016-08-30 04:06:11,2017-09-28 06:18:04
IS,Convert VGG CNN M 1024 Error,I can convert VGG16 and VGG19 but when I run python convert model py VGG CNN M 1024 deploy prototxt VGG CNN M 1024 caffemodel VGG1024 it gives It contains norm1 mx symbol LRN name 'norm1' data relu1 alpha 0 000500 beta 0 750000 knorm 2 000000 nsize 5 in VGG CNN M 1024 model Is that means this layer do not belong to the L2LayerParameter so the tool can not support,,phunterlau,2016-08-30 05:03:37,2017-09-28 06:18:06
IS,MapReduceKeepHighDim reduction dimension do not match,Hi I'm adding some modifications to the structure of a pretrained network I added a skip connection and a 1x1 conv after the concat to make the final feature dimension unchanged Here are the visualization of the modified part of the network original net qq20160831 1 modified qq20160831 2 I changed visualization py a bit so it displays batch size which is the first dimension since shared conv reduce is a new symbol and does not exist in the pretrained model I initialized its weight using normal distribution with shape 1024 1536 1 1 but during forwarding I suppose mxnet reports this error dmlc core include dmlc logging h 235 23 16 07 src engine threaded engine h 306 23 16 07 mshadow mshadow tensor gpu inl h 146 Check failed eshape dimkeep dshape 0 MapReduceKeepHighDim reduction dimension do not match and I do not know why,,"neodooth,phunterlau",2016-08-31 15:25:30,2017-09-28 06:18:08
IS,is there any example of LSTM image classification not a issue,Is there any example of LSTM image classification Or is any other method for image classification except CNN,,"tornadomeet,phunterlau",2016-08-29 07:59:32,2017-09-28 06:18:10
IS,about unpack patch2col and swapaxis,When I read the Forward function in convolution inl h I want to find out where unpack patch2col and swapaxis exectutor is The time cost of these two functions are very huge Or I did not get the right part because of Async Thanks in advance,,"piiswrong,phunterlau",2016-08-31 11:36:28,2017-09-28 06:18:11
IS,example problem where is train model,yech1990 I found an unknown package on line 5 in mxnet example kaggle ndsb1 train dsb py L5 I can not find train model below mxnet example kaggle ndsb1 directory,,"ysh329,ysh329",2017-04-18 10:13:38,2017-09-28 06:18:12
IS,how to accelerate training process of lstm model,I am playing with the example rnn lstm py char rnn codes When I use 4 gpus to run it only use 10 30 on each of them and cpu is around 200 I have tried kvstore local all reduce cpu all reduce device etc nothing better some may be even worse then the default option local update cpu Any tricks to accelerate this More details data set 132w Chinese sentences with Jieba to cut words 4 5w words params batch size 128 buckets 40 num hidden 64 num embed 16 num lstm layer 3 devices Tesla K40c 4 mxnet building with USE CUDA 1 and USE CUDNN 0 some speed logs 02 48 47 INFO Auto select kvstore type local update cpu 02 48 47 INFO Start training with gpu 0 gpu 1 gpu 2 gpu 3 02 57 50 INFO Epoch 0 Batch 1000 Speed 235 90 samples sec Train Perplexity 129 630862 03 06 47 INFO Epoch 0 Batch 2000 Speed 238 64 samples sec Train Perplexity 15 946662 03 15 47 INFO Epoch 0 Batch 3000 Speed 237 14 samples sec Train Perplexity 14 958449,,"pluskid,phunterlau",2016-08-27 07:27:58,2017-09-28 06:18:12
IS,How to test using mx mod Module,I have the following code to test my model However it takes as long to test as to train even a bit longer and I'm worried I'm training it by mistake or doing something way wrong,,"pluskid,phunterlau",2016-08-27 08:29:16,2017-09-28 06:18:13
IS,Jupyter notebook DeprecationWarning encoder py 207 Interpreting naive datetime as local,When I execute below codes in Jupyter Notebook screen printed below warning message However when I execute in ipython or python command everything is okay,,"ysh329,ysh329",2017-04-15 14:00:56,2017-09-28 06:18:59
IS,How can I scale resize the rec format data cifar 10 before train,,,"ysh329,ysh329,ysh329",2017-03-21 07:43:20,2017-09-28 06:19:30
IS,training accuracy decrease slightly at the start of every epoch,I train a Inception v2 network on my dataset with around 600 classes and 1M images At the beginning every thing is fine The training accuracy increases consistently However after some epochs the training accuracy begins to drop slightly at the start of every epoch image,,"pluskid,phunterlau",2016-08-24 08:56:05,2017-09-28 06:21:04
IS,Temporal k max pooling,I need to construct a temporal k max pooling layer which outputs k features I had a quick search and I think my problem is similar to this Basically the layer gets an input of 1 height x 125 width x 512 number of filters and outputs 1 height x k width x 512 number of filters In my mind this is different to a standard max pooling layer and I dont know if that can be reduced to this,,"CNevd,phunterlau",2016-08-27 18:10:12,2017-09-28 06:21:06
IS,Global memory allocation policy,Is that possible to support global memory pool in mxnet Current memory share policy between different executors is based on each NDArray If new executor has more NDArray numbers even the total memory consumption is lower than the shared executor it will still alloc new memory space For example to improve the efficiency of full sequence lstm we can use a large batch size for short sequence and small batch size for long sequence however due to the memory allocation policy of mxnet this will cacuse OOM,,phunterlau,2016-09-03 12:03:58,2017-09-28 06:21:07
IS,mxnet csharp Interface,I make a csharp Interface with mxnet,,"yajiedesign,tqchen,phunterlau",2016-09-01 14:13:56,2017-09-28 06:21:09
IS,Is there a bug when I use visualization py in mxnet,I want to visualize my net structure using mx viz plot network My net structure include such code How to solve this problem thanks,,phunterlau,2016-09-05 04:25:52,2017-09-28 06:21:10
IS,I want to run warp ctc with flexible length data Is there any suggestion,I want to run warp ctc with flexible length data Is there any suggestion,,phunterlau,2016-09-05 02:11:02,2017-09-28 06:21:12
IS,get each layer output,whether i can get each neural output when i finish my training,,"tornadomeet,tornadomeet,phunterlau",2016-09-01 01:58:39,2017-09-28 06:21:13
IS,Download pretrained model,Sorry to ask the problem here but I do not know how to get the pretrained model vgg 16 after read the caffe documents because I do not know what is the gist id crresponding to model Can anyone help rd idc01 rank gpu 01 scripts download model from gist sh usage download model from gist sh gist id dirname,,"tornadomeet,tornadomeet,phunterlau",2016-09-05 02:08:25,2017-09-28 06:21:15
IS,toy ctc report error while using more than one gpu,I used gpu1 and gpu0 when running toy ctc and report error like this,,phunterlau,2016-09-06 11:59:34,2017-09-28 06:21:16
IS,Does warp ctc support multiple input feature length,Does warp ctc support multiple input feature length I tried rnn is bucket iterator but it does not work,,phunterlau,2016-09-05 02:31:06,2017-09-28 06:21:17
IS,when release a new stable version,the last stable version is 0 7 0 when will be the new stable version released,,"zihaolucky,Ldpe2G,zihaolucky,phunterlau",2016-09-06 05:52:19,2017-09-28 06:21:18
IS,question about the pad design,The pad in convolution seems to add the pad by two side means that every time I add the pad number 1 then the output size will plus 2 given the stride is 1 1 So it seems not so flexible to get what output size I want like there is a isame' setting in keras border mode Why it is designed to like this,,"winstywang,yajiedesign",2016-09-07 01:57:45,2017-09-28 06:44:43
IS,BUG The performance of multi GPU training is decreased dramatically at current revision,The performance of multi GPU training is decreased dramatically after I update the code from 5f278803303d5019b76c5aaecfd8c8f27758558e to the latest revision In particular using four GPUs and 5f278803303d5019b76c5aaecfd8c8f27758558e I got 242 26 samples sec for Inception BN network at training However using four GPUs and the latest version I got only about 90 samples sec for Inception BN network at training,,"tqchen,winstywang,Trangle,yajiedesign",2016-09-06 03:43:02,2017-09-28 06:44:46
IS,raw in MXNet Android Example,Dear mxnet contributors I have trouble in generating the params files Could you tell me how to generate this file I just try to change it into an acoustic model using your interface and made some experiment with that qq 20160909145759 Please help me with that Thank you very much Xin Wang nextowang gmail com Xidian University September 9th 2016,,yajiedesign,2016-09-09 07:01:36,2017-09-28 06:44:49
IS,How can I implement the RCNN model by mxnet,I want to impelement the RCNN model by mxnet My code is as follows def rcnn factory data num filter kernel stride 1 1 pad 0 0 name None suffix '' conv conv factory data data num filter num filter kernel 3 3 name 'conv s' name conv mx symbol Convolution data data num filter num filter kernel kernel stride stride pad pad name 'conv s s' name suffix bn mx symbol BatchNorm data conv name 'bn s s' name suffix act mx symbol LeakyReLU data bn act type 'rrelu' name 'rrelu s s' name suffix conv a mx symbol Convolution data act num filter num filter kernel kernel stride stride pad pad name 'conv a s s' name suffix weight 'conv s s weight' name suffix no bias True sum a mx symbol ElementWiseSum conv conv a bn a mx symbol BatchNorm data sum a name 'bn a s s' name suffix act a mx symbol LeakyReLU data bn a act type 'rrelu' name 'rrelua s s' name suffix conv b mx symbol Convolution data act a num filter num filter kernel kernel stride stride pad pad name 'conv b s s' name suffix weight 'conv s s weight' name suffix no bias True sum b mx symbol ElementWiseSum conv conv b act b mx symbol LeakyReLU data sum b act type 'rrelu' name 'rrelub s s' name suffix conv c mx symbol Convolution data act b num filter num filter kernel kernel stride stride pad pad name 'conv c s s' name suffix weight 'conv s s weight' name suffix no bias True sum c mx symbol ElementWiseSum conv conv c act c mx symbol LeakyReLU data sum c act type 'rrelu' name 'rreluc s s' name suffix return act c The conv conv a conv b conv c are shared weights But there are some bugs bug The RCNN is proposed by url and the caffe model is as follows model RCNN128 txt,,"sxjscience,yajiedesign",2016-09-10 09:29:46,2017-09-28 06:44:52
IS,low accuracy of example kaggle ndsb1,I follow all the instructions if the README md file of example kaggle ndsb1 Everything goes fine but the accuracy of the training models is weird image,,yajiedesign,2016-09-11 11:11:22,2017-09-28 06:44:56
IS,using im2rec cc in VS2013 with the error of im2rec obj error LNK2019 public static class dmlc Stream cdecl dmlc Stream Create char const char const const bool Create Stream dmlc SAPEAV12 PEBDQEBD N Z main,Hello everyone I want to transform the data form to rec It is only the tool of im2rec cc that can transform data with multilabels in one picture However I do not know how to use it I open the MxnetTestApp sln in VS and add the im2rec cpp in it It goes with the errors following ERROR 1 im2rec obj error LNK2019 public static class dmlc Stream cdecl dmlc Stream Create char const char const const bool Create Stream dmlc SAPEAV12 PEBDQEBD N Z main 1 im2rec obj error LNK2019 public static class dmlc InputSplit cdecl dmlc InputSplit Create char const unsigned int unsigned int char const Create InputSplit dmlc SAPEAV12 PEBDII0 Z main 1 im2rec obj error LNK2019 public void cdecl dmlc RecordIOWriter WriteRecord void const unsigned int64 WriteRecord RecordIOWriter dmlc QEAAXPEBX K Z main 1 D MXNet windows vs MxnetTestApp x64 Debug MxnetTestApp exe fatal error LNK1120 52 0 1 0 0,,"piiswrong,yajiedesign",2016-09-09 08:24:21,2017-09-28 06:45:03
IS,How to use amalgamation w pre trained model,You might find this question easy however mxnet amalgamation link does not tell how to include a pre trained model json param file into this mechanism For example Lelina is app whatsthis uses imagenet but I'm not sure how that has been compiled to libmxnet predict so,,"SlipknotTN,yajiedesign",2016-08-28 20:48:37,2017-09-28 06:46:04
IS,Problem in model py when predict,when a computer had limited memory but want to predict a large dataset the predict function use list as its reture value cound this be revised as iterators for predict data label,,"Trangle,yajiedesign",2016-09-12 15:36:02,2017-09-28 06:46:08
IS,How to construct a new network from a model loaded,I have a pre trained mxnet model and I can also load it Now i want to change the network for example inserting a new layer to the existed network How can I do Preferably I want to know how to get the symbol of every layer such as a symbol of convolution layer from a loaded model I think this can be done definitely in mxnet but I do not know,,"vchuravy,yajiedesign",2016-09-12 07:51:24,2017-09-28 06:46:11
IS,suggest to add 'iou loss layer' operator,UnitBox An Advanced Object Detection Network,,yajiedesign,2016-09-14 07:20:51,2017-09-28 06:46:14
IS,suggest to add 'roi warp layer' operator,Instance aware Semantic Segmentation via Multi task Network Cascades,,"sxjscience,yajiedesign",2016-09-14 09:01:49,2017-09-28 06:46:18
IS,Crash on predict Check failed out data size 2,I first trained a network using the symbol alexnet that can be found in the examples The size of each exemplar is 3 256 256 I end up with the following error 17 19 14 data Programming Libraries mxnet mxnet dmlc core include dmlc logging h 235 17 19 14 src operator lrn inl h 59 Check failed out data size 2 17 19 14 data Programming Libraries mxnet mxnet dmlc core include dmlc logging h 235 17 19 14 src engine threaded engine h 306 17 19 14 src operator lrn inl h 59 Check failed out data size 2 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 17 19 14 src engine threaded engine h 306 17 19 14 src operator lrn inl h 59 Check failed out data size 2 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Aborted core dumped Any ideas on what could cause this problem Thanks,,yajiedesign,2016-09-12 16:23:45,2017-09-28 06:46:24
IS,the bug of the adversary example,Hi I run the adversary code in the example adversary adversaty generation ipynb but it has some bugs as follows bug1 bug2 bug3 Thanks,,"piiswrong,yajiedesign",2016-09-18 02:46:17,2017-09-28 06:46:36
IS,How to set default bucket key while using bucket iterator,when I set default key like this What is the problem,,yajiedesign,2016-09-19 01:02:21,2017-09-28 06:46:39
IS,Proposal on building tensorboard like tool,By using Kibana Logstash tech stack It is a very common and matured tool also open sourced in industry It provides the interface to show collect logs from various machines Therefore what we only need to do is design the format of mxnet is output log about what to export to the tool And it can be deployed easily independently by docker Just a very naive idea But I believe this tool will makes mxnet easier to use It is also useful in real industry community,,"piiswrong,piiswrong,yajiedesign",2016-09-15 02:17:31,2017-09-28 06:46:42
IS,Cannot run rcnn training,the log is like this INFO root TRAIN RPN WITH IMAGENET INIT INFO root GENERATE RPN DETECTION voc 2007 trainval gt roidb loaded from D Data faster rcnn cache voc 2007 trainval gt roidb pkl prepare roidb generating detections 0 5011 generating detections 10 5011 generating detections 20 5011 generating detections 30 5011 generating detections 40 5011 generating detections 50 5011 generating detections 60 5011 generating detections 70 5011 generating detections 80 5011 generating detections 90 5011 generating detections 100 5011 generating detections 110 5011 generating detections 120 5011 generating detections 130 5011 generating detections 140 5011 generating detections 150 5011 generating detections 160 5011 21 57 37 D MyCoding DeepLearning OpenSource dmlc mxnet dmlc core include dmlc logging h 235 21 57 37 D MyCoding DeepLearning OpenSource dmlc mxnet src operator cudnn convolution cc 225 Check failed cudnnFindConvolutionF orwardAlgorithm s dnn handle in desc filter desc conv desc out desc kMaxAlgos nalgo fwd algo CUDNN STATUS SUCCESS 21 57 37 D MyCoding DeepLearning OpenSource dmlc mxnet dmlc core include dmlc logging h 235 21 57 37 d mycoding deeplearning opensource dmlc mxnet src engine threaded engine h 306 21 57 37 D MyCoding DeepLearning Ope nSource dmlc mxnet src operator cudnn convolution cc 225 Check failed cudnnFindConvolutionForwardAlgorithm s dnn handle in desc filter desc conv desc out desc kMaxAlgos nalgo fwd algo CUDNN STATUS SUCCESS An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all oper ations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging i use gtx980m with 8g graph memory and use model vgg16 as default it seems that the memory was not free during the forward waiting period cause the batchsize is only 1,,"Trangle,piiswrong,precedenceguo,Trangle,yajiedesign",2016-09-18 14:07:30,2017-09-28 06:46:49
IS,How do I know if mxnet was compiled with USE CUDA 1,I'm working on a remote cluster environment where mxnet is installed but not by me This page says it will only work with CUDA if it was compiled accordingly Is there a way I can check if my version of mxnet was compiled with USE CUDA 1,,"tornadomeet,yajiedesign",2016-09-18 10:21:56,2017-09-28 06:46:52
IS,Low validation accuracy if I split the training data manually when training with multi machine,I training my model with a mpi cluster of 10 machines Because the training dataset is very large distributing the whole train rec to each machine requires much time and space So I manually split the train lst into 10 non overlap text file such as train 0 lst train 1 lst Then I use im2rec py to make 10 ImageRecordIO dataset When training with multi machine in each machine it will download train OMPI COMM WORLD RANK rec and use it as the source of training iterator I do not forget to change the parameters num parts and part index as 1 0 of the training ImageRecordIter I do not change the validation dataset I distribute the complete val rec to each machine So I believe the training procedure should be roughly same as when I use only one train rec But the result is disappointing The training accuracy curve is good even slightly better than previous However the validation curve is much worse image The blue curves are the training and val curves when I use one complete train rec while red curves are those when using splited train 0 rec train 1 rec Is there any mistake that I may have,,"tornadomeet,yajiedesign",2016-09-19 04:56:31,2017-09-28 06:46:55
IS,Is there any typo in Dependency Engine for Deep Learning,case study on multi gpu neural net The original sample code is There is same problem in the graph show,,yajiedesign,2016-09-22 02:20:39,2017-09-28 06:47:28
IS,Win10 DLL for R and MATLAB conflict,Hoping to get some insight Now I have used MXNET with R and Python on Win 10 64 bit for 2 weeks The R DRAT and binary install method seems to work I wanted to check is there is a conflict as mxnet is added as a library in R with code in folder under R USERLIB not in R HOME and in Python under Anaconda Do the DLL get placed in various folders with chance of conflict If I am running a example in R and in Python using MXNET at same time will it be Ok In R there is a weekly update possibility What anout python Do I need to do it manually And I keep getting issues with MATLAB as reported earlier,,yajiedesign,2016-09-22 10:28:18,2017-09-28 06:47:31
IS,Why is LOG INFO not working as supposed in mxnet C code,I ran example image classification train mnist py with batch size 10000 so there would be 6 batches for the forword training val is set to None the Forword function of the first Fully connect layer should be called 6 times I added some debug codes as follows p hayw f d 0 t 11 i u3j o466yu t tq yrezt3oy But the log printed is not what I supposed o smuqtw sdk ou 6 ji why is logger info load123 in python executed 6 times as supposed while LOG INFO forward123 in fully connected inl h just once,,"piiswrong,yajiedesign",2016-09-22 13:10:33,2017-09-28 06:47:35
IS,Features in mysql database,Hello I am new in mxnet and trying to figure out the way how to train my model with features in a large mysql table Is there any iterator code for mysql,,"piiswrong,yajiedesign",2016-09-22 22:28:40,2017-09-28 06:47:41
IS,amalgamation for full convolution network memory leak,I use url framework and recompiled jni libmxnet predict so to support my own network There are convolution batchnorm relu deconvolution residual block in my network and it is a fully convolution network which has a random input data shape not fixed to 1 3 224 224 So I change a little the url java code and some core code like this Predictor predictor WhatsApplication mInstance getPredictor inWidth inHeight predictor forward data colors float outputData predictor getOutput 0 predictor free for the input image may has a size of 512 512 and a forward calculation would need about 200M memory size in android phone Usually I did forward calculation three times or more the apk will break because of out of memory I have do predictor free as you see and I doubt there is a memory leak in mxnet predict all cc I also found this situalation in url as using my fcn looking for helps about how to solve this memory leak or the right way to do fcn in android using mxnet,,yajiedesign,2016-09-24 08:21:45,2017-09-28 06:48:20
IS,Can I pass non tensor data as a c object through layers,I want to implement a layer which can output non tensor data as a c object that is stored in cpu memory In caffe I can store the object as a data member of a layer and pass the address of the object in a Blob to subsequent layers Storing the memory address in a Blob is a hack but it works because in caffe every thread has separate layer instances and the computation of a minibatch does not cross over different threads machines Can I do the same thing in mxnet,,"piiswrong,yajiedesign",2016-09-25 11:49:57,2017-09-28 06:48:24
IS,redefine CHECK in sframe plugin,I got this error in log when I make with sframe plugin In file included from home papillon SFrame oss src serialization iarchive hpp 42 0 from home papillon SFrame oss src serialization serialize hpp 34 from home papillon SFrame oss src serialization serialization includes hpp 33 from home papillon SFrame oss src parallel atomic hpp 38 from home papillon SFrame oss src flexible type flexible type hpp 17 from home papillon SFrame oss src unity lib image util hpp 12 from plugin sframe iter sframe cc 16 home papillon SFrame oss src logger assertions hpp 119 0 warning CHECK redefined define CHECK condition,,"winstywang,yajiedesign",2016-09-25 17:17:31,2017-09-28 06:48:27
IS,Checked failed cuudnnConvolutionForward,Hi All MXNET users Recently I downloaded the latest MXNET I tried to train ILSVRC2012 data with MXNET but I could not train with alexnet and inception bn network If I use inception bn network tools launch py H hosts n 2 launcher ssh python train imagenet py network inception bn gpus 0 1 2 3 batch size 144 num epochs 1 lr 0 05 lr factor 0 94 data shape 256 data dir data ilsvrc12 kv store dist sync it has the following error 10 45 00 cm shared scratch rengan DL mxnet dmlc core include dmlc logging h 235 10 45 00 src operator cudnn convolution inl h 113 Check failed cudnnConvolutionForward s dnn handle alpha in desc data ptr data offset g filter desc wmat ptr weight offset g conv desc algo workspace dptr forward workspace byte beta out desc out ptr out offset g CUDNN STATUS SUCCESS 10 45 00 cm shared scratch rengan DL mxnet dmlc core include dmlc logging h 235 10 45 00 src engine threaded engine h 306 10 45 00 src operator cudnn convolution inl h 113 Check failed cudnnConvolutionForward s dnn handle alpha in desc data ptr data offset g filter desc wmat ptr weight offset g conv desc algo workspace dptr forward workspace byte beta out desc out ptr out offset g CUDNN STATUS SUCCESS An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging The alexnet has the same error as inception bn Does anyone know why I got these errors Thanks Regards Rengan,,yajiedesign,2016-09-28 18:43:29,2017-09-28 06:48:30
IS,problem about setup py on nnvm branch,when using L7 to install mxnet by then the libmxnet so is not installed because when using import mxnet is will export error if we change from from distutils core import setup to from setuptools import setup then it will be ok how to fix this,,"tornadomeet,tqchen,yajiedesign",2016-09-29 02:55:23,2017-09-28 06:48:53
IS,speech demo baselines,Guys someone pointed out this repo to me I'm a little confused where you are getting your kaldi baselines for AMI from they seem quite a bit worse than what we have checked in right now BTW the AMI s5b recipe is probably a better starting point than AMI s5 But in either case your results are quite a lot worse than what we have checked in,,"antinucleon,antinucleon,yzhang87,yzhang87,yajiedesign",2016-09-29 03:35:55,2017-09-28 06:48:57
IS,alexnet parameter amount inconsistent,I tried to utilize the example code in example image classification symbol alexnet py However when I checked the shape to compute the parameter amount of the network I found that it was not consist to the number presented in the original paper 60M I guessed the problem occurs between the pooling layer and the fully connected layer but I had no idea how to solve it Does anyone know how to fix it to make it correspond to what the original paper mentions Thanks,,yajiedesign,2016-09-22 02:45:57,2017-09-28 06:49:00
IS,Model Parallelism,Does mxnet have the plan to support model parallelism on distributed environment It means to split cerntain layers into sublayers which run across different machines although much larger communication cost is introduced while under situations where model couldnot be contained on single machine it is a useful feature,,"sxjscience,sxjscience,piiswrong,mli,yajiedesign",2016-09-29 03:16:28,2017-09-28 06:49:04
IS,C predict with GPU,Can you provide example how I have to load input data and download output from GPU In API I see only mx float data MXNet loads data to GPU by himself,,"thirdwing,yajiedesign",2016-09-29 18:48:45,2017-09-28 06:49:07
IS,Master test failing,javelinjs Scala Julia and R tests are failing in master Could you guys look into it,,"piiswrong,pluskid,piiswrong,mli,thirdwing,mli,thirdwing,mli,yajiedesign",2016-09-29 23:33:44,2017-09-28 06:49:23
IS,Neural art,Neural art mxnet Neural art deepart io Neural art mxnet,,yajiedesign,2016-10-01 07:07:54,2017-09-28 06:49:27
IS,use fit with a network with two inputs with different sized images,Hi I can build a model with two input but i can not train it I would build one model with two different input and in next levels the network merge the Feature map Thank you,,yajiedesign,2016-09-29 21:54:14,2017-09-28 06:49:30
IS,Training testing only layers,I am now trying to implement the paper which detects faces and regresses their bounding boxes at the same time For BB regression I use L 2 distance metric based loss but in testing I want to use different evaluation metric rather than simply using L 2 distance For that I want to use some intermediate layers within the network but I found that I can only use the outputs from get outputs function in evaluation Caffe for instance has the phase keyword so that the network structure can vary between training and testing phases Dose mxnet have a similar functionality,,yajiedesign,2016-10-04 07:51:26,2017-09-28 06:49:33
IS,request move add L1 L2 regularization to layers symbols,I think it would be great to move the L2 and adding L1 regularization from the solver to the layers symbols I believe Keras does this In this way L1 L2 can be specified per layer and not global Also all solvers will just work Gradient will just be modified by each layer What do you think Is that easy to do,,yajiedesign,2016-10-04 22:16:15,2017-09-28 06:49:36
IS,How to make a prediction of heatmap matrix when y can only be dimension 1,How to make a prediction of heatmap matrix when y can only be dimension 1 I intend to predict the pixel location of the body pose where each pixel is a probability of the pose Hence I should have a matrix of the same size as the input image However I am encountering the following error when y is a matrix ValueError Label must be 1D or 2D with 2nd dimension being 1,,"piiswrong,yajiedesign",2016-10-04 18:18:34,2017-09-28 06:51:20
IS,Must the last layer of mxnet model be output layers like Softmax LinearRegression etc,Must the last layer of mxnet model be output layers like Softmax LinearRegression etc I want to create a model where the output layer is actually a group of the outputs of convolution layers however I bump into the problem of my data label not matching Symbol InferShapeKeyword argument name heatmap not found After my research on the issues that other posted it seems that the name of the output layer should match the name of my output layer However Group layer does not have a name My group layer consists of c1 c2 c8 where each of the c is are output of a convolution layer hence they are images All the examples on mxnet seems to have output layers defined in mxnet like Softmax and LinearRegression as the last layer Is Group as the last layer not supported For reference the following is my code,,"piiswrong,yajiedesign",2016-10-06 04:48:43,2017-09-28 06:51:34
IS,Why x y in python will call SimpleBinaryOperator but not ElementWiseSum,Hi if write x y for resnet I found that it will call to SimpleBinaryOperator but not ElementWiseSum Anyone know where is this being defined and how to change,,"zhenlinluo,piiswrong,zhenlinluo,yajiedesign",2016-10-06 05:35:07,2017-09-28 06:51:47
IS,How to set constraints for the arguments,How can I set constraints for the parameter updating for example restricting the gamma parameters of PReLU within range 0 1 Is it possible to do this within existing MXNet framework,,"taoari,piiswrong,taoari,yajiedesign",2016-10-06 14:38:00,2017-09-28 06:52:00
IS,empty shapes returned in RNN example,Following the code in example rnn rnn cell demo py I create a RNN symbol like this the returned shapes seem to be right now But according to the instructions in rnn cell demo py both c and h are time major rather than batch major I am not familiar with RNN symbol in mxnet can anyone provide some help about this Should I define h and c without using SwapAxis,,"nicklhy,yajiedesign",2016-10-08 08:28:07,2017-09-28 06:52:14
IS,How to reshape data from a line to k 120,My data is in line format each line is k x 120 length how to write a reader how to set bucket key default bucket key and shapes Is there an example,,yajiedesign,2016-10-08 09:12:31,2017-09-28 06:52:27
IS,How can I make a facial landmark prediction using the params and json files on windows,I have trained a usable facial landmark model on linux using python but now I want to use the model to make prediction by C on windows I am not familiar with C very much is anyone can give me some instructions,,"thirdwing,yajiedesign",2016-09-29 10:19:57,2017-09-28 06:52:41
IS,Does evaluation on the last batch have ignored the padded data,I am wondering how to exactly evaluate the testing images using python code Following example image classification I use python code to train and evaluate the model I find that mx model does not explicitly handle the padded data of the last batch for validation or testing data In mx model predict L613 the padded data have been removed from the outputs which is what I expected I have tried some methods to evaluate the testing datset 1 Using a proper testing batch size but it is not feasible when the test set size is a large prime number And I got an error of Slice end index out of range when testing batch size training batch size related to this issue 1978 2 Write my own code to call the mx model predict after each epoch to evaluate the validation set Please tell me if I am wrong and hope mxnet has handled this problem for exact evaluation,,yajiedesign,2016-10-10 05:33:36,2017-09-28 06:53:11
IS,How to give the head gradient properly when last layer is not loss layer,Above is my code I am not sure if my head gradient is provided properly Even if it is not right now the training is not working For all the 8 outputs the predicted values are the same This is definitely not right Does anyone know how can I train a network whereby the last layer is not a loss layer,,"winstywang,yajiedesign",2016-10-08 12:55:17,2017-09-28 06:53:14
IS,What does Please pass all the input Symbols via positional arguments instead of keyword arguments mean,I am having this problem when using the UpSampling layer,,"piiswrong,winstywang,yajiedesign",2016-10-04 18:03:52,2017-09-28 06:53:18
IS,Getting more done in GitHub with ZenHub,Hola has created a ZenHub account for the dmlc organization ZenHub is the only project management tool integrated natively in GitHub created specifically for fast moving software driven teams How do I use ZenHub To get set up with ZenHub all you have to do is download the browser extension and log in with your GitHub account Once you do you ll get access to ZenHub s complete feature set immediately What can ZenHub do ZenHub adds a series of enhancements directly inside the GitHub UI Real time customizable task boards for GitHub issues Multi Repository burndown charts estimates and velocity tracking based on GitHub Milestones Personal to do lists and task prioritization Time saving shortcuts like a quick repo switcher a Move issue button and much more Add ZenHub to GitHub Still curious See more ZenHub features or read user reviews This issue was written by your friendly ZenHub bot posted by request from ZenHub Board,,yajiedesign,2016-10-11 03:53:18,2017-09-28 06:53:21
IS,can it possible run loop in mxnet,I would like to write loop code to tuning dnn parameters in case that to avoid loading large data everytime The code as below qq 20161006104232 And I found when the first loop finished it will be hunt after starting to the second loop qq 20161006104541 Ca not it possbile to run loop in python code the command as below launch py H hosts2 n2 s1 python train py kv store dist sync train dir train path val dir val path save model prefix model dnn batch size 800 num epochs 100 gpu 0 1 2 3 momentum 0 8 num examples train examples lr factor 0 4,,"tornadomeet,yajiedesign",2016-10-06 02:48:15,2017-09-28 06:53:24
IS,How should I create the functions provide data and provide label properly in DataIter class,I have encountering the following error when I run my program Traceback most recent call last File stacked hourglass v2 mxnet py line 111 in module eval metric mx metric MSE File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 742 in fit self init params dict data provide data data provide label TypeError unsupported operand type s for 'instancemethod' and 'instancemethod' I am not sure what I am doing wrong,,"tornadomeet,yajiedesign",2016-10-05 14:32:36,2017-09-28 06:53:28
IS,is it ok for loadding data only once in mxnet,I would like to make sure data are loaded once How can i change the python code It can not work above the code,,"tornadomeet,yajiedesign",2016-10-09 12:41:41,2017-09-28 06:53:31
IS,What will mxnet convolution do if the dilate shape is greater than the input shape,For example the input shape is 1 100 and the dilate is 3 3 I have tried to set the dilate to 1 3 and the error is mxnet mshadow mshadow extension pack col2patch h 53 Check failed sshape 1 o height o width imshape ProdShape 0 dstdim 3 PackColToPatchEx p src size 1 mismatch,,"winstywang,winstywang,winstywang,yajiedesign",2016-10-08 09:12:51,2017-09-28 06:53:38
IS,NCE Loss has dead argument,xlvector the argument num label is not used anywhere in the function definition nce loss data label label weight embed weight vocab size num hidden num label in nce py Should be removed,,"sbodenstein,tornadomeet,xlvector,yajiedesign",2016-10-05 09:57:18,2017-09-28 06:53:41
IS,Implementing multi instance and hierarchical multi instance learning,Hello all I have written two papers about solving multi instance learning problems with neural network formalism Simple mil and an extension to two multi instance problems one embedded into the other multi mil I have implemented everything in my library but as you can expect it is loosing its breath Would it be possible to implement such functionality in mxnet such that it can run gpu Thank you very much for the answer Tomas,,"piiswrong,piiswrong,piiswrong,yajiedesign",2016-10-13 07:38:44,2017-09-28 06:53:44
IS,Low efficiency of variable lstm,The gpu utilization of variable lstm is very low and unstable it seems symbol construction causes this low efficiency We do not use bucket strategy so sym gen is called every batch 1 The blank between batches is very huge which cause gpu utilization to 0 And the cudamemset green part costs most of the compute time So how to eliminate the blank between computations and reduce the cudamemset times,,"sxjscience,pluskid,sxjscience,pluskid,yajiedesign",2016-09-01 02:38:04,2017-09-28 06:53:48
IS,i want to know how to speed up when i use gpu mode to predict,i want to know how to speed up when i use gpu mode to predict we cost a lot when we use gpu to wait for gpu preparition,,yajiedesign,2016-10-14 07:14:51,2017-09-28 06:53:54
IS,The predict function in 'base module py' of the new interface module some wrong,I train a model using the new interface 'module' Here if the batch size is smaller than 100 then it will output the following error MXNetError 20 14 41 include mxnet ndarray h 244 Check failed shape 0 end Slice end index out of range But it will be all right if the batch size is not smaller than 100 Why,,"piiswrong,piiswrong,yajiedesign",2016-10-06 12:21:21,2017-09-28 06:53:58
IS,ValueError Unknown initialization pattern for rnn0 state,Xavier init mx init Xavier rnd type gaussian factor type in magnitude 2 0 init rnn state mx nd zeros shape 177 10 256 ctx mx gpu 0 Xavier init 'fc8 weight' init rnn state I tried Xavier Normal Uniform initializer all raise the error seems mx symbol rnn does not support mx symbol initializer,,"pluskid,yajiedesign",2016-10-16 16:23:55,2017-09-28 06:54:01
IS,Bash scripts for easy installation of MXNet,It would be very help full if we can provide a bash script that users can run and get all dependencies and mxnet installed and ready to use With this we will have high conversion rate from people seeing getting started page and actually getting started To start with we can cover common OS and simple installation mechanism But this would cover maximum customer base At minimum following flavors should be covered 1 Ubuntu MXNet for Python CPU Only 2 Amazon Linux RHEL MXNet for Python CPU Only 3 Mac OS MXNet for Python CPU Only Following this we can cover script based installation for R Scala Julia Then GPU based MXNet installation Where do you think these scripts reside Another separate repo dmlc mxnet ezinstall or a folder within this repo say utilities,,"sandeep-krishnamurthy,sandeep-krishnamurthy,yajiedesign",2016-10-16 18:28:22,2017-09-28 06:54:04
IS,Improvement in set up and installation guide,We can improve set up and installation guide to make it more easier smoother and faster for users to install mxnet TODOs Have a separate install chapter under getting started Have quick installation script for common installation requirement Example MXNet for Python on Ubuntu RHEL Amazon Linux Mac OS for CPU only This would cover 80 of our user base and get them started and running initial examples of mxnet in minutes We need more re structuring of contents OS wise set up wise like pre built binary or build from source etc Language proof read and improve content quality Move set up issues to Common Problems section in this guide,,"sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,yajiedesign",2016-10-14 14:01:23,2017-09-28 06:54:08
IS,About time major indexing speedup,Hi I tried to reproduce the speedup of the time major indexing for sequence learning However I did not observe speedup I am using the latest clone from the master branch compiled with CUDA 7 5 and CuDNN v5 under Windows Server 2012R2 The training log Any ideas about this Thanks,,"piiswrong,pluskid,yajiedesign",2016-10-16 08:10:12,2017-09-28 06:54:11
IS,How to get error in each batch iteration,I am now using mxnet and want to get error in each update in optimizer py But can not find any method to get the current error in each iteration there are no function like def get lr to get the error directly Thank you very much,,yajiedesign,2016-10-12 09:36:40,2017-09-28 06:54:14
IS,Pooling formula seems wrong to cause Googlenetv1 not working,If trying using mxnet the error report mxnet base MXNetError InferShape Error in pooling4 13 02 47 src operator pooling inl h 200 Check failed param kernel 0 dshape 2 2 param pad 0 param kernel 1 dshape 3 2 param pad 1 kernel size exceed input I think the reason is that the pooling formula in Mxnet may be wrong If change the formula to oshape 2 1 dshape 2 2 param pad 0 param kernel 0 param stride 0 1 param stride 0 Then it run well Are you aware of this issue,,"zhenlinluo,sxjscience,zhenlinluo,sxjscience,piiswrong,zhenlinluo,sxjscience,yajiedesign",2016-10-06 00:11:59,2017-09-28 06:54:19
IS,will lr scheduler FactorScheduler bring different loss,I am fine tuning a model using I found it very strange that for first several batches using model args and not using it brings me two different loss same training data 0 01 and 0 09 respectively Here self epoch size 40000 self lr factor epoch 2 self lr factor 0 1 Anyone could explain why this happens,,yajiedesign,2016-10-17 22:14:56,2017-09-28 06:54:22
IS,Can it affect the accurary if skip the code in model py,I would like to skip the train metric in model py because it cost too much time during trainning And can it possible to affect the program result evaluate at end so we can lazy copy executor manager update metric eval metric data batch label,,"piiswrong,yajiedesign",2016-10-17 09:25:19,2017-09-28 06:54:29
IS,Something wrong with CUDA driver version and CUDA runtime,There is an exception which someone reported several days ago I try to train a model with gpu it crashes and throws an exception saying raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 10 34 49 src storage storage cc 48 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA CUDA driver version is insufficient for CUDA runtime version with centos 7 0 cuda 8 0 Driver Version 361 77 2 days ago I fixed this problem But it is appear again when changing the cuda toolkit to cuda 7 5 Making matters even worse attempt to recovering the toolkit to cuda 8 0 is useless I have no idea what happend,,"miguelgfierro,yajiedesign",2016-10-11 02:29:44,2017-09-28 06:54:36
IS,Any API support python generator function,Hi is there any API for python that supports getting data from generator Specifically like the fit generator API in keras,,yajiedesign,2016-09-04 09:10:14,2017-09-28 06:54:39
IS,How to get forward error in optimizer py,Is there are anyone know how to get forward error in optimizer py as currently we could only get gradient results in optimizer py Many thanks in advance,,"piiswrong,piiswrong,piiswrong,yajiedesign",2016-10-17 15:15:43,2017-09-28 06:54:42
IS,Install on linux,The installation guide of linux is missed How can i find it,,"Ldpe2G,yajiedesign",2016-10-19 04:12:02,2017-09-28 06:54:48
IS,IndentationError in the file example fcn xs image segmentaion py line 22,Problem IndentationError unindent does not match any outer indentation level in line 22 You should delete a space in line 22 to run the demo,,yajiedesign,2016-10-19 06:27:29,2017-09-28 06:54:51
IS,how to just train final layer,I would like to train final layer with previous layer fixed Can I do something like this in symbol file Now what will the learning rate be in layer 6 and layer 7 There might be 3 cases layer 6 and 7 both are 0 001 layer 6 is 0 while layer 7 is 0 001 layer 6 is 0 while layer 7 is 0 01 default value Which one is correct,,"wangg12,yajiedesign",2016-10-18 03:45:30,2017-09-28 06:54:59
IS,convert caffe model failed,the newest caffe has changed their data format in tools caffe converter convert symbol py the code if len proto input dim 0 input dim proto input dim elif len proto input shape 0 input dim proto input shape 0 dim is not suitable yet for example in older version of caffe alexnet model name AlexNet input data input shape dim 10 dim 3 dim 227 dim 227 now changed to name AlexNet layer name data type Input top data input param shape dim 10 dim 3 dim 227 dim 227,,yajiedesign,2016-10-21 08:14:20,2017-09-28 06:59:28
IS,link not available,,,"piiswrong,yajiedesign",2016-10-22 14:01:29,2017-09-28 06:59:32
IS,Speedometer log always start from batch 2,I feed the data with python io interface set the speedometer to 1 the mx callback Speedometer log always start from batch 2 I think it should start from 1 I can not find the reason and I am not sure whether it will affect the result Here is code DataIter code is,,yajiedesign,2016-10-24 07:19:11,2017-09-28 06:59:39
IS,Symbol GetName only works for non grouped symbol,I use the following code to construct a grouped symbol got the Error symbol cc 489 Check failed node e source get Symbol GetName only works for non grouped symbol I want to get the softmax activation to compute metrics how should I do this,,yajiedesign,2016-10-24 22:04:51,2017-09-28 06:59:42
IS,How to set the mxnet environment variable,Since the official Docs just show the types of the Environment Variable It dose not tell us how to set,,yajiedesign,2016-10-24 02:20:38,2017-09-28 06:59:46
IS,The DataIter problem with symbol list arguments,In a LSTM bucketing network we have tow forms of data Iterator to provide the data One is provide the data wrapped together I believe the error arises from the change of bucket but I'm not quite sure Look forward to your reply,,yajiedesign,2016-10-25 07:36:23,2017-09-28 06:59:49
IS,Asking for advices about tuning wavenet,I am trying to reproduce the result of WaveNet and the code is in I have worked on this for several days but the net still can not converge I beg some advices to help me tune the training process At present I am suspicious of three points 1 MNXet convolution op can not pad only one side so I pad the data by Concat 2 The SoftmaxOutput I am not familiar with multi output I think there may be something wrong in the current implementation 3 the dilate bug I modified the convolution code according to Is there some one can confirm the fix,,"piiswrong,zihaolucky,zihaolucky,yajiedesign",2016-10-18 08:41:53,2017-09-28 06:59:53
IS,Could not open readthedocs documents,Could not open below linkage provided by cifar10 recipe ipynb for Image RecordIO Iterator Seems that I also could not open all the MXNET document on the readthedocs io such as the documents about using symbol Would you please help Thanks,,"tornadomeet,tornadomeet,yajiedesign",2016-10-25 04:08:52,2017-09-28 06:59:57
IS,cifar10 32 32 does not work in ipython,When I modified the image size in the ipython notebook in the example for the cifar10 to 32 32 The kernel always restarts Would you please help Thanks,,yajiedesign,2016-10-26 01:26:34,2017-09-28 07:00:00
IS,How to use the parameters in im2rec py,I have some problems when I use the exts in parametes of im2rec py It is right if I use the default value but It is error if I use it like exts ' jpg' ' jpeg' Why,,yajiedesign,2016-10-26 03:10:01,2017-09-28 07:00:07
IS,GPU accelerated javascript,Does this library use GPU with javascript too If yes is it restricted to a library like CUDA or is it flexible,,yajiedesign,2016-10-26 13:49:25,2017-09-28 07:00:11
IS,multi gpu support of joint training of rcnn example is broken,not sure if this is due to the modifications to executor group py in 23bf60cab87ff0e4cf02deccdbc2ca9791d9d821 the error message is AssertionError all data must have the same batch size batch size 2 but label has shape 1 72900 raised from decide slices in executor group py the maximum shape is wouldata' 2 3 1440 1440 'label' 1 72900 'bbox target' 1 36 90 90 'bbox inside weight' 1 36 90 90 'bbox outside weight' 1 36 90 90 'gt boxes' 2 500,,"neodooth,winstywang,yajiedesign",2016-10-27 10:20:15,2017-09-28 07:00:18
IS,what are the values in kernel dshape pad,Hi All MXNET users When I run googlenet and other networks many times I have the following errors mxnet dmlc core include dmlc logging h 235 11 27 37 src operator pooling inl h 200 Check failed param kernel 0 dshape 2 2 param pad 0 param kernel 1 dshape 3 2 param pad 1 kernel size exceed input For ILSVRC2012 ImageNet data I resized the images to 256 when creating rec database When I run train imagenet py with 4 GPUs I tried several settings 1 batch size 144 and data shape 256 2 batch size 128 and data shape 224 3 batch size 256 and data shape 224 only 1 was working and the other two had the above errors The gdb for python cannot be used on my server now and I cannot install it So I cannot debug the program to check the values in these parameters Could anyone explain what are the values in kernel dshape and pad So that I can know which values should be chosen Thanks Regards Rengan,,"piiswrong,winstywang,winstywang,yajiedesign",2016-09-29 18:44:37,2017-09-28 07:00:21
IS,Could not set FactorSchedule,When I tried to set a factor schedule in cifar10 recipe ipynb I got below error message Would you please help Thanks AttributeError Traceback most recent call last ipython input 10 01000f16f948 in module 9 model mx model FeedForward ctx mx gpu symbol softmax num epoch num epoch 10 learning rate 0 00003 momentum 0 9 wd 0 00001 11 lr scheduler mx misc FactorScheduler 2 12 In this example learning rate will be reduced to 0 1 previous learning rate for every 2 epochs AttributeError 'module' object has no attribute 'misc',,"piiswrong,piiswrong,yajiedesign",2016-10-26 04:32:36,2017-09-28 07:00:29
IS,How can I use Xavier to initial parameters while not using model FeedForward,Now I want to train CIFAR 10 and I use simple bind to do it because I want to add some operations more flexibly However currently I can only get about 48 validation error which is not good at all I guess the problem might be using random initialization without carefully chosen The below is my initialization for r in exec arg arrays r np random randn r shape So how can I apply Xavier to better initialize in this situation Thanks,,yajiedesign,2016-10-25 05:27:29,2017-09-28 07:00:32
IS,Question on SequenceMask OP,mxnet symbol SequenceMask It said If sequence length is false then each example in the batch is assumed to have the max sequence length and this operator becomes the identity operator First I think the sequence length parameter here mentioned should be use sequence length Then this document means this op works only if I set use sequence length is true otherwise it is just an identity OP Therefore why do we need this parameter Why it should not be true by default,,"sbodenstein,yajiedesign",2016-10-28 19:45:15,2017-09-28 07:00:36
IS,one problem occurred when trainning fcn xs the log as follows I have no idea what happend and I just did it according to readme how to fix it would someone like to help me out thanks so much,dl dl mxnet example fcn xs run fcnxs sh INFO root Namespace epoch 74 init type 'vgg16' model 'fcn32s' prefix 'VGG FC ILSVRC 16 layers' retrain False home dl mxnet example fcn xs data py 79 VisibleDeprecationWarning using a non integer number instead of an integer will result in an error in the future img img rand start rand start min hw home dl mxnet example fcn xs data py 80 VisibleDeprecationWarning using a non integer number instead of an integer will result in an error in the future label label rand start rand start min hw INFO root Start training with gpu 0 13 04 12 home dl mxnet dmlc core include dmlc logging h 235 13 04 12 src operator crop inl h 117 Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height 13 04 12 home dl mxnet dmlc core include dmlc logging h 235 13 04 12 src engine threaded engine h 306 13 04 12 src operator crop inl h 117 Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 13 04 12 src engine threaded engine h 306 13 04 12 src operator crop inl h 117 Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging run fcnxs sh line 3 5298 Aborted core dumped python u fcn xs py model fcn32s prefix VGG FC ILSVRC 16 layers epoch 74 init type vgg16,,"tornadomeet,tornadomeet,yajiedesign",2016-10-27 05:37:25,2017-09-28 07:00:43
IS,fcn xs on multi gpu,when i run the example fcn xs i replace the ctx mx gpu 0 with ctx mx gpu 0 mx gpu 1 there is the error message below Traceback most recent call last File fcn xs py line 73 in module main File fcn xs py line 29 in main fcnxs args fcnxs auxs init fcnxs init from vgg16 ctx fcnxs fcnxs args fcnxs auxs File home bingbing example fcn xs init fcnxs py line 26 in init from vgg16 fcnxs args k mx nd zeros v shape ctx File usr local mllib mxnet python mxnet ndarray py line 842 in zeros arr empty shape ctx dtype File usr local mllib mxnet python mxnet ndarray py line 578 in empty return NDArray handle new alloc handle shape ctx False dtype File usr local mllib mxnet python mxnet ndarray py line 65 in new alloc handle ctypes c int ctx device typeid AttributeError 'list' object has no attribute wouldevice typeid' when i just use ctx mx gpu 0 it works well i want to know whether this example just can use single gpu,,yajiedesign,2016-10-29 12:45:54,2017-09-28 07:00:46
IS,Some questions about Embedding Layer,Questions about Embedding layer 1 What is the function of Embedding layer In Keras it says Turn positive integers indexes into dense vectors of fixed size eg 4 20 0 25 0 1 0 6 0 2 But what is the meaning When will it be used 2 I try some simple codes to see how the Embedding layer works I use the code in the xlvector blog 201 seriesdata 1 i for i in range 100 101 i for i in range 100 for i in range 10000 k random randint 0 199 count 1000 seriesdata k for j in range count dis random random 10 price seriesdata k math sqrt 1 0 dis print str price ' t' str dis ' t' str k dis dis mx symbol Variable wouldis' price price mx symbol Variable 'price' price interval price interval mx symbol Variable 'price interval' series series mx symbol Variable iseries' series out mx symbol Embedding data series input dim 200 output dim 100 name series embed ne mx symbol Flatten series out name series flatten arg out aux ne infer shape series 6 1 print arg print ne list arguments print out execute ne bind ctx mx cpu args wouldis flatten' seriesdata execute forward d out execute outputs 0 print d out asnumpy IN console 6L 1L 200L 100L iseries' iseries embed weight' 6L 100L Then throw an error raise ValueError 'Must specify all the arguments in s' arg key ValueError Must specify all the arguments in args I know it means that I did not give values to the series embed weight But in this example which values to give it,,yajiedesign,2016-10-30 04:32:56,2017-09-28 07:00:49
IS,Process finished with exit code 139 interrupted by signal 11 SIGSEGV,I am building a CNN RNN model for speech recognition But it seems a lot problems with MXNet is rnn API I followed the rnn example and trained my network finally after someone is help But when it comes to the predict step some issues occur Here are my codes x mx io NDArrayIter data Pxxs arr label None batch size 10 shuffle False last batch handle 'pad' pred self model predict x hint Pxx shape 10 1 x y batch size chnnel w h Then Process finished with exit code 139 interrupted by signal 11 SIGSEGV Since My codes is work pretty well in my single cnn network experienments pred self model predict Pxx hint Pxx is a numpy array with shape 1 1 x y So what is the true skills of MXNet is RNN API here are my codes of the symbol CNN RNN data mx symbol Variable wouldata' rnn h init mx sym SwapAxis mx sym Variable 'LSTM init h' dim1 0 dim2 1 rnn c init mx sym SwapAxis mx sym Variable 'LSTM init c' dim1 0 dim2 1 rnn params mx sym Variable 'LSTM bias' print infer shape data conv1 mx symbol Convolution data data kernel 3 5 num filter 64 stride 1 4 bn1 mx symbol BatchNorm data conv1 fix gamma False momentum 0 9 eps 1e 5 relu1 mx symbol Activation data bn1 act type relu pool1 mx symbol Pooling data relu1 pool type max kernel 2 2 stride 2 2 print infer shape pool1 conv2 mx symbol Convolution data pool1 kernel 3 3 num filter 128 stride 1 2 bn2 mx symbol BatchNorm data conv2 fix gamma False momentum 0 9 eps 1e 5 relu2 mx symbol Activation data bn2 act type relu pool2 mx symbol Pooling data relu2 pool type max kernel 2 2 stride 2 2 print infer shape pool2 conv3 mx symbol Convolution data pool2 kernel 3 3 num filter 256 stride 1 2 bn3 mx symbol BatchNorm data conv3 fix gamma False momentum 0 9 eps 1e 5 relu3 mx symbol Activation data bn3 act type relu pool3 mx symbol Pooling data relu3 pool type max kernel 1 2 stride 1 2 print infer shape pool3 conv4 mx symbol Convolution data pool3 kernel 2 2 num filter 512 stride 1 1 bn4 mx symbol BatchNorm data conv4 fix gamma False momentum 0 9 eps 1e 5 relu4 mx symbol Activation data bn4 act type relu print infer shape relu4 reshape1 mx symbol Reshape data relu4 shape 10 512 1 swapaxis1 mx symbol SwapAxis data reshape1 dim1 0 dim2 1 swapaxis2 mx symbol SwapAxis data swapaxis1 dim1 0 dim2 2 bilstm mx symbol RNN data swapaxis2 bidirectional False num layers 3 mode 'lstm' state size 128 state outputs False state rnn h init state cell rnn c init parameters rnn params print infer shape bilstm swapaxis3 mx symbol SwapAxis data bilstm dim1 0 dim2 1 print infer shape swapaxis3 mean state mx symbol sum data swapaxis3 axis 1 177 print infer shape mean state fc1 mx symbol FullyConnected data mean state num hidden num classes neonscript withrnn mx symbol SoftmaxOutput data fc1 label label name isoftmax',,yajiedesign,2016-10-30 08:44:00,2017-09-28 07:00:53
IS,Matlab c api gives wrong predict result for binary classification,Thanks for providing Matlab binding for mxnet The Matlab binding works well when I use to predict multi class classification but gives wrong result when I try to do binary classification The predicted probability is always like 0 9998 0 0002 the index 0 is near 1 while index 1 is near zero Can someone help me solve the problem,,yajiedesign,2016-10-31 02:43:09,2017-09-28 07:00:57
IS,I have a question about the startup speed of the mxnet,I test 4 desktops with the same project as follow 1 gtx 1080 cuda 8 0 cudnn v5 A 2 gtx 960 cuda 7 5 cudnn v5 B 3 gtx 750ti cuda 7 5 cudnn v4 C 4 gtx 750ti cuda 7 5 cudnn v4 D time cost by the prepared stage is different for A 37s for B 2s for C 3s for D 80s is there any dependency can influence this,,"piiswrong,antinucleon,piiswrong,yajiedesign",2016-10-26 08:32:30,2017-09-28 07:01:00
IS,How can I shuffle the iterator and split it to several sets,Is there any function to shuffle an iterator and split it into several sets I want to split the Cifar training data into 3 sets but only find a shuffle parameter in mxnet io ImageRecordIter How can I split it Many thanks,,yajiedesign,2016-10-28 02:16:25,2017-09-28 07:01:07
IS,How to choose batch size when using multi GPU,Hi All I used MXNet to run the GoogleNet and Inception BN networks in examples image classification I got good speedup in both networks but the batch size choice is different in these two models In GoogleNet the batch size is constant 144 for 1 GPU 2 GPUs and 4 GPUs But in Inception BN the batch size is 64 for 1 GPU 128 for 2 GPUs and 256 for 4 GPUs Based on the description in the batch size should be constant for 1 GPU and multiple GPUs This is because when multiple GPUs are used the samples in one batch is distributed to all GPUs But why for Inception BN we need to change the batch size Another post also changed the batch size Regards Rengan,,"piiswrong,zihaolucky,mli,yajiedesign",2016-10-31 21:23:59,2017-09-28 07:01:16
IS,accuracy level on 50,I tried to learn mxnet on my data but achieved only 50 This is my arch But i rly need mxnet so my question is What am i doing wrong,,"piiswrong,mli,yajiedesign",2016-10-29 10:48:27,2017-09-28 07:02:30
IS,How to slice a tensor along its first axis axis 0,For some reason I have to slice the feature tensor from N C to N separate feature tensor 1 C I know that I can do it like f out mx sym SliceChannel data f in num outputs N axis 0 if the batch size N is known But that requires me to add a parameter in my symbol building function like get symbol num classes batch size Can I get that value dynamically when calling this symbol building function Or just set num outputs 1 to let mx sym SliceChannel slice each row as a default way,,"nicklhy,piiswrong,nicklhy,yajiedesign",2016-11-03 02:44:18,2017-09-28 07:02:33
IS,reldiff in NNVM branch,I find that the reldiff in NNVM has been changed and the 1e07 here seems to be a typo L100 Should we make it the same as the master,,"sxjscience,piiswrong,yajiedesign",2016-11-03 08:44:55,2017-09-28 07:02:43
IS,The training speed of Mxnet suddenly decrease,When I train a regression Network the beginning speed is 2200 samples sec But in the processing of the first eopch the speed suddenly decrease to 220 samples sec then it keep the low speed I do not open other software to take up the GPU 2016 11 02 15 50 19 936 Node Start training with gpu 0 begin 2016 11 02 15 51 19 011 Node Epoch 0 Batch 50 Speed 2017 78 samples sec Train mae 31 301492 2016 11 02 15 51 24 811 Node Epoch 0 Batch 100 Speed 2206 82 samples sec Train mae 20 411160 2016 11 02 15 51 30 775 Node Epoch 0 Batch 150 Speed 2146 44 samples sec Train mae 20 642383 2016 11 02 15 51 36 021 Node Epoch 0 Batch 200 Speed 2440 01 samples sec Train mae 20 432849 2016 11 02 15 51 41 300 Node Epoch 0 Batch 250 Speed 2425 11 samples sec Train mae 20 718793 2016 11 02 15 51 46 459 Node Epoch 0 Batch 300 Speed 2481 18 samples sec Train mae 20 457319 2016 11 02 15 51 51 770 Node Epoch 0 Batch 350 Speed 2410 05 samples sec Train mae 20 486700 2016 11 02 15 51 57 262 Node Epoch 0 Batch 400 Speed 2330 90 samples sec Train mae 20 212012 2016 11 02 15 52 03 188 Node Epoch 0 Batch 450 Speed 2160 01 samples sec Train mae 20 293134 2016 11 02 15 52 08 851 Node Epoch 0 Batch 500 Speed 2260 49 samples sec Train mae 20 548091 2016 11 02 15 52 15 020 Node Epoch 0 Batch 550 Speed 2074 89 samples sec Train mae 20 329031 2016 11 02 15 52 20 982 Node Epoch 0 Batch 600 Speed 2147 09 samples sec Train mae 20 594568 2016 11 02 15 52 26 703 Node Epoch 0 Batch 650 Speed 2237 56 samples sec Train mae 20 301958 2016 11 02 15 53 16 995 Node Epoch 0 Batch 700 Speed 254 51 samples sec Train mae 20 488441 2016 11 02 15 54 15 848 Node Epoch 0 Batch 750 Speed 217 49 samples sec Train mae 20 526115 2016 11 02 15 55 13 267 Node Epoch 0 Batch 800 Speed 222 92 samples sec Train mae 20 733269 2016 11 02 15 56 09 373 Node Epoch 0 Batch 850 Speed 228 14 samples sec Train mae 20 550915 2016 11 02 15 57 10 218 Node Epoch 0 Batch 900 Speed 210 37 samples sec Train mae 20 642852 2016 11 02 15 58 08 562 Node Epoch 0 Batch 950 Speed 219 39 samples sec Train mae 20 234574 2016 11 02 15 59 05 891 Node Epoch 0 Batch 1000 Speed 223 28 samples sec Train mae 20 285285 2016 11 02 16 00 08 299 Node Epoch 0 Batch 1050 Speed 205 10 samples sec Train mae 20 178782 2016 11 02 16 01 05 101 Node Epoch 0 Batch 1100 Speed 225 35 samples sec Train mae 20 175192 2016 11 02 16 02 02 977 Node Epoch 0 Batch 1150 Speed 221 16 samples sec Train mae 20 575023 2016 11 02 16 03 01 791 Node Epoch 0 Batch 1200 Speed 217 64 samples sec Train mae 20 448277 2016 11 02 16 03 57 848 Node Epoch 0 Batch 1250 Speed 228 34 samples sec Train mae 20 382529 2016 11 02 16 04 55 615 Node Epoch 0 Batch 1300 Speed 221 58 samples sec Train mae 20 583595 2016 11 02 16 05 52 932 Node Epoch 0 Batch 1350 Speed 223 32 samples sec Train mae 20 528768 2016 11 02 16 06 51 230 Node Epoch 0 Batch 1400 Speed 219 57 samples sec Train mae 20 258879 2016 11 02 16 07 48 905 Node Epoch 0 Batch 1450 Speed 221 93 samples sec Train mae 20 676554 2016 11 02 16 08 45 845 Node Epoch 0 Batch 1500 Speed 224 80 samples sec Train mae 20 580773 2016 11 02 16 09 42 883 Node Epoch 0 Batch 1550 Speed 224 41 samples sec Train mae 20 413611 2016 11 02 16 10 39 731 Node Epoch 0 Batch 1600 Speed 225 16 samples sec Train mae 20 254984 2016 11 02 16 11 36 080 Node Epoch 0 Batch 1650 Speed 227 16 samples sec Train mae 20 607114 2016 11 02 16 12 32 962 Node Epoch 0 Batch 1700 Speed 225 03 samples sec Train mae 20 629045 2016 11 02 16 13 29 893 Node Epoch 0 Batch 1750 Speed 224 84 samples sec Train mae 20 326704 2016 11 02 16 14 27 821 Node Epoch 0 Batch 1800 Speed 220 96 samples sec Train mae 20 359617 2016 11 02 16 15 24 795 Node Epoch 0 Batch 1850 Speed 224 67 samples sec Train mae 20 677470 2016 11 02 16 16 22 750 Node Epoch 0 Batch 1900 Speed 220 86 samples sec Train mae 20 715191 2016 11 02 16 17 19 177 Node Epoch 0 Batch 1950 Speed 226 84 samples sec Train mae 20 459832 2016 11 02 16 18 16 480 Node Epoch 0 Batch 2000 Speed 223 38 samples sec Train mae 20 572316 2016 11 02 16 19 12 243 Node Epoch 0 Batch 2050 Speed 229 54 samples sec Train mae 20 498098 2016 11 02 16 20 09 616 Node Epoch 0 Batch 2100 Speed 223 10 samples sec Train mae 20 377475 2016 11 02 16 20 20 782 Node Epoch 0 Resetting Data Iterator 2016 11 02 16 20 20 782 Node Epoch 0 Time cost 1748 278,,"piiswrong,piiswrong,yajiedesign",2016-11-02 08:21:28,2017-09-28 07:02:46
IS,Where the req type is defined when do backward,Hi In Backward definition if OpReqType req is not what I want how I can change it and where Thanks,,"zhenlinluo,piiswrong,zhenlinluo,yajiedesign",2016-11-03 04:44:52,2017-09-28 07:02:49
IS,can pooling convention be set as full by default,Currently most symbol still does not have pooling convention parameter the default value is valid Why can it be set as full by default It can help all future training to base on the new formula and avoid potential shape issue like googlenet in deepmark can not be run by default until it add pooling convention full But this require all pre trained model to be retrained again What do you think about this,,"zhenlinluo,piiswrong,yajiedesign",2016-11-03 21:07:03,2017-09-28 07:02:52
IS,Does mxnet provide Model Average for distributed training,Now I would like to train the example code train cifar10 resnet py with two local workers is there any existed model average implementation I can directly use For example CNTK has this kind of feature Or should I write the corresponding update rules for the kv store Thanks a lot,,"piiswrong,yajiedesign",2016-11-04 07:12:05,2017-09-28 07:02:58
IS,mean operator,Why is there no mxnet symbol mean operator May be I'm missing something but I do not see how to easily replicate it with sum as I would need to infer shapes and thus connect the symbols to data already,,"piiswrong,yajiedesign",2016-11-04 21:57:34,2017-09-28 07:03:01
IS,Cudnn of two different GPU cards,My computer has two different GPU cards The GFX 1060 supports Cudnn and NVS 290 does not I will only use the 1060 to run MXNET Is there any problem if I keep both cards and make the installation with Cudnn supported,,"piiswrong,yajiedesign",2016-11-04 20:54:38,2017-09-28 07:03:05
IS,Second order derivative,Any ideas how to calculate second order derivative a mx sym Variable 'a' b mx sym FullyConnected data a name 'fc1' num hidden 64 gr b grad wrt 'a' gr2 gr grad wrt 'a' fails with Do not support Backward of Backward,,yajiedesign,2016-11-05 01:40:09,2017-09-28 07:03:13
IS,Problem with SoftmaxOutput in multi output prediction,Hi all I want to use a network to predict a 19 19 matrix After using the fully connected layer I use a SoftmaxOutput layer with multi output is True Each label is a 19 19 matrix which has only one position equal to 1 Then I tried to flatten the labels and the predictions changed to 1 361 def getnet data mx symbol Variable wouldata' conv1 ConvFactory data data num filter 128 kernel 5 5 pad 2 2 conv2 ConvFactory data conv1 num filter 128 kernel 3 3 pad 1 1 conv3 ConvFactory data conv2 num filter 128 kernel 3 3 pad 1 1 conv4 ConvFactory data conv3 num filter 128 kernel 3 3 pad 1 1 conv5 ConvFactory data conv4 num filter 128 kernel 3 3 pad 1 1 conv6 ConvFactory data conv5 num filter 128 kernel 3 3 pad 1 1 conv7 ConvFactory data conv6 num filter 128 kernel 3 3 pad 1 1 conv8 ConvFactory data conv7 num filter 128 kernel 3 3 pad 1 1 conv9 ConvFactory data conv8 num filter 128 kernel 3 3 pad 1 1 conv10 ConvFactory data conv9 num filter 128 kernel 3 3 pad 1 1 conv11 ConvFactory data conv10 num filter 128 kernel 3 3 pad 1 1 conv12 ConvFactory data conv11 num filter 128 kernel 3 3 pad 1 1 conv13 ConvFactory data conv12 num filter 1 kernel 1 1 flatten mx symbol Flatten data conv13 name 'flatten' mlp mx symbol SoftmaxOutput data conv13 name isoftmax' multi output True return mlp The input data shape is 48 19 19 and the output label shape is 19 19 I have seen the 1404 and tried the accuracy function def array acc label pred return float np sum np square label pred Can anyone please advise how I can make it work correctly Thanks,,"piiswrong,yajiedesign",2016-11-04 10:45:06,2017-09-28 07:03:17
IS,3D Support for Pooling and Convolution,We would really like to have full 3D support for convolutions and pooling for both GPU CPU and will start contributing the missing pieces This issue is to track the various tasks and progress get comment and feedback and coordinate efforts if others would also like to contribute The following needs to be implemented CPU 3D convolution already have cuDNN GPU CPU GPU 3D dilated convolution CPU 3D Max Mean Total Pooling already have cuDNN for GPU CPU GPU 3d Pad layer The strategy adapt corresponding Torch layers Everything will be NCDHW format D depth of course the order of the spatial dims does not matter,,"sbodenstein,piiswrong,yajiedesign",2016-11-03 09:42:28,2017-09-28 07:03:20
IS,Ca not use mxnet after Quick Installation,I' can not use mxnet after Quick Installation I'm using Ubuntu 16 04 GTX1060 cuda8 0 cudnnv5 1 import mxnet as mx a mx nd ones 2 3 mx gpu 11 28 30 home quantumliu MXNet mxnet dmlc core include dmlc logging h 235 11 28 30 src storage storage cc 78 Compile with USE CUDA 1 to enable GPU usage Traceback most recent call last File stdin line 2 in module File home quantumliu MXNet mxnet python mxnet ndarray py line 894 in ones arr empty shape ctx dtype File home quantumliu MXNet mxnet python mxnet ndarray py line 611 in empty return NDArray handle new alloc handle shape ctx False dtype File home quantumliu MXNet mxnet python mxnet ndarray py line 69 in new alloc handle ctypes byref hdl File home quantumliu MXNet mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 11 28 30 src storage storage cc 78 Compile with USE CUDA 1 to enable GPU usage,,yajiedesign,2016-11-06 03:32:11,2017-09-28 07:03:23
IS,element wise logical operator,is there any element wise logical operator in mxnet for example greater or less than or how to implement by current function example code a mx nd array 0 1 1 0 b mx nd array 1 0 1 0 print type a b current output is a normal python bool type type 'bool' and only one value but i want output to be an element wise array like False True False False,,yajiedesign,2016-11-06 02:39:54,2017-09-28 07:03:27
IS,Is it possible to change mx symbol SoftmaxOutput is grad scale on the fly,For an algorithm I am developing I need to scale the gradients starting backpropagating from the SoftmaxOutput units by a scalar which changes in every iteration of the training Since I believe SoftmaxOutput does not accept gradients from below layers as it is meant to be the last operation calling backward with a proper out grads parameter does not work Therefore currently in each iteration I connect a new mx symbol SoftmaxOutput to the last feature layer with the newly calculated scalar for that iteration in the grad scale parameter and then bind a new executor object Is there any way to change grad scale parameter of the SoftmaxOutput once the executor has been bound I obvserved that binding a new executor in each iteration with the way I explained above introduces a significant amount of overhead What would be the best way to that instead Thanks in advance,,"piiswrong,yajiedesign",2016-11-07 00:54:08,2017-09-28 07:03:30
IS,Add new optimizer Eve Adam improvement,Hi there is new improvement of Adam algorithm called Eve I would be nice to add this as a new optimizer results are interesting,,"piiswrong,yajiedesign",2016-11-07 16:01:46,2017-09-28 07:03:37
IS,Change threading module to multiprocessing module in PrefetchingIter python,I use a lot of small images and process some random crop and resize which are very cpu oriented works I find that PrefetchingIter in python module are implemented using threading module which does not work like real thread as we expect Can you change threading module to multiprocessing module in PrefetchingIter to get full performance of multicore CPU I try to change it but somehow it hang and can not debug,,"piiswrong,yajiedesign",2016-11-07 23:29:42,2017-09-28 07:03:40
IS,Backward OpReqType is not respected by a few layers,The following layers seem not to use the Assign macro or any custom code to actually implement the OpReqType that they are passed during Backward Spatial Transfomer Correlation Crop SoftmaxOutput SwapAxis I may be missing some I did not check exhaustively This is a bit troubling though it should not be hard to fix We have not actually verified with tests that these layers will do the wrong thing or crash in the case of no ingrad being allocated but that seems probable what do you make of this,,"taliesinb,piiswrong,tqchen,taliesinb,yajiedesign,taliesinb",2016-11-07 14:14:30,2017-09-28 07:03:44
IS,How can I get a local version of the documentation,,,yajiedesign,2016-11-08 11:34:22,2017-09-28 07:03:47
IS,train cifar10 resnet py multiple machines training error,try to run the train cifar10 resnet with 2 machines using command like tools launch py n 2 H hosts sync dst dir tmp mxnet python tmp mxnet train cifar10 resnet py kv store dist sync batch size 64 gpus 0 it spits multiple error msgs of Traceback most recent call last File ctypes callbacks c line 314 in 'calling callback function' File tmp mxnet mxnet kvstore server py line 38 in server controller optimizer pickle loads cmd body File usr lib python2 7 pickle py line 1382 in loads return Unpickler file load File usr lib python2 7 pickle py line 858 in load dispatch key self File usr lib python2 7 pickle py line 1090 in load global klass self find class module name File usr lib python2 7 pickle py line 1126 in find class klass getattr mod name AttributeError 'module' object has no attribute 'Nesterov' I can successfully run the training on each of the machine locally without problem However when running using two machines it can not work What could be the problem,,"winstywang,mli,mli,antinucleon,yajiedesign",2016-10-06 00:16:01,2017-09-28 07:03:50
IS,mx apply crash,Something as simple as this crashes both on Mac and Windows require mxnet a mx symbol Variable name a b mx symbol Variable name b c a b res mx apply x c a 1 b 4 name sum ctx mx ctx default Anything forbidden here Should not crash the whole thing in any case,,yajiedesign,2016-11-08 14:16:18,2017-09-28 07:03:53
IS,NDArray slice axis bug unary ndarray function got an unexpected keyword argument 'axis',Hello I just downloaded the latest source code for the mxnet library and compiled it Using the new libmxnet dll my code breaks where I use slice axis method of mx ndarray module I tried it for the following simple example as well array mx ndarray zeros shape 10 20 30 sliced mx ndarray slice axis src array axis 0 begin 2 end 3 I get following error sliced mx ndarray slice axis src array axis 0 begin 2 end 3 TypeError unary ndarray function got an unexpected keyword argument 'axis' This was not present until I recently started to use the new mxnet code Could there be a bug introduced somewhere,,"piiswrong,sxjscience,yajiedesign",2016-11-07 12:00:44,2017-09-28 07:03:56
IS,Problems using csviter,In the example module mnist mlp py validation accuracy is computed as follows The validation sample number is 9534 and the batchsize is 128 9534 128 74 48 which means there should be 74 full batch and the last batch is not full This error happens because the the preds abandons the last batch I do not know how to fix this MnistIter does not have this problem Then a strange thing happens if we uncomment the commented line which means we compute the preds twice which seems to me overwriting which makes no difference no error will be reported but the computed validation accuracy is very low wrong I think it should because of the last batch causes the label shift but can not understand why it happens and how to fix it Could anyone give some explanation,,yajiedesign,2016-11-08 15:36:24,2017-09-28 07:03:59
IS,Plan to migrate from mxnet readthedocs io to mxnet io for mxnet docs,Team Today I spent time in diving deep in to task of migrating mxnet readthedocs io to mxnet io Below is the summary of issues and solution proposal Please do let me know your thoughts and suggestions Issue 1 Google search for mxnet and a concept brings mxnet readthedocs io site up in the search results Many links are actually broken Why are we getting slowed down from implementing 301 redirects Limitation of readthedocs site and not able to modify the md file since they are common across mxnet io and readthedocs site 1 Readthedocs site Admin panel will allow configuring re directs only within same domain It will not allow re directs to other domain Example We can redirect to but not to 2 Readthedocs site will do redirection only on 404 pages It will not allow redirection of valid webpages This means with readthedocs settings we can never redirect from pages that exists both in mxnet readthedocs io site and mxnet io site 3 We cannot have redirection code written in our docs file because we are using same md files in our mxnet io site as well However one interesting functionality of readthedocs site is that we can specify branch from which docs needs to be built Proposed solution 1 We create a new page for notifying users that we have moved to new site mxnet io 2 Set up configuration in readthedocs site admin panel for redirecting all 404 to this migration page From this migration page we will bring users to mxnet io site 3 We cut a branch for docs migration Update all md file with http 301 direct html code Update readthedocs site to pick this branch for docs site Since we continue to have mxnet io site on latest branch It continues to be independent 4 Bring down mxnet readthedocs io site at right time when we see search engine is prioritizing mxnet io over mxnet readthedocs io Feedback suggestions welcome We need to fix this broken links wrong links from search engine issue as soon as possible This might be a good first step towards it skm,,"sandeep-krishnamurthy,yajiedesign",2016-11-09 08:36:10,2017-09-28 07:04:02
IS,LSTM Regression model,Will it be possible to fit a regression model using current implementation of LSTM in mxnet I tried it but not able to run it I am able to run MLP implementation using rmse as activation engine Training works fine but when predicted gives only one value as output In case of feed forward model I am not getting any value when fitting the train data All the eval metrics ie train rmse values are NA is Same with predictions I use the same variable R array for all three models I am not able to understand why mlp can fit a model Though it gives only single prediction value whereas Feedforwards fits NA values and LSTM throws error P S I am new to learning neural nets may be the question is too basic or amaetuer Excuse if it is so,,yajiedesign,2016-11-09 12:38:26,2017-09-28 07:08:11
IS,CSVIter batchsize directly influences the validation,Following is the definition of the data validation iterator Cannot figure out why,,yajiedesign,2016-11-10 03:42:56,2017-09-28 07:08:15
IS,Multiple node training never ends,image As shown above the process of training ends but the program does not I set PS VERBOSE 2 and these are the outputs after training The lower Terminal shows the current network traffic status I have no idea what is going on here I notice that there are ' id' in it does it lose track of certain processes My running script DMLC INTERFACE em3 PS VERBOSE 2 python mxnet tools launch py launcher ssh n 2 s 1 H hosts2 python train mnist py lr 0 05 kv store dist sync batch size 4096 num epochs 4 gpus 0 1 2 3,,"mli,yajiedesign",2016-11-10 06:40:33,2017-09-28 07:08:19
IS,lstm inference simple bind error,I write an symbol for lstm like When I do simple bind I will get following error mxnet base MXNetError 12 03 50 src symbol graph executor cc 578 Check failed graph InferNodeShapes topo order out shapes aux shapes false Shape inference cannot be complete in bind But if I let output mx FullyConnected hidden num hidden xxxx everything is ok When node name casue above error is mul5 0 out grad agg I find it has two input shapes but both of them is empty,,"xlvector,piiswrong,xlvector,xlvector,xlvector,yajiedesign",2016-11-11 04:05:15,2017-09-28 07:08:22
IS,gradient cost lots of memory when large batch size,why gradient cost so lots of memory is that normal symbol bind ctx mx gpu 0 args args args grad grad grad req ' ' batch size 10,,yajiedesign,2016-11-11 01:36:55,2017-09-28 07:08:26
IS,installation error for scala package,I try to install mxnet for scala package following the installation guide After building the shared library then execute the command make scalapkg But I got an error Failed to execute goal net alchim31 maven scala maven plugin 3 2 2 compile default on project mxnet init 2 11 Execution default of goal net alchim31 maven scala maven plugin 3 2 2 compile failed CompileFailed Anybody knows how to solove it Thank you so much,,yajiedesign,2016-11-14 01:48:30,2017-09-28 07:08:32
IS,how does mxnet process multi branches network,take inception network for example a calculates them parallelly as much as possible b calculates them one after one which one,,yajiedesign,2016-11-11 15:44:19,2017-09-28 07:08:35
IS,cudnn v5 problem on win7,I am running mxnet on python win7 cuda8 0 It is good Then I added cudnn v5 15 The program output an error mxnet src operator cudnn c onvolution inl h 296 Check failed cudnnSetFilter4dDescriptor filter desc dt ype format param num filter param num group data shape 1 param num group param kernel 0 param kernel 1 CUDNN STATUS SUCCESS Does the current version support cudnn v5 15,,"piiswrong,yajiedesign",2016-11-09 03:20:19,2017-09-28 07:08:38
IS,Numerical Gradient Improvement,sxjscience I noticed that the numerical gradient checker L260 uses f x h f x h instead of the centered difference method f x h f x h 2h The numerical error in the latter is of order O h 2 and works much better in practice whilst the former is O h I have noticed that one needs to give a very large tolerance when doing grad checks in the test suite perhaps because of this method Is there a reason for using the less precise method Or can I change it,,"sbodenstein,sxjscience,sbodenstein,sxjscience,sxjscience,sbodenstein,yajiedesign",2016-11-02 07:52:53,2017-09-28 07:08:48
IS,SSD example has missing files,Havent you forgotten to add the files related to the scale layer Here you can see them in the original mxnet ssd repository image Thanks,,"piiswrong,piiswrong,zhreshold,zhreshold,zhreshold,yajiedesign",2016-11-12 16:51:41,2017-09-28 07:08:51
IS,Wrong hierarchy in the mxnet doc,345 should be the sub point of 2,,"piiswrong,sandeep-krishnamurthy,yajiedesign",2016-11-10 04:50:14,2017-09-28 07:08:54
IS,CustomOp hangs while converting numpy array to NDArray,I'm writing a new mx operator CustomOp and in the forward method the engine hangs no CPU activity when converting my numpy array to an NDArray mxy mx nd array y I have MXNET CPU WORKER NTHREADS 2 as the example and docs say which smells like there is some kind of race condition at play And in fact if I turn this off then the sample code in example numpy ops custom softmax py fails in the same operation So I'm guessing I'm hitting whatever bug can sometimes be avoided by setting the using multiple worker threads but in my case that is not a sufficient workaround What else can I try,,"leopd,piiswrong,leopd,leopd,yajiedesign",2016-11-13 22:52:02,2017-09-28 07:08:57
IS,Anyone can help me with the build check after an PR,I have make an PR 3811 but fails in checks It is very stange to me Could anyone help me with this I check the all log of the build check it seem to be noting with my commits the build logs 8080 job mxnet 412 consoleText 8080 job mxnet 412 consoleText,,"burness,zhreshold,burness,yajiedesign",2016-11-16 01:53:44,2017-09-28 07:09:01
IS,im2rec with unchanged 1 report error when prediction,Report error when produce image record file train rec with parameter with unchanged 1 and test the iterator will report following error 23 10 24 home jjzhao mxnet dmlc core include dmlc logging h 235 23 10 24 src io iter batchloader h 83 Check failed unit size i d data i Size But if produce train rec without unchanged 1 will have no such issue may I know why,,yajiedesign,2016-11-16 15:29:22,2017-09-28 07:09:04
IS,ResNet for ILSVRC12 dataset,I was looking for the ResNet config file for the ILSVRC12 dataset but I only find the one for the Cifar10 dataset Do we have a ResNet config file for the ILSVRC12 dataset Thank you very much,,"piiswrong,tornadomeet,mli,yajiedesign",2016-11-15 15:44:57,2017-09-28 07:09:08
IS,How to use mxnet RNN symbol,Hi all I have found RNN symbol is added in mxnet v0 7 Now I'm trying to use it to impl lstm in example rnn with python But I have no idea because there is no document or any information of the input and output Can anyone give me any advice thanks,,"pluskid,philipskokoh,yajiedesign",2016-08-25 10:02:31,2017-09-28 07:09:11
IS,Documentation error in image aug default cc,In src io image aug default cc documentations between parameter declaration part and default value setting DMLC DECLARE PARAMETER do not match For instance fill value should be filled color while padding but it is described as Maximum value of illumination variation and so on,,"piiswrong,yajiedesign",2016-11-14 09:03:11,2017-09-28 07:09:15
IS,What rpc framework is used in Mxnet,What RPC framework is used in Mxnet Does Mxnet implement RPC by self Where is the RPC code of Mxnet Thanks,,"piiswrong,tqchen,yajiedesign",2016-11-18 00:22:41,2017-09-28 07:09:18
IS,Expandable sidebars are collapsed by default,Now it requires multiple clicks to expand the sidebars to see the page layout,,"andremoeller,piiswrong,leopd,yajiedesign",2016-11-18 16:56:19,2017-09-28 07:09:21
IS,Auto generated Scala Package API docs,We are using Doxygen to auto generate API docs for CPP package This is very helpful for developers and power users of mxnet We need something similar for Scala and R packages For Scala package there is not out of box support from Doxygen May be we should use scaladoc Feedback and suggestions from the community is welcome,,"sandeep-krishnamurthy,piiswrong,Ldpe2G,yzhliu,andremoeller,yajiedesign",2016-11-17 10:43:19,2017-09-28 07:09:24
IS,Embedinig operator failed when export MXNET BACKWARD DO MIRROR 1,How to fix this problem And is that necessary to check the req type in forward pass I think it is use only useful in backward,,yajiedesign,2016-11-18 09:25:03,2017-09-28 07:09:27
IS,cudaMalloc failed out of memory while training fcn32s,My device is TitanX Maxwell and dataset is VOC2012 When I run run fcns sh it will raise Error cudaMalloc failed out of memory So I set cut off size 400 and I get the same error At last I changes cut off size 300 and it woks I want to know why mxnet is fcnxs costs much more GPU memory than caffe,,yajiedesign,2016-11-21 06:33:23,2017-09-28 07:09:30
IS,auto module index js does not work properly,Some changes made in API pages for example changing the link content of left sidebar makes auto module index js failed Now some entries such as Symbol Creation API Reference has no child level any more,,"kevinthesun,piiswrong,yajiedesign",2016-11-21 07:36:28,2017-09-28 07:09:34
IS,Heteroscedastic dropout uncertainty,Can heteroscedastic dropout uncertainty be enabled in MXNet This follows from 3821 I now have networks predicting with dropout uncertainty but I believe this is homoscedastic uncertainty see Yarin Gal is blog post Yarin provides an example ConvnetJS code for heteroscedastic uncertainty L971 but I do not know enough to apply this to MXNet myself Can anyone advise on how to implement heteroscedastic regression layers in MXNet It seems this would be a very useful feature for all regression purposes Many thanks,,"tornadomeet,tqchen,yajiedesign",2016-11-16 16:07:29,2017-09-28 07:09:40
IS,What unit test have for rnn,mli do you have unit test operator for rnn so far,,"zhenlinluo,piiswrong,sxjscience,yajiedesign",2016-11-21 18:59:23,2017-09-28 07:10:46
IS,There are two predictions when a picture run twice Why,There are two predictions when a picture run twice I train a OCR model use CNN net when I predict a pic one time the result is correct but run twice the first is correct and the second is wrong code batch size 1 sym arg params aug params mx model load checkpoint ocr 0 data shape data batch size 3 50 200 input shapes dict data shape sym getnet executor sym simple bind ctx mx gpu input shapes for key in executor arg dict keys if key in arg params arg params key copyto executor arg dict key executor forward is train False data mx nd array img probs executor outputs 0 asnumpy a np argmax probs 0 b np argmax probs 2 print a b when run testRecognizeOne 0 jpg testRecognizeOne 0 jpg the output 2 3 0 0 why,,"xlvector,yajiedesign",2016-11-21 15:33:20,2017-09-28 07:10:49
IS,Can an ImageRecord pack images as formats other than jpg or png,I have a dataset that consists of millions of int16 images actual range is about 4000 4000 ints To my knowledge opencv stores and loads jpgs pngs as uint8 only pack img L244 implies that imagerecord can only handle these two format types Is there another way to load int16 images for training I do not wish to convert to uint8 Tiff for example can store this range I would also like to utilize the random crop and random mirror functions as millions of int16 images already take up significant disk space Best GPS,,"piiswrong,jmerkow,yajiedesign",2016-11-18 23:52:16,2017-09-28 07:10:52
IS,About SequenceLast symbol and asnumpy Segmentation fault core dumped,it reports following mistake Segmentation fault core dumped Look forward to your reply Thanks,,"pluskid,piiswrong,sbodenstein,yajiedesign",2016-11-22 15:14:50,2017-09-28 07:10:55
IS,OpenCV3 in installing MXNet on Mac OS X 10 11 6,I am going to install MXNet Python on a Mac based on the installation steps from standard installation But I already have OpenCV3 installed with Homebrew Q1 Can I use my installed OpenCV3 for MXNet installation If I can then do I need to make some change from the standard installation procedures Q2 If I have to use OpenCV2 4 for MXNet can I install OpenCV 2 4 with Homebrew Wonder if this may cause some problems because of two OpenCV of different versions are installed by Homebrew Not sure if this is the right place to ask the question please help Thanks,,"piiswrong,yajiedesign",2016-11-23 06:25:36,2017-09-28 07:11:10
IS,Question Slow calculation and mulit thread issues pyhton,Hi for learning reasons I implement the stochastic gradient descent by myself with mxnet Basically I do in my learning loop the following steps 1 Select a sample from the learning data by random 2 Copy it to the executor memory 3 do forward calculation 4 calculate head gradient 5 do backward calculation 6 adapt weights of executor I iterate this code around 50000 times and it takes around 15 seconds on my computer 15 seconds does not sound much however if I need to search 50 different arguments for the best fit with 10 fold crossvalidation then we talk about around 2 hours of training time And this is bad I guess the internal learning algorithms from the library are much faster but by using them there is only little I can learn I also notices that it really does not matter how many samples are calculated per iteration If I calculate for validation 200 samples in the same loop does not increase the learning time much as long as I copy the data just once to the test executor memory So setting up the memory for the train executor seems to be costly here not the calculation of the executor Does anyone has an idea how to speed up my learning I also tried to learn different arguments learning rate regularization rate on different threads at the same time Here every thread has its own training and test executors and writes only local data So they should not share any resources However I got race condition errors and the threads slows down it calculation enormously when running in parallel So I guess even by using different executors they have some kind of shared resource by mxnet I would be happy if someone can give me some hints about multi threading with mxnet,,yajiedesign,2016-11-23 08:07:29,2017-09-28 07:11:14
IS,When I compiled mxnet with the ssd example I got erro,3 E opensource mxnet master build Debug libmxnet lib E opensource mxnet master build Debug libmxnet exp 3 multibox detection obj error LNK2019 class mxnet Operator cdecl mxnet op CreateOp struct mshadow gpu struct mxnet op MultiBoxDetectionParam int CreateOp Ugpu mshadow op mxnet YAPEAVOperator 1 UMultiBoxDetectionParam 01 H Z public virtual class mxnet Operator cdecl mxnet op MultiBoxDetectionProp CreateOperatorEx struct mxnet Context class std vector class mxnet TShape class std allocator class mxnet TShape class std vector int class std allocator int const CreateOperatorEx MultiBoxDetectionProp op mxnet UEBAPEAVOperator 3 UContext 3 PEAV vector VTShape mxnet V allocator VTShape mxnet std std PEAV vector HV allocator H std 7 Z 3 multibox prior obj error LNK2019 class mxnet Operator cdecl mxnet op CreateOp struct mshadow gpu struct mxnet op MultiBoxPriorParam int CreateOp Ugpu mshadow op mxnet YAPEAVOperator 1 UMultiBoxPriorParam 01 H Z public virtual class mxnet Operator cdecl mxnet op MultiBoxPriorProp CreateOperatorEx struct mxnet Context class std vector class mxnet TShape class std allocator class mxnet TShape class std vector int class std allocator int const CreateOperatorEx MultiBoxPriorProp op mxnet UEBAPEAVOperator 3 UContext 3 PEAV vector VTShape mxnet V allocator VTShape mxnet std std PEAV vector HV allocator H std 7 Z 3 multibox target obj error LNK2019 class mxnet Operator cdecl mxnet op CreateOp struct mshadow gpu struct mxnet op MultiBoxTargetParam int CreateOp Ugpu mshadow op mxnet YAPEAVOperator 1 UMultiBoxTargetParam 01 H Z public virtual class mxnet Operator cdecl mxnet op MultiBoxTargetProp CreateOperatorEx struct mxnet Context class std vector class mxnet TShape class std allocator class mxnet TShape class std vector int class std allocator int const CreateOperatorEx MultiBoxTargetProp op mxnet UEBAPEAVOperator 3 UContext 3 PEAV vector VTShape mxnet V allocator VTShape mxnet std std PEAV vector HV allocator H std 7 Z 3 cuda compile 1 generated convolution cu obj error LNK2019 cudnnConvolutionBackwardFilter v3 public virtual void cdecl mxnet op CuDNNConvolutionOp float Backward struct mxnet OpContext const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector enum mxnet OpReqType class std allocator enum mxnet OpReqType const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector class mxnet TBlob class std allocator class mxnet TBlob const Backward CuDNNConvolutionOp M op mxnet UEAAXAEBUOpContext 3 AEBV vector VTBlob mxnet V allocator VTBlob mxnet std std 11AEBV vector W4OpReqType mxnet V allocator W4OpReqType mxnet std 6 11 Z 3 cuda compile 1 generated deconvolution cu obj error LNK2001 cudnnConvolutionBackwardFilter v3 3 cuda compile 1 generated convolution cu obj error LNK2019 cudnnConvolutionBackwardData v3 public virtual void cdecl mxnet op CuDNNConvolutionOp float Backward struct mxnet OpContext const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector enum mxnet OpReqType class std allocator enum mxnet OpReqType const class std vector class mxnet TBlob class std allocator class mxnet TBlob const class std vector class mxnet TBlob class std allocator class mxnet TBlob const Backward CuDNNConvolutionOp M op mxnet UEAAXAEBUOpContext 3 AEBV vector VTBlob mxnet V allocator VTBlob mxnet std std 11AEBV vector W4OpReqType mxnet V allocator W4OpReqType mxnet std 6 11 Z 3 cuda compile 1 generated deconvolution cu obj error LNK2001 cudnnConvolutionBackwardData v3 3 E opensource mxnet master build Debug libmxnet dll fatal error LNK1120 5,,"piiswrong,yajiedesign",2016-11-23 08:08:15,2017-09-28 07:11:17
IS,problem with CNN for text classification with multiple convolutional layers,Dear all I would like to create a CNN for text classification with multiple convolutional layers Here is my code but I am lost when I want to do the second convolutional layer input x mx sym Variable wouldata' this is a set of sentences each sentence is composed of DNA letters A T G C or N input y mx sym Variable isoftmax label' label for each sentence num embed 5 A T G C or N batch size 50 sentence size 100 filter size 8 embed layer mx sym Embedding data input x input dim num embed output dim num embed name 'vocab embed' conv input mx sym Reshape data embed layer shape batch size 1 sentence size num embed output shape 50 1 100 5 1st convolutional layer conv1 mx sym Convolution data conv input kernel filter size num embed num filter num filter output shape should be 50 100 93 1 no I do not know how to check the output shape relu1 mx sym Activation data conv1 act type arelu' pooled1 mx sym Pooling data relu1 pool type 'max' kernel 3 1 stride 3 1 output shape should be 50 100 31 1 no 2nd convolutional layer i would like to convert shape 50 100 31 1 in shape 50 1 31 100 so that I could rerun convolution on it but i do not know how to do at all Thanks in advance Raphael,,"pluskid,yajiedesign",2016-11-23 19:45:43,2017-09-28 07:11:20
IS,train faster rcnn use two gpus error occured like this,C WinPython python 2 7 10 amd64 python exe D MyCoding DeepLearning test example mxnet example rcnn train end2end py root path H data faster rcnn devkit path H data faster rcnn VOCdevkit frequent 50 kv store local prefix H data faster rcnn model faster rcnn pretrained H data faster rcnn model vgg16 load epoch 1 INFO root Namespace devkit path 'H data faster rcnn VOCdevkit' factor step 50000 frequent 50 gpu ids '1 0' image set 'trainval' kv store 'local' load epoch 1 lr 0 001 mom 0 9 monitor False no flip False num classes 21 num epoch 10 prefix 'H data faster rcnn model faster rcnn' pretrained 'H data faster rcnn model vgg16' resume False root path 'H data faster rcnn' test image set 'test' wd 0 0005 work load list None year '2007' INFO root TRAIN FASTER RCNN WITH APPROXIMATE JOINT END2END 'providing maximum shape' wouldata' 2 3 1000 1000 'label' 1L 34596L 'bbox target' 1L 36L 62L 62L 'bbox inside weight' 1L 36L 62L 62L 'bbox outside weight' 1L 36L 62L 62L 'gt boxes' 2 500 voc 2007 trainval gt roidb loaded from H data faster rcnn cache voc 2007 trainval gt roidb pkl append flipped images to roidb prepare roidb Traceback most recent call last File D MyCoding DeepLearning test example mxnet example rcnn train end2end py line 178 in module args work load list args resume not args no flip args factor step File D MyCoding DeepLearning test example mxnet example rcnn train end2end py line 125 in end2end train arg params args aux params auxs begin epoch begin epoch num epoch num epoch File C WinPython python 2 7 10 amd64 lib site packages mxnet 0 7 0 py2 7 egg mxnet module base module py line 338 in fit for training True force rebind force rebind File D MyCoding DeepLearning test example mxnet example rcnn rcnn module py line 137 in bind force rebind False shared module None File C WinPython python 2 7 10 amd64 lib site packages mxnet 0 7 0 py2 7 egg mxnet module module py line 282 in bind grad req grad req input types input types File C WinPython python 2 7 10 amd64 lib site packages mxnet 0 7 0 py2 7 egg mxnet module executor group py line 170 in init self label layouts self decide slices label shapes File C WinPython python 2 7 10 amd64 lib site packages mxnet 0 7 0 py2 7 egg mxnet module executor group py line 196 in decide slices s has shape s name shape AssertionError all data must have the same batch size batch size 2 but label has shape 1L 34596L Process finished with exit code 1 training end2end,,"Trangle,piiswrong,precedenceguo,tornadomeet,Trangle,precedenceguo,precedenceguo,precedenceguo,precedenceguo,yajiedesign",2016-11-15 15:36:30,2017-09-28 07:11:24
IS,example cpp image classfication can not work in Windows,compile sucessfully but found same runtime error for processing images D Program Files x86 Jenkins workspace mxnet mxnet dmlc core includ e dmlc logging h 235 12 42 16 d program files x86 jenkins workspace mxnet mxnet src operator concat inl h 152 Check failed dshape j tmp j Inc orrect shape 2 1 320 15 24 first input shape 1 576 16 25,,"piiswrong,yajiedesign",2016-11-27 04:46:32,2017-09-28 07:11:27
IS,Differences simliarity between mxnet is runtime dependency engine and neon is transformer,Quite new to mxnet and neon so the question might sound naive but it puzzles me recently In mexnet runtime dependency engine do the tricky things of scheduling parallel computing tasks which is the key difference from other ML DL libs While a quick glance of transformer from ngraph it seems this component based on llvm do the optimization with the same effect Did not jump into the code yet but in essence what is the differences and similarity of those two components from mxnet and neon,,yajiedesign,2016-11-28 05:59:53,2017-09-28 07:11:30
IS,rcnn train end2end py error Cannot find custom operator type proposal,I have fixed all the import error and print print compatibility issue to run train end2end py with Python 3 will not submit the changes since I will not test the code with Python 2 However the training will still not run,,"piiswrong,precedenceguo,precedenceguo,yajiedesign",2016-11-28 03:52:38,2017-09-28 07:11:33
IS,numpy indexing and symbolic shapes,I want to slice a 50 x N x 1 symbolic tensor to 50 x N 1 x 1 and as far as I see there is no way to do that without knowing the exact value of N at definition time You must know N at symbol definition time or otherwise you can not to the discard 1 row operation This is very easy to do in Theano you just have to do tensor 1 Also I found that there are no symbolic shapes in MXNet which is surprising because it is very useful to do operations with them a lot of the time What I mean by symbolic shapes is that when you do tensor shape where tensor is a symbol it returns a symbolic tuple containing symbolic integers lazily evaluated at runtime So I request 1 numpy indexing for symbolic tensors 2 symbolic shapes to do symbolic operations with,,"zhreshold,tqchen,tqchen,sxjscience,yajiedesign",2016-11-28 09:22:24,2017-09-28 07:11:37
IS,CPU cocurrency problem,environment ubuntu x64 16 04 LTS with mxnet installed according to the install document using the ubuntu installation script run the example train mnist py on 8 logical cpus with OPENBLAS NUM THREADS 8 MXNET CPU WORKER NTHREADS 8 MXNET CPU PRIORITY NTHREADS 8 still only 2 logical cpus used why why not get the cpu count at runtime and use them all by default as tensorflow does,,"piiswrong,yajiedesign",2016-11-27 08:31:36,2017-09-28 07:11:40
IS,C image classification of GPU data,Hi From looking at the C image classification example it looks like there is no way of signifying that the data already resides on the GPU I would like to access the very low level elements of mxnet through a C api such that I can load directly into the input gpu tensor and read from the output gpu tensor I would also like access to the streams so I can correctly utilize streams in my application Currently this looks possible but on windows a fair bit of the mxnet api is not decorated for dll export Has anyone else run into this issue,,"dtmoodie,piiswrong,dtmoodie,piiswrong,dtmoodie,piiswrong,yajiedesign",2016-11-29 21:10:02,2017-09-28 07:11:43
IS,How to save model if using a low level training loop,I follow this to train my own model But I get confused about how to save my model I know one way is set up a model when I want to save But is there a better way to do this,,"piiswrong,yajiedesign",2016-11-29 14:16:58,2017-09-28 07:11:50
IS,Feeding video frames data into LSTM,I am new to MXNET I have a video dataset in which each video has different length of frames I want to use LSTM to train a feature descriptor of the video For each moment of LSTM the input is one frame of the video Is there anyone who can show me an easy way to feed video frame data to LSTM Any suggest will be appreciated,,"sxjscience,yajiedesign",2016-11-30 12:35:24,2017-09-28 07:11:54
IS,Ca not run example code with Python 3,Python example code can not be run as they are written with Python 2 Steps to reproduce just run with Python 3 image classification python3 train mnist py 2016 11 27 01 37 30 367 Node 0 start with arguments Namespace batch size 128 data dir 'mnist ' gpus None kv store 'local' load epoch None lr 0 1 lr factor 1 lr factor epoch 1 model prefix None network 'mlp' num epochs 10 num examples 60000 save model prefix None Traceback most recent call last File train mnist py line 161 in module train model fit args net get iterator data shape File D work tmp mxnet master example image classification train model py line 51 in fit train val data loader args kv File train mnist py line 89 in get iterator impl download data dir File train mnist py line 17 in download urllib urlretrieve zippath AttributeError module 'urllib' has no attribute 'urlretrieve' This is because Python 3 no longer has urllib please migrate to Python 3 It is painful to have 2 and 3 on the system especially on Windows,,"piiswrong,leopd,jostep,yajiedesign",2016-11-27 01:41:11,2017-09-28 07:11:56
IS,Link address typo on python data loading api,On parameters for data iterator page some links such as are incorrect,,"kevinthesun,yajiedesign",2016-11-30 23:54:12,2017-09-28 07:12:00
IS,Is there a good way to free the GPU memory in python,I found that after I delete the executor the param array still in GPU is memory Even I reset python they still in GPU memory Unless I restart python it seems not possible free the GPU memory,,yajiedesign,2016-12-01 06:19:39,2017-09-28 07:12:03
IS,spell error,In src kvstore comm h line 292 It may affect the perofmrance perofmrance performance,,yajiedesign,2016-12-01 10:46:12,2017-09-28 07:12:06
IS,bin im2rec rotating images 90 degrees counter clockwise,I am using the bin im2rec tool to generate a rec file for the Kaggle Dogs vs Cats image challenge I used the following code to visualize the output of ImageRecordIter pre construct the training image iterator trainIter mx io ImageRecordIter path imgrec config TEST MX REC data shape 3 227 227 batch size 128 3 rand crop True rand mirror True preprocess threads 4 grab the first batch of images from the generator images trainIter getdata asnumpy loop over a few images for i in np arange 0 100 grab the image and transpose it to RGB ordering image images i transpose 0 2 1 astype uint8 R G B image 0 image 1 image 2 image np dstack R G B write the image to file p args output png format i 1 io imsave os path sep join p image pre However when I inspected the output files all of the images were rotated exactly 90 degrees counter clockwise I have attached a few of the examples to this note I'm just curious if I am doing something wrong or if this is a bug in the im2rec binary 1 2 3,,"jrosebr1,jrosebr1,jrosebr1,jrosebr1,jrosebr1,jrosebr1,yajiedesign",2016-12-01 00:24:13,2017-09-28 07:12:10
IS,Add CI support for Android,Now android verison are broken,,yajiedesign,2016-12-01 21:25:44,2017-09-28 07:12:12
IS,About running image segmentation py for image with size 500 281 the program crashed,Here is the error output Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height The error happened in exector forward is train False I tried some other images some succeed It seems that it has a problem with feeding in different image size,,yajiedesign,2016-12-01 22:04:04,2017-09-28 07:12:15
IS,Check failed from shape to shape operands shape mismatch,I still can not find where is the problem Can anyone help me File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 811 in fit sym gen self sym gen File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 236 in train multi device executor manager load data batch data batch File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 410 in load data batch self curr execgrp load data batch data batch File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 257 in load data batch load data data batch self data arrays File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 93 in load data load general batch data targets File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 89 in load general d src slice idx copyto d dst File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 533 in copyto return internal copyto self out other File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 1225 in unary ndarray function c array ctypes c char p c str str i for i in kwargs values File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 07 56 44 src ndarray ndarray cc 231 Check failed from shape to shape operands shape mismatch,,"Ldpe2G,yajiedesign",2016-12-01 08:01:13,2017-09-28 07:13:12
IS,How to run trained mxnet model in Java,I want to develop and experiment with models in Python but execute them in Java e g in an Android App How do I accomplish something analogous to this,,"thirdwing,zihaolucky,yajiedesign",2016-12-02 01:51:24,2017-09-28 07:13:19
IS,Master frozen until v0 9 release Please redirect pull requests to nnvm branch,We are freezing the master branch until v0 9 release With the exception of changes limited to docs folder all pull requests should be redirected to the nnvm branch v0 9 is currently under development in the nnvm branch It contains many new features and a major refactor to move the backend to nnvm Development is almost completed we now need to add more tests to ensure nothing breaks Specifically we would like to add around 50 of the examples to maintained examples which means we will add automatic tests for them An example is here Training takes too long to test so I think we can just train for a few iterations to make sure the code works but does not need to verify convergence We can probably train to convergence once every month or so Tests should pass on both master and nnvm without modification so that we can be confident there is no break Here is a tentative list of tasks to be completed before 0 9 release verify that distributed training works Maybe add it to test Simulation on a single machine is probably good enough add test for rcnn add test for ssd add some rnn models to test add dcgan to test add neural style to test add speech demo to test add caffe module to test add torch module to test test optimizer Please feel free to add more We should also manually go through the rest of the examples to make sure they run on nnvm branch,,"piiswrong,pluskid,tqchen,piiswrong,pluskid,piiswrong,zhreshold,yajiedesign",2016-12-01 05:44:11,2017-09-28 07:13:22
IS,Making mxnet examples compatible for python2 3,This is inspired by the issue 3995 as most examples in mxnet examples still only support python2 currently common incompatible issues in the mxnet examples are 1 print 2 raise errors 3 urllib 4 StringIO 5 iterator 6 dict keys to list I realized you planned to work on this Let me know your opinions on this I will try to create a pr for each compatibility update for each example Please let me know if there are any questions or concerns,,"jostep,yajiedesign",2016-12-02 17:39:58,2017-09-28 07:13:25
IS,Installation error,There is an error while installing Can u include a new setup for Android,,"kevinthesun,yajiedesign",2016-12-02 02:09:28,2017-09-28 07:13:29
IS,Loss increase when running another program on same GPU,I tried to run the following program on my GTX980 When running one instance everything works perfectly However when running anther instance the first one is loss will increase to NAN very fast so does the second one What may be the cause of this problem is this a known problem,,"sxjscience,yajiedesign",2016-11-29 04:53:02,2017-09-28 07:13:36
IS,compile with USE DIST KVSTORE 1 to use dist sync on Windows binary distribution,It seems Windows distributions do not have no distributed training support Is this due to platform problem Steps to reproduce run the commands tools launch py runs on Windows mxnet export DMLC NUM WORKER 2 mxnet export DMLC ROLE server mxnet export DMLC PS ROOT PORT 9098 mxnet export DMLC PS ROOT URI mxnet ps0 mxnet export DMLC NUM SERVER 2 python3 train mnist py kv store dist sync 21 25 50 D chhong mxnet dmlc core include dmlc logging h 235 21 25 50 D chhong mxnet src kvstore kvstore cc 41 compile with USE DIST KVSTORE 1 to use dist sync Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File D work mxnet example image classification common fit py line 95 in fit kv mx kvstore create args kv store File C Program Files Python35 lib site packages mxnet 0 7 0 py3 5 egg mxnet kvstore py line 378 in create ctypes byref handle File C Program Files Python35 lib site packages mxnet 0 7 0 py3 5 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 21 25 50 D chhong mxnet src kvstore kvstore cc 41 compile with USE DIST KVSTORE 1 to use dist sync,,yajiedesign,2016-12-04 03:28:49,2017-09-28 07:13:39
IS,error when compiling using clang omp on mac,My mac system is osx 10 11 and I brew install clang omp I change mxnet configure that cc clang omp and CXX clang omp when compiling mxnet there are many errors Part of them are shown below clang omp c O3 Wall Wno unknown pragmas Iinclude std c 0x fopenmp fPIC DDMLC USE HDFS 0 DDMLC USE S3 0 DDMLC USE AZURE 0 msse2 o line split o src io line split cc In file included from src io line split cc 2 In file included from include dmlc io h 8 usr local Cellar clang omp 2015 04 01 libexec bin include c v1 cstdio 125 9 error no member named 'FILE' in the global namespace using FILE usr local Cellar clang omp 2015 04 01 libexec bin include c v1 cstdio 126 9 error no member named 'fpos t' in the global namespace using fpos t,,"piiswrong,yajiedesign",2016-12-02 17:19:44,2017-09-28 07:13:42
IS,speech demo for decoding with kaldi,After I trained the lstm model using speech demo in mxnet I want to decode for eval data But it occurs an error The following is the one of the log file for decoding,,"piiswrong,pluskid,yzhang87,yzhang87,yzhang87,yzhang87,yzhang87,yzhang87,yzhang87,yajiedesign",2016-11-13 05:45:55,2017-09-28 07:13:46
IS,error in im2rec py Line 51,Line 51 if args shuffle is True when use shuffle True this line is false just delete is True,,yajiedesign,2016-12-05 13:40:53,2017-09-28 07:13:53
IS,do bucketing support variable length input for bidirectional lstm,It seems to use 0 padding to get fixed length in every bucket But bi lstm needs 2 direction to input could mxnet know the difference,,"piiswrong,yajiedesign",2016-12-06 07:32:23,2017-09-28 07:14:00
IS,Mxnet amalgamation project rely on intel MKL,I did not install the intel mkl lib and have installed the mxnet without mkl After i build the openblas for android make the amalgamation project for android will generate such error,,"piiswrong,zhenlinluo,glingyan,piiswrong,arank,glingyan,leezu,glingyan,yajiedesign",2016-10-31 10:34:30,2017-09-28 07:14:03
IS,How set transform in speech demo,In speech demo the default setting is not use feature transform If I want to use feature transform from Kaldi nnet1 how to set it,,yajiedesign,2016-12-07 01:30:02,2017-09-28 07:14:06
IS,caffe converter tools does not support PReLU layer in caffe,It will generate the error when run the convert model py Any ideas to solve this thanks,,yajiedesign,2016-12-07 09:10:05,2017-09-28 07:14:10
IS,Different outputs of the bn layer when converting ResNet from caffe,I am trying to convert the pre trained ResNet 50 network from deep residual networks to mxnet But it turns out that the predicted results are always slightly different from the original one Then I found the outputs of a BN layer between caffe and mxnet are always different Since caffe is BN layer does not really learn gamma beta I copied the parameters from the Scale layer to mxnet BN operators I set use global stats True and also tried fix gamma True False but the outputs are still different Any help would be appreciated,,"nicklhy,piiswrong,nicklhy,taoari,nicklhy,mli,yajiedesign",2016-08-25 08:08:26,2017-09-28 07:14:13
IS,are there some tutorials for distributed training with kubernetes,are there some tutorials for distributed training with kubernetes,,"pineking,zihaolucky,piiswrong,mli,pineking,yajiedesign",2016-12-05 11:57:02,2017-09-28 07:14:16
IS,mxnet base MXNetError InferShape Error in score is weight argument,I fine tuned fcn xs with my dataset there is no error during training but when I try to segment a new image with python image segmentation py error occurs Traceback most recent call last File image segmentaion py line 58 in module main File image segmentaion py line 49 in main exector fcnxs bind ctx fcnxs args args grad None grad req null aux states fcnxs args File home hliu MXNet mxnet python mxnet symbol py line 844 in bind ctypes byref handle File home hliu MXNet mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError,,"tornadomeet,yajiedesign",2016-12-08 03:25:58,2017-09-28 07:14:20
IS,is there anyone trianed fcn xs successed why I failed with an error,INFO root Namespace epoch 74 init type 'vgg16' model 'fcn32s' prefix 'VGG FC ILSVRC 16 layers' retrain False INFO root Start training with gpu 0 16 40 25 home dl mxnet dmlc core include dmlc logging h 235 16 40 25 src operator crop inl h 117 Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height 16 40 25 home dl mxnet dmlc core include dmlc logging h 235 16 40 25 src engine threaded engine h 306 16 40 25 src operator crop inl h 117 Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 16 40 25 src engine threaded engine h 306 16 40 25 src operator crop inl h 117 Check failed static cast int param offset 0 data shape 2 out shape 2 offset 0 should be less than the residual space of height An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging run fcnxs sh line 3 13455 Aborted core dumped python u fcn xs py model fcn32s prefix VGG FC ILSVRC 16 layers epoch 74 init type vgg16,,"tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,II-Matto,II-Matto,II-Matto,II-Matto,yajiedesign",2016-10-27 09:01:43,2017-09-28 07:14:23
IS,illumination and contrast as image augmentation parameters explained,I am trying to understand the valid values for the illumination and contrast image augmentation parameters but the docs are quite succint A value of zero means no change but what are the reasonable range of values that I can use I tried looking at the code and the relevant section seems to be in src io iter normalize h copied bellow but I get confused by the rand uniform rnd and I fail to understand the valid ranges of contrast and illumination,,"Gelu74,yajiedesign",2016-12-08 09:32:04,2017-09-28 07:14:26
IS,weird out of memory error on GPU slowdown on CPU,I got back to benchmarking various DL tools on the airline dataset For this use case I use fully connected networks though I cannot reach the accuracy of GBMs even after extensive tuning deep neural networks I have experimented with this before and that is when I suggested mxnet to implement sparse features I know this is still in the works However now with the larger EC2 GPU instances I can try to run the larger dataset 10M rows with the dense representation now it will fit in RAM with 1 hot encoding which was not the case before For comparison the 1M dataset is fast 35 sec on p2 xlarge 1 GPU and 70 sec on r3 8xlarge CPU 32 cores On the 10M dataset however on p2 8xlarge using 1 GPU only while on r3 8xlarge CPU 32 cores it runs for about 1 hour i e 100x more time then on 1M dataset How can this be For GPU I would expect same memory usage since I'm doing mini batches of 128 For CPU I would expect 10x runtime not 100x Any ideas Code here,,"piiswrong,piiswrong,yajiedesign",2016-12-03 04:42:32,2017-09-28 07:14:30
IS,when I compile mxnet with cmake an error occured,CMake Warning at D opencv3 build OpenCVConfig cmake 166 message Found OpenCV Windows Pack but it has no binaries compatible with your configuration You should manually point CMake variable OpenCV DIR to your build of OpenCV library Call Stack most recent call first CMakeLists txt 59 find package CMake Error at CMakeLists txt 59 find package Found package configuration file D opencv3 build OpenCVConfig cmake but it set OpenCV FOUND to FALSE so package OpenCV is considered to be NOT FOUND,,"hjk41,yajiedesign",2016-12-08 02:24:53,2017-09-28 07:15:10
IS,An error occured when I complie mxnet in VS2013 could you tell me how to fix it,2 symbol cc 2 G OpenSource mxnet build Release libmxnet lib G OpenSource mxnet build Release libmxnet exp 2 2 g opensource mxnet src engine threaded engine cc 298 fatal error C1001 2 f dd vctools compiler utc src p2 ehexcept c 956 2 2 Visual C 2 2 LINK fatal error LNK1257,,"kevinthesun,yajiedesign",2016-12-08 12:47:24,2017-09-28 07:15:13
IS,How to extract features of an image from a pretrained CNN model and I try but get an error mxnet base MXNetError InferType Error in transpose0 15 48 18 src operator operator util cc 402 At least one input type needs to be specified,I trained a CNN model and want to use the model to extract features of an specify image from a full connected layer Here is the code img cv2 imread code release1 1 random part val 157wpot 68 0 jpg cv2 IMREAD GRAYSCALE img img reshape 1 1 64 64 astype float32 img np multiply img 1 255 0 train lbl np array 1 dtype float32 print train lbl shape train lbl dtype img dtype print type train lbl type train lbl 0 testiter mx io NDArrayIter img train lbl 1 shuffle False testiter label mx io init data train lbl allow empty True default name 'label' prefix cnn ocr iterator 30 model mx model FeedForward load prefix iterator prob data1 label1 model predict testiter return data True then I meet the strange error like this File lstm ocr part py line 392 in module prob data1 label1 model predict testiter return data True File python mxnet model py line 616 in predict self init predictor data shapes type dict File python mxnet model py line 536 in init predictor self ctx 0 grad req 'null' type dict type dict dict input shapes File python mxnet symbol py line 685 in simple bind arg types aux types self infer type type dict File python mxnet symbol py line 417 in infer type ctypes byref complete File python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError InferType Error in transpose0 15 48 18 src operator operator util cc 402 At least one input type needs to be specified If you can give some advice I will be very glad,,yajiedesign,2016-12-08 08:04:43,2017-09-28 07:15:16
IS,BatchNorm Dropout may fail when os environ 'MXNET BACKWARD DO MIRROR' '1',Given os environ 'MXNET BACKWARD DO MIRROR' '1' mxnet being built with cudnn 8 0 v5 1 a Dropout layer directly after a BatchNorm layer will cause the below error Environment info Operating System Linux Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 7907e452752a2d2d12d167076cdf7d375c269ade Python version and distribution 2 7 5 Error Message Steps to reproduce 1 Run the above modified test case What have you tried to solve it 1 Inserting an ReLU layer between the BatchNorm and Dropout layers will suppress the error 2 Building mxnet with cudnn 7 0 v4 0 7 will suppress the error 3 Without setting os environ 'MXNET BACKWARD DO MIRROR' '1' the error will not occur,,"piiswrong,tqchen,pineking,piiswrong,pineking,tqchen,yajiedesign",2016-12-11 12:59:26,2017-09-28 07:15:23
IS,An error appear when call predictor with libmxnet predict so,I had do a make in amalgamation folder and a libmxnet predict so appear in lib with OpenBlas default config I had copy libmxnet predict so and mxnet predict py to a new folder Files list Anyway If I use libmxnet so to instead of libmxnet predict so in this point everything working well My mxnet source code is already up to date in master Thank you,,"BobLiu20,yajiedesign",2016-12-12 09:24:11,2017-09-28 07:15:26
IS,Freeze weights during training,Is there any solution to freeze weights For example I want to keep some weights unchanged when them satisfy some condition like weight 0 5 during training,,"kevinthesun,yajiedesign",2016-12-12 12:19:31,2017-09-28 07:15:33
IS,Detailed traceback information on exception,We need to have more detailed traceback on errors and exceptions For example in this questions on stackoverflow Error message is very abstract Error in mx io internal arrayiter as array data as array label unif rnds basic string M replace aux It is difficult for users to understand that the issues is related to mismatch in index value when creating a matrix or any shape of a data structure Example of similar issue and fix in Tensorflow package,,"sandeep-krishnamurthy,piiswrong,yajiedesign",2016-12-12 18:36:34,2017-09-28 07:15:36
IS,nnvm any h check type failed,nnvm branch failed when training seq2seq task following is the error info The strange thing is when i setting engine type to naive nnvm will be ok,,"piiswrong,sxjscience,piiswrong,piiswrong,tqchen,tqchen,piiswrong,piiswrong,tqchen,piiswrong,tqchen,piiswrong,tqchen,piiswrong,yajiedesign",2016-12-06 08:09:30,2017-09-28 07:15:40
IS,Something wrong with dilate Conv,I think there should be something wrong with the dilate Conv I try to run this demo below but an error raised src engine threaded engine h 306 16 51 50 mxnet mshadow mshadow extension pack col2patch h 51 Check failed sshape 1 o height o width imshape ProdShape 0 dstdim 3 PackColToPatchExp src size 1 mismatch It should mean that I feed the net with wrong shape of data But I can run this demo without part 1 or part 2 which means I should feed the net with proper data because as you can see that the two parts would not change the shapes of input and output So it is strange that why shape mismatch error will occur when combine the two parts Besides the error occurs in the process of backward If the data shape is improper it should be more reasonable that the error occurs in the process of forward Traceback most recent call last File train py line 189 in module epoch end callback mx callback do checkpoint models File home mxnet python mxnet model py line 806 in fit sym gen self sym gen File home mxnet python mxnet model py line 227 in train multi device executor manager backward File home mxnet python mxnet executor manager py line 418 in backward self curr execgrp backward File home mxnet python mxnet executor manager py line 268 in backward texec backward File home mxnet python mxnet executor py line 147 in backward ndarray File home mxnet python mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError Code batch size 1 length 41000 shape input batch size 1 2 length label batch size length net mx symbol Variable name input label mx symbol Varible name label part 1 zero 0 mx sym Variable name 'zero 0' shape 'zero 0' batch size 1 2 1 concat 0 mx sym Concat zero 0 net dim 3 name 'concat 0' net mx sym Convolution data concat 0 kernel 1 2 stride 1 1 num filter 1 part 2 zero 1 mx sym Variable name 'zero 1' shape 'zero 1' batch size 1 2 2 concat 1 mx sym Concat zero 1 net dim 3 name 'concat 1' net mx sym Convolution data concat 1 kernel 1 2 dilate 1 2 stride 1 1 num filter 1 net mx symbol Convolution data net kernel 2 1 num filter 5 name post conv net mx symbol SoftmaxOutput data net label label name softmax multi output True,,"piiswrong,yajiedesign",2016-12-12 09:05:55,2017-09-28 07:15:43
IS,9 error MSB6006 cmd exe 1 C Program Files x86 MSBuild Microsoft Cpp v4 0 V120 Microsoft CppCommon targets 170 5 mxnet,i follow the instruction from using the thirdparty containing the opencv openblas and cudnn v3 there is no problem using cmake but the problem came when i compile the mxnet cpp i dont know why it happens anyone can help me,,"kevinthesun,yajiedesign",2016-12-12 07:30:32,2017-09-28 07:15:46
IS,How to convert SSD caffemodel to mxnet,I want convert a SSD caffemodel to mxnet I have tried the caffe converter to deal with deplopy prototxt and SSD caffemodel but I get errors Traceback most recent call last File convert model py line 110 in module main File convert model py line 37 in main prob input dim proto2symbol args caffe prototxt File home ubuntu mxnet tools caffe converter convert symbol py line 194 in proto2symbol sym output name input dim proto2script proto file File home ubuntu mxnet tools caffe converter convert symbol py line 169 in proto2script raise Exception 'Unknown Layer s ' layer i type Exception Unknown Layer Normalize,,"zhreshold,yajiedesign",2016-12-13 07:12:52,2017-09-28 07:15:52
IS,does Warp CTC support bucketing,I try to use bucketing lstm and Warp CTC to do OCR task and I find the output never update when training However if I only use one bucket the output works fine So I doubt whether Warp CTC could use in a bucketing way,,yajiedesign,2016-12-14 08:57:46,2017-09-28 07:15:56
IS,GPU training error with CUDA driver version is insufficient for CUDA runtime version,Ubuntu 16 04 CUDA 8 CUDNN5 1 Driver Version 375 20 complie success but Failed in mnist example INFO root Start training with gpu 0 10 23 55 home sun mxnet dmlc core include dmlc logging h 235 10 23 55 src storage storage cc 38 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA CUDA driver version is insufficient for CUDA runtime version Traceback most recent call last File home sun LearnMxnet mnist1 py line 115 in module batch end callback mx callback Speedometer batch size 200 output progress for each 200 data batches File usr local lib python2 7 dist packages mxnet model py line 811 in fit sym gen self sym gen File usr local lib python2 7 dist packages mxnet model py line 206 in train multi device logger logger File usr local lib python2 7 dist packages mxnet executor manager py line 324 in init self slices train data File usr local lib python2 7 dist packages mxnet executor manager py line 236 in init input types data types File usr local lib python2 7 dist packages mxnet executor manager py line 150 in bind exec arg arr nd zeros arg shape i ctx dtype arg types i File usr local lib python2 7 dist packages mxnet ndarray py line 876 in zeros arr empty shape ctx dtype File usr local lib python2 7 dist packages mxnet ndarray py line 612 in empty return NDArray handle new alloc handle shape ctx False dtype File usr local lib python2 7 dist packages mxnet ndarray py line 69 in new alloc handle ctypes byref hdl File usr local lib python2 7 dist packages mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 10 23 55 src storage storage cc 38 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA CUDA driver version is insufficient for CUDA runtime version Process finished with exit code 1,,yajiedesign,2016-12-13 02:27:49,2017-09-28 07:15:59
IS,How to get the parameters of each layer using MATLAB,I want to get the parameters of each layer from a pretrained model such as the weights and biases of a convolution layer What functions can be used to attain this goal in MATLAB,,yajiedesign,2016-12-12 14:36:55,2017-09-28 07:16:02
IS,mx symbol RNN in bidirectional mode what is the layout of its output,If the hidden size is set to H and the mode is set to bidirectional the output size of mx sym RNN will be batch size sequence length H 2 Then what is the layout of this output tensor Which parts correspond to the forward output and backward output,,yajiedesign,2016-12-15 10:11:59,2017-09-28 07:16:05
IS,How to initialize parameter of mx sym RNN in Orthogonal mode,mx sym RNN accepts one single parameter vector then passes it to CuDNN is RNN interface It is unclear that which parts of this vector correspond to those weights and biases in RNN Neither MXNet nor CuDNN is documents provide a hint to the layout of this vector So how could I apply orthogonal initialization to the RNN is weights,,yajiedesign,2016-12-15 13:07:03,2017-09-28 07:16:09
IS,Keras Add support for mean operator,Add support for mean operator Refer to 4207 on how this could be done,,"shivarajugowda,yajiedesign",2016-12-15 16:41:50,2017-09-28 07:16:12
IS,Keras Add support for std deviation operator,Add support for std deviation operator Refer 4207 on how this could be done,,"shivarajugowda,yajiedesign",2016-12-15 16:42:43,2017-09-28 07:16:16
IS,training process becomes zombie process and keeps occupying GPU memory,My training program creates a child process for data IO while the model mx model FeedForward is running in the parent process But for some unknown reason I just find the parent process becomes a zombie process which keeps occupying the GPU memory The child process for io is still running background and can be killed When I run nvidia smi in cmd line part of the displayed message is Processes GPU Memory GPU PID Type Process name Usage 0 31794 C Unknown Error 5371MiB 1 7653 C python 5369MiB 1 13386 C xx xxx caffe 1205MiB 1 27707 C Unknown Error 1011MiB 2 18049 C python 5809MiB 3 11827 C python 6169MiB It seems that two different processes 31794 and 27707 became zombies at the same time top p 31794 27707 shows the process is state is Z BTW the parent of these two zombie processes is init pid 1 now Anyone seen this before Is it possible to clear them and release the GPU memory without rebooting Environment info Operating System Ubuntu 16 04 LTS Compiler GCC 5 3 Package used Python R Scala Julia Python Or if installed from source Yes MXNet commit hash git rev parse HEAD 2fefed4692007a34197df655f4e54db8942eb4ab Python version and distribution Python 2 7,,"nicklhy,nicklhy,nicklhy,nicklhy,tornadomeet,nicklhy,yajiedesign",2016-12-15 06:19:16,2017-09-28 07:16:19
IS,build mxnet in RHEL,Environment info Operating System RHEL6 Compiler GCC I have a question here Can current mxnet source code be built and installed on RHEL6 OR has anyone built it on RHEL6 I tried to build on RHEL6 but could not find any installation documentation for RHEL in installing mxnet or other websites I googled Thank you very much for your help,,"philipskokoh,mli,yajiedesign",2016-12-15 17:37:06,2017-09-28 07:16:22
IS,cudaMalloc failed out of memory,Environment info Operating System ubuntu14 04 Compiler gcc4 8 4 Package used Python R Scala Julia Python MXNet version 0 7 0 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 6 If you are using R package please provide R sessionInfo Error Message faster rcnn train error when I python train end2end py I have downsize the VOC2007 train sets to1096 images my 980 have 4G MEMORY but it still report out of memory I want to know how to solve this problem Thank you yx yx X8DTL mxnet example mx rcnn master python train end2end pyCalled with argument Namespace begin epoch 0 dataset 'PascalVOC' dataset path wouldata VOCdevkit' end epoch 10 epoch 1 flip True frequent 20 gpus '0' image set '2007 trainval' kvstore wouldevice' lr 0 001 lr step 50000 network 'vgg' prefix 'model e2e' pretrained 'model vgg16' resume False root path wouldata' work load list None 'EPS' 1e 14 'IMAGE STRIDE' 0 'PIXEL MEANS' array 123 68 116 779 103 939 'SCALES' 600 1000 'TEST' 'BATCH IMAGES' 1 'HAS RPN' False 'NMS' 0 3 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'BATCH IMAGES' 1 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' True 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'END2END' True 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 7 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 6000 'RPN PRE NMS TOP N' 12000 num images 1096 voc 2007 trainval gt roidb loaded from data cache voc 2007 trainval gt roidb pkl append flipped images to roidb 20 41 11 src engine engine cc 36 MXNet start using engine NaiveEngine providing maximum shape wouldata' 1 3 1000 1000 'gt boxes' 1 100 5 'label' 1 34596 'bbox target' 1 36 62 62 'bbox weight' 1 36 62 62 output shape 'bbox loss reshape output' 1L 128L 84L 'blockgrad0 output' 1L 128L 'cls prob reshape output' 1L 128L 21L 'rpn bbox loss output' 1L 36L 37L 50L 'rpn cls prob output' 1L 2L 333L 50L 20 41 15 home yx mxnet dmlc core include dmlc logging h 235 20 41 15 src storage pooled storage manager h 79 cudaMalloc failed out of memory Traceback most recent call last File train end2end py line 185 in main File train end2end py line 182 in main lr args lr lr step args lr step File train end2end py line 133 in train net arg params arg params aux params aux params begin epoch begin epoch num epoch end epoch File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet module base module py line 379 in fit self update File home yx mxnet example mx rcnn master rcnn core module py line 183 in update self curr module update File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet module module py line 419 in update kvstore self kvstore File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 115 in update params updater index num device k g w File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet optimizer py line 822 in updater optimizer update index weight grad states index File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet optimizer py line 298 in update grad grad self rescale grad File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 138 in mul return multiply self other File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 744 in multiply None File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 655 in ufunc helper return lfn scalar lhs float rhs File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet ndarray py line 1263 in generic ndarray function c array ctypes c char p c str str i for i in kwargs values File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 20 41 15 src storage pooled storage manager h 79 cudaMalloc failed out of memory terminate called without an active exception Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python train end2end py 2 3 What have you tried to solve it 1 Downsize the train sets but the same error still occur 2 3,,"piiswrong,mli,yajiedesign",2016-12-13 15:42:30,2017-09-28 07:16:25
IS,Keras Map and reduce functions,Does MXNET support higher order map reduce kind of functions map foldl foldr like described below in TensorFlow Theno has similar functionality described here If yes any pointers on where to look is appreciated if not any pointers on how to get started implementing them would be helpful Thanks,,"shivarajugowda,piiswrong,shivarajugowda,shivarajugowda,shivarajugowda,piiswrong,shivarajugowda,piiswrong,shivarajugowda,shivarajugowda,piiswrong,piiswrong,shivarajugowda,piiswrong,shivarajugowda,yajiedesign",2016-12-08 20:10:53,2017-09-28 07:16:29
IS,I write a bucket iterator using warp ctc but during training it reported killed only nothing else,nothing else printed,,yajiedesign,2016-09-20 01:37:46,2017-09-28 07:16:32
IS,divide a tensor by the max value of this tensor in forward,i want to divide a tensor by the max value of this tensor Tensor xpu 1 DType m aux args 0 get xpu 1 DType s ASSIGN DISPATCH m kWriteTo reduce except dim 1 mshadow red maximum reshape data Shape2 1 data shape Size Assign out req activation kOut data broadcast 1 1 m data shape but the compiler gives me an error as following image,,"piiswrong,yajiedesign",2016-12-18 12:45:29,2017-09-28 07:16:35
IS,bind fail for the GPU RNN operator,Hi I have some trouble to run simple RNN operator test code in GPU Below code fails when it binds The error message is as below Could you please help to make it running Thanks Error message ec2 user ip 10 0 0 12 unittest python test operator rnn py wouldata' 'LSTM bias' 'LSTM init h' 'LSTM init c' 2L 4L 3L 304L 2L 4L 4L 2L 4L 4L 2L 4L 4L 18 44 59 home ec2 user src mxnet dmlc core include dmlc logging h 235 18 44 59 src symbol graph executor cc 578 Check failed graph InferNodeShapes topo order out shapes aux shapes false Shape inference cannot be complete in bind Traceback most recent call last File test operator rnn py line 185 in module test lstm File test operator rnn py line 174 in test lstm args grad grad dict LSTM init h init hc shape LSTM init c init hc shape File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet symbol py line 844 in bind ctypes byref handle File usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 18 44 59 src symbol graph executor cc 578 Check failed graph InferNodeShapes topo order out shapes aux shapes false Shape inference cannot be complete in bind 18 44 59 home ec2 user src mxnet dmlc core include dmlc logging h 235 18 44 59 home ec2 user src mxnet mshadow mshadow stream gpu inl h 98 Check failed err CUBLAS STATUS SUCCESS Create cublas handle failed 18 44 59 home ec2 user src mxnet dmlc core include dmlc logging h 235 18 44 59 home ec2 user src mxnet mshadow mshadow stream gpu inl h 98 Check failed err CUBLAS STATUS SUCCESS Create cublas handle failed terminate called after throwing an instance of wouldmlc Error' what 18 44 59 home ec2 user src mxnet mshadow mshadow stream gpu inl h 98 Check failed err CUBLAS STATUS SUCCESS Create cublas handle failed Code def test lstm seq length 2 batch size 4 num hidden 4 num lstm layer 2 input size 3 init hc shape num lstm layer batch size num hidden data shape seq length batch size input size bias shape 4 num hidden input size num hidden 2 num lstm layer 1 4 num hidden 2 num hidden 2 inp shapes wouldata' data shape inp shapes 'LSTM init h' init hc shape inp shapes 'LSTM init c' init hc shape inp shapes 'LSTM bias' bias shape param shape data np random random data shape data wouldata' data shape init h 'LSTM init h' init hc shape init c 'LSTM init c' init hc shape data mx sym Variable data rnn h init mx sym Variable 'LSTM init h' rnn c init mx sym Variable 'LSTM init c' rnn params mx sym Variable 'LSTM bias' rnn params mx sym Variable mx symbol norm scale 0 1 shape 4 num hidden num hidden num hidden 2 num layers name LSTM bias grad dict ddata mx nd empty data shape grad dict wouldata' ddata dLSTM init h mx nd empty init hc shape grad dict 'LSTM init h' dLSTM init h dLSTM init c mx nd empty init hc shape grad dict 'LSTM init c' dLSTM init c dLSTM bias mx nd empty bias shape grad dict 'LSTM bias' dLSTM bias rnn native mx sym RNN data data state size num hidden num layers num lstm layer mode 'lstm' name 'LSTM' The following params can be omitted provided we do not need to apply the workarounds mentioned above state rnn h init state cell rnn c init parameters rnn params arg names rnn native list arguments print arg names arg shape out shape aux shape rnn native infer shape inp shapes arg dict dict zip arg names mx nd ones shape ctx mx gpu for shape in arg shape print arg shape print out shape embed mx sym Embedding data data input dim in dim output dim out dim name embed rnn test rnn native bind ctx mx gpu args arg dict args grad grad dict LSTM init h init hc shape LSTM init c init hc shape forward rnn test forward print rnn test outputs 0 asnumpy,,yajiedesign,2016-12-13 19:21:14,2017-09-28 07:16:39
IS,Support for single machine multi process KVStore based on Shared Memory,I have been trying to implement the A3C algorithm However I have found that it is impossible to create an ndarray that is shared among different processes Howevery sharing a ndarray object between different processes could be very useful since the multi threading module on Python is so limited due to GIL As far as I know the Chainer framework supports this feature and here is an example May I ask if MXNet support similar features or anyone has been working on it Thanks,,"peterzcc,piiswrong,peterzcc,piiswrong,mli,yajiedesign",2016-10-13 10:35:49,2017-09-28 07:16:46
IS,how to predict input in variable length using mxnet predict all api,Use mxnet predict all cc is very convenient to make the model run in smart device In my project speech synthesis I had used dnn to predict the output But the input is with variable length I had not found api for this type input so I predict the output one by one In fact it is very slow The code is like this I want to know if mxnet have something like placeholder in tensorflow in mxnet c api If no how to make it more efficient Use bucket Thank you,,"vsooda,vsooda,vsooda,yajiedesign",2016-11-17 03:27:26,2017-09-28 07:16:49
IS,Mxnet io doc problem,1 On this page the API dropdown in headline does not work 2 Unfinished content in extension multiple labels for a single image,,yajiedesign,2016-11-21 05:39:36,2017-09-28 07:16:54
IS,any advice of hyper parameters to train resnet and inception bn with IMAGENET,thses days I am training resnet and inception bn with IMAGENET both of them converge very slowly after train accuracy around 0 6 is any advice of hyper parameters I used the default hyper parameters with,,"tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,yajiedesign",2016-12-20 09:37:49,2017-09-28 07:16:57
IS,block in Batch 2150,Environment info Operating System ubuntu14 04 Package used Python R Scala Julia Python2 7 MXNet version 0 7 0 Error Message i run the train imagenet py in the example and i modified some code it seems blocked as follows image i attached the preocess in gdb and excute bt the result is as follows image Minimum reproducible example my code of line 619 in optimizer py is as follows image I edit adam class and it seems some problems in SyncCopyToCpu as 3724 This not happens always but i think it is a bug I do not know if it is fixed in nnvm branch so if it is need to be fixed yet i can try by best to provide more infomation,,yajiedesign,2016-12-20 03:32:14,2017-09-28 07:17:01
IS,Is that possible to use batchnorm for data parallelisation,Is it possible to make batch norm working on the whole batch when doing data split To be more specific batch norm calculates the mean and var for the current batch If for example we use batch size 8 and use 4 gpus then each gpu will have 2 images and the batch norm will only calculates the mean and var based on these 2 Any possibility the batch norm can work on the whole 8 images,,"piiswrong,yajiedesign",2016-12-20 13:02:08,2017-09-28 07:17:05
IS,why not use const reference in ElementwiseSum template function,why not use const reference for the first argument of the following function,,"howard0su,yajiedesign",2016-12-20 11:59:23,2017-09-28 07:17:09
IS,Keras Support binding a value in Symbol Constructor,NDArray supports binding a value in a constructor v mx ndarray array value For Symbol the only time you can bind a value is during a bind call Providing a interface to bind it in a constructor will make it easy in some scenarios like in integration with Keras,,"shivarajugowda,yajiedesign",2016-12-21 18:47:50,2017-09-28 07:17:13
IS,MultiGPU Evaluation Problem,The evaluation accuracies do not match when resuming from a previous checkpoint with multi gpus training However this problem does not occur in a single gpu case For example I am training ResNet 8 on Cifar10 dataset For a single GPU case the following is the log file for training at 40 epoch As can be seen however for Validation accuracy 0 847756 resuming from a checkpoint is larger than 0 838041 original around 1 for multiple gpus This phenomenon when resuming from a multi gpus training the validation accuracy gets a little higher around 1 than the original one appears multiple times when I am training the neural network in other cases Is this a bug in the Multi GPU synchronization process,,"taoari,Trangle,taoari,mli,taoari,zhreshold,mli,piiswrong,piiswrong,taoari,yajiedesign",2016-12-14 06:48:29,2017-09-28 07:17:16
IS,A strange thing while training with googlenet,Hi all while training with googlenet the training curve is very strange image Is there anyone good suggestions Net is googlenet lr is set to 0 05 lr factor is set to 0 95 lr factor epoch is set to 5 num epochs is set to 30 using mx io NDArrayIter to load npy files standardized by myself,,"piiswrong,yajiedesign",2016-12-22 04:45:36,2017-09-28 07:17:19
IS,The implement of LRN layer that similar to caffe,There are two version of LRN implement in caffe within channel and cross channel version When porting pretrained caffemodel to mxnet params they perform quite different with exactly same input Here is a sample of code from Caffe from I'm wondering the explanation of LRN layer in mxnet compared to caffe implmentation especially why the additional knorm variable is defined to 2 0 by default Or is there any reference article that tightly related to the implementation,,"liangfu,yajiedesign",2016-12-21 08:24:12,2017-09-28 07:17:27
IS,Discussion Should Github Issues still be used for bug installation issues,Would not it be better to discuss or ask questions related to installation issues and bugs in the users code on a different platform like Google groups forum mxnet or SO I just realized this has been discussed a year before in 1020 and 976 and a Google group has been created in fact Please go ahead and close this if there is not a need for a discussion again The reason I thought of starting a discussion is because the github issue still encourages people to share usage installation newbie questions here Probably promoting mentioning the usage of Google groups for those will help github issues be more focused towards the development of mxnet only I have seen scikit learn do something like this image,,"mli,leopd,howard0su,thirdwing,yajiedesign",2016-12-21 19:05:49,2017-09-28 07:17:30
IS,Prediction fails in Handwritten Digit Recognition example,When I am trying to reproduce the example of Handwritten Digit Recognition with python and prebuild MXNet version 20160321 I encounter a preblem in prodiction i e executing the following code prob model predict val img 0 1 astype np float32 255 0 I copied the code from url and here is my environment info and error message Environment info Operating System Windows 10 Package used Python R Scala Julia python MXNet version windows binary build 20160321 20160321 win10 x64 gpu Python version and distribution python 2 7 CUDA version 8 0 44 CUDNN version CUDNN v3 Error Message What have you tried to solve it I tried other versions of prebuild MXNet When using version 20160223 the problem is the same When I used later version 20160531 or 20160419 with CUDNN v5 1 or v3 prediction was fine for CPU training of multilayer perceptron but the GPU training of CNN yielded a poor accuracy same as url How can I solve this problem,,"kevinthesun,kevinthesun,yajiedesign",2016-12-21 08:41:51,2017-09-28 07:17:34
IS,Is it possible to manually push and to pull of dist kv store,When using dist kv store launch py while create several subprocesses with different DMLC ROLE schedule worker or server And after that kv store would works automatically I want to manually push and to pull of dist kv store just staring several process with configured environments and doing kv push Is that possible,,"piiswrong,yajiedesign",2016-12-22 15:27:58,2017-09-28 07:17:37
IS,Is there a python ordinary classification example,I am a new user to mxnet Recently I want to do mulitclasses using mxnet so I look the examples by mxnet However it has many examples about image I can not find an example about ordinary classification such as binary multiclass labels in traditional machine learning scenario can you provide such an example for the new user to understand more about mxnet,,"piiswrong,yajiedesign",2016-12-23 08:04:53,2017-09-28 07:17:40
IS,Installation problem related to OpenCV3,Hi I am having trouble installing MXNet it seems unable to find my opencv3 folder Walids MacBook Pro mxnet walidahmed make j sysctl n hw ncpu make No rule to make target usr local Cellar opencv 2 4 13 2 include opencv2 opencv hpp' needed by build src io image aug default o' Stop OpenCV is already installed at usr local Cellar opencv3 opencv hpp is at usr local Cellar opencv3 HEAD dc9602e 4 include opencv2 opencv hpp if I uninstall OpenCV3 nad install OpenCV2 4 MXNet installation works fine the issue that I need OpenCV3 Environment info Operating System MacOs Sierra Package used Python R Scala Julia Python Python version and distribution 2 7 Error Message Please paste the full error message including stack trace Walids MacBook Pro mxnet walidahmed make j sysctl n hw ncpu make No rule to make target usr local Cellar opencv 2 4 13 2 include opencv2 opencv hpp' needed by build src io image aug default o' Stop Minimum reproducible example if you are using your own code please provide a short script that reproduces the error,,"piiswrong,yajiedesign",2016-12-23 15:16:44,2017-09-28 07:17:43
IS,'module' object has no attribute io,I have followed the installation instructions for Python on Ubuntu 14 04 found here python package installation I am able to run the following Python test successfully python example image classification train mnist py But when I attempt to use the MLP on MNIST data found here I get an eror AttributeError 'module' object has no attribute 'io' From the line train mx io MNISTIter There is an io py file in mxnet python mxnet Any help as to why I am seeing this error would be appreciated,,yajiedesign,2016-10-18 17:02:45,2017-09-28 07:17:47
IS,nnpack performance,i tried nnpack but fount it performs much slower than mkldnn env ec2 c4 4xlarge ubuntu 14 04 i compiled nnpack with enable shared and set use nnpack 1 and use nnpack num threads 16 then in the mxnet example image classification folder run python benchmark score py it looks like only a single thread is used,,"mli,mli,tornadomeet,tornadomeet,mli,tornadomeet,yajiedesign",2016-12-20 23:28:46,2017-09-28 07:17:50
IS,Build error with CUDA build,I am seeing following error when trying to build with CUDA support Same error shows up when trying to build CUDA docker container g std c 11 DMSHADOW FORCE STREAM Wall O3 I root mxnet mshadow I root mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 pkg config cflags opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE NVRTC 0 MM MT build src resource o src resource cc build src resource d g std c 11 c DMSHADOW FORCE STREAM Wall O3 I root mxnet mshadow I root mxnet dmlc core include fPIC Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 pkg config cflags opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE NVRTC 0 c src resource cc o build src resource o In file included from root mxnet mshadow mshadow tensor h 16 0 from include mxnet base h 13 from src resource cc 9 root mxnet mshadow mshadow base h 242 13 error conflicting declaration 'typedef int cudnnTensorFormat t' typedef int cudnnTensorFormat t In file included from root mxnet mshadow mshadow base h 152 0 from root mxnet mshadow mshadow tensor h 16 from include mxnet base h 13 from src resource cc 9 usr include cudnn h 146 3 error 'cudnnTensorFormat t' has a previous declaration as 'typedef enum cudnnTensorFormat t cudnnTensorFormat t' cudnnTensorFormat t In file included from root mxnet mshadow mshadow tensor h 16 0 from include mxnet base h 13 from src resource cc 9 root mxnet mshadow mshadow base h 325 50 error invalid conversion from 'int' to 'cudnnTensorFormat t' fpermissive static const cudnnTensorFormat t kCudnnFlag 1 root mxnet mshadow mshadow base h 335 50 error invalid conversion from 'int' to 'cudnnTensorFormat t' fpermissive static const cudnnTensorFormat t kCudnnFlag 1 root mxnet mshadow mshadow base h 347 50 error invalid conversion from 'int' to 'cudnnTensorFormat t' fpermissive static const cudnnTensorFormat t kCudnnFlag 1 root mxnet mshadow mshadow base h 357 50 error invalid conversion from 'int' to 'cudnnTensorFormat t' fpermissive static const cudnnTensorFormat t kCudnnFlag 1 make build src resource o Error 1,,yajiedesign,2016-12-01 00:10:33,2017-09-28 07:17:57
IS,mxnet io search bar can not be selected after zooming page,To reproduce 1 Go to 2 Zoom in CTRL 3 Try to click on search bar Tried with Chrome and Firefox on Ubuntu,,"andremoeller,yajiedesign",2016-12-25 23:03:02,2017-09-28 07:18:00
IS,CSVIter fails in NNVM branch,Environment info Operating System ubuntu 16 04 Compiler g Ubuntu 5 4 0 6ubuntu1 16 04 4 5 4 0 20160609 Package used Python R Scala Julia Python MXNet version nnvm branch 76445465eb6e77206876e4d4eba5883148d8151c Python version and distribution Python 3 5 2 default Nov 17 2016 17 05 23 GCC 5 4 0 20160609 on linux Error Message Please paste the full error message including stack trace What have you tried to solve it Originally built from nnvm with profiler NVRTC CUDA CUDNN Rebuilt from nnvm branch with different configs including default config and run on CPU only still fails Rebuilt and reinstalled from master branch works fine,,"andremoeller,piiswrong,andremoeller,piiswrong,piiswrong,yajiedesign",2016-12-25 22:37:37,2017-09-28 07:18:04
IS,docker for cpu version too old,the mxnet version on docker hub seems way too old hope that you can update the docker image together with the code base Environment Operating System Ubuntu 16 04 LTS with docker MXNet version Python Steps to reproduce 1 docker pull dmlc mxnet 2 docker run dmlc mxnet python mxnet example image classification train mnist py 3 404 not found,,"piiswrong,yajiedesign",2016-12-26 02:36:48,2017-09-28 07:18:08
IS,Extremely large perplexity loss when implementing memory network for language modeling,Recently I am trying to implement the End To End Memory Networks for language modeling according to carpedm20 is Tensorflow implementation and you can find my code here But even after I set all the parameters same with his my mxnet version does not work properly and produce extremely large perplexity loss in training phase from the very beginning I tried to change the learning rate and gradient clip to much smaller values i e init lr 0 0005 max grad norm 1 Now the validation perplexity first fall to 200 300 but then increase to 400 500 again I also tried to use ADAM instead of SGD This change could give a better validation result around 160 but still much higher than the TF implementation 114 Currently the only difference between these two implementations I know is the lr factor method The TF version scales the learning rate when the loss stops to decrease But I do not know how to implement it in MXNet Instead I use a normal mx lr scheduler FactorScheduler Questions 1 Is there any difference between tensorflow is SGD and mxnet is SGD when both momentum and wd are set to 0 2 Why does the ADAM produces a not too bad result while the SGD fails to converge from the very beginning even their initial learning rates are the same,,"nicklhy,piiswrong,piiswrong,nicklhy,yajiedesign",2016-12-26 08:05:53,2017-09-28 07:18:11
IS,Could NOT find CUDA Found unsuitable version 8 0 but required is exact version 7 5 found usr local cuda,Could NOT find CUDA Found unsuitable version 8 0 but required is exact version 7 5 found usr local cuda I clone the repository and run cmake The error occurs,,yajiedesign,2016-12-26 12:09:10,2017-09-28 07:18:14
IS,Error when trying to create my own data iterator,I am trying to create my own data iterator each data item is created by reading two image files reshaping them and concatenating them along the channel axis thus creating a 6 channel image See code below When I try to train using this data I get this error Traceback most recent call last File opt pycharm helpers pydev pydevd py line 1596 in module globals debugger run setup 'file' None None is module File opt pycharm helpers pydev pydevd py line 974 in run pydev imports execfile file globals locals execute the script File opt project train py line 23 in module batch end callback mx callback Speedometer batch size 1 output progress for each 200 data batches File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 811 in fit sym gen self sym gen File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet model py line 236 in train multi device executor manager load data batch data batch File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 410 in load data batch self curr execgrp load data batch data batch File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 257 in load data batch load data data batch self data arrays File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 93 in load data load general batch data targets File usr local lib python2 7 dist packages mxnet 0 7 0 py2 7 egg mxnet executor manager py line 89 in load general d src slice idx copyto d dst AttributeError 'numpy ndarray' object has no attribute 'copyto',,"kevinthesun,yajiedesign",2016-12-26 14:07:37,2017-09-28 07:18:19
IS,The try except code in python mxnet notebook callback py caused a lots of error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Mac Ubuntu Compiler gcc Package used Python R Scala Julia Python MXNet version 0 7 Or if installed from source Yes MXNet commit hash git rev parse HEAD 5434387d60ee121d7bd2221ff710de2a5db67a54 If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet notebook callback pyc in init self metric name display freq frequent 292 'eval' 'elapsed' 293 294 super LiveLearningCurve self init None metric name display freq frequent 295 296 def setup chart self usr local lib python2 7 site packages mxnet 0 7 0 py2 7 egg mxnet notebook callback pyc in init self pandas logger metric name display freq batch size frequent 199 NOTE would be nice to auto detect the metric name if there is only one 200 self metric name metric name 201 bokeh io output notebook 202 self handle self setup chart 203 NameError global name 'bokeh' is not defined Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Just run the example recommenders demo1 MF ipynb Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Start Jupyter 2 Run example recommenders demo1 MF ipynb 3 What have you tried to solve it 1 Track the error log found that module bokeh was not installed 2 Check the source code found the follow code try import bokeh plotting except ImportError pass 3 Do not understand why we just pass it when we can not import bokeh plotting is not it a better way to let the system report that we can not import this module which is not installed and let the users to install it with pip install bokeh,,"DamonDeng,kevinthesun,yajiedesign",2016-12-26 15:48:27,2017-09-28 07:18:22
IS,the value of executor arg dict bn conv1 gamma is different in different position of one iteration,I use the symbol simple bind to train the network batch by batch below is the code snippet exe self symbol simple bind ctx self ctx input shapes for batch in train data exe forward is train True exe backward print before print exe arg dict 'bn conv1 gamma' asnumpy for i pair in enumerate zip exe arg arrays exe grad arrays weight grad pair if i 3 print arr arrays print weight asnumpy print arg dict print exe arg dict 'bn conv1 gamma' asnumpy self updater i grad weight where 'bn conv1 gamma' is the 3rd from 0 of self symbol list arguments i found that the value of exe arg dict 'bn conv1 gamma' asnumpy is different in different position which really confused me,,"piiswrong,piiswrong,yajiedesign",2016-11-24 15:21:51,2017-09-28 07:18:25
IS,When I use the 'mx symbol Convolution' with 3d kernel I get the problem Volume convolution is not implmented in mshadow,Does mxnet still not support 3D convolution,,"tornadomeet,tornadomeet,yajiedesign",2016-12-20 07:10:52,2017-09-28 07:18:28
IS,im2rec,im2rec py image lst integer image index,,yajiedesign,2016-11-21 07:00:16,2017-09-28 07:18:31
IS,imagenet hanging in the end,I have cloned the latest mxnet and tried running imagenet a very small subset of the images What I found is that mxnet in distributed training using dist device sync hangs in the end with two workers Running cifar was ok and didnt hang in the end I get something like on two workers 11 19 09 src io iter image recordio cc 221 ImageRecordIOParser tmp amithr imagenet mini train rec use 4 threads for decoding 11 16 32 src io iter image recordio cc 221 ImageRecordIOParser tmp amithr imagenet mini train rec use 4 threads for decoding INFO root Start training with gpu 1 gpu 2 INFO root Start training with gpu 1 gpu 2 INFO root Epoch 0 Batch 20 Speed 33 72 samples sec Train accuracy 0 515625 INFO root Epoch 0 Batch 20 Speed 33 80 samples sec Train accuracy 0 467187 INFO root Epoch 0 Resetting Data Iterator INFO root Epoch 0 Time cost 35 018,,"piiswrong,mli,mli,yajiedesign",2016-12-27 16:30:18,2017-09-28 07:18:35
IS,How to set net use different input shape,the network use different image size as input how to write the dataiter,,"andremoeller,formath,yajiedesign",2016-12-24 06:12:28,2017-09-28 07:18:38
IS,Is it possible to compose existing operators to create new operators,For example in TensorFlow InstanceNorm and BatchNorm can be created as I am just wondering if this is possible in MXNet If possible is there any example for illustrating this The good part of composing existing operators to create a new operator is that one no longer need to write the backward function It will be also much easier to help understand the source code For example comparing against the above implementation the backward function of BatchNorm is quite daunting to read,,"taoari,piiswrong,taoari,taoari,formath,yajiedesign",2016-12-18 02:32:14,2017-09-28 07:18:41
IS,How to use the function mx nd slice axis,Description speaks about 4 arguments What does parameters X0 out of the function How to use these parameters,,yajiedesign,2016-12-28 17:03:54,2017-09-28 07:18:44
IS,Difficult to get save symbols in new API mxnet module,I have been using MXNET for a while and get used to the interface mxnet model FeedForward which has a method save checkpoint This method returns a JSON file of symbol definition and params for saving args and aux params But with the new API mxnet module I can not figure out how to get save the symbol easily in this module and it makes more complex to use or finetune on models based on this interface since loading a model requires a JSON file for symbol definition In mxnet module BaseModule there is a property symbol which allows us to get the symbol in module But in other inherit classes like mxnet module Module and mxnet module SequentialModule we can comprise stack modules to build more complex networks This makes harder to access the whole symbol in these modules even though we can get symbols of each submodules in recursive way and concat them but this is still not straightforward for me,,"piiswrong,pluskid,yajiedesign",2016-10-31 11:24:59,2017-09-28 07:18:48
IS,NDArrayIter building one more iter even if last batch handle set to discard,Hi I use the following call train iter mx io NDArrayIter mydata mylbl 1 shuffle True last batch handle wouldiscard' with mydata shape 33 1 28 28 mylbl shape 33 then the following prints return NDArray 34x1x28x28 0 NDArray 1x1x28x28 0 NDArray 1x1x28x28 0 should not the first print be NDArray 3 3 x1x28x28 0 as the last batch handle set to discard and the batch size is 1 many thanks AL,,yajiedesign,2016-12-29 02:01:52,2017-09-28 07:18:51
IS,problem with bucketing lstm and cnn for text recognition,Hi everyone I want to use lstm and cnn to implement text recognition from image for example recognize 'good' from a text image bounding box and I use warp ctc as loss function but I find my softmax out are always the same Environment info Operating System ubuntu 16 04 Compiler gcc 5 4 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 32cb6bc Python version and distribution Python 2 7 11 Anaconda 4 0 0 x86 64 Error Message The following is my output of final fullyconnected layer as pred What have you tried to solve it 1 I try to use bi lstm but get the same output problem 2 do mxnet provide API to get the middle layers' output when fitting the model,,"piiswrong,formath,yajiedesign",2016-12-12 07:11:59,2017-09-28 07:18:54
IS,binutils 2 24 bin ld cannot find lopenblas,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos 6 5 x86 64 Compiler g GCC 4 8 5 ldd GNU libc 2 12 opencv 2 4 13 openblase 0 2 15 binutils 2 24 Error Message Please paste the full error message including stack trace opt pb binutils 2 24 bin ld cannot find lopenblas collect2 error ld returned 1 exit status make lib libmxnet so Error 1 make Waiting for unfinished jobs opt pb binutils 2 24 bin ld cannot find lopenblas collect2 error ld returned 1 exit status make bin im2rec Error 1 What have you tried to solve it 1 reinstall openblas 2 set mxnet config mk 31 ADD LDFLAGS L opt openblas L opt openblas include 34 ADD CFLAGS L opt openblas L opt openblas include 96 USE BLAS openblas 3 cd mxnet and make stills fail,,"howard0su,howard0su,yajiedesign",2016-12-28 11:22:03,2017-09-28 07:18:58
IS,it is not compatible between v0 7 0 and v0 8 0,I have been used v0 7 0 for a long time But It is failed when I try to use v0 8 0 to load and run the model which is trained from v0 7 0 So the two versions are not compatible,,yajiedesign,2016-12-29 03:59:30,2017-09-28 07:19:01
IS,Does mxnet has the matrix inverse symbol,I want to implement some new operations by mxnet And I need to use the matrix inverse operation Dose mxnet has the symbol mx symbol matrix inverse to implement the inverse of matrix,,"xlvector,yajiedesign",2016-12-29 13:08:23,2017-09-28 07:19:04
IS,Undefined symbols for architecture x86 64 when compiling using Cuda,Environment info Operating System MacOS Package used Python R Scala Julia MXNet version 0 7 Python version and distribution 2 7 my graphics card is Intel Iris Pro 1536 MB Error Message ld warning directory not found for option ' L usr local cuda lib64' Undefined symbols for architecture x86 64 mxnet op TuneCudnnConvolution mxnet op ConvolutionParam std 1 vector mxnet TShape std 1 allocator mxnet TShape std 1 vector mxnet TShape std 1 allocator mxnet TShape mxnet Context cudnnDataType t cudnnConvolutionFwdAlgo t cudnnConvolutionBwdDataAlgo t cudnnConvolutionBwdFilterAlgo t referenced from mxnet op CuDNNConvolutionOp float CuDNNConvolutionOp mxnet op ConvolutionParam std 1 vector mxnet TShape std 1 allocator mxnet TShape std 1 vector mxnet TShape std 1 allocator mxnet TShape mxnet Context in convolution gpu o mxnet op CuDNNConvolutionOp double CuDNNConvolutionOp mxnet op ConvolutionParam std 1 vector mxnet TShape std 1 allocator mxnet TShape std 1 vector mxnet TShape std 1 allocator mxnet TShape mxnet Context in convolution gpu o mxnet op CuDNNConvolutionOp mshadow half half t CuDNNConvolutionOp mxnet op ConvolutionParam std 1 vector mxnet TShape std 1 allocator mxnet TShape std 1 vector mxnet TShape std 1 allocator mxnet TShape mxnet Context in convolution gpu o ld symbol s not found for architecture x86 64 clang error linker command failed with exit code 1 use v to see invocation make lib libmxnet so Error 1,,"kevinthesun,yajiedesign",2016-12-29 16:37:12,2017-09-28 07:19:08
IS,A note on data augmenation,In the following figure I compare commonly used data augmentation techniques for ResNet 50 on ImageNet Random crop random flip and scale augmentations were shown to be very effective and they provide as the baseline scale aug For JPEG compression with quality 90 and aspect ratio augmentation the curves indicates they almost have no effects For color augmentation it is shown that the accuracy is actually decreased notes on aug Compare to state of the art implementation For ResNet 50 on ImageNet top 1 accuracy of 74 55 were achieved As suggested in canceling scale color aspect augmentation will further improve the results For a fair comparison actually achieved 74 20 accuracy at the 95 th epoch right before the cancellation of scale color aspect augmentations Hence we can conclude that augmentations beyond random crop random flip and scale actually hurt the performance as 74 55 74 20,,"taoari,piiswrong,taoari,tornadomeet,taoari,yajiedesign",2016-12-30 06:55:40,2017-09-28 07:19:12
IS,Check failed from shape to shape operands shape mismatch,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System mac Compiler Package used Python R Scala Julia python MXNet version Or if installed from source pip install mxnet MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 14 51 23 dmlc core include dmlc logging h 208 14 51 23 src ndarray ndarray cc 159 Check failed from shape to shape operands shape mismatch Traceback most recent call last File Users weiwe Desktop alexa planestrike mxnet train py line 181 in module board position log action log hit log play game training TRAINING first game game 0 File Users weiwe Desktop alexa planestrike mxnet train py line 89 in play game tmp model predict X predict iter 0 File Users weiwe tensorflow lib python2 7 site packages mxnet model py line 641 in predict self init predictor data shapes File Users weiwe tensorflow lib python2 7 site packages mxnet model py line 576 in init predictor pred exec copy params from self arg params self aux params File Users weiwe tensorflow lib python2 7 site packages mxnet executor py line 179 in copy params from array copyto self arg dict name File Users weiwe tensorflow lib python2 7 site packages mxnet ndarray py line 351 in copyto return NDArray copyto self out other File Users weiwe tensorflow lib python2 7 site packages mxnet ndarray py line 645 in unary ndarray function c array NDArrayHandle out handle File Users weiwe tensorflow lib python2 7 site packages mxnet base py line 74 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 14 51 23 src ndarray ndarray cc 159 Check failed from shape to shape operands shape mismatch Process finished with exit code 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error model mx model FeedForward symbol mlp num epoch 1 learning rate reward ALPHA current board ndarray np array current board dtype np int8 action ndarray np array action dtype np int8 train iter mx io NDArrayIter current board ndarray action ndarray batch size 1 shuffle False model fit X train iter game board ndarray np array game board flatten dtype np int8 predict iter mx io NDArrayIter game board ndarray batch size 1 shuffle False tmp model predict X predict iter 0 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error I probably have missed sth obvious 'current board' is 1x36 'game board' is 6x6 so after flattening the dimension should be the same w 'current board' I do not understand why I got the shape mismatch error What have you tried to solve it 1 2 3,,"piiswrong,yajiedesign",2016-12-30 06:55:07,2017-09-28 07:19:56
IS,MXNetError 16 38 44 src operator cudnn batch norm cc 20 CuDNNBatchNorm is merged into BatchNorm for cudnn version above v5 Use the later instead,OS Linux Ubuntu 16 04LTS MXNET version from GIT today CUDA version 8 DNN version 5 1 compile for GPU 100 ok no error GPU 1070 nvidia deep3d version from git today when I run the deep3d ipynb with jupyter notebook I get this error MXNetError 16 38 44 src operator cudnn batch norm cc 20 CuDNNBatchNorm is merged into BatchNorm for cudnn version above v5 Use the later instead Any working solution for this error message Any fix or patch working idea,,"yajiedesign,yajiedesign",2016-12-28 15:48:28,2017-09-28 07:19:59
IS,fcn xs py data py Crashes Semantic Segmentation Setup,Hi I was trying to reproduce results on semantic segmentation given in example folder fcn xs But unfortunately I am having multiple issues trying to understand the code and the code keeps crashing Environment info Operating System Windows Linux Package used Python MXNet version Master I am totally confused about the structure of train lst and val lst used for simulating fcn xs example on VOC 2012 dataset I am using VOC2012 dataset as mentioned in the code with my root dir VOCdevkit VOC2012 SegmentationClass flist name VOCdevkit VOC2012 SegmentationClass train lst I created the train lst and val lst as follows 2008 000033 0 2008 000197 0 2008 000716 0 2008 001719 0 2008 002221 0 2008 002551 0 2008 002719 0 2008 003196 0 2008 003703 0 2008 003729 0 2008 006140 0 First column is Segmented ImageID and Second Column is the Class Here 0 mean aeroplane When I feed this file to the project provided in fcn xs I get following error I modified the data py file little to read the lst files data img name label img name self f readline strip ' n' split t data img name label img name self f readline strip ' n' split t Error Message runfile 'H MXNet mxnet master mxnet master example fcn xs fcn xs py' args ' model fcn32s prefix VGG FC ILSVRC 16 layers epoch 74 init type vgg16' wdir 'H MXNet mxnet master mxnet master example fcn xs' 32 32 2008 000033 0 Traceback most recent call last File ipython input 4 034dc04e8c56 line 1 in module runfile 'H MXNet mxnet master mxnet master example fcn xs fcn xs py' args ' model fcn32s prefix VGG FC ILSVRC 16 layers epoch 74 init type vgg16' wdir 'H MXNet mxnet master mxnet master example fcn xs' File C Users asdlfj Anaconda2 lib site packages spyderlib widgets externalshell sitecustomize py line 714 in runfile execfile filename namespace File C Users asdlfj Anaconda2 lib site packages spyderlib widgets externalshell sitecustomize py line 74 in execfile exec compile scripttext filename 'exec' glob loc File H MXNet mxnet master mxnet master example fcn xs fcn xs py line 73 in module main File H MXNet mxnet master mxnet master example fcn xs fcn xs py line 36 in main rgb mean 123 68 116 779 103 939 File data py line 44 in init self data self label self read File data py line 54 in read data self data name label self label name self read img data img name label img name File data py line 59 in read img label Image open os path join self root dir label name File C Users asdlfj Anaconda2 lib site packages PIL Image py line 2272 in open fp builtins open filename rb IOError Errno 2 No such file or directory 'H AWS VOCdevkit VOC2012 SegmentationClass 0' Steps to reproduce Problem is here in data py file where it tries to read the images and labels img Image open os path join self root dir img name label Image open os path join self root dir label name Here it tries to read images for both image and labels But I only have Image and Labels as numeric class value I am really unable to understand what img name and label name mean here Does JPEGImages map to img name and SegmentationClass map to label name in VOC2012 folder Obviously some link in understanding is missing between JPEGImages SegmentationClass images Label Values in my setup Clarification will be really helpful Thanks,,yajiedesign,2016-12-30 18:23:02,2017-09-28 07:20:02
IS,cudaMalloc Issue for semantic segmentation,i am doing something about semantic segmentation and use network definition below data mx sym Variable wouldata' VGG part group 1 conv1 1 mx sym Convolution data data kernel 3 3 num filter 64 name 'conv1 1' workspace 2048 relu1 1 mx sym Activation data conv1 1 act type arelu' name arelu1 1' conv1 2 mx sym Convolution data relu1 1 kernel 3 3 num filter 64 name 'conv1 2' workspace 2048 relu1 2 mx sym Activation data conv1 2 act type arelu' name arelu1 2' pool1 mx sym Pooling data relu1 2 pool type 'max' kernel 2 2 stride 2 2 name 'pool1' group 2 conv2 1 mx sym Convolution data pool1 kernel 3 3 num filter 128 name 'conv2 1' workspace 2048 relu2 1 mx sym Activation data conv2 1 act type arelu' name arelu2 1' conv2 2 mx sym Convolution data relu2 1 kernel 3 3 num filter 128 name 'conv2 2' workspace 2048 relu2 2 mx sym Activation data conv2 2 act type arelu' name arelu2 2' pool2 mx sym Pooling data relu2 2 pool type 'max' kernel 2 2 stride 2 2 name 'pool2' group 3 conv3 1 mx sym Convolution data pool2 kernel 3 3 num filter 256 name 'conv3 1' workspace 2048 relu3 1 mx sym Activation data conv3 1 act type arelu' name arelu3 1' conv3 2 mx sym Convolution data relu3 1 kernel 3 3 num filter 256 name 'conv3 2' workspace 2048 relu3 2 mx sym Activation data conv3 2 act type arelu' name arelu3 2' conv3 3 mx sym Convolution data relu3 2 kernel 3 3 num filter 256 name 'conv3 3' workspace 2048 relu3 3 mx sym Activation data conv3 3 act type arelu' name arelu3 3' pool3 mx sym Pooling data relu3 3 pool type 'max' kernel 2 2 stride 2 2 name 'pool3' group 4 without pool4 conv4 1 mx sym Convolution data pool3 kernel 3 3 num filter 512 name 'conv4 1' workspace 2048 relu4 1 mx sym Activation data conv4 1 act type arelu' name arelu4 1' conv4 2 mx sym Convolution data relu4 1 kernel 3 3 num filter 512 name 'conv4 2' workspace 2048 relu4 2 mx sym Activation data conv4 2 act type arelu' name arelu4 2' conv4 3 mx sym Convolution data relu4 2 kernel 3 3 num filter 512 name 'conv4 3' workspace 2048 relu4 3 mx sym Activation data conv4 3 act type arelu' name arelu4 3' dilation part conv5 1 mx sym Convolution data relu4 3 kernel 3 3 num filter 512 dilate 2 2 name 'conv5 1' workspace 2048 relu5 1 mx sym Activation data conv5 1 act type arelu' name arelu5 1' conv5 2 mx sym Convolution data relu5 1 kernel 3 3 num filter 512 dilate 2 2 name 'conv5 2' workspace 2048 relu5 2 mx sym Activation data conv5 2 act type arelu' name arelu5 2' conv5 3 mx sym Convolution data relu5 2 kernel 3 3 num filter 512 dilate 2 2 name 'conv5 3' workspace 2048 relu5 3 mx sym Activation data conv5 3 act type arelu' name arelu5 3' fc6 mx sym Convolution data relu5 3 kernel 7 7 num filter 4096 dilate 4 4 name 'fc6' workspace 2048 relu6 mx sym Activation data fc6 act type arelu' name arelu6' drop6 mx sym Dropout data relu6 p 0 5 name wouldrop6' fc7 mx sym Convolution data drop6 kernel 1 1 num filter 4096 name 'fc7' workspace 2048 relu7 mx sym Activation data fc7 act type arelu' name arelu7' drop7 mx sym Dropout data relu7 p 0 5 name wouldrop7' final mx sym Convolution data drop7 kernel 1 1 num filter num class name 'final' workspace 2048 ctx conv1 1 mx sym Convolution data final kernel 3 3 pad 1 1 num filter num class name 'ctx conv1 1' workspace 2048 ctx relu1 1 mx sym Activation data ctx conv1 1 act type arelu' name 'ctx relu1 1' ctx conv1 2 mx sym Convolution data ctx relu1 1 kernel 3 3 pad 1 1 num filter num class name 'ctx conv1 2' workspace 2048 ctx relu1 2 mx sym Activation data ctx conv1 2 act type arelu' name 'ctx relu1 2' ctx conv2 1 mx sym Convolution data ctx relu1 2 kernel 3 3 pad 2 2 num filter num class dilate 2 2 name 'ctx conv2 1' workspace 2048 ctx relu2 1 mx sym Activation data ctx conv2 1 act type arelu' name 'ctx relu2 1' ctx conv3 1 mx sym Convolution data ctx relu2 1 kernel 3 3 pad 4 4 num filter num class dilate 4 4 name 'ctx conv3 1' workspace 2048 ctx relu3 1 mx sym Activation data ctx conv3 1 act type arelu' name 'ctx relu3 1' ctx conv4 1 mx sym Convolution data ctx relu3 1 kernel 3 3 pad 8 8 num filter num class dilate 8 8 name 'ctx conv4 1' workspace 2048 ctx relu4 1 mx sym Activation data ctx conv4 1 act type arelu' name 'ctx relu4 1' ctx conv5 1 mx sym Convolution data ctx relu4 1 kernel 3 3 pad 16 16 num filter num class dilate 16 16 name 'ctx conv5 1' workspace 2048 ctx relu5 1 mx sym Activation data ctx conv5 1 act type arelu' name 'ctx relu5 1' ctx conv6 1 mx sym Convolution data ctx relu5 1 kernel 3 3 pad 32 32 num filter num class dilate 32 32 name 'ctx conv6 1' workspace 2048 ctx relu6 1 mx sym Activation data ctx conv6 1 act type arelu' name 'ctx relu6 1' ctx conv7 1 mx sym Convolution data ctx relu6 1 kernel 3 3 pad 64 64 num filter num class dilate 64 64 name 'ctx conv7 1' workspace 2048 ctx relu7 1 mx sym Activation data ctx conv7 1 act type arelu' name 'ctx relu7 1' ctx fc1 mx sym Convolution data ctx relu7 1 kernel 3 3 pad 1 1 num filter num class name 'ctx fc1' workspace 2048 ctx fc1 relu1 mx sym Activation data ctx fc1 act type arelu' name 'ctx fc1 relu1' ctx final mx sym Convolution data ctx fc1 relu1 kernel 1 1 num filter num class name 'ctx final' workspace 2048 softmax mx sym SoftmaxOutput data ctx final multi output True use ignore True ignore label 255 name softmax i use memonger to compute the memory required for one image as input cost memonger get cost softmax data 1 3 1396 1396 the required memory is 6954MB which is much less than 12GB but when i use 2 images as input and 2 Titan X gpu cudaMalloc failed occurred i do not understand why the memory is not enough i use mx model FeedForward to train the network thanks,,"piiswrong,yajiedesign",2016-12-31 05:28:40,2017-09-28 07:20:05
IS,Error occured when compiling with vs2013 and mxnet 2016 12 30,Environment info Operating System Win7 64 Compiler VS 2013 12 0 21005 1 REl Package used python MXNet version master in 2016 12 30 Python version and distribution Anaconda python 2 7 11 Error Message 1 in dmlc core include dmlc optional h namespace dmlc brief dummy type for assign null to optional struct nullopt t brief dummy constructor constexpr nullopt t int error expected an identifier error expected a 2 in mshadow base h base h 243 error invalid redeclaration of type name cudnnTensorFormat t 3 dmlc core include dmlc logging h 95 error C2040 void const const int Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Download mxnet master dmlc core and mshadow 2 CMAKE configure and generate success OpenCV cudnn 4 and openblas is OK 3 all build and error occured,,"yajiedesign,yajiedesign",2016-12-30 05:46:19,2017-09-28 07:20:08
IS,a question about the new Image APIs,In the release note it says this will be automatically processed in parallel So I am wondering if the other preprocessing functions can be used in the same way Like crop random scale can I make them parallel as well,,"piiswrong,yajiedesign",2017-01-01 23:47:39,2017-09-28 07:20:11
IS,from layer mx symbol Scale data from layer mode spatial AttributeError 'module' object has no attribute 'Scale',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2016-12-27 02:47:09,2017-09-28 07:20:15
IS,running bug typeerror init got an unexpected keyword argument 'factor1',I want to change the factor in lr scheduler py to 3 different factors so I changed lr scheduler py like this image and changed train model py like this image and changed some other files which use factor But there is a bug like this image I do not know what is the reason,,yajiedesign,2017-01-02 14:36:40,2017-09-28 07:20:19
IS,mxnet scala2 11,hi I'm using scala 2 11 in my project and i want to know is your mxnet suitable for my project,,"yzhliu,yzhliu,yajiedesign",2016-10-27 07:42:43,2017-09-28 07:20:22
IS,Avoid overflow of sigmoid,In practice sigmoid is easy to overflow Maybe we can add an upper bound parameter for sigmoid like Activaction x act type isigmoid' upper bound MAX X,,"lx75249,yajiedesign",2017-01-03 15:27:09,2017-09-28 07:20:25
IS,Evaluation metric train or test score,Hi Mxnet I am a bit confused Looking at the examples and at the callback py file I can see that the same function param eval metric get is used for both the Test metric and the Train metric Can you clarify What makes the result test versus train the used callback entry point batch end callback versus eval batch end callback versus eval end callback In other words how to retrieve the Test score the train score and the train loss in a single custom callback for example from the eval batch end callback Many thanks AL,,"piiswrong,yajiedesign",2017-01-02 22:37:32,2017-09-28 07:20:28
IS,what is correct way to stop the subprocess created by launcher py,In distributed experiments for example when I press the Ctrl C the subprocess created by launcher py still running and must be killed manually by pid on each node kill 9 PID what is the best way to stop the launcher py any ideas,,"pineking,piiswrong,yajiedesign",2016-12-11 09:55:37,2017-09-28 07:20:32
IS,Get a core dump when trying to run the Scala example from the website,I have been trying to get the MNist Scala example from the website to run with no luck I have fixed a compilation error but now when I run it I get a core dump I do not know how to proceed Environment info Operating System Ubuntu 16 10 amd64 Compiler sbt Package used Python R Scala Julia Scala package scalaVersion 2 11 8 MXNet version ml dmlc mxnet mxnet full 2 10 linux x86 64 gpu 0 1 1 Error Message,,yajiedesign,2017-01-05 11:03:38,2017-09-28 07:20:41
IS,How to print loss and LR for current batch,Shall i write custom eval metric,,"howard0su,piiswrong,howard0su,yajiedesign",2017-01-01 16:40:15,2017-09-28 07:20:44
IS,Generate the Amalgamation for android with NNPACK,If I generate the amalgamation for android without other librarys the running time of WhatsThis App is about 700ms However when I generate Amalgamation with libnnpack a or others librarys generate by ndk build the running time of WhatsThis became more than 6000ms some times almost 7s here is what I have done 1 add these to the makefile in amalgamation export NNPACK ROOT MXNET ROOT nnpack NNPACK mxnet itself CFLAGS I MXNET ROOT mxnetroot CFLAGS I MXNET ROOT dmlc core include mxnetroot dmlc core include CFLAGS I MXNET ROOT include mxnetroot include CFLAGS I MXNET ROOT mshadow mxnetroot mshadow nnpack CFLAGS DMXNET USE NNPACK 1 CFLAGS DMXNET USE NNPACK NUM THREADS 8 CFLAGS I NNPACK ROOT include LDFLAGS L NNPACK ROOT obj local armeabi v7a LDFLAGS lnnpack lpthreadpool lnnpack ukernels lnnpack reference lgtest lfp16 utils lbench utils lcpufeatures LDFLAGS lnnpack lpthreadpool lnnpack ukernels lcpufeatures nnpack dependence googletest CFLAGS I NNPACK ROOT third party gtest 1 7 0 include CFLAGS I NNPACK ROOT third party gtest 1 7 0 nnpack dependence pthreadpool CFLAGS I NNPACK ROOT third party pthreadpool include CFLAGS I NNPACK ROOT third party FXdiv include other define used CFLAGS DMSHADOW STAND ALONE 1 CFLAGS DMSHADOW USE CUDA 0 CFLAGS DMSHADOW USE MKL 0 CFLAGS DSHADOW RABIT PS 0 CFLAGS DMSHADOW DIST PS 0 CFLAGS DMSHADOW USE SSE 0 CFLAGS DMXNET USE OPENCV 0 CFLAGS DMXNET PREDICT ONLY 0 CFLAGS DDISABLE OPENMP 1 2 add these to Amalgamation py the line 123 if MXNET USE NNPACK 1 include src operator nnpack nnpack convolution inl h endif MXNET USE NNPACK 3 run make ANDROID 1 and than get the jni mxnet predict so rename this and replace the one in WhatsThis the running time became almost 7s Who can help me to solve this,,"tornadomeet,xlvector,yajiedesign",2016-12-29 09:09:04,2017-09-28 07:20:51
IS,some csv bugs,Bug1 upgrade mxnet to 0 91 csv can not traversal all lines only traversal part lines For example a csv given as listed,,yajiedesign,2017-01-05 14:59:08,2017-09-28 07:20:54
IS,Different results on different machines,Hi I run lstm regression model using mxnet on different machines and get different results specifically I run on 3 machines and one of which is always worse than the other two 1 CentOS 6 5 64 bit gcc 4 8 1 openblas mxnet 0 7 0 9 pearson correlation 0 96 2 Windows 10 32 bit vs 2015 openblas mkl mxnet 0 7 pearson correlation 0 96 3 fedora 25 ubuntu 16 10 32 bit gcc 6 3 clang 3 8 openblas mxnet 0 7 0 9 pearson correlation 0 90 0 93 The data scripts etc on 3 machines are just the same but the result on the third machine is much worse than others can anybody please help Thanks,,"piiswrong,piiswrong,piiswrong,piiswrong,yajiedesign",2017-01-01 07:31:06,2017-09-28 07:20:57
IS,use Inception bn to train ilsvrc12 dataset the val accuracy will fluctuate strongly,Why do I use Inception bn to train ilsvrc12 dataset the val accuracy will fluctuate strongly ps dist async 5 parameter servers 5 wokers thx,,"mli,yajiedesign",2017-01-06 02:07:06,2017-09-28 07:21:03
IS,Python 3 5 2 Compabilitiy with mxnet,Hi I just wonder if current mxnet git compatible with Python 3 5 2 Environment info Operating System Ubuntu 16 04 1 Compiler GCC G 5 4 0 Package used Python R Scala Julia Python 3 5 2 MXNet version 0 9 1 Current git Or if installed from source Yes install from source MXNet commit hash git rev parse HEAD 1ae2905cc3c2e35541d45edab18ddffb9cf455da If you are using python package please provide Python 3 5 2 Python version and distribution 3 5 2 Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Trying to convert all py files from version 2 to version 3 by using command 2to3 Cheers Pei,,yajiedesign,2017-01-06 09:05:28,2017-09-28 07:21:06
IS,GT750M is a bit slower than i5 3230M is that normal,I just installed the newest pre compiled windows version 20170105 mxnet x64 vc14 gpu I have cuda 8 and cudnn5 installed Then I run the following python code GPU is a bit slower than CPU I know GT750M is not a high end GPU but shall it be that slow Or something else is wrong I am sure GPU did work because GPU z tell me that GPU load is about 90,,"mli,yajiedesign",2017-01-06 04:44:24,2017-09-28 07:21:10
IS,ImageData Layer in MXNet,I remember someone has implemented a layer similar to caffe is ImageData layer which takes a list of images as input without recordio file does someone remember this implementation,,yajiedesign,2017-01-06 11:34:40,2017-09-28 07:21:13
IS,Unknown CMake command detect cuDNN,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Debian Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace The system is Linux 4 5 0 0 bpo 1 amd64 x86 64 Compiling the C compiler identification source file CMakeCCompilerId c succeeded Compiler usr bin cc Build flags Id flags The output was 0 Compilation of the C compiler identification source CMakeCCompilerId c produced a out The C compiler identification is GNU found in u suhubdyd code mxnet CMakeFiles 3 0 2 CompilerIdC a out Compiling the CXX compiler identification source file CMakeCXXCompilerId cpp succeeded Compiler usr bin c Build flags Id flags The output was 0 Compilation of the CXX compiler identification source CMakeCXXCompilerId cpp produced a out The CXX compiler identification is GNU found in u suhubdyd code mxnet CMakeFiles 3 0 2 CompilerIdCXX a out Determining if the C compiler works passed with the following output Change Dir u suhubdyd code mxnet CMakeFiles CMakeTmp Run Build Command usr bin make cmTryCompileExec2478541284 fast usr bin make f CMakeFiles cmTryCompileExec2478541284 dir build make CMakeFiles cmTryCompileExec2478541284 dir build make 1 Entering directory ' u suhubdyd code mxnet CMakeFiles CMakeTmp' usr bin cmake E cmake progress report u suhubdyd code mxnet CMakeFiles CMakeTmp CMakeFiles 1 Building C object CMakeFiles cmTryCompileExec2478541284 dir testCCompiler c o usr bin cc o CMakeFiles cmTryCompileExec2478541284 dir testCCompiler c o c u suhubdyd code mxnet CMakeFiles CMakeTmp testCCompiler c Linking C executable cmTryCompileExec2478541284 usr bin cmake E cmake link script CMakeFiles cmTryCompileExec2478541284 dir link txt verbose 1 usr bin cc CMakeFiles cmTryCompileExec2478541284 dir testCCompiler c o o cmTryCompileExec2478541284 rdynamic make 1 Leaving directory ' u suhubdyd code mxnet CMakeFiles CMakeTmp' Detecting C compiler ABI info compiled with the following output Change Dir u suhubdyd code mxnet CMakeFiles CMakeTmp Run Build Command usr bin make cmTryCompileExec1232553970 fast usr bin make f CMakeFiles cmTryCompileExec1232553970 dir build make CMakeFiles cmTryCompileExec1232553970 dir build make 1 Entering directory ' u suhubdyd code mxnet CMakeFiles CMakeTmp' usr bin cmake E cmake progress report u suhubdyd code mxnet CMakeFiles CMakeTmp CMakeFiles 1 Building C object CMakeFiles cmTryCompileExec1232553970 dir CMakeCCompilerABI c o usr bin cc o CMakeFiles cmTryCompileExec1232553970 dir CMakeCCompilerABI c o c usr share cmake 3 0 Modules CMakeCCompilerABI c Linking C executable cmTryCompileExec1232553970 usr bin cmake E cmake link script CMakeFiles cmTryCompileExec1232553970 dir link txt verbose 1 usr bin cc v CMakeFiles cmTryCompileExec1232553970 dir CMakeCCompilerABI c o o cmTryCompileExec1232553970 rdynamic Using built in specs COLLECT GCC usr bin cc COLLECT LTO WRAPPER usr lib gcc x86 64 linux gnu 4 9 lto wrapper Target x86 64 linux gnu Configured with src configure v with pkgversion 'Debian 4 9 2 10' with bugurl file enable languages c c java go d fortran objc obj c prefix usr program suffix 4 9 enable shared enable linker build id libexecdir usr lib without included gettext enable threads posix with gxx include dir usr include c 4 9 libdir usr lib enable nls with sysroot enable clocale gnu enable libstdcxx debug enable libstdcxx time yes enable gnu unique object disable vtable verify enable plugin with system zlib disable browser plugin enable java awt gtk enable gtk cairo with java home usr lib jvm java 1 5 0 gcj 4 9 amd64 jre enable java home with jvm root dir usr lib jvm java 1 5 0 gcj 4 9 amd64 with jvm jar dir usr lib jvm exports java 1 5 0 gcj 4 9 amd64 with arch directory amd64 with ecj jar usr share java eclipse ecj jar enable objc gc enable multiarch with arch 32 i586 with abi m64 with multilib list m32 m64 mx32 enable multilib with tune generic enable checking release build x86 64 linux gnu host x86 64 linux gnu target x86 64 linux gnu Thread model posix gcc version 4 9 2 Debian 4 9 2 10 COMPILER PATH usr lib gcc x86 64 linux gnu 4 9 usr lib gcc x86 64 linux gnu 4 9 usr lib gcc x86 64 linux gnu usr lib gcc x86 64 linux gnu 4 9 usr lib gcc x86 64 linux gnu LIBRARY PATH usr lib gcc x86 64 linux gnu 4 9 usr lib gcc x86 64 linux gnu 4 9 x86 64 linux gnu usr lib gcc x86 64 linux gnu 4 9 lib lib x86 64 linux gnu lib lib usr lib x86 64 linux gnu usr lib lib usr lib gcc x86 64 linux gnu 4 9 lib usr lib COLLECT GCC OPTIONS ' v' ' o' 'cmTryCompileExec1232553970' ' rdynamic' ' mtune generic' ' march x86 64' code mxnet CMakeFiles CMakeOutput log 359L 31963C Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error Just run cmake CMakelist What have you tried to solve it Not yet,,"yajiedesign,yajiedesign",2017-01-06 22:36:30,2017-09-28 07:21:19
IS,mxnet 0 9 1 seems to be slower and use much more memory,Hi I just updated my system to the newest version of mxnet from the sources It seems to be using more memory and it is even slower compared to the previous version that I was using cf00ca064d5f6e46b062c2027ae2905c6147a04c I'm not using CUDNN because I need to deploy on a system that has cudnn2 I'm using just the classic cuda backend and testing on inception v3 network Same thing on example memcost passing from 41 to 74 MB Why is that Are you still optimizing nnvm Do you have any suggestion,,"piiswrong,tqchen,tqchen,tqchen,yajiedesign",2017-01-04 14:40:22,2017-09-28 07:21:26
IS,how to use mx sym dot,In mxnet I HAVE no idea about how to use the mx sym dot hope u can help me,,"sxjscience,sxjscience,yajiedesign",2017-01-07 09:21:52,2017-09-28 07:21:29
IS,how to save and load a DataIter or mx io NDArrayIter for the python program,the following code is the DataIter of lstm py I want to save the local filesytem After saving when the training I load the DataIter I do not want to build the DataIter again when train the model Thank you very much for a help,,yajiedesign,2017-01-08 10:42:44,2017-09-28 07:21:35
IS,src ndarray ndarray cc 831 Check failed mean shape 0 buff shape 1,,,yajiedesign,2017-01-09 02:21:37,2017-09-28 07:21:38
IS,Grad Error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OSX 10 12 2 Compiler gcc 4 2 1 Package used Python R Scala Julia Python If you are using python package please provide Python version and distribution 2 7 12 Error Message Please paste the full error message including stack trace MXNetError Traceback most recent call last ipython input 15 7c0c69847072 in module 1 x c grad wrt 'a' 2 print x list arguments 3 print x list outputs 4 5 ex2 x bind ctx mx cpu args c name ' 0 grad' mx nd ones 3 4 5 usr local lib python2 7 site packages mxnet 0 9 1 py2 7 egg mxnet symbol pyc in grad self wrt 951 mx uint len wrt 952 c wrt 953 ctypes byref handle 954 return Symbol handle 955 pylint enable no member usr local lib python2 7 site packages mxnet 0 9 1 py2 7 egg mxnet base pyc in check call ret 73 74 if ret 0 75 raise MXNetError py str LIB MXGetLastError 76 77 if sys version info 0 3 MXNetError 20 02 11 src c api c api symbolic cc 534 not implemented Stack trace returned 4 entries bt 0 0 libmxnet so 0x000000010e345b18 ZN4dmlc15LogMessageFatalD2Ev 40 bt 1 1 libmxnet so 0x000000010e8ee938 MXSymbolGrad 88 bt 2 2 ctypes so 0x000000010bd097f7 ffi call unix64 79 bt 3 3 0x00007fff549ab0f0 0x0 140734612812016,,"piiswrong,yajiedesign",2017-01-08 16:18:39,2017-09-28 07:21:41
IS,Check failed cudnnFindConvolutionBackwardDataAlgorithm,20170109 mxnet x64 vc14 gpu cudnn cudnn 8 0 windows10 x64 v5 1 13 19 31 D Program Files x86 Jenkins workspace mxnet mxnet src io iter image recordio cc 221 ImageRecordIOParser D MXNet Bin data image train rec use 3 threads for decoding 13 19 32 D Program Files x86 Jenkins workspace mxnet mxnet src io iter image recordio cc 221 ImageRecordIOParser D MXNet Bin data image val rec use 3 threads for decoding 13 19 44 d program files x86 jenkins workspace mxnet mxnet dmlc core include dmlc logging h 300 13 19 44 d program files x86 jenkins workspace mxnet mxnet src operator cudnn convolution inl h 557 Check failed cudnnFindConvolutionBackwardDataAlgorithm s dnn handle filter desc out desc conv desc in desc kMaxAlgos nalgo bwd data algo CUDNN STATUS SUCCESS 4 vs 0 13 19 44 d program files x86 jenkins workspace mxnet mxnet dmlc core include dmlc logging h 300 13 19 44 d program files x86 jenkins workspace mxnet mxnet src engine threaded engine h 336 13 19 44 d program files x86 jenkins workspace mxnet mxnet src operator cudnn convolution inl h 557 Check failed cudnnFindConvolutionBackwardDataAlgorithm s dnn handle filter desc out desc conv desc in desc kMaxAlgos nalgo bwd data algo CUDNN STATUS SUCCESS 4 vs 0 An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,yajiedesign,2017-01-09 06:23:44,2017-09-28 07:21:44
IS,win Validation accuracy,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System win10 Compiler Package used Python R Scala Julia python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace image Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"mli,mli,yajiedesign,yajiedesign,yajiedesign,yajiedesign,yajiedesign,yajiedesign",2017-01-03 12:47:15,2017-09-28 07:21:48
IS,Async execution causes errors in distributed computation,bind exec method in python module executor group py may cause libzmq communication errors in distributed computation since output arrays of executors are tried to be read before they are filled completely To my findings the related code part may be fixed as below pre if self for training self grad arrays exec grad arrays i for exec in self execs for i name in enumerate self arg names if b exec grad arrays i wait to read or b name in self param names else self grad arrays None data names x 0 for x in data shapes if self inputs need grad self input grad arrays exec grad arrays i for exec in self execs for i name in enumerate self arg names if b exec grad arrays i wait to read or b name in data names pre,,yajiedesign,2017-01-09 01:30:33,2017-09-28 07:21:52
IS,error when built mxnet with torch,I came to a problems when built mxnet with torch Here are the information and my configeration Anyone can help me to solve it please Environment info Operating System centos 7 Compiler gcc 4 8 5 Package used Python R Scala Julia python MXNet version Dec 12 2016 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 5 If you are using R package please provide R sessionInfo Error Message plugin torch torch base cc In member function void mxnet TorchState SetStream mshadow Stream Device with xpu mshadow gpu plugin torch torch base cc 45 16 error THCState has no member named currentStream CudaState currentStream mshadow Stream gpu GetStream s config mk 23 export CC gcc 24 export CXX g 25 export NVCC nvcc 30 the additional link flags you want to add 31 ADD LDFLAGS ' L opt OpenBLAS lib' 32 33 the additional compile flags you want to add 34 ADD CFLAGS ' I opt OpenBLAS include' 41 USE CUDA 1 42 43 add the path to CUDA library to link and compile flag 44 if you have already add them to environment variable leave it as NONE 45 USE CUDA PATH usr local cuda 46 USE CUDA PATH usr local cuda 47 48 whether use CuDNN R3 library 49 USE CUDNN 1 54 whether use opencv during compilation 55 you can disable it however you will not able to use 56 imbin iterator 57 USE OPENCV 1 58 59 use openmp for parallelization 60 USE OPENMP 1 65 UNAME S shell uname s 66 ifeq UNAME S Darwin 67 USE BLAS apple 68 else 69 USE BLAS openblas 70 endif 119 whether to use torch integration This requires installing torch 120 You also need to add TORCH PATH install lib to your LD LIBRARY PATH 121 TORCH PATH HOME torch 122 MXNET PLUGINS plugin torch torch mk 123 TORCH PATH HOME torch 124 MXNET PLUGINS plugin torch torch mk,,"piiswrong,yajiedesign",2017-01-09 14:14:27,2017-09-28 07:21:56
IS,The batch size of training data should no greater than the batch size of test data,Environment info Operating System Linux Mint 18 1 Compiler python 2 7 MXNet version Maybe 0 8 0 It was downloaded several days ago MXNet commit hash git rev parse HEAD a3a928c21ab91b246a5fab7c9ec135f6e616f899 If you are using python package please provide numpy sklearn Python version and distribution 2 7 Error Message Please paste the full error message including stack trace MXNetError 15 57 49 include mxnet ndarray h 259 Check failed shape 0 end Slice end index out of range Minimum reproducible example Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Open svm mnist py 2 Change the batch size in line 55 and 56 For example we can change them to 100 99 respectively 3 If the batch size of training data is larger than it in the test the code would fail to run What have you tried to solve it 1 2 3,,yajiedesign,2017-01-07 08:14:52,2017-09-28 07:21:59
IS,c win 20170110 mxnet x64 vc14 gpu is disable,C Users admin python Python 2 7 12 Continuum Analytics Inc default Jun 29 2016 11 07 13 MSC v 1500 64 bit AMD64 on win32 Type help copyright credits or license for more information Anaconda is brought to you by Continuum Analytics Please check out and import mxnet Traceback most recent call last File stdin line 1 in module File C Miniconda2 lib site packages mxnet 0 9 1 py2 7 egg mxnet init py line 7 in module from base import MXNetError File C Miniconda2 lib site packages mxnet 0 9 1 py2 7 egg mxnet base py line 43 in module LIB load lib File C Miniconda2 lib site packages mxnet 0 9 1 py2 7 egg mxnet base py line 35 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File C Miniconda2 lib ctypes init py line 362 in init self handle dlopen self name mode WindowsError Error 1114 DLL,,yajiedesign,2017-01-10 06:25:11,2017-09-28 07:22:02
IS,The key words of mx symbol Variable,In the document mxnet symbol Variable url It says that Variable has a key word dtype But in the Python implement code there is not,,"WellyZhang,yajiedesign",2017-01-10 07:55:11,2017-09-28 07:22:05
IS,Poor CPU scaling NDArray asnumpy bottleneck,I am having a performance scaling issue in CPU context From what I can tell it looks like this issue could suggest a bottleneck in the call NDArray asnumpy I initially noticed this problem using the old FeedForward class and more recently duplicated it using the newer Module class all details below are using Module When switching MXNET CPU WORKER NTHREADS and OMP NUM THREADS from 1 to 4 on a 4 core machine I only get a 13 training speed improvement My CPU usage does increase about threefold so the settings would seem to be working Also I get a 268 speed improvement in a numpy dot operation so I do see improvements there Given that multithreading seems to be working but there is not much performance improvement I tried using cProfile to identify any potential bottlenecks Using a test function code below here are the most expensive calls ncalls tottime percall cumtime percall filename lineno function 1 0 011 0 011 11 299 11 299 string 1 module 1 0 000 0 000 10 997 10 997 base module py 287 fit 21 0 000 0 000 10 752 0 512 module py 521 update metric 21 0 000 0 000 10 752 0 512 executor group py 413 update metric 21 0 002 0 000 10 751 0 512 metric py 134 update 42 10 745 0 256 10 748 0 256 ndarray py 488 asnumpy So it looks like the program is spending almost all of its time in the two calls to asnumpy made in the metric is update function Can anyone explain this effect My environment Operating System Windows 7 SP1 Compiler Visual Studio 2015 Package used Python R Scala Julia Python Multi threading library used OpenBLAS MXNet commit hash git rev parse HEAD 440b6379708693c5e8c1537b0ded65be9f2011bb from December 30th Python version and distribution WinPython 2 7 10 64 bit Code for numpy array multiplication test def numpy test A np random rand 4000 4000 B np random rand 4000 4000 C A dot B Sample code for mxnet test def asnumpy test hidden layer count X np random rand 10000 1000 y np random randint 2 size 10000 train iter mx io NDArrayIter X y 500 X eval np random rand 500 1000 y eval np random randint 2 size 500 eval iter mx io NDArrayIter X eval y eval 500 net mx sym Variable wouldata' for i in range hidden layer count layer mx sym FullyConnected data net num hidden 20 layer mx sym LeakyReLU data layer act type 'prelu' layer mx sym BatchNorm data layer net mx sym Concat net layer num args 2 label mx sym Variable isoftmax label' net mx sym SoftmaxOutput data net label label model mx mod Module net wouldata' isoftmax label' model bind train iter provide data train iter provide label model init params mx init MSRAPrelu optimizer params 'learning rate' 0 1 'momentum' 0 9 'wd' 0 0001 model fit train iter eval iter 'acc' optimizer params optimizer params num epoch 1,,"piiswrong,yajiedesign",2017-01-05 21:24:56,2017-09-28 07:22:08
IS,ImageNet Issues on Pascal GPUs,Hi All I was able to compile MXNET on the Pascal GPUs after adding gencode arch compute 60 code compute 60 flags The system uses cuda 8 0 I found that when I compile OpenCV with CUDA support turned off and run ImageNet I get only 20 25 Images Sec with two GPUs I thought that OpenCV was limiting performance so I used OpenCV with CUDA support turned on But when I do that I get seg faults When I did a back trace I found that simple functions such as CudaSetDevice Fail Attaching the backtrace below Not sure if it is a bug in OpenCV or MXNET 0 0x00003fffb7c9af54 in pthread mutex lock from lib64 libpthread so 0 1 0x00003fff6388d588 in cudbgApiDetach from usr lib nvidia libcuda so 1 2 0x00003fff638600f8 in cudbgApiDetach from usr lib nvidia libcuda so 1 3 0x00003fff63886cd0 in cudbgApiDetach from usr lib nvidia libcuda so 1 4 0x00003fff63972360 in cuVDPAUCtxCreate from usr lib nvidia libcuda so 1 5 0x00003fff638907c4 in cudbgApiDetach from usr lib nvidia libcuda so 1 6 0x00003fff638924dc in cudbgApiDetach from usr lib nvidia libcuda so 1 7 0x00003fff63849368 in cudbgApiDetach from usr lib nvidia libcuda so 1 8 0x00003fff63744644 in from usr lib nvidia libcuda so 1 9 0x00003fff638bbd30 in cuInit from usr lib nvidia libcuda so 1 10 0x00003fff9fdf4b9c in cudaInitManagedRuntime from usr local cuda lib64 libcudart so 8 0 11 0x00003fff9fdf7618 in cudaInitManagedRuntime from usr local cuda lib64 libcudart so 8 0 12 0x00003fffb7c9fa2c in pthread once from lib64 libpthread so 0 13 0x00003fff9fe378c8 in cudaGraphicsVDPAURegisterOutputSurface from usr local cuda lib64 libcudart so 8 0 14 0x00003fff9fdee9f8 in cudaInitManagedRuntime from usr local cuda lib64 libcudart so 8 0 15 0x00003fff9fdf8fa4 in cudaInitManagedRuntime from usr local cuda lib64 libcudart so 8 0 16 0x00003fff9fe13760 in cudaSetDevice from usr local cuda lib64 libcudart so 8 0 17 0x00003fffa22b3924 in mxnet StorageImpl ActivateDevice ctx at src storage storage cc 47 18 0x00003fffa22b1754 in mxnet StorageImpl Alloc this 0x3fff0c0073d0 size 1204224 ctx at src storage storage cc 95 19 0x00003fffa14e73c0 in mxnet NDArray Chunk CheckAndAlloc this 0x111dcea8 at include mxnet ndarray h 346 20 0x00003fffa14e731c in mxnet NDArray Chunk Chunk this 0x111dcea8 size 301056 ctx delay alloc false dtype 0 at include mxnet ndarray h 341,,"piiswrong,mli,yajiedesign",2017-01-06 16:38:39,2017-09-28 07:22:11
IS,dtype of Convolution weights,Can I use weight of Convolution with uint8 dtype Or there is any solution to make mxnet surport this,,"Godricly,piiswrong,yajiedesign",2017-01-10 07:57:49,2017-09-28 07:22:15
IS,GPU based Scala package on OS X,Is there a way that I can build a GPU based Scala package on OS X,,"Ldpe2G,Ldpe2G,yzhliu,yzhliu,yajiedesign",2017-01-05 21:21:45,2017-09-28 07:22:18
IS,I use multilayer bidirectional LSTM and CTC to train but Train Accuracy is 0,Here is my code def bi lstm unroll concat first seq len num hidden num label num lstm layer forward param cell backward param cell forward last states backward last states for i in range num lstm layer forward param cell append LSTMParam i2h weight mx sym Variable forward l d i2h weight i i2h bias mx sym Variable forward l d i2h bias i h2h weight mx sym Variable forward l d h2h weight i h2h bias mx sym Variable forward d h2h bias i forward state LSTMState c mx sym Variable forward l d init c i h mx sym Variable forward l d init h i forward last states append forward state backward param cell append LSTMParam i2h weight mx sym Variable backward l d i2h weight i i2h bias mx sym Variable backward l d i2h bias i h2h weight mx sym Variable backward l d h2h weight i h2h bias mx sym Variable backward d h2h bias i backward state LSTMState c mx sym Variable backward l d init c i h mx sym Variable backward l d init h i backward last states append backward state data mx sym Variable wouldata' label mx sym Variable 'label' wordvec mx sym SliceChannel data data num outputs seq len squeeze axis 1 hidden wordvec i for i in range seq len forward hidden range seq len backward hidden range seq len for layeridx in range num lstm layer for seqidx in range seq len forward next state lstm num hidden indata hidden seqidx prev state forward last states layeridx param forward param cell layeridx seqidx seqidx layeridx layeridx forward hidden seqidx forward next state h forward last states layeridx forward next state k seq len seqidx 1 backward next state lstm num hidden indata hidden k prev state backward last states layeridx param backward param cell layeridx seqidx k layeridx layeridx backward hidden k backward next state h backward last states layeridx backward next state for seqidx in range seq len hidden seqidx mx sym Concat forward hidden i backward hidden i dim 1 hidden concat mx sym Concat hidden dim 0 pred mx sym FullyConnected data hidden concat num hidden 70 label mx sym Reshape data label shape 1 label mx sym Cast data label dtype 'int32' sm mx sym WarpCTC data pred label label label length num label input length seq len return sm,,yajiedesign,2017-01-11 07:24:42,2017-09-28 07:22:21
IS,Distributed training problem,For distributed training I am facing the following issue the default dmlc tracker ssh does not source ' bashrc' Therefore it cannot use ananconda is package and cannot find cuda is so package position and report error like this OSError libcudart so 8 0 cannot open shared object file No such file or directory Is there anyway to solve this problem like add isource bashrc' into the beginning ssh commands Solution Need to modify dmlc core ssh py Add isource zshrc' to prog New problem After launch distributed learning In htop we can clearly see the process is running However in nvidia smi we cannot see the activity And the root machine is printout does not have anything What may be the cause Best Zhou,,yajiedesign,2017-01-09 15:15:53,2017-09-28 07:22:24
IS,Why I get different training performance with same NVIDIA cards,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 desktop 64bit Compiler gcc Package used Python R Scala Julia python MXNet version 0 7 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message EARLY VERSION I have two PC both Ubuntu 14 04 64bit one with GTX750Ti 2GB Intel I5x4 and the other with GTX1060 6GB Intel I7x8 I trained a customized SSD example with GTX750Ti and I got the speed at around 90 samples sec but when I tried GTX1060 I got only around 40 samples sec Why GTX1060 should get better performance right UPDATE Just now I get another GTX750Ti 2GB and I build a CUDA8 CUDNN5 1 MXNET environment on another computer all the same except the CPU Intel I7x8 The same thing happened On this card I get the poor performance too max to about 45 samples sec when training my own SSD example while I get about 80 samples sec on my first GTX750Ti I checked nvidia smi of both GTX750Ti per second when training and I found that the GTX750Ti with good performance showed stable volatile GPU Util around 95 but the one with bad performance showed quite a unstable volatile GPU Util ranged from 20 to 95 Later I checked GTX1060 too and its volatile GPU Util ranged from 0 to 49 So it seems like there is something wrong in my latest cuda mxnet environment built not GTX1060 is problem Environment CUDA 8 0 CUDNN 5 1 NVIDIA DRIVER 375 20 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python train py 2 3 What have you tried to solve it 1 I tried the nvidia 367 driver but it showed no difference 2 I reinstalled CUDA8 0 CUDNN5 1 and recompiled mxnet but still no difference,,"piiswrong,yajiedesign",2017-01-09 11:43:31,2017-09-28 07:22:31
IS,mxnet example autoencoder mnist sae py not converge with the specified parameters,Version of the code with the working parameters Please fix this part of the example,,yajiedesign,2017-01-11 17:52:58,2017-09-28 07:22:34
IS,issue using sgd with momentum,When I use sgd with momentum I train for 10 epoches first and save the parameters but the state created by mx optimizer Updater for gradient updates with momentum is not saved so if I want to train the same model from 10th epoch the state will be initialized with all zeros which is not what i want so i want to know whether there is the solution to this problem Thanks,,"piiswrong,yajiedesign",2017-01-11 09:24:42,2017-09-28 07:22:37
IS,about example error,I run lstm ocr py but name 'prefix' is not defined Could you tell me how to set the 'prefix' in lstm ocr py Thank you very much,,yajiedesign,2017-01-12 08:54:40,2017-09-28 07:22:40
IS,the validation curve of imagenet has oscillation anyone else met this problem,Environment info Operating System ubuntu Package used Python R Scala Julia python MXNet version 0 7 Or if installed from source yes Python version and distribution 2 7 Error Message when I train the imagenet using inception bn using sync and async I met a problem that the validation accuracy of imagenet has oscillation but the train accuracy is normal For example from epoch 0 to epoch 10 the validation accuracy is increased from 0 to 0 5 however at epoch 11 the validation is suddently decreased to 0 2 it can happen many times during the train making me feel confused I want to ask if there was anyone else who met the same problem or who can show me the right curve of validation accuracy of imagenet include sync and async I want to know the right curve of validation accuracy of iamgenet Thanks a lot,,"tornadomeet,yajiedesign",2017-01-11 07:46:12,2017-09-28 07:22:43
IS,base import MXNetError,ImportError Traceback most recent call last ipython input 2 36f7894d9f39 in module 1 import mxnet as mx 2 3 def to4d img 4 return img reshape img shape 0 1 28 28 astype np float32 255 5 home d mxnet python mxnet init py in module 5 6 from context import Context current context cpu gpu 7 from base import MXNetError 8 from import base 9 from import ndarray home d mxnet python mxnet base py in module 8 import atexit 9 import numpy as np 10 from import libinfo 11 12 all 'MXNetError' ImportError cannot import name libinfo,,"tornadomeet,yajiedesign",2017-01-11 07:16:36,2017-09-28 07:22:46
IS,Shared gamma for BatchNorm,I was wondering if it is possible to have multiple BatchNorm layers that use the same gamma variable I could not find how to define the variable name similar to what is done for convolution layer here,,"piiswrong,yajiedesign",2017-01-11 13:46:56,2017-09-28 07:22:49
IS,bug import mxnet error OSError dlopen mxnet python mxnet lib libmxnet so 6 Symbol not found ZN4half5 eLutE,while import mxnet report error import mxnet Traceback most recent call last File stdin line 1 in module File Users ppj Documents mxnet python mxnet init py line 7 in module from base import MXNetError File Users ppj Documents mxnet python mxnet base py line 43 in module LIB load lib File Users ppj Documents mxnet python mxnet base py line 35 in load lib lib ctypes cdll LoadLibrary lib path 0 File anaconda lib python3 5 ctypes init py line 425 in LoadLibrary return self dlltype name File anaconda lib python3 5 ctypes init py line 347 in init self handle dlopen self name mode OSError dlopen Users ppj Documents mxnet python mxnet lib libmxnet so 6 Symbol not found ZN4half5 eLutE Referenced from usr local opt openexr lib libIlmImf 2 2 22 dylib Expected in usr local opt openexr lib libIlmImf 2 2 22 dylib in usr local opt openexr lib libIlmImf 2 2 22 dylib,,"piiswrong,yajiedesign",2016-08-24 14:40:20,2017-09-28 07:22:53
IS,my mxnet was automatically installed under python 2 7 while I use anaconda for python 3 5,Hi I installed mxnet in linux mint I use anaconda for python 3 5 I followed the instruction and it was successfully installed Both mxnet and the anaconda are latest version However when I tried the code import mxnet as mx res mx nd array 1 2 3 I got the error AttributeError module 'mxnet' has no attribute 'nd' if I typed mx I got module 'mxnet' namespace after repeating the installation and checking the scripts I saw mxnet was installed under python 2 7 How can change it to python 3 5 Thanks Regards,,yajiedesign,2017-01-12 13:57:13,2017-09-28 07:22:56
IS,Are the training samples in each batch randomly picked from the whole bag of training data,I am just curious about if the samples in each batch are randomly sampled from the bag of whole training data If so is there a way to ensure the ratio of positive and negative samples in each batch Thanks,,yajiedesign,2017-01-12 19:34:03,2017-09-28 07:22:59
IS,Cudnn error when training faster rcnn,I'm trying to use my own datasets to train faster rcnn The code I use is based on the example code url I modified the labels to fit my dataset I compile MXnet with cuda 8 0 and cudnn The command I use to train is Environment info Operating System Ubuntu 14 04 Package used Python R Scala Julia Python MXNet version 0 9 1 MXNet commit hash git rev parse HEAD fbb68859699861bc104f4a692c660d74cff72f66 Python version and distribution Python 2 7,,yajiedesign,2017-01-13 02:26:29,2017-09-28 07:23:02
IS,Is there a tanh loss layer in mxnet,In some classification problems it is useful to train a network with the last layer being a tanh and minimizing the mean square error Is there anything like that,,"Godricly,piiswrong,yajiedesign",2017-01-13 01:09:47,2017-09-28 07:23:04
IS,cudnn convolution performance tests on cnn lstm ctc,Environment info Operating System ubuntu 14 04 Compiler gcc 4 8 Package used Python R Scala Julia python MXNet version 0 9 1 0 7 1 nightly 32cb6bc Or if installed from source MXNet commit hash git rev parse HEAD 32cb6bc If you are using python package please provide Python version and distribution anaconda 2 7 11 Error Message I used cnn lstm ctc to do ocr tasks and it can work well with mxnet 0 7 version but when I use the latest mxnet version 0 9 1 it will block here forever Minimum reproducible example Steps to reproduce 1 2 3 What have you tried to solve it,,"piiswrong,yajiedesign",2017-01-13 07:08:16,2017-09-28 07:23:07
IS,How to upsample a feature map,I want to unsample a feature map by a factor of 4 e g 9x9 36x36 nearest method I find the mxnet symbol upsamping but I do not know exactly how to use it Could somebody give me an example,,yajiedesign,2017-01-13 17:20:47,2017-09-28 07:23:10
IS,very small mistake in mx rcnn debug mode,A little mistake in Loader py file Line 304 missing before len label list len new label list format len label list len new label list url L304 But still awesome code which I have learned a lot from Thanks,,"piiswrong,precedenceguo,yajiedesign",2017-01-11 06:05:54,2017-09-28 07:23:13
IS,where to find hostd3 pl and ami local bptt cfg in the file mxnet example speech demo run ami sh,Hi when i tried to run mxnet example speech demo run ami sh the files hostd3 pl in line 16 and ami local bptt cfg in line 35 were not found can you tell me where to find them thank you,,yajiedesign,2016-09-04 12:34:23,2017-09-28 07:23:16
IS,how to print job status of worker in distributed training,I use the launcher to to start distributed training in AWS instances for my customed training code,,yajiedesign,2017-01-15 09:09:58,2017-09-28 07:23:19
IS,problem when running warp ctc example on latest mxnet version,Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 Package used Python R Scala Julia Python MXNet version latest 0 9 1 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Anaconda 2 7 11 Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error run example warp ctc toy ctc py What have you tried to solve it 1 toy ctc py work fine on mxnet 0 7,,"piiswrong,yajiedesign",2017-01-14 03:03:47,2017-09-28 07:23:23
IS,Different performance of Deconvolution layer with or without num group,Hi I tried to use deconvolution layer to do upsampling I guess a good way is to use deconvolution layer with bilinear kernels There could be two ways of doing it one is to set num group equal to the channel number and the other one is to use num group 1 and use a multi channel kernel However I observed there was a big difference using these two The test code is as follows Using num group 1 seems quicker but if the channel number is very larger the kernel will be very big Since most of the values are 0 setting num group channel seems a good solution but is this suppposed to be slow,,yajiedesign,2017-01-02 11:33:41,2017-09-28 07:23:32
IS,Occasionally dead cycle for distributed training,Sometimes the distributed training will be blocked With gdb attatch the following code causes the dead cycle Is that a bug or something related to my environment,,"mli,yajiedesign",2016-11-08 01:01:09,2017-09-28 07:23:35
IS,Bug in src resource cc function GetHostSpace,L56 This line should be,,yajiedesign,2017-01-16 12:01:04,2017-09-28 07:23:38
IS,Feature request Can anyone implement an operator equivalent to tensorflow multinomial,Tensorflow use operator multinomial for sampling inside computational graph It is useful for schedule sampling training of RNN Recent work on generating sentences by GAN seqGAN also relies on tf multinomial in their implementation I suppose that without such operator it is troublesome to integrate sampling into computation flow during training phase So I try to implement it myself However after some research I found no way to perform discrete distribution sampling in cuda API Here is a unsuccessful trail Generating sample from a non uniform discrete distribution Off course I can use mx operator CustomOp to write a Numpy version but it is the last choice for me,,"sxjscience,sxjscience,asmushetzel,sxjscience,asmushetzel,sxjscience,asmushetzel,piiswrong",2016-11-15 10:27:02,2017-09-28 07:43:15
IS,c win wrong predict results for image classification,MXNet 20160531 win10 x64 cpu VS2013 As image classification predict cc ALL the image MXPredGetOutput return the same result image,,"mli,mli,yajiedesign",2017-01-04 05:24:06,2017-09-28 08:00:11
IS,Check failed it node2index end it first nptr get,Hi I'm trying to do some reimplementation of Yin Sch tze is paper ABCNN I wrote these functions to calculate attention matrix and attention based pooling L29 L43 Using these lines to debug these functions and there was no error L208 But when I started training the model it returned I found the 2317 it said concat op cannot handle correctly the case when the inputs contain duplicates But when I debug the similarity and ABPooling function separately there is no problem with Concat And there is no other Concat sym in model structure except these functions Thanks a lot,,"piiswrong,piiswrong,tqchen,yajiedesign",2016-12-29 12:34:16,2017-09-28 08:00:27
PR,Enable fused softmax smoothing,MXNet modifications to support softmax smoothing,,KellenSunderland,2017-09-28 09:00:26,2017-09-28 09:00:42
IS,error in demo py,Dear community I'm using mx net 0 11 1 under windows x64 with cuda 8 Executing the ssd example i encountered serveral problems As i have seen in other related issues some will get fixed I fixed a few myself but now i have a problem File C local Anaconda3 4 1 1 Windows x86 64 envs mxnet lib site packages mxnet io py line 395 in prefetch func self next batch i self iters i next File D MachineLearning MxNet incubator mxnet example ssd dataset iterator py line 215 in next pad self getpad index self getindex File C local Anaconda3 4 1 1 Windows x86 64 envs mxnet lib site packages mxnet io py line 156 in init assert isinstance data list tuple Data must be list of NDArrays AssertionError Data must be list of NDArrays I get those error is and i must admit i do not see the cause of this The test images and models get loaded correctly At least i'm thinking that after debugging Does anyone know how to solve this issue I can reproduce this easily by typing demo py epoch 0 images data demo dog jpg thresh 0 5 Best Regards,,zhreshold,2017-09-25 19:46:05,2017-09-28 12:56:58
IS,CURAND Gen Uniform float failed,Help,,"sxjscience,piiswrong,phunterlau",2016-12-04 09:24:21,2017-09-28 17:29:24
IS,cannot find aws s3 file,The following code return the error 09 38 25 src io input split base cc 163 Check failed files size 0U 0 vs 0 Cannot find any files that matches the URI patternz s3 What did I go wrong,,"mli,phunterlau",2017-01-07 09:46:21,2017-09-28 17:29:26
IS,mx gpu function when install without GPU,It seems that even when mxnet is installed without gpu mx gpu still gives gpu 0 which is confusing,,"kevinthesun,phunterlau",2017-01-17 00:37:27,2017-09-28 17:29:27
IS,Compiler error C2664 in mxnet cpp io h struct MXDataIterBlob constructor,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Microsoft Windows 10 Compiler Visual Studio 2015 C 15 x64 Error Message 1 C MxNet MXNet cpp include mxnet cpp io hpp 21 error C2664 istd shared ptr mxnet cpp MXDataIterBlob std make shared mxnet cpp MXDataIterBlob nullptr nullptr ' cannot convert argument 1 from 'nullptr' to 'nullptr ' Minimum reproducible example I was building mlp example as a DLL with Microsoft CLR support What have you tried to solve it I resolved the bug by providing static cast to nullptr as shown below in io hpp line 21 MXDataIter MXDataIter const std string mxdataiter type creator mxdataiter map GetMXDataIterCreator mxdataiter type blob ptr std make shared MXDataIterBlob nullptr this original code gives compiler error C2664 blob ptr std make shared MXDataIterBlob static cast DataIterHandle nullptr,,"piiswrong,howard0su,phunterlau",2017-01-16 22:38:27,2017-09-28 17:29:28
IS,Training validation test data division of ImageRecordIter,Hi MXNet ImageRecordIter divides CIFAR 10 data into a training set of 50K samples and a test set of 10K samples There are numerous papers that divide CIFAR 10 data into 45K training samples 5K validation samples and 10K test samples I would like to adopt an identical division in order to produce comparable results Any solution I cannot abandon ImageRecordIter because I am using it for data argumentation Thank you very much,,phunterlau,2017-01-17 09:22:44,2017-09-28 17:29:39
IS,One vs All Classification,Hi I'm using a CNN in R to perform image classification with K different classes However I want to try another approach and do a one vs all classification with each label so one label would be 1 and the rest would be 0 My question is what output layer should I use If I use mx symbol SoftmaxOutput after a FullyConnected layer with one neuron is useless because it always predict 1 or even NaN for every example LinearRegressionOutput gives a similar result My basic idea is that I need an output layer that performs 0 5 threshold on the single output neuron from the last FullyConnected layer Sorry if this is a dumb question I'm still new to this field Thanks in advanced,,"piiswrong,phunterlau",2016-11-29 23:56:45,2017-09-28 17:29:41
IS,CSV Iterator errors number of input must be bigger than batch size,High level issue description There appeared to be a bug in the most recent version of MxNet which affects CSV Iterator and makes it virtually unusable In fact this issue is very critical because NDArrayIter cannot be used for very large data sets that are not fitting in memory entirely It invalidates one of the most attractive features of MxNet so unless this issue is resolved we cannot use MxNet for many of our clients Please review Environment info Operating System Both Ubuntu 14 04 3 MS Windows 8 1 Compiler GCC Ubuntu 4 8 4 2ubuntu1 14 04 3 4 8 4 on Ubuntu VS 2015 Update 2 on Windows Package used Python MXNet version Installed from source the moist recent version taken from on January 10 2017 MXNet commit hash git rev parse HEAD 837fe9b972c8e92cd5d8a8bafd9f788fd3693dbf If you are using python package please provide No Python version and distribution Anaconda3 Python 3 5 1 If you are using R package please provide No Error Message src io iter batchloader h 100 Check failed base Next number of input must be bigger than batch size libmxnet dll mxnet io BatchLoader Next Line 100 C libmxnet dll mxnet io PrefetcherIter Init l2 lambda mxnet DataBatch dptr Line 78 libmxnet dll dmlc ThreadedIter mxnet DataBatch Init l2 lambda Line 338 C C Minimum reproducible example I m using the minimalistic code from original kaggle ndsb2 example provided with MxNet This is the only example in the latest package which is using CSVIter I simplified this code and provided a small CSV data sets to reproduce the error please see attachments both Train min py txt please remove txt extension before running py code test data txt and test label txt are attached Steps to reproduce 1 Just run Train min py as is without any parameters to reproduce the error The most critical line here is line 45 which sets the batch size batch size 181 Please keep in mind that the data set contains 681 lines in CSV files I assume that the batch size up to 681 inclusive should be supported This is the behavior we have seen in the previous version of MxNet built from source on September 14 2016 Behavior is slightly different on Windows and Linux On Linux the highest batch size I can specify is 180 Everything higher than 181 inclusive failed with the above error On Windows the highest batch size I can specify is 91 Everything higher than 92 inclusive failed with the above error Using smaller batch size 180 on Linux and 91 on Windows helps us to have the run completed without the errors However in this case most of the data is ignored and only first 180 91 records are used in training What have you tried to solve it 1 I was able to debug the source code in C and can see that MxNet calculates the number of labels as 86 instead of 681 which it really is It looks like the system assumes that 681 is not the number of records in the file but the number of bytes 681 8 85 125 or something like that 2 I tried to switch from ThreadedEngine to NaiveEngine but it did not help 3 I ve also tried turning round batch option of CSVIter on and off but it did not help either 4 I stopped my research at this point because it looks like the affected code goes beyond MxNet itself into DMLC s ThreadedIter and fixing the bug might affect other MxNet functionality which should be thoroughly tested and verified Train min py txt test data txt test label txt Regards Vladimir Drozdetski,,phunterlau,2017-01-17 18:43:44,2017-09-28 17:29:43
IS,About std thread In ThreadPool is Very slow to Start the Thread Entry Function,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux version 3 10 0 327 el7 x86 64 Compiler gcc version 4 8 3 20140911 Red Hat 4 8 3 9 GCC Package used Python R Scala Julia Python MXNet version 0 7 Or if installed from source Yes I Install from Source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 5 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 mxnet face master 2 3 What have you tried to solve it 1 Debug 2 3,,phunterlau,2017-01-18 03:28:48,2017-09-28 17:29:45
IS,small amount memory cost in GPU device 0 even if context is set to mx gpu 1,I just noticed that running mx mod Module fit can produce a small amount GPU memory cost in device 0 even if I set context to mx gpus 1 when creating the mx mod Module variable mod mx mod Module context mx gpu 1 symbol net The problem seems to be caused at line 410 in python mxnet module module py when it tries to copy initialized local parameters to kvstore I loaded my pre trained weights using mx nd load which means these parameters are stored in cpu originally Thus when copying them to kv store they are copied to mx gpu 0 by default I am not sure if it is a bug or so But maybe we should check and transfer arg params aux params to the target device in mx mod Module init params,,"nicklhy,phunterlau",2017-01-18 03:37:50,2017-09-28 17:29:46
IS,Why cudaStreamCreate will consume a long time in mxnet while with a short time in a simple c demo,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,phunterlau,2017-01-18 07:08:07,2017-09-28 17:29:47
IS,About MXNDArrayCreateEx The Performence with std make shared,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System x86 64 Red Hat Linux Compiler gcc4 8 5 Package used Python R Scala Julia Python MXNet version v0 7 Or if installed from source Yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3 when I call this Function as followed arg params data mx nd array img ctx it will take a long time for a return from this function and I has debug with gdb args and python m pdb between python and c then I found that It is mainly because the std make shared when allocated Thank you very much for giving me some suggestion thank you sir,,"piiswrong,phunterlau",2017-01-01 16:42:50,2017-09-28 17:29:48
IS,did datahandle in kvstore dist server h and pull push in kvstore dist h both run on the same node could we also profiler the datahandle event in kvstore dist server h,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,phunterlau,2017-01-18 08:45:23,2017-09-28 17:29:50
IS,Library built with VS2015 causes python a hang,Hi I built mxnet from master branch on a Windows 7 machine with Visual Studio 2015 Update 3 CUDA 8 CuDNN 5 1 and MKL There was no error and only a few warnings when I built it When I tested it with the examples for instance image classification the task could finish I mean I can get the results but python could not quit properly There is no error message and I had to kill the python process manually I tried GPU and CPU modes both have this issue,,"tornadomeet,phunterlau",2016-10-05 11:08:47,2017-09-28 17:29:51
IS,Core installation success but Python interface fail Win7 VS2015 CUDA 8 Cudnn 5 1,I am trying to install with win7 vs 2015 The core libraries build libmxnet lib libmxnet dll is successful but when I tried to install Python interface it failed python01 My sys info win 7 VS 2015 cuda 8 cudnn 5 1 Cmake 3 7 2 Python 2 7 Anaconda 4 2 OpenCV 3 2 Discussion I tried to solve it by installing Visual C 9 0 and re running setup py script but it certainly does not work neither I am wondering if building VS2015 is possible since on the website it specifically says uisng VS 2013 The reason is that VS2015 was not compatible with CUDA back then Since now VS 2015 is working with CUDA 8 I am trying to build it with VS 2015 Another possible reason is about OpenBlas I read that to use OpenBlas using mingw64 dll is a must details are in the link However I did not use it and the building of the library seems fine SoI do not know if this causes the failure of the Python interface Also I use the latest OpenCV 3 2 available 2016 12 so I am curious if that is an issue Anyway would someone know how to solve this issue Thanks,,"piiswrong,piiswrong,tqchen,phunterlau",2017-01-18 18:52:03,2017-09-28 17:29:52
IS,Tree LSTM implementation,Is that possible and easy to implement Tree LSTM in MXNet I have noticed that DyNet can do this dynamically Is that also easy for MXNet with imperative programming,,phunterlau,2017-01-19 01:57:01,2017-09-28 17:29:53
IS,Sometimes in some environment load params is very slow it seem as blocking in somewhere,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"piiswrong,phunterlau",2017-01-18 09:14:57,2017-09-28 17:29:54
IS,MakeLoss should accept optional output symbol,I would like to suggest that the MakeLoss symbol constructor takes an optional output symbol When it is provided the forward output of MakeLoss would be the same as the provided output symbol while the backward operation would be using the gradient of the input loss The value of this approach is that it means you can more easily retrieve the prediction values when using an executor bound to a custom loss without a second executor for the earlier prediction symbol In particular Module is predict method would then correctly output the prediction not the error when built around a custom MakeLoss loss symbol Example usage for custom squared error on liner regression,,"piiswrong,piiswrong,phunterlau",2017-01-19 23:19:57,2017-09-28 17:29:55
IS,Using NDArrayIter causes Huge Memory Usage,I have got a network that I'm trying to train but I'm running into weird memory issues When using the following code Is it normal to have this amount of memory usage It seems extraordinarily high to me As I mentioned above I want to get this training on my GPU but with things currently this does not seem like it will be a possibility Even if I set the batch size to 1 it is still too much for my GPU So my questions regarding this are as follows Is this amount of memory usage normal Am I using NDArrayIter wrong Is there another type of iterator I can use with less memory footprint if this is normal Thanks in advance,,phunterlau,2017-01-20 01:07:18,2017-09-28 17:29:57
IS,slow IO,Hi All I was running imagenet with test io 1 and found that I get only 22 samples sec which is very slow On profiling with cProfile it shows that the majority of time is spent in 21 70 439 3 354 70 441 3 354 io py 565 next Not sure what occurs behind the covers am I missing anything basic The raw IO performance seems fine def next self if self debug skip load and not self debug at begin return DataBatch data self getdata label self getlabel pad self getpad index self getindex if self first batch is not None batch self first batch self first batch None return batch self debug at begin False next res ctypes c int 0 check call LIB MXDataIterNext self handle ctypes byref next res if next res value return DataBatch data self getdata label self getlabel pad self getpad index self getindex else raise StopIteration,,phunterlau,2017-01-19 17:45:28,2017-09-28 17:29:58
IS,could not use gpu,I have compiled the source file and installed the python interface But when i tried to use gpu it failed 847231828126022107 My environment is windows 7 64bit vs2015 with update3 cmake 3 7 1 cuda7 5 cudnn v3 opencv 3 0 openblas python3 4 winpython when I install the python interface I encounter an error c program files x86 microsoft visual studio 10 0 vc bin x86 amd64 cl exe fatal error with statue 2 I modified the regedit change the root of microsoft visual studio 10 0 to microsoft visual studio 12 0 and successfully installed the python interface Are there something wrong,,"howard0su,phunterlau",2017-01-19 15:15:31,2017-09-28 17:30:00
IS,BlockGrad Bug,Environment info Operating System Windows Compiler Visual Studio Community 2015 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD Error Message,,"sxjscience,piiswrong,tqchen,piiswrong,tqchen,phunterlau",2017-01-19 14:28:30,2017-09-28 17:30:01
IS,Why rescale the gradient with batch size,I am training a network for semantic segmentation by using the following softmax symbol and setting softmax mx symbol SoftmaxOutput data upscore label label multi output True normalization 'valid' use ignore True ignore label 255 name isoftmax' model mx model FeedForward ctx devs symbol net symbol num epoch args num epochs learning rate args lr momentum 0 9 wd 0 0001 optimizer 'nag' initializer mx init MSRAPrelu model args I found that the default value of rescale grad is setting to rescale grad 1 0 batch size in model py L794 I am confused about the setting of rescale grad Because I think the gradient in the softmax output L175 has been normalized properly by valid cnt Could someone explain the rescale grad setting Thanks,,phunterlau,2017-01-22 07:02:58,2017-09-28 17:30:04
IS,How to use native Infiniband instead of IPoIB,Hi All I could use distributed training with IPoIB But since using IPoIB is still slower than using native Infiniband is there any way to use native Infiniband Thanks Regards Rengan,,"howard0su,phunterlau",2017-01-22 15:31:05,2017-09-28 17:30:05
IS,unable to run train end2end py,I had created the data and model directories in examples rcnn downloaded the datasets and pre trained models when I run train end2end it give me the following error line 99 in load pascal annotation size cv2 imread roi rec 'image' shape AttributeError 'NoneType' object has no attribute ishape' opencv cannot read the file have no clue why I started debugging the script and although the path is there opencv does not read the file one more question why should I open each image to get it is width and height these data is recorded in the annotation file,,"precedenceguo,phunterlau",2017-01-20 22:11:44,2017-09-28 17:30:06
PR,Skip adding control deps if nodes are the same,The following code would fail before this PR due to introducing a cycle into the gradient graph See this issue for analysis This PR also fixes this issue of the same root cause,,"reminisce,tqchen,piiswrong,reminisce,tqchen,tqchen",2017-09-27 05:46:32,2017-09-28 17:33:23
IS,rcnn custom dataset tutorial,Is there any tutorial on how to train rcnn on custom dataset thanks,,"precedenceguo,phunterlau",2017-01-21 21:04:18,2017-09-28 17:42:16
IS,im2rec resize 100 original image have various width and height how to get ImageRecordIter datashape,I using Mxnet bin im2rec set resize to 100 my original image width and height are different for each single image so how do I set datashape in ImageRecordIter Here is what I do Anybody know why,,"Godricly,phunterlau",2017-01-23 06:03:25,2017-09-28 17:42:17
IS,DistilledSGLD regression algorithm shows the MSE is nan,I use my own data insteed of the sample toy data and the MSE is nan I debug in the python source code and find that the student network output is nan Environment info Operating System WIN7 Compiler Prebuild 20160531 Package used Python R Scala Julia Python MXNet version 0 93 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7,,phunterlau,2017-01-23 00:45:54,2017-09-28 17:42:18
IS,the 'for' loop is not neccessary in roi pooling operator,L187,,phunterlau,2017-01-21 03:27:36,2017-09-28 17:42:19
IS,Typo in tutorial,mx save temp ndarray a should be mx nd save temp ndarray a,,phunterlau,2017-01-23 11:56:32,2017-09-28 17:42:20
IS,About creat new operator Is it true that I do not need write cu at all time using c,Hi I think I just need write x inl h and x cc when creating a new operator using mshadow in mxnet using c It can be tanslated to cu when I set the CUDA 1 and make automatically When is it translated after make So Are xx cu s in src operator created as above Or when should I write xx cu manually Some code in xx cu s is look like writing manually Thank you,,"Godricly,phunterlau",2017-01-23 08:21:03,2017-09-28 17:42:22
IS,How to run MXNet with less than 1 epoch,Hi All Is there any way to run MXNet with fewer than 1 epoch Because Nvidia profiler nvprof crashed even with 1 epoch And the possible reason is that it generated too much profiling data So can I run MXNet with only a specific number of iterations which is less than 1 epoch Regards Rengan,,"piiswrong,phunterlau",2017-01-23 20:17:41,2017-09-28 17:42:23
IS,Compilation fails with cuDNN and cuda 7 5 18,If I compile the current master with cuDNN enable I encounter many of these errors Versions gcc 4 9 2 cuDNN 5 0 4 according to header file cuda 7 5 18 mxnet b12f957ac1,,"vchuravy,mli,vchuravy,phunterlau",2016-08-25 16:03:11,2017-09-28 17:42:24
IS,validation accuracy of imagenet,When I train the imagenet on 5 PCs distributed training I find that the validation of imagenet is about 68 which is lower than 72 the mxnet doc gives Here are my params dist kvstore sync num epoch 100 batch size 28 network inception bn and other params are default And the following is my results val accuracy 67 2 train accuracy 90 6 top5 val accuracy 87 top5 accuracy 98 I want to know whether my result is right Because it is lower than the offical guide the doc gives I an not sure of my result If there is any one can give me some information thank a lot,,phunterlau,2017-01-24 06:14:36,2017-09-28 17:42:26
IS,Howto cloud md instructions do not work,I followed the examples but there was no test conv file under tests python gpu When I am trying to import mxnet here is the error I got ubuntu ip 172 31 47 238 mxnet python python Python 2 7 6 default Oct 26 2016 20 30 19 GCC 4 8 4 on linux2 Type help copyright credits or license for more information import mxnet Traceback most recent call last File stdin line 1 in module File mxnet init py line 7 in module from base import MXNetError File mxnet base py line 9 in module import numpy as np File usr lib python2 7 dist packages numpy init py line 153 in module from import add newdocs File usr lib python2 7 dist packages numpy add newdocs py line 13 in module from numpy lib import add newdoc File usr lib python2 7 dist packages numpy lib init py line 18 in module from polynomial import File usr lib python2 7 dist packages numpy lib polynomial py line 19 in module from numpy linalg import eigvals lstsq inv File usr lib python2 7 dist packages numpy linalg init py line 50 in module from linalg import File usr lib python2 7 dist packages numpy linalg linalg py line 29 in module from numpy linalg import lapack lite umath linalg usr lib liblapack so 3 undefined symbol ATL chemv How out of date are the instructions in this md file Please make it clear to people I wasted more than 1 day trying to follow the directions and debug what might be wrong,,"mli,phunterlau",2017-01-26 01:19:03,2017-09-28 17:42:31
IS,Are there any other distributed training examples,I only find one MNIST lenet example from here I prefer to run it in the AWS,,"mli,phunterlau",2017-01-24 07:30:33,2017-09-28 17:42:32
IS,Source operators prevent fast symbol resizing,Stochastic source ops sample ops are useful obviously While it is possible to sample from random distributions outside of your network and provide the randomness as input this is slower more complicated and makes the graph less self contained However there is a problem with using the source ops like 'normal' and 'uniform' as it stands which is that they break an important property of an symbol graph resizability It is highly desirable to be able to resize an existing symbol by simply doing shape inference using a different batch size of the relevant input tensors rather than the alternative of generating a completely new symbol graph each time you want to use a different batch size A very important application of fast resizing is when evaluating a net on a large set of user inputs One obviously should break the input into batches that match the batch size one chose for the evaluation network but there will be in general a 'leftover' part at the end and to generate a whole new graph to evaluate that leftover part or using junk data will often be slower than the ideal thing which is to fast resize to the length of the final piece then evaluate There are others examples of the value of fast resizability probably the most important one is for me is dealing with variable length sequence networks when unrolling can be avoided e g using the RNN op where creating buckets quickly is again very desirable For a concrete example of how resizability can be broken by sample ops let is take variational dropout That is straightforward to implement by sampling from a bernoulli distribution and then doing a broadcast multiply onto the sequence tensor The problem here is that the batch size needs to be hardcoded into the sample op is ishape' parameter because backward shape inference does not pass through broadcast layers as broadcast is 'polymorphic' it has multiple input shapes that would work with the output shape And even if we make a variational dropout op or modify the existing dropout op to be able to do broadcasting internally we can not depend on backward shape inference to solve this problem for user defined networks that come from a high level framework like that of Mathematica The user might want to do a reshape or broadcast of a random source and then again MXNet will not be able to figure out the batch size to use for the sample op I have prototyped a solution for this already was pretty straightforward to implement and simple to reason about but it does require making a small modification to the nnvm backprop engine so first I would first like to ask what the preference of the core MXNet developers is to solve this problem I can go into detail about my proposed solution if that is desired,,"taliesinb,taliesinb,piiswrong,piiswrong,taliesinb,taliesinb,phunterlau",2017-01-07 11:39:22,2017-09-28 17:42:33
IS,mxnet very bad accuracy compared to other packages simply when learning a b c,Hello I find that learning how to do an addition works well in mxnet but not how to do a multiplication Can anyone see what training parameters to use I tried various learning rates batch sizes etc with overall very bad performance compared to other deeplearning packages Here is a little example in R library mxnet x matrix runif 3000 1000 3 y x 1 x 2 x 3 training set of 1000 samples x2 matrix runif 3000 1000 3 y2 x2 1 x2 2 x2 3 testing set m mx mlp x y hidden node 100 out node 1 out activation rmse learning rate 0 01 training print sqrt mean predict m x2 y2 2 test around 0 05 i e about 40 error example of comparison H2O h2o init m h2o deeplearning x 1 3 y 4 training frame as h2o cbind x y print sqrt mean as data frame predict object m y 4 newdata as h2o x2 predict y2 2 relative error on the order of 10 of note even that is much some other packages get 1 error,,phunterlau,2017-01-24 23:46:24,2017-09-28 17:42:34
IS,ImageNet expected speed on Pascal GPUs with resnet 50,Hi All Wondering if any body knows the expected rate with resnet 50 for imagenet on Pascal GPUs I am debugging a problem most likely an I O issue but knowing in what range will it be can help me bs 128 Even a single node or single GPU rate would also help thanks,,"mli,phunterlau",2017-01-23 20:12:14,2017-09-28 17:42:36
IS,OSError home sojoyoo mxnet python mxnet lib libmxnet so undefined symbol ZN2cv12 OutputArrayC1ERNS 3MatE,Environment info Operating System Ubuntu 16 04 Compiler gcc g 5 4 Package used Python R Scala Julia python Or if installed from source 0 9 3 MXNet commit hash git rev parse HEAD 64bdd25665019cec2ca38009b1e1a1cdf96d3cb2 If you are using python package please provide Python version and distribution Python 2 7 12 miniconda2 Error Message Traceback most recent call last File train mnist py line 8 in module from common import find mxnet fit File home sojoyoo mxnet example image classification common find mxnet py line 4 in module import mxnet as mx File home sojoyoo mxnet python mxnet init py line 7 in module from base import MXNetError File home sojoyoo mxnet python mxnet base py line 43 in module LIB load lib File home sojoyoo mxnet python mxnet base py line 35 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File home sojoyoo miniconda2 lib python2 7 ctypes init py line 362 in init self handle dlopen self name mode OSError home sojoyoo mxnet python mxnet lib libmxnet so undefined symbol ZN2cv12 OutputArrayC1ERNS 3MatE Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I installed opencv 2 4 13 as the docmunts mentioned 2 I installed mxnet by runing install mxnet ubuntu python sh 3 The installation was finished successfully 4 I want to make sure the setup is ok so i tried to run train mnist py as below 1 cd home sojoyoo mxnet examples image classification 2 python train mnist py What have you tried to solve it 1 I have no idea of this error,,phunterlau,2017-01-27 04:54:24,2017-09-28 17:42:37
IS,Installation problem while make j nproc,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler gcc 5 4 Package used Python R Scala Julia R MXNet version 0 9 1 Or if installed from source check out from github MXNet commit hash git rev parse HEAD b67a4457ffd162ce8e7b7eca96285111362e74b9 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 follow standard installation step to build mxnet 2 make j nproc the following compilation error appear build src operator convolution gpu o In function mxnet op CuDNNConvolutionOp float SelectAlgo mxnet Context const std vector nnvm TShape std allocator nnvm TShape const std vector nnvm TShape std allocator nnvm TShape const lambda mxnet RunContext 1 operator mxnet RunContext const' home clive mxnet src operator cudnn convolution inl h 568 undefined reference to mxnet op CuDNNAlgoReg Get ' build src operator convolution gpu o In function mxnet op CuDNNConvolutionOp double SelectAlgo mxnet Context const std vector nnvm TShape std allocator nnvm TShape const std vector nnvm TShape std allocator nnvm TShape const lambda mxnet RunContext 1 operator mxnet RunContext const' home clive mxnet src operator cudnn convolution inl h 568 undefined reference to mxnet op CuDNNAlgoReg Get ' build src operator convolution gpu o In function mxnet op CuDNNConvolutionOp mshadow half half t SelectAlgo mxnet Context const std vector nnvm TShape std allocator nnvm TShape const std vector nnvm TShape std allocator nnvm TShape const lambda mxnet RunContext 1 operator mxnet RunContext const' home clive mxnet src operator cudnn convolution inl h 568 undefined reference to mxnet op CuDNNAlgoReg Get ' build src operator convolution gpu o In function mxnet op CuDNNConvolutionOp float SelectAlgo mxnet Context const std vector nnvm TShape std allocator nnvm TShape const std vector nnvm TShape std allocator nnvm TShape const ' home clive mxnet src operator cudnn convolution inl h 471 undefined reference to mxnet op CuDNNAlgoReg Get ' home clive mxnet src operator cudnn convolution inl h 472 undefined reference to mxnet op CuDNNAlgoReg Get ' build src operator convolution gpu o home clive mxnet src operator cudnn convolution inl h 471 more undefined references to mxnet op CuDNNAlgoReg Get ' follow collect2 error ld returned 1 exit status Makefile 259 recipe for target 'bin im2rec' failed make bin im2rec Error 1 What have you tried to solve it 1 2 3,,"piiswrong,jingpengw,phunterlau",2017-01-06 16:31:19,2017-09-28 17:42:38
IS,Question about LSTM implementation Perplexity convergence differs between lstm bucketing py and rnn cell demo py,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 03 running on docker docker host is Ubuntu 16 04 Compiler gcc 4 8 4 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD b6e8eec8b94c70d9e116b3a4443ce75ce3e07aa2 If you are using python package please provide Python version and distribution Python 2 7 6 Question I think that the following three objects implement the same purpose differently lstm bucketing rnn cell demo batch major rnn cell demo time major But the results of Perplexity convergence are different lstm bucketing What is the difference between these three implementations,,phunterlau,2017-01-23 09:21:17,2017-09-28 17:42:39
IS,Running of ImageNet with multiple nodes,I am planning to do some large scale runs up to 64 GPUs Is there a good recommendation to get a good accuracy for the Imagenet I saw a pointer recommending 0 5 as initial rate but 0 1 towards 30 60 epochs Will this improve accuracy or just training speed And how do I set this in my run script Also the batch size Can the batch size go beyond 2048 for eg 32 Batch size GPU 64 2048 or 4096 if I use 64 Batch size GPU Are there any pointers on how large can this batch size be Accordingly the batch size GPU can be adjusted,,"piiswrong,phunterlau",2017-01-26 16:02:18,2017-09-28 17:42:40
IS,Performace regression issue on current mxnet with latest nnvm,Compared with the version before NNVM refactored a epoch requires 216s while the previous value is 155s performance downgrade up to 40 GPU Z shows PCI bus load and GPU load is lower while CPU occupy is higher Previously one CPU core is fullly running now more than one cores are actively running I copied dmlc nnvm to overwirte dmlc mxnet nnvm otherwise the program cannot run Anyone encounters similar performance issue,,"piiswrong,phunterlau",2017-01-14 14:29:07,2017-09-28 17:42:41
IS,Mxnet has no mx img ImageIter,On mxnet official documents site I searched something about put single image into iter but this link gives me this NEW mx img ImageIter implemented in python easily customizable can load from both rec files and raw image files OLD mx io ImageRecordIter implemented in backend C less customizable but can be used in all language bindings load from rec files Custom iterator by inheriting mx io DataIter However when I run something like this I wonder is this method was deprecated or merged into other methods Anyone could give me a help,,"howard0su,phunterlau",2017-01-29 09:02:42,2017-09-28 17:42:43
IS,Aggregation of hidden layer representations from multiple GPUs to CPU using Module Model API,Hi Thanks for creating such an awesome library it is simply the best one out in the market in terms of speed memory and ease of use I want to be able to create multiple towers with same weights parameters like in Tensorflow and pass different mini batches of data to each tower and in the end be able to concatenate the hidden layer representations to create a bigger batch on which I would like to run my loss function and propagate the gradients back For example Number of GPUs 2 gpu 0 gpu 1 Network definition data mx symbol Variable wouldata' fc1 mx symbol FullyConnected data name 'fc1' num hidden 128 act1 mx symbol Activation fc1 name arelu1' act type arelu' fc2 mx symbol FullyConnected act1 name 'fc2' num hidden 64 Now I want to supply a mini batch of 100 samples such that fc2 for 50 samples a 50 by 64 matrix is computed on gpu 0 lets call it fc2 0 and fc2 for the rest of the 50 samples is computed on gpu 1 lets call it fc2 1 Lastly I want to concatenate fc2 0 and fc 1 to get fc a 100 by 64 matrix ie fc mx symbol concat fc1 0 fc1 1 2 0 While doing so I want various parameters of the models to be the same across the two gpus ie fc1 weight fc1 bias fc2 weight and fc2 bias should be the same for both the GPUs Now I want to apply a loss function on fc say I have a loss function customLoss loss customLoss fc Lastly I would like to use Module Model API to train the network by using something like model mx model FeedForward create loss X data set num epoch num epoch learning rate 0 01 OR mod mx mod Module loss mod fit train dataiter eval data eval dataiter optimizer params 'learning rate' 0 01 'momentum' 0 9 num epoch n epoch The reason I want to aggregate the representations for loss computation is because the loss computations depends on the number of samples in the mini batch and simply computing the loss individually on separate GPUs and averaging the gradient the current multi GPU of MxNet will lead to sub optimal performance This is one major feature that I could not figure out in MxNet may be it already exists but I am not sure how to achieve it Any help would be deeply appreciated,,"piiswrong,phunterlau",2017-01-27 12:03:54,2017-09-28 17:42:45
IS,Alexnet and googlenet symbol in example image classification is wrong,piiswrong Symbol for alexnet and googlenet should be switched Pls check,,"zhenlinluo,piiswrong,phunterlau",2017-01-29 20:43:01,2017-09-28 17:42:46
IS,opencv error of SSD,zhreshold Environment info Operating System ubuntu14 04 Compiler gcc 4 8 4 g 4 8 4 Package used Python R Scala Julia Python IDE Spyder MXNet version 0 9 2 Python version and distribution 2 7 Opencv version 3 0 Error Message In Spyder runfile ' home guorui soft code mxnet mxnet example ssd demo py' args r' cpu' wdir r' home guorui soft code mxnet mxnet example ssd' UMD has deleted common tools tools find mxnet detect dataset testdb dataset imdb symbol vgg16 reduced tools rand sampler dataset iterator detect detector dataset 23 01 59 src nnvm legacy json util cc 153 Loading symbol saved by previous version v0 8 0 Attempting to upgrade Detection time for 1 images 4 4787 sec OpenCV Error The function feature is not implemented Unknown unsupported array type in type file home guorui soft opencv Install OpenCV master Ubuntu 3 0 OpenCV opencv 3 0 0 modules core src matrix cpp line 1837 Traceback most recent call last File stdin line 1 in module File usr lib python2 7 dist packages spyderlib widgets externalshell sitecustomize py line 540 in runfile execfile filename namespace File home guorui soft code mxnet mxnet example ssd demo py line 100 in module CLASSES args thresh args show timer File detect detector py line 170 in detect and visualize img cv2 imread im list k cv2 error home guorui soft opencv Install OpenCV master Ubuntu 3 0 OpenCV opencv 3 0 0 modules core src matrix cpp 1837 error 213 Unknown unsupported array type in function type or runfile ' home guorui soft code mxnet mxnet example ssd demo py' args r' cpu' wdir r' home guorui soft code mxnet mxnet example ssd' 23 10 05 src nnvm legacy json util cc 153 Loading symbol saved by previous version v0 8 0 Attempting to upgrade Detection time for 1 images 5 2720 sec OpenCV Error Assertion failed d 2 sizes 0 1 sizes 1 1 sizes 0 sizes 1 0 in create file home guorui soft opencv Install OpenCV master Ubuntu 3 0 OpenCV opencv 3 0 0 modules core src matrix cpp line 2294 Traceback most recent call last File stdin line 1 in module File usr lib python2 7 dist packages spyderlib widgets externalshell sitecustomize py line 540 in runfile execfile filename namespace File home guorui soft code mxnet mxnet example ssd demo py line 100 in module CLASSES args thresh args show timer File detect detector py line 170 in detect and visualize img cv2 imread im list k cv2 error home guorui soft opencv Install OpenCV master Ubuntu 3 0 OpenCV opencv 3 0 0 modules core src matrix cpp 2294 error 215 d 2 sizes 0 1 sizes 1 1 sizes 0 sizes 1 0 in function create Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 demo py cpu What have you tried to solve it import cv2 is ok and the installation of opencv is correct sometimes when close and restart spyder it can work and if it works once it can work forever without closing the spyder Thank you,,"zhreshold,zhreshold,phunterlau",2017-01-21 15:23:49,2017-09-28 17:42:47
IS,Memory allocation failed,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Deep Learning on Amazon Linux with MXNet ami a382c3b4 Compiler unknown Package used Python R Scala Julia Python MXNet version 0 7 0 Python version and distribution python3 Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error I am trying to use the NMT to do a seq2seq generation Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Clone the repository I am running it on an AWS g2 2xlarge instance 2 cd MXNMT nmt 3 python3 main py mode train What have you tried to solve it 1 I try to use a tiny corpus change the data root model root log root in nmt xconfig py from wouldata' to 'tinydata' the problem disappears I think it is related to the corpus size But I do not know how to decouple the correlation What kind of data is stored in GPU memory,,"mli,phunterlau",2017-01-29 15:47:03,2017-09-28 17:42:48
IS,NDArray invocations with scalar arguments,There are many complaints about passing scalar arguments to MXNet is ndarray calls What I am thinking about is to have a wrapper for every registered NDArray function in MXNet such that if the given argument is scalar but supposed to be array do an automatic conversion My question is should we do it on python side or on C side If done on python side each ndarray call will have an extra wrapper another function redirect which may introduce overhead I do not know whether this is supported on C side since I guess C cannot have arguments of either NDArray or scalar type,,"jermainewang,piiswrong,phunterlau",2017-02-01 04:03:58,2017-09-28 17:42:51
IS,why do not use etcd for distribute communication,Why do not use etcd for distribute communication Why create a special parameters server I think that etcd is very suitable for distribute communication Etcd has very good read performance and good write performance We can use etcd to store meta data We can use etcd as a mq server for async communication Peer to peer and topic all both supported by etcd,,"mli,phunterlau",2016-12-06 01:57:43,2017-09-28 17:42:52
PR,Issue 7750 MXNet 0 11 0 Release Feedback README File Part2,nswamy Fix Point 5 of this issue which mentions The README md refers to the copyright being owned by Contributors Needs updating to a license statement with NOTICE handling the copyright side of things However some other Apache projects do not have this license section in the README md file at all,,"mbaijal,piiswrong,mbaijal",2017-09-27 17:37:58,2017-09-28 18:27:04
IS,Define a mx sym Variable which is constant in each mini batch but still trainable,Consider the following code The mx sym Variable 'extra bias' is expected to be small small large small after training which is indeed the case However currently the inferred shape has an extra dimension mini batch size and hence there will be mini batch number of copies of it For example run the code and you will see this output 0 37707356 0 37662297 1 12921023 0 37551349 0 37635165 0 37655994 1 12926149 0 37634984 0 37645847 0 37651965 1 12986588 0 37688756 0 37663144 0 37624913 1 12919986 0 37631923 0 3760089 0 3764413 1 12953496 0 37708497 How can we tell mxnet that this variable should be constant in each mini batch so there is no need to train such a large array while making sure it is still trainable Thanks,,"piiswrong,phunterlau",2017-01-31 15:56:25,2017-09-28 18:33:49
IS,Data shape inconsistent,Hi I'm trying to run MXNet to compare it to Keras I want to reproduce this simple example of Iris data classification However I'm stuck at the very beginning as I can not prepare the data properly iris data txt,,phunterlau,2017-02-01 14:58:17,2017-09-28 18:33:50
IS,mx sym Convolution weight xxxxx no longer works,It seems mx sym Convolution weight xxxxx no longer works probably because the parameter is removed The error as we can see weight is no longer in the parameter list mxnet base MXNetError Cannot find argument 'weight' Possible Arguments kernel Shape tuple required convolution kernel size h w or d h w stride Shape tuple optional default convolution stride h w or d h w dilate Shape tuple optional default convolution dilate h w or d h w,,"piiswrong,sxjscience,phunterlau",2017-02-01 08:50:24,2017-09-28 18:33:51
IS,how to debug the error,python main py 41084 3 299 299 5672 3 299 299 23 56 46 dev disk2 mxnet dmlc core include dmlc logging h 235 23 56 46 include mxnet tensor blob h 742 Check failed this shape Size shape Size TBlob get with shape new and old shape do not match total elements Traceback most recent call last File main py line 32 in module train iter mx io NDArrayIter data train data label train label batch size batch size shuffle True File dev disk2 mxnet python mxnet io py line 430 in init self data init data data allow empty False default name wouldata' File dev disk2 mxnet python mxnet io py line 398 in init data should be NDArray or numpy ndarray TypeError Invalid type ' type 'numpy ndarray' ' for data should be NDArray or numpy ndarray,,"piiswrong,phunterlau",2016-11-24 16:06:44,2017-09-28 18:33:53
IS,Distributed Training How to set the model output directory,I have read the doc below to run the distributed training example But I have not find the parameter to set the place where output the model file Thanks,,phunterlau,2017-02-03 07:06:08,2017-09-28 18:33:55
IS,Python CustomOp limited to default type float32,This issue is related to 2300 which fixes it for SimpleOp I need to implement an operator via CustomOp whose arguments input output all have the same type but this is not np float32 In fact I need np float64 for numerical reasons The default implementation of InferType in include mxnet operator h requires all types to be float32 This is fixed in src operator operator util cc where InferType now does the sensible thing namely requires that all types are the same or unspecified But this does not extend to CustomOp Python API I tried to insert this code into src operator custom inl h With this simple bind works without exception but the kernel crashes when any computation is done So there seems to be a deeper reason for the type to be float32 What I find funny is that casting things from float32 to float64 works totally fine when I build my operator using mxnet symbol only It is just once I use CustomOp somewhere that things break This means that float64 is supported in principle but not everywhere Thanks for some help,,"mseeger,Godricly,mseeger,piiswrong,phunterlau",2017-02-01 16:38:47,2017-09-28 18:33:56
IS,LSTM Batch Normalization with Multi GPU,I have implemented LSTM with Batch Normalization as followed def lstm self prefix num hidden indata prev state param seqidx layeridx dropout 0 if dropout 0 indata mx sym Dropout data indata p dropout i2h mx sym FullyConnected data indata weight param i2h weight num hidden num hidden 4 no bias True name prefix t d l d i2h seqidx layeridx i2h self BatchNorm name prefix t d l d i2h bn seqidx layeridx data i2h gamma param i2h gamma num channel num hidden 4 h2h mx sym FullyConnected data prev state h weight param h2h weight num hidden num hidden 4 no bias True name prefix t d l d h2h seqidx layeridx h2h self BatchNorm name prefix t d l d h2h bn seqidx layeridx data h2h gamma param h2h gamma beta param beta num channel num hidden 4 gates i2h h2h slice gates mx sym SliceChannel gates num outputs 4 name prefix t d l d slice seqidx layeridx in gate mx sym Activation slice gates 0 act type sigmoid in transform mx sym Activation slice gates 1 act type tanh forget gate mx sym Activation slice gates 2 act type sigmoid out gate mx sym Activation slice gates 3 act type sigmoid next c forget gate prev state c in gate in transform next h out gate mx sym Activation self BatchNorm name prefix 't d l d c bn' seqidx layeridx data next c gamma param c gamma beta param c beta num channel num hidden act type tanh return LSTMState c next c h next h def BatchNorm self name data gamma beta None kargs net data if not self bn return net if self minibatch num channel kargs pop 'num channel' net mx symbol Reshape net shape 1 self num subject num channel net mx symbol BatchNorm name name ' norm' data net fix gamma True momentum 0 9 attr 'wd mult' '0' 'lr mult' '0' net mx symbol Reshape data net shape 1 num channel else net mx symbol BatchNorm name name ' norm' data net fix gamma True momentum 0 9 attr 'wd mult' '0' 'lr mult' '0' net mx symbol broadcast mul net gamma if beta is not None net mx symbol broadcast plus net beta return net Setting the gpu as 0 the train accuracy and validation accuracy are Any idea why Thanks,,phunterlau,2017-01-13 13:47:01,2017-09-28 18:33:57
IS,Googlenet crash,1 Build with no MKL on CPUs 2 Run benchmark score py It ll run Alexnet first and crash right away Again Alexnet is in quotes as it is really Googlenet,,"zhenlinluo,phunterlau",2017-01-29 20:44:36,2017-09-28 18:33:58
IS,LSTM in predictor c API,Is it possible to implement LSTM networks in this predictor,,"piiswrong,phunterlau",2017-01-01 12:52:13,2017-09-28 18:33:59
IS,PReLU gamma initialization with gamma 0 0 should be better,Currently the PReLU gamma is initialized to be 1 0 because of the following code in Initializer scala I think it will be better to initialize PReLU gamma to 0 0 or 0 25 as in the original paper as this will be closer to ReLU which is a good starting point When gamma 1 0 the unit is simply linear and hence not a very good starting point So I suggest adding a parameter for PReLU is initialize value,,"piiswrong,phunterlau",2017-02-05 07:03:03,2017-09-28 18:34:01
IS,Build failed on EC2 g2 instances,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler default Package used Python R Scala Julia default MXNet version latest master branch Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message usr local cuda bin nvcc c o build src operator tensor matrix op gpu o std c 11 Xcompiler D FORCE INLINES g O3 ccbin g Xcompiler DMSHADOW FORCE STREAM Wall O3 I home ubuntu mxnet mshadow I home ubuntu mxnet dmlc core include fPIC I home ubuntu mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE PROFILER 1 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE DIST KVSTORE I home ubuntu mxnet ps lite include I home ubuntu mxnet deps include I home ubuntu mxnet cub DMXNET USE NVRTC 0 src operator tensor matrix op cu src operator tensor indexing op inl cuh 84 error identifier shfl xor is undefined Was working okay Stopped compiling when updated to the latest master branch,,"piiswrong,piiswrong,ap-hynninen,piiswrong,cjolivier01,cjolivier01,ap-hynninen,cjolivier01,ap-hynninen,phunterlau",2017-01-31 00:26:14,2017-09-28 18:34:02
IS,MXNet Project Ideas,This issue is intent to maintain a list of some ongoing projects or some project ideas This is a prefect places for anyone who want to contribute to the projects No matter you are beginner to Deep learning or you are expert in this area you can always find some projects to work with Fix Issues There are many user reported small issues in the GitHub Grab one fix it and submit PR Remove deprecated API usage in Examples Contact Skill Needed DL basic Several APIs functions or function argument is deprecated in 0 8 We need clean up example so that we do not carry those deprecated API forward Some common used Deprecation Warning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead src operator tensor matrix op inl h 155 Using target shape will be deprecated Build binary Python package Contact Skill Needed Python some c In order to release binary package to support various hardware and software configurations like GPU CUDNN presents we need some code to handle this gracefully some issues reported on the problem that fails run binary compiled with CUDA on non CUDA machine are fixed Need some volunteer to look at this issue and build package If publish to some common repo it will be even greater Tensorboard for MXnet Contact Skill Needed Python some c skill This project is aiming to bring Tensorboard as the virtualization tool for MXNet It is already working but need help to support more features Documentation Contact Skill Needed English Writing MXNet documentation can be improved in many aspects For example it is great that this article can have a similar one for mxnet Chinese Documentation Contact Skill Needed English Read Chinese writing There are already several documents are translated into Chinese But we need more and a lot more This will help us growing our community in Chinese community Data Pipe Line optimization Contact Skill needed C Performance profiling In order to scale better on multi GPU when training the model with complex data pipeline the performance of CPU executing operation becomes very critical MXNet need more tuning in this area Improve IPython Support Contact Skill needed Python DL basic Although you can use mxnet in ipython today fine however there are several low hang fruit can improve the experience a lot Improve to string for various important objects like NDArray symbol model iter etc Improve the default values for common used functions like model fit and provides more sensible default values,,"howard0su,zihaolucky,zihaolucky,zihaolucky,phunterlau",2017-01-09 14:46:23,2017-09-28 18:34:03
IS,Does Mxnet for windows support multi device training,I read Mxnet dmlc core tracker dmlc tracker mpi py and do not know how to install mpirun for windows I tried MS MPI it has mpiexec but not mpirun,,phunterlau,2017-02-06 06:47:29,2017-09-28 18:34:04
IS,Train accuracy 1 000000,Environment info Operating System centos 7 1 MXNet version v0 9 Python versio v2 7 Error Message When I train with the googlenet the Train accuracy 1 000000 all the time anyone can help INFO root Epoch 4 Batch 20 Speed 436 48 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 40 Speed 418 70 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 60 Speed 419 75 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 80 Speed 418 40 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 100 Speed 421 43 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 120 Speed 419 66 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 140 Speed 415 42 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 160 Speed 415 30 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 180 Speed 416 12 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 200 Speed 420 10 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 220 Speed 416 36 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 240 Speed 416 22 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 260 Speed 418 99 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 280 Speed 415 44 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 300 Speed 414 69 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 320 Speed 415 61 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 340 Speed 417 83 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 360 Speed 412 90 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 380 Speed 417 75 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 400 Speed 416 55 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 420 Speed 416 80 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 440 Speed 415 29 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 460 Speed 415 40 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 480 Speed 415 84 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 500 Speed 417 56 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 520 Speed 424 66 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 540 Speed 416 23 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 560 Speed 414 90 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 580 Speed 423 35 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 600 Speed 408 58 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 620 Speed 418 28 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 640 Speed 424 21 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 660 Speed 418 61 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 680 Speed 412 77 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 700 Speed 415 54 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 720 Speed 416 24 samples sec Train accuracy 1 000000 INFO root Epoch 4 Batch 740 Speed 415 85 samples sec Train accuracy 1 000000 INFO root Epoch 4 Resetting Data Iterator INFO root Epoch 4 Time cost 113 786 Steps to reproduce 1 The data is ILSVRC 2012 50000 jpg images 2 prepare the lst files,,"Godricly,Godricly,phunterlau",2017-02-06 05:17:28,2017-09-28 18:34:05
IS,scala source package is published but empty on Maven,Hey guys Publishing empty source package for Scala makes it difficult to browse the sources in the IDE Should not be a difficult fix For example see the contents of,,"piiswrong,yzhliu,CodingCat,phunterlau",2017-02-05 20:53:38,2017-09-28 18:34:07
IS,Some error occured when installing mxnet on centos6 5,usr local cuda 7 5 bin nvcc c o build src operator broadcast reduce op gpu o std c 11 Xcompiler D FORCE INLINES g O3 ccbin g Xcompiler DMSHADOW FORCE STREAM Wall O3 I root Desktop mxnet mshadow I root Desktop mxnet dmlc core include fPIC Iinclude msse3 funroll loops Wno unused parameter Wno unknown pragmas I usr local cuda 7 5 include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 pkg config cflags opencv fopenmp DMSHADOW USE CUDNN 1 DMXNET USE DIST KVSTORE I root Desktop mxnet ps lite include I root Desktop mxnet deps include DMXNET USE NVRTC 1 src operator broadcast reduce op cu ERROR certificate common name u201cwww github com u201d doesn u2019t match requested host name u201craw githubusercontent com u201d To connect to raw githubusercontent com insecurely use u2018 no check certificate u2019 make 1 root Desktop mxnet deps include zmq h Error 5 make 1 Leaving directory root Desktop mxnet ps lite' make PSLITE Error 2 make Waiting for unfinished jobs,,"jingpengw,phunterlau",2016-09-06 04:48:25,2017-09-28 18:34:08
IS,batch size cpu num memory train time,I found that the larger the batch size is the less the memory will be why otherwise the train time become less obviously,,phunterlau,2017-01-20 01:27:58,2017-09-28 18:34:09
IS,This is a bug I do not understand the eval metric is reseted so the epoch train metric value is 0 or nan,in the mxnet 0 9 3 py2 7 egg mxnet module base module py in the param eval metric reset when a epoch is end the eval metric of the last batch is reseted next the callback do not caculate the new metric of the epoch next Where to calculate this epoch eval metric when a epoch end So in my training the epoch eval metric is nan or 0 all the time,,phunterlau,2017-02-07 09:09:36,2017-09-28 18:34:12
IS,Detecting for MACRO DMLC USE CXX11 is not properly for MSVC,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows Compiler vs 2015 Package used Python R Scala Julia C MXNet commit hash git rev parse HEAD 85636ff1a015d04d3a8f960bc644b85ee5157135,,phunterlau,2017-02-07 13:06:32,2017-09-28 18:34:13
IS,mxnet MATLAB interface error R Works Win 10 64 Binary install,Too intimidating posting on github for newbie issues Just starting and need help and posted in Google group first Repeating Installed mxnet on win 10 64 bit using binary option R demo worked fine with MNIST data Can get predictions and data is read in MATLAB demo which is where I want to do most work is having issues The demo fails in calls to libmxnet Failure in callmxnet m at line 26 24 assert ischar func 25 ret calllib 'libmxnet' func varargin 26 assert ret 0 func is MXPredCreatePartialOut varargin is cell of 12 elements The folder mxnet lib has the libmxnet dll and lib files The path includes these folders Demo m 21 pred model forward img,,"mli,phunterlau",2016-08-31 18:58:41,2017-09-28 18:34:14
IS,Asynchronous mini batch allocation for DMA to GPU,hello I read this ' ' and found the author says lower hardware configuration can achieve very good speed if using asyn mini batch DMA to GPU has mxnet already supported it or in the TODO Asynchronous mini batch allocation Once your GPU finished computation on the current mini batch it wants to immediately work on the next mini batch You can now of course initiate a DMA transfer and then wait for the transfer to complete so that your GPU can continue to crunch numbers But there is a much more efficient way Prepare the next mini batch in advance so that your GPU does not have to wait at all This can be done easily and asynchronously with no degradation in GPU performance CUDA Code for asynchronous mini batch allocation The first two calls are made when the GPU starts with the current batch the last two calls are made when the GPU finished with the current batch The transfer of the data will be completed long before the stream is synchronized in the second step so there will be no delay for the GPU to begin with the next batch cudaMemcpyAsync m next batch X data m next buffer X data copy range bytes X cudaMemcpyHostToDevice m streamNext batch X cudaMemcpyAsync m next batch Y data m next buffer Y data copy range bytes Y cudaMemcpyHostToDevice m streamNext batch Y cudaStreamSynchronize m streamNext batch X cudaStreamSynchronize m streamNext batch Y,,"piiswrong,minazou,phunterlau",2017-01-13 10:28:17,2017-09-28 18:34:16
IS,Is RNN operator of CPU version in developing,Though the speed of RNN and 3D CNN of cpu version may be very slow I think high level operator of CPU version is in need for algorithm tests and little projects,,"phunterlau,gautamkmr,szha",2017-02-08 08:24:40,2017-09-28 18:34:18
IS,No module named bbox when running rcnn demo py,I have met probelm when running the demo py in rcnn The exception says File mnt gelu mxnet example rcnn rcnn processing bbox transform py line 2 in from cython bbox import bbox overlaps cython ImportError No module named bbox I'm pretty sure that i have compiled and installed all operators I have solve the problem by doing the following steps I make the following changes to the project and fix the problem 1 add following lines of code before import ' cython bbox import bbox overlaps cython' in rcnn processing bbox transform py import pyximport pyximport install 2 edit the setup py in rcnn cython change the code from 'nms kernel cu' 'gpu nms pyx' to 'nms kernel cu' 'gpu nms cpp' 3 run the command python setup py install under the above directory 4 copy 3 so files from rcnn cython build lib linux x86 64 2 7 to pyxbld lib linux x86 64 2 7 rcnn cython I just want to ask is there any more nice way to solve this problem,,"sxjscience,phunterlau",2017-02-07 10:02:30,2017-09-28 18:34:19
PR,Revert Kill running PR builds when a new build is triggered for the,nswamy Since the Apache Infra team said that all the script approvals are not possible we need to find a workaround to this Reverting changes until the issue is fixed This reverts commit 4c0df6249d03841f5eb30e1428aa25fc230fed30,,"mbaijal,gautamkmr",2017-09-28 17:57:56,2017-09-28 18:45:56
IS,Bug on sum,Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia python MXNet version '0 11 0' pip3 install mxnet cu80 0 11 0 I got a very strange crash in my code Even though you have a clean crash line code you will probably have an hard time with this bug as this is probably a bug about the core of the graph Have fun I managed to reproduce the crash with the simple version below Here is the crash 17 25 45 home username mxnet dmlc core include dmlc logging h 308 17 25 45 src core graph cc 50 Check failed it node2index end it first nptr get And the associated Stack trace Stack trace returned 10 entries bt 0 home username mxnet python mxnet lib libmxnet so 0x27364e1 0x7f08a4b5b4e1 bt 1 home username mxnet python mxnet lib libmxnet so ZN4nnvm12IndexedGraphC1ERKNS 5GraphE 0x2ed 0x7f08a4b5c4dd bt 2 home username mxnet python mxnet lib libmxnet so ZN4nnvm5Graph13indexed graphEv 0x2f 0x7f08a4b5dc1f bt 3 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13AssignContextEN4nnvm5GraphERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ mm 0x5f 0x7f08a3b5c47f bt 4 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13GraphExecutor9InitGraphEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSN INS 9OpReqTypeESaISS EE 0xe6 0x7f08a3b61736 bt 5 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13GraphExecutor4InitEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSt13unordered mapISD NS2 6TShapeESt4hashISD ESt8equal toISD ESaISG ISH ST EEERKSS ISD iSV SX SaISG ISH iEEERKSN INS 9OpReqTypeESaIS18 EERKSt13unordered setISD SV SX SaISD EEPSN INS 7NDArrayESaIS1I EES1L S1L PSS ISD S1I SV SX SaISG ISH S1I EEEPNS 8ExecutorERKSS INS2 9NodeEntryES1I NS2 13NodeEntryHashENS2 14NodeEntryEqualESaISG IKS1S S1I EEE 0x11f 0x7f08a3b6d4ff bt 6 home username mxnet python mxnet lib libmxnet so ZN5mxnet8Executor10SimpleBindEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ RKSt13unordered mapISC NS1 6TShapeESt4hashISC ESt8equal toISC ESaISF ISG SS EEERKSR ISC iSU SW SaISF ISG iEEERKSM INS 9OpReqTypeESaIS17 EERKSt13unordered setISC SU SW SaISC EEPSM INS 7NDArrayESaIS1H EES1K S1K PSR ISC S1H SU SW SaISF ISG S1H EEEPS0 0x233 0x7f08a3b6e313 bt 7 home username mxnet python mxnet lib libmxnet so MXExecutorSimpleBind 0x2d4a 0x7f08a3b2b2da bt 8 home username venvmxnet lib python3 5 lib dynload ctypes cpython 35m x86 64 linux gnu so ffi call unix64 0x4c 0x7f08bfbb1e20 bt 9 home username venvmxnet lib python3 5 lib dynload ctypes cpython 35m x86 64 linux gnu so ffi call 0x2eb 0x7f08bfbb188b Traceback most recent call last File home username mxnet python mxnet symbol py line 1473 in simple bind ctypes byref exe handle File home username mxnet python mxnet base py line 129 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 17 25 45 src core graph cc 50 Check failed it node2index end it first nptr get Stack trace returned 10 entries bt 0 home username mxnet python mxnet lib libmxnet so 0x27364e1 0x7f08a4b5b4e1 bt 1 home username mxnet python mxnet lib libmxnet so ZN4nnvm12IndexedGraphC1ERKNS 5GraphE 0x2ed 0x7f08a4b5c4dd bt 2 home username mxnet python mxnet lib libmxnet so ZN4nnvm5Graph13indexed graphEv 0x2f 0x7f08a4b5dc1f bt 3 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13AssignContextEN4nnvm5GraphERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ mm 0x5f 0x7f08a3b5c47f bt 4 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13GraphExecutor9InitGraphEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSN INS 9OpReqTypeESaISS EE 0xe6 0x7f08a3b61736 bt 5 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13GraphExecutor4InitEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSt13unordered mapISD NS2 6TShapeESt4hashISD ESt8equal toISD ESaISG ISH ST EEERKSS ISD iSV SX SaISG ISH iEEERKSN INS 9OpReqTypeESaIS18 EERKSt13unordered setISD SV SX SaISD EEPSN INS 7NDArrayESaIS1I EES1L S1L PSS ISD S1I SV SX SaISG ISH S1I EEEPNS 8ExecutorERKSS INS2 9NodeEntryES1I NS2 13NodeEntryHashENS2 14NodeEntryEqualESaISG IKS1S S1I EEE 0x11f 0x7f08a3b6d4ff bt 6 home username mxnet python mxnet lib libmxnet so ZN5mxnet8Executor10SimpleBindEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ RKSt13unordered mapISC NS1 6TShapeESt4hashISC ESt8equal toISC ESaISF ISG SS EEERKSR ISC iSU SW SaISF ISG iEEERKSM INS 9OpReqTypeESaIS17 EERKSt13unordered setISC SU SW SaISC EEPSM INS 7NDArrayESaIS1H EES1K S1K PSR ISC S1H SU SW SaISF ISG S1H EEEPS0 0x233 0x7f08a3b6e313 bt 7 home username mxnet python mxnet lib libmxnet so MXExecutorSimpleBind 0x2d4a 0x7f08a3b2b2da bt 8 home username venvmxnet lib python3 5 lib dynload ctypes cpython 35m x86 64 linux gnu so ffi call unix64 0x4c 0x7f08bfbb1e20 bt 9 home username venvmxnet lib python3 5 lib dynload ctypes cpython 35m x86 64 linux gnu so ffi call 0x2eb 0x7f08bfbb188b During handling of the above exception another exception occurred Traceback most recent call last File bugbroadcast py line 50 in module bugBroadcast File bugbroadcast py line 36 in bugBroadcast m bind data shapes DataDesc data bs inpsize File home username mxnet python mxnet module module py line 417 in bind state names self state names File home username mxnet python mxnet module executor group py line 231 in init self bind exec data shapes label shapes shared group File home username mxnet python mxnet module executor group py line 327 in bind exec shared group File home username mxnet python mxnet module executor group py line 603 in bind ith exec shared buffer shared data arrays input shapes File home username mxnet python mxnet symbol py line 1479 in simple bind raise RuntimeError error msg RuntimeError simple bind error Arguments data 16 784 17 25 45 src core graph cc 50 Check failed it node2index end it first nptr get Stack trace returned 10 entries bt 0 home username mxnet python mxnet lib libmxnet so 0x27364e1 0x7f08a4b5b4e1 bt 1 home username mxnet python mxnet lib libmxnet so ZN4nnvm12IndexedGraphC1ERKNS 5GraphE 0x2ed 0x7f08a4b5c4dd bt 2 home username mxnet python mxnet lib libmxnet so ZN4nnvm5Graph13indexed graphEv 0x2f 0x7f08a4b5dc1f bt 3 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13AssignContextEN4nnvm5GraphERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ mm 0x5f 0x7f08a3b5c47f bt 4 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13GraphExecutor9InitGraphEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSN INS 9OpReqTypeESaISS EE 0xe6 0x7f08a3b61736 bt 5 home username mxnet python mxnet lib libmxnet so ZN5mxnet4exec13GraphExecutor4InitEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorIS4 SaIS4 EESR SR RKSt13unordered mapISD NS2 6TShapeESt4hashISD ESt8equal toISD ESaISG ISH ST EEERKSS ISD iSV SX SaISG ISH iEEERKSN INS 9OpReqTypeESaIS18 EERKSt13unordered setISD SV SX SaISD EEPSN INS 7NDArrayESaIS1I EES1L S1L PSS ISD S1I SV SX SaISG ISH S1I EEEPNS 8ExecutorERKSS INS2 9NodeEntryES1I NS2 13NodeEntryHashENS2 14NodeEntryEqualESaISG IKS1S S1I EEE 0x11f 0x7f08a3b6d4ff bt 6 home username mxnet python mxnet lib libmxnet so ZN5mxnet8Executor10SimpleBindEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorIS3 SaIS3 EESQ SQ RKSt13unordered mapISC NS1 6TShapeESt4hashISC ESt8equal toISC ESaISF ISG SS EEERKSR ISC iSU SW SaISF ISG iEEERKSM INS 9OpReqTypeESaIS17 EERKSt13unordered setISC SU SW SaISC EEPSM INS 7NDArrayESaIS1H EES1K S1K PSR ISC S1H SU SW SaISF ISG S1H EEEPS0 0x233 0x7f08a3b6e313 bt 7 home username mxnet python mxnet lib libmxnet so MXExecutorSimpleBind 0x2d4a 0x7f08a3b2b2da bt 8 home username venvmxnet lib python3 5 lib dynload ctypes cpython 35m x86 64 linux gnu so ffi call unix64 0x4c 0x7f08bfbb1e20 bt 9 home username venvmxnet lib python3 5 lib dynload ctypes cpython 35m x86 64 linux gnu so ffi call 0x2eb 0x7f08bfbb188b,,"piiswrong,tqchen,reminisce,reminisce,reminisce,tqchen,reminisce",2017-09-25 15:54:51,2017-09-28 19:20:12
IS,The accuracy of imagenet1k inception bn,I use inception bn to train imagenet 2012 after 100 epochs the train accuracy 0 90 top1 0 98 top5 and the val accuracy 0 67 top1 0 87 top5 are lower than 0 7245 top1 0 9079 top5 given by Single Center Crop Accuracy so I doubt it can you give the values of the following parameters for training to get the given result 1 batch size 2 lr 3 lr factor 4 data shape 5 num epoch and the following parameters for train data and val data 1 resize 2 quality 3 rand crop 4 rand mirror Thx,,szha,2017-02-08 11:30:30,2017-09-28 21:54:41
IS,Typo for output names in module is document,Hi I am a beginner for mxnet Just found a typo in your mxnet online document When initialize module it requires you to provide both of data names and output names However it should be label names here Otherwise it will give error msg,,szha,2017-02-08 21:45:38,2017-09-28 21:54:42
IS,SIGSEGV in libmxnet,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 1 Compiler Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 12 from ubuntu repositories Full python environment information python V Python 2 7 12 pip list appdirs 1 4 0 cycler 0 10 0 functools32 3 2 3 post2 graphviz 0 5 2 matplotlib 2 0 0 mxnet 0 9 3 numpy 1 12 0 olefile 0 44 opencv python 3 2 0 6 packaging 16 8 pandas 0 19 2 Pillow 4 0 0 pip 9 0 1 pkg resources 0 0 0 pydicom 0 9 9 pyparsing 2 1 10 python dateutil 2 6 0 pytz 2016 10 scipy 0 18 1 setuptools 34 1 0 six 1 10 0 subprocess32 3 2 7 wheel 0 30 0a0 Error Message Please paste the full error message including stack trace See error message in stack trace from gdb in Steps to reproduce below Minimum reproducible example import mxnet as mx import cv2 img cv2 imread 'mean png' 0 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error gdb python GNU gdb Ubuntu 7 11 1 0ubuntu1 16 04 7 11 1 Copyright C 2016 Free Software Foundation Inc License GPLv3 GNU GPL version 3 or later This is free software you are free to change and redistribute it There is NO WARRANTY to the extent permitted by law Type show copying and show warranty for details This GDB was configured as x86 64 linux gnu Type show configuration for configuration details For bug reporting instructions please see Find the GDB manual and other documentation resources online at For help type help Type apropos word to search for commands related to word Reading symbols from python no debugging symbols found done gdb r Starting program home dmg virtualenvs mxnet bin python Thread debugging using libthread db enabled Using host libthread db library lib x86 64 linux gnu libthread db so 1 Python 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 on linux2 Type help copyright credits or license for more information import mxnet as mx New Thread 0x7ffff3451700 LWP 9466 New Thread 0x7fffed30d700 LWP 9467 import cv2 img cv2 imread 'mean png' 0 Thread 1 python received signal SIGSEGV Segmentation fault 0x00007fffeed47142 in cv Mat copyTo cv OutputArray const const from home dmg virtualenvs mxnet local lib python2 7 site packages mxnet libmxnet so gdb bt 0 0x00007fffeed47142 in cv Mat copyTo cv OutputArray const const from home dmg virtualenvs mxnet local lib python2 7 site packages mxnet libmxnet so 1 0x00007fffe98fcbf5 in object pyopencv from cv Mat cv Mat const from home dmg virtualenvs mxnet local lib python2 7 site packages cv2 cv2 so 2 0x00007fffe98fd874 in pyopencv cv imread object object object from home dmg virtualenvs mxnet local lib python2 7 site packages cv2 cv2 so 3 0x00000000004c468a in PyEval EvalFrameEx 4 0x00000000004c2765 in PyEval EvalCodeEx 5 0x00000000004c2509 in PyEval EvalCode 6 0x00000000004f1def in 7 0x000000000044c6ed in PyRun InteractiveOneFlags 8 0x000000000044c4b2 in PyRun InteractiveLoopFlags 9 0x000000000042e88a in 10 0x000000000049e14a in Py Main 11 0x00007ffff7811830 in libc start main main 0x49dab0 main argc 1 argv 0x7fffffffdc98 init optimized out fini optimized out rtld fini optimized out stack end 0x7fffffffdc88 at csu libc start c 291 12 0x000000000049d9d9 in start I do not get a segfault if I do not import mxnet And if I import cv2 before I import mxnet there is similarly no problem Any clues What have you tried to solve it 1 2 3,,"piiswrong,szha",2017-02-07 18:21:32,2017-09-28 21:54:43
IS,Decrease the learning rate when the cv accuracy is not going down,Hello When training the models using sgd I want to decrease the learning rate when the cv accuracy is not going down how to achieve this Thanks,,szha,2017-02-09 06:14:00,2017-09-28 21:54:44
IS,Problems when adding a new type of leaky ReLU operator,Hi I am trying to add a new type of leaky ReLU operator by modifying leaky relu inl h And it works fine if I simply replicate the PReLU code fragments change it a bit names computations etc and add the correct MSHADOW XINLINE forward and grad functions for my new ReLU However when I try to add a new trainable parameter 'Beta' as in enum LeakyReLUOpInputs kData kGamma kBeta I run into problems The code compiles fine but I get this while using my new ReLU 15 42 23 Users Apple mxnet dmlc core include dmlc logging h 300 15 42 23 Users Apple mxnet mshadow mshadow extension broadcast h 73 Check failed ShapeCheckDim1SrcExp Check src self 0 shape dimcast 0 vs 5 broadcast shape mismatch I suspect this have something to do with the understanding of the following code in Backward On the other hand DeclareBackwardDependency ListArguments etc should be fine as the correct modifications seems more obvious So I think the mistake shall lies in what I listed before,,szha,2017-02-05 08:10:41,2017-09-28 21:54:46
IS,What is the meaning of index in batch norm cc,In src operator batch norm cc line 50 there is a script for initializing some internal variables Here variables are indexed by the argument const int index but I have no idea how to compute the correct index to access each variable to be initialized Can anyone explain me about this,,"piiswrong,szha",2017-02-10 04:17:45,2017-09-28 21:54:47
IS,Error while building with USE CUDA set to 1,Environment info Operating System Ubuntu Compiler gcc Package used Python R Scala Julia python MXNet version Error Message,,szha,2017-02-10 07:07:09,2017-09-28 21:54:48
IS,MXNet Symbol Implementation details 2,Dear all Do you have any idea of what is the meaning of arg nodes and node row ptr heads Example code import mxnet as mx a mx sym Variable 'a' b mx sym Variable 'b' c a b print c tojson output nodes op null name a inputs op null name b inputs op elemwise add name plus1 inputs 0 0 0 1 0 0 arg nodes 0 1 node row ptr 0 1 2 3 heads 2 0 0 attrs mxnet version int 904,,szha,2017-02-10 14:10:34,2017-09-28 21:54:49
IS,GPU build notes for Ubuntu 16 10,Hello It took some tweaks to get mxnet built with gpu support on Ubuntu 16 10 so I wanted to document things I had to do for possible incorporation into the build instructions build the shared library and for others' reference This is based on the source code as of 2017 02 10 commit 70df5ad736f1cd462b6b583917032e8f2dd127be 1 Ubuntu 16 10 uses gcc 6 but CUDA 8 0 currently requires gcc 5 4 or earlier to compile We need to tell the make system to use gcc 5 2 Instructions install the Atlas BLAS but we append OpenBLAS to the config mk file We need to either make sure we have OpenBLAS sudo apt get install openblas or remove edit the config mk line that specifies openblas USE BLAS openblas so that we use atlas 3 Ubuntu 16 10 uses OpenCV 2 4 but the installation instructors add a config part that requires OpenCV 3 Remove lopencv imgcodecs from ADD LDFLAGS in config mk or manually install OpenCV 3 With those tweaks I was able to build the shared library and everything worked out from there,,szha,2017-02-11 18:26:36,2017-09-28 21:54:50
IS,Android amalgamation mxnet predict all cc cblas error when execute forward function,When execute the line p exec Forward false Because i use five convolution layer in network so i guess perhaps this error relate to the cblas function cblas sgemm which do the matrix multiplication Anyone can help me Thanks,,szha,2016-11-17 11:53:25,2017-09-28 21:54:51
IS,combining demo3 dssm ipynb with rating based solutions,How would you combine the additional user data for example search queries in the demo2 solution i e rating based solution Would the item movie embedding be simply augemented with its title and image like the augmentation done for the user,,szha,2017-02-12 15:05:04,2017-09-28 21:54:52
IS,Loss metrics do not have gradient calculations,Im trying to figure out how gradient in the loss layer being calculated in mxnet and still failing documentation does not help Can you please describe this logic or give me some link What i wanted to do is to define custom numpy loss layer but its intefrace does not allow me to give out gradients wrt inputs meaning i wont be able to control this when i calculate my loss Or i miss some part in how mxnet is implemented I have read about splitting SoftmaxLoss to SoftmaxOutput and something else but that something else seems to be missing in metrics py For now i switched to using Accuracy metric but it does not ideally fit my needs so i wanted to modify it a bit but modifying it without doing gradient calcs kind of does not make sense Could you please guide me,,szha,2016-12-21 01:22:27,2017-09-28 21:54:54
IS,Multiple softmax outputs targets or using multi output in SoftmaxOutput layer,Let is say we will predict two types of labels X and Y for a dataset and set the loss function to be the sum of the individual softmax loss I have read all the issues on multi target prediction here yet unfortunately I still can not make it work Please let me know if I made any silly mistakes Here is the example code,,szha,2017-01-31 08:58:40,2017-09-28 21:54:55
IS,OP request softmax with mask,Hi I think that attention mechanism can be acccelerated if we have a softmax OP with mask Currently the input sentences are of different length and I fix this ugly as follows But if there is softmax with mask we can change to a faster implementation without computing energe for each source sentence word,,"sbodenstein,szha",2017-02-10 06:42:58,2017-09-28 21:54:56
IS,I find that the version 0 9 3 really has a big promotion,the more CPUs I set the faster it performed little while ago the more CPUs i set the slower it was though the cpu utilization percentage was so high and the distributed training is faster obviously than other deep learning frame like paddle,,"piiswrong,szha",2017-02-07 01:16:14,2017-09-28 21:54:57
IS,Code freezing with joblib,I'm trying to parallelize predictions using joblib and noticing that the code will freeze when I try to use the predict method in parallel This was reported in joblib a while ago and they said that the issue was with MKL and that making the number of threads equal to 1 would solve the problem I'm using atlas sse3 and tried setting atlas threads to 1 ATL NTHREADS but it is not working for me I was wondering if there could be an issue in mxnet explaining this I tried setting MXNET GPU WORKER NTHREADS to 1 MXNET CPU PRIORITY NTHREADS to 1 and MXNET ENGINE TYPE to 'NaiveEngine',,"piiswrong,piiswrong,piiswrong,zihaolucky,zihaolucky,szha",2016-10-19 06:22:18,2017-09-28 21:54:59
IS,mxnet example error,Dear all I have found an error when running fine tuning pre trained example with the latest mxnet version Is there any way to fix it Traceback most recent call last File FineTune py line 76 in module mod score fit new sym new args aux params train val batch size num gpus File FineTune py line 66 in fit return mod score val TypeError score takes at least 3 arguments 2 given,,szha,2017-02-09 13:05:16,2017-09-28 21:55:01
IS,Mxnet symbol implementation details,Dear all I am trying to implement a structure as follows Image1 Res Net except for last layer diff predict result Image2 Res Net except for last layer or Image Combined slice channel Image1 Res Net except for last layer diff predict result Image2 Res Net except for last layer Which way is easier I am now trying the second way The problem is that how to use the previous resnet symbols to do this as the original resnet symbol get a variable named wouldata' as the input how to make the output of slice channel transmit data to the variable wouldata' and how to initialize the network with pretrained parameters Or is there are any examples or smarter ways of doing this,,"nicklhy,szha",2017-02-10 14:07:46,2017-09-28 21:55:02
IS,Missing model files,Most of the symbol and params files for the models in the model gallery are missing,,szha,2017-01-30 22:43:40,2017-09-28 21:55:03
PR,New tutorial of implementing operators in MXNet backend,A new tutorial for implementing operators in MXNet backend tailored for users interested in knowing about and contributing to MXNet C code base See this link for a friendly view haibin lin,,"reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,reminisce,rahul003,rahul003,rahul003,rahul003,reminisce,reminisce,reminisce,reminisce,zheng-da,reminisce,anirudh2290,anirudh2290,anirudh2290,madjam,madjam,madjam,madjam,madjam,madjam,reminisce,reminisce,madjam,madjam,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,TaoLv,cjolivier01,TaoLv,reminisce,TaoLv,reminisce,TaoLv,piiswrong,reminisce,reminisce,reminisce",2017-09-10 04:30:15,2017-09-28 23:10:52
PR,fix path expand in model zoo,zhreshold,,szha,2017-09-28 22:56:54,2017-09-28 23:13:19
PR,Perl do not merge just a test,,,"sergeykolychev,sergeykolychev,sergeykolychev",2017-09-28 20:10:33,2017-09-28 23:22:34
IS,Not expanded default root location root ' mxnet models' in gluon model zoo vision networks,Environment info Operating System ubuntu 16 04 Compiler root ' mxnet models' Package used Python R Scala Julia python MXNet version Or if installed from source Downloading mxnet 0 11 1b20170928 py2 py3 none manylinux1 x86 64 whl 21 0MB MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 on linux2 Error Message Please paste the full error message including stack trace when the correct file exits on disk I got Model file is not found Downloading Minimum reproducible example if you are using your own code please provide a short script that reproduces the error from mxnet gluon import nn from mxnet gluon model zoo import vision as models deep dog net models densenet121 pretrained True Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error What have you tried to solve it In this pull request there are some net load params get model file remove default variable root root or use os path expanduser to expand default root location,,"szha,szha,szha",2017-09-28 22:47:25,2017-09-28 23:24:03
IS,Error when building mxnet on ubuntu 16 04 gcc 5 4 0 python 3 5 2,Environment info Operating System Ubuntu 16 04 Compiler gcc 5 4 0 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 3ffbc3588b16a7538c74d980adc610218c7a19b0 Python version 3 5 2 Error Message Here is output I get when running make j12 in the mxnet directory What have you tried to solve it 1 I verified my installation of CUDA toolkit I am using Nvidia tesla k80 gpu,,"piiswrong,szha",2017-01-31 08:24:40,2017-09-28 23:48:49
IS,The self define loss function support to the GPU need the cuda code in the ndarray softmax py I do not know the cuda code How the difine the loss function,,,szha,2017-02-12 14:07:25,2017-09-28 23:48:50
IS,Question Setting the updater of PS in backend c,I'm wondering if it is possible to set the updater in parameter server using KVStoreDistServer set updater directly in the backend c and currently this seems to be called from the C API from Python calls that I do not quite understand I'm wondering whether I can achieve the same thing by calling into that method directly If so what updater should be used and how Many thanks,,"piiswrong,piiswrong,piiswrong,szha",2017-02-13 22:30:43,2017-09-28 23:48:52
IS,EvalMetric such as Accuracy shall check ignore label,If the SoftmaxOutput layer is using ignore label then the EvalMetric computation shall take it into account Right now ignored labels are the same as error in EvalMetric which leads to a low accuracy score,,szha,2017-02-12 12:21:21,2017-09-28 23:48:53
IS,Weird Mxnet Model Zoo Domain Problem,1 2 3 I tried to download the VGG16 model from model zoo However the link does not work sometimes Sometimes there is VGG folder but sometimes not It seems there are two version of data dmlc ml and the DNS server will randomly resolved into one However they have different content I'm not sure what is the real reason of this,,"piiswrong,szha",2017-01-21 00:27:12,2017-09-28 23:48:54
IS,Question about the scalable training,Suppose we start a distributed mxnet training job with 3 ps and 10 workers In the training I need to scale the job to 1 ps and 5 workers Does MXNet support this scalable training If it is supported How we can do it,,"pineking,piiswrong,pineking,szha",2017-02-13 08:03:10,2017-09-28 23:48:55
IS,can not convert from numpy array to mx nd array,I met the error with newest mxnet version mxnet 0 9 3 with python 2 7 in Ubuntu 16 04 1 LTS gcc version 5 4 0 Here is my code to generate the error import mxnet as mx import numpy as np t1 mx nd array np zeros 12000 3 224 224 dtype 'float32' t2 mx nd array np zeros 120000 3 224 224 dtype 'float32' t1 is fine but t2 will produce error msg like this MXNetError Traceback most recent call last in 1 t2 mx nd array np zeros 120000 3 224 224 dtype 'float32' home yaping mxnet python mxnet ndarray pyc in array source array ctx dtype 1121 raise TypeError isource array must be array like object' 1122 arr empty source array shape ctx dtype 1123 arr source array 1124 return arr 1125 home yaping mxnet python mxnet ndarray pyc in setitem self in slice value 269 internal set value float value out self 270 elif isinstance value np ndarray np generic 271 self sync copyfrom value 272 else 273 raise TypeError 'type s not supported' str type value home yaping mxnet python mxnet ndarray pyc in sync copyfrom self source array 342 self handle 343 source array ctypes data as ctypes c void p 344 ctypes c size t source array size 345 346 def slice self start stop home yaping mxnet python mxnet base pyc in check call ret 73 74 if ret 0 75 raise MXNetError py str LIB MXGetLastError 76 77 if sys version info 0 3 MXNetError 10 06 08 include mxnet tensor blob h 243 Check failed this shape Size shape Size 18063360000 vs 883490816 TBlob get with shape new and old shape do not match total elements,,"sxjscience,szha",2017-02-12 15:17:00,2017-09-28 23:48:56
IS,Matrix Factorization MF part 2 Getting Fancy,How would the examples presented here work in case the number of users is a few millions The embeddings would potentially be very large is it feasible for not just a toy example,,szha,2017-02-14 10:47:02,2017-09-28 23:48:57
IS,scala package Running example on spark has the error java bin,javelinjs I run the spark example of mxnet on spark cluster The error raised in the staring of scheduler The main error is 17 02 10 15 17 03 INFO FileInputFormat Total input paths to process 1 17 02 10 15 17 03 INFO MXNet repartitioning training set to 10 partitions 17 02 10 15 17 03 INFO MXNet Starting scheduler on 10 16 43 26 38348 17 02 10 15 17 03 INFO ParameterServer Start process java cp data2 hadoop yarn nm local dir usercache hadoop pay dev appcache application 1486701291637 25560 spark 4b7478a0 ca31 48ca 8e6d 62540f7935ae userFiles 855124ea e736 4690 9781 82b35e3e58bd mxnet core 2 11 0 1 2 SNAPSHOT jar data2 hadoop yarn nm local dir usercache hadoop pay dev appcache application 1486701291637 25560 spark 4b7478a0 ca31 48ca 8e6d 62540f7935ae userFiles 855124ea e736 4690 9781 82b35e3e58bd mxnet full 2 11 linux x86 64 cpu 0 1 2 SNAPSHOT jar data2 hadoop yarn nm local dir usercache hadoop pay dev appcache application 1486701291637 25560 spark 4b7478a0 ca31 48ca 8e6d 62540f7935ae userFiles 855124ea e736 4690 9781 82b35e3e58bd mxnet spark 2 11 0 1 2 SNAPSHOT jar ml dmlc mxnet spark ParameterServer role scheduler root uri 10 16 43 26 root port 38348 num server 1 num worker 10 timeout 300 java io IOException Cannot run program java error 2 No such file or directory at java lang ProcessBuilder start ProcessBuilder java 1047 at java lang Runtime exec Runtime java 617 at java lang Runtime exec Runtime java 450 at java lang Runtime exec Runtime java 347 at ml dmlc mxnet spark ParameterServer startProcess ParameterServer scala 132 at ml dmlc mxnet spark MXNet fit MXNet scala 130 at com meituan pay ClassificationExample main App scala 51 at com meituan pay ClassificationExample main App scala at sun reflect NativeMethodAccessorImpl invoke0 Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 57 at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43 at java lang reflect Method invoke Method java 606 at org apache spark deploy yarn ApplicationMaster anon 2 run ApplicationMaster scala 637 Caused by java io IOException error 2 No such file or directory at java lang UNIXProcess forkAndExec Native Method at java lang UNIXProcess init UNIXProcess java 186 at java lang ProcessImpl start ProcessImpl java 130 at java lang ProcessBuilder start ProcessBuilder java 1028 12 more 17 02 10 15 17 03 ERROR ClassificationExample requirement failed Failed to start ps scheduler process java lang IllegalArgumentException requirement failed Failed to start ps scheduler process,,"CodingCat,yzhliu,yzhliu,szha",2017-02-10 09:02:05,2017-09-28 23:48:58
IS,SSD example crashes on CPUs,Running the example given here crashes on CPUs both with and without MKL flags being set but runs fine on GPUs Steps to reproduce are described in the above link Just follow the steps as is and when you run this it will crash python demo py The crash is show here demo crash txt Eric can you take a look,,"glingyan,howard0su,glingyan,glingyan,glingyan,glingyan,szha",2017-02-01 22:33:14,2017-09-28 23:48:59
IS,A little bug in mxnet python code,Hi I'm new to mxnet and I am walking through a kaggle project to familiar with its python interface But something confused me when I try to call mx mod Module predict function to complete my final work Since there was no headache when I called mx mod Module score function predict function should also work fine unless bug around Then I read through python source code to find the bug following info displayed when I run the code best model predict eval data test MXNetError Traceback most recent call last ipython input 14 6eee4a79ad10 in module 1 best model predict eval data test usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet module base module pyc in predict self eval data num batch merge batches reset always output list 291 self forward eval batch is train False 292 pad eval batch pad 293 outputs out 0 out shape 0 pad copy for out in self get outputs 294 295 output list append outputs usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet ndarray pyc in getitem self in slice 317 raise ValueError 'NDArray only support continuous slicing on axis 0' 318 if in slice start is not None or in slice stop is not None 319 return self slice in slice start in slice stop 320 else 321 return self usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet ndarray pyc in slice self start stop 358 stop mx uint stop if stop else mx uint self shape 0 359 check call LIB MXNDArraySlice 360 self handle start stop ctypes byref handle 361 return NDArray handle handle writable self writable 362 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 if sys version info 0 3 MXNetError 23 38 09 include mxnet ndarray h 262 Check failed shape 0 end 200 vs 201 Slice end index out of range Notice it is an index out of range problems and the fault is directed by outputs out 0 out shape 0 pad copy for out in self get outputs I check the variable pad and find it equals to 1 That is the key problem Next I find where pad defined Since I initialize test variable by following code test mx img ImageIter batch size batch size data shape data shape path imgrec 'test test rec' resize resize mean mean img std std img I check image py and find pad is assigned by batch size 1 i from return io DataBatch batch data batch label batch size 1 i of 455 line By just change the batch size 1 i to max batch size 1 i 0 I successfully fixed the bug But I still not sure whether the right code is max batch size 1 i 0 or batch size i the second way also fix the problem Any idea about this little bug and if I certainly fix the bug could you give me a chance to help me pull request to merge my little work into mxnet Thank you,,szha,2017-02-14 16:01:20,2017-09-28 23:49:00
IS,MxNet can not fully utilize CPU in multithread programs,Hi all I'm using mxnet scala package and runs prediction in a multithread program and each thread has a thread local FeedForward instance It seems that mxnet is not able to utilize multi cores since only one core is busy while I have 20 threads in this program The java stacktrace show that all threads are in this mxNDArraySyncCopyToCPU method Is this method waiting some lock for all threads in the same process One quick solution may be to use multi process instead of multi thread but jvm has much heavy overhead and this is may be my last resort Is there any way to solve this issue,,"yzhliu,szha",2017-02-14 14:34:46,2017-09-28 23:49:02
IS,why DMLC server process utilize all CPU but not limit by nthread reduction,mli When I run below command on BDW server which have 44 cores export PS VERBOSE 1 export OMP NUM THREADS 40 export KMP AFFINITY granularity fine compact 1 0 tools launch py n 1 launcher ssh H hosts python train imagenet py batch size 144 lr 0 05 lr factor 94 num epoch 1 kv store dist sync network inception bn data train data train rec data val data val rec I found there have two major process running One is DMLC worker and another one is DMLC server However I found DMLC server process will init subthread on all of the CPU cores This is not happen when I set kv store local When running local I only see 4 subthreads which equal to nthread reduction Could you pls let me know how to limit the threads when kvstore dist sync,,"zhenlinluo,szha",2017-02-14 18:37:21,2017-09-28 23:49:03
IS,error more than one instance of overloaded function max matches the argument list,cpchung cam linx 100 mxnet gcc version gcc GCC 4 8 5 20150623 Red Hat 4 8 5 11 Copyright C 2015 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE cpchung cam linx 100 mxnet g version g GCC 4 8 5 20150623 Red Hat 4 8 5 11 Copyright C 2015 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE cpchung cam linx 100 mxnet uname a Linux cam linx 100 3 10 0 327 10 1 el7 x86 64 1 SMP Tue Feb 16 17 03 50 UTC 2016 x86 64 x86 64 x86 64 GNU Linux src operator tensor indexing op inl cuh 170 error more than one instance of overloaded function max matches the argument list function max int int function max unsigned int unsigned int function max int unsigned int function max unsigned int int function max long long long long function max unsigned long long unsigned long long function max long long unsigned long long function max unsigned long long long long function max float float function max double double function max float double function max double float argument types are size t size t detected during instantiation of void mxnet op AddTakeGradLargeBatchCaller const mxnet OpContext mshadow Tensor xpu 2 DType const mshadow Tensor xpu 1 IndexType const mshadow Tensor xpu 2 DType with xpu mxnet gpu IndexType float DType float src operator tensor indexing op h 234 here instantiation of void mxnet op EmbeddingOpBackward xpu const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob std allocator mxnet TBlob const std vector mxnet OpReqType std allocator mxnet OpReqType const std vector mxnet TBlob std allocator mxnet TBlob with xpu mxnet gpu src operator tensor indexing op cu 15 here sudo yum check update Install gcc 4 8 make and other development tools sudo yum install y gcc sudo yum install y gcc c sudo yum install y clang Install Python Numpy pip and set up tools sudo yum install python devel sudo pip install numpy curl o get pip py python get pip py pip V install graphviz jupyter sudo E pip install graphviz sudo E pip install jupyter Install OpenBLAS at usr local openblas git clone cd OpenBLAS sudo make j nproc 1 sudo make PREFIX usr local install cd Install cmake for building opencv sudo yum install y cmake Install OpenCV at usr local opencv git clone cd opencv mkdir p build cd build cmake D BUILD opencv gpu OFF D WITH EIGEN ON D WITH TBB ON D WITH CUDA OFF D WITH 1394 OFF D CMAKE BUILD TYPE RELEASE D CMAKE INSTALL PREFIX usr local sudo make PREFIX usr local install Download MXNet source code to mxnet directory git clone mxnet recursive Move to source code parent directory cd mxnet cp make config mk Replace this line if you use other BLAS libs echo USE BLAS openblas config mk echo ADD CFLAGS I usr include openblas config mk echo ADD LDFLAGS lopencv core lopencv imgproc lopencv imgcodecs config mk echo USE CUDA 1 config mk echo USE CUDA PATH usr local cuda config mk echo USE CUDNN 1 config m make j nproc Assuming you are in mxnet directory run below commands Install MXNet Python package cd python sudo python setup py install,,"piiswrong,piiswrong,szha",2017-02-14 20:25:04,2017-09-28 23:49:04
IS,how to use mx sym choose element 0index and mx sym fill element 0index I do not find the using method any more,mxnet symbol choose element 0index mxnet symbol fill element 0index There are no instructions for use and how to use them,,"piiswrong,szha",2017-02-15 03:34:44,2017-09-28 23:49:05
IS,Add defaultt value for parser in train mnist example to use inception bn,I want use inception bn in train mnist example to test multi node performance And inception bn symbol need three parameters as follow def get symbol num classes image shape kwargs So I add these three values passing through parser set defaults in train mnist py I hope it helps,,szha,2017-02-15 12:54:16,2017-09-28 23:49:06
IS,dist async preform different speed of multi node training,I use two machines with Ubuntu 16 04 LTS and mxnet 0 9 4 to train MNIST The command is It is easy to spot that in dist async job host machine maybe is far slower than the other and is almost the same as its in the dist sync job But I do not know what slow down the host machine Anyone can explain it,,"mli,szha",2017-02-08 08:04:18,2017-09-28 23:49:07
IS,How to calculate floating point operations FLOPS in mxnet,Are there any tools to calculate floating point operations FLOPS in mxnet If not then how to do this manually Any suggestions would be appreciated,,szha,2017-02-15 14:17:19,2017-09-28 23:49:08
IS,Compilation Error with the latest version,update to the latest version of mxnet I face a compilation error as follows include mxnet tensor blob h 39 error cannot determine the exception specification of the default constructor due to a circular dependency My gcc is 4 8 5 and cuda 6 5 Everything works well under the previous version,,"tornadomeet,lazyparser,szha",2016-08-29 06:30:17,2017-09-28 23:49:09
IS,error when running predict with pretrained model ipynb,I have not GPU envrionment so use CPU for this example I change this line ctx mx cpu model mx model FeedForward load prefix num round ctx mx cpu numpy batch size 1 then get the error infomation MXNetError Traceback most recent call last ipython input 4 ee7dbb5b02b9 in module 2 prefix inception bn Inception BN 3 num round 126 4 model mx model FeedForward load prefix num round ctx mx cpu numpy batch size 1 C Users leo Anaconda2 lib site packages mxnet 0 7 0 py2 7 egg mxnet model pyc in load prefix epoch ctx kwargs 832 prefix epoch params will be saved for parameters 833 834 symbol arg params aux params load checkpoint prefix epoch 835 return FeedForward symbol ctx ctx 836 arg params arg params aux params aux params C Users leo Anaconda2 lib site packages mxnet 0 7 0 py2 7 egg mxnet model pyc in load checkpoint prefix epoch 361 parameters will be loaded from prefix epoch params 362 363 symbol sym load ' s symbol json' prefix 364 save dict nd load ' s 04d params' prefix epoch 365 arg params C Users leo Anaconda2 lib site packages mxnet 0 7 0 py2 7 egg mxnet symbol pyc in load fname 956 raise TypeError 'fname need to be string' 957 handle SymbolHandle 958 check call LIB MXSymbolCreateFromFile c str fname ctypes byref handle 959 return Symbol handle 960 C Users leo Anaconda2 lib site packages mxnet 0 7 0 py2 7 egg mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 def c str string MXNetError Failed loading Op conv 1 of type Convolution Cannot find argument 'cudnn off' Possible Arguments kernel Shape tuple required convolution kernel size y x stride Shape tuple optional default 1 1 convolution stride y x dilate Shape tuple optional default 1 1 convolution dilate y x pad Shape tuple optional default 0 0 pad for convolution y x num filter int non negative required convolution filter channel number num group int non negative optional default 1 Number of groups partition This option is not supported by CuDNN you can use SliceChannel to num group apply convolution and concat instead to achieve the same need workspace long non negative optional default 512 Tmp workspace for convolution MB no bias boolean optional default False Whether to disable bias parameter,,"piiswrong,szha",2016-11-24 08:34:06,2017-09-28 23:49:10
IS,Error while building Python wrapper for speech demo,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 LTS Compiler g Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source MXNet commit hash git rev parse HEAD 2413a8ed421b5a917eed544b08d434839b8c812d If you are using python package please provide Python version and distribution Python 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I was trying the example speech demo While building Python wrapper for kaldi toolkits after installing the kaldi in the system successfully I got the following error g msse msse2 Wall I pthread DKALDI DOUBLEPRECISION 0 Wno sign compare Wno unused local typedefs Winit self DHAVE EXECINFO H 1 rdynamic DHAVE CXXABI H DHAVE ATLAS I home beibei qian kaldi tools ATLAS include I home beibei qian kaldi tools openfst include g fPIC DHAVE CUDA I usr local cuda include c o ctypes o ctypes cc ctypes cc In function kaldi nnet1 Nnet t Nnet new char float int ctypes cc 206 25 error class kaldi nnet1 Nnet has no member named SetDropoutRetention nnet nnet transf SetDropoutRetention dropout retention ctypes cc 209 25 error class kaldi nnet1 Nnet has no member named SetDropoutRetention nnet nnet transf SetDropoutRetention 1 0 In file included from base kaldi common h 38 0 from util table types h 23 from ctypes cc 3 base kaldi math h At global scope base kaldi math h 130 17 warning kaldi kLogZeroBaseFloat defined but not used Wunused variable const BaseFloat kLogZeroBaseFloat std numeric limits BaseFloat infinity make ctypes o Error 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Download and install Kaldi 2 Copy example speech demo python wrap to kaldi src 3 cd kaldi src python wrap 4 make What have you tried to solve it 1 Pull the latest Kaldi and reinstall 2 Pull the latest mxnet 3,,szha,2017-02-10 06:41:05,2017-09-28 23:49:11
IS,mxnet on centos 7,I have tried it on centos 7 I dont see python27 but sudo yum install python devel sudo pip install numpy Does anyone try following the rest of the tutorial on centos 7 Or the document is out of date Environment info Operating System centos 7 with kernel 3 10,,"qiyuangong,qiyuangong,luoyetx,szha",2017-02-14 00:16:26,2017-09-28 23:49:12
IS,How to configure a GBDT model by mxnet,it is said that mxnet supports the GBDT algorithm how can I invoking it,,"piiswrong,szha",2017-02-16 10:04:06,2017-09-28 23:49:13
IS,Installation error of Installation MXNet for Python Installing Python package for MXNet,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler gcc version 4 8 4 g version 4 8 4 Package used Python R Scala Julia Python 2 7 6 MXNet version 20170113 download If you are using python package please provide Python version and distribution Python 2 7 6 Cython version 0 20 1post0 Error Message Quick Installation Install MXNet for Python bash install mxnet ubuntu python sh Installing Numpy python numpy fonts font awesome ipython notebook common kde l10n engb kde l10n zhcn libjs highlight libjs jquery ui libjs marked libpgm 5 1 0 libzmq3 python simplegeneric python zmq Use 'apt get autoremove' to remove them 0 0 0 698 Installing Python setuptools python setuptools python pip fonts font awesome ipython notebook common kde l10n engb kde l10n zhcn libjs highlight libjs jquery ui libjs marked libpgm 5 1 0 libzmq3 python simplegeneric python zmq Use 'apt get autoremove' to remove them 0 0 0 698 Installing Python package for MXNet Compiling mxnet cython symbol pyx because it changed Compiling mxnet cython ndarray pyx because it changed Cythonizing mxnet cython ndarray pyx Error compiling Cython file svec i vec i c str return svec cdef extern from nnvm c api h const char NNGetLastError mxnet cython base pyi 54 32 Syntax error in C variable declaration Traceback most recent call last File setup py line 72 in module ext modules config cython File setup py line 53 in config cython return cythonize ret File usr lib python2 7 dist packages Cython Build Dependencies py line 798 in cythonize cythonize one args 1 File usr lib python2 7 dist packages Cython Build Dependencies py line 915 in cythonize one raise CompileError None pyx file Cython Compiler Errors CompileError mxnet cython ndarray pyx Thank you very much,,"piiswrong,szha",2017-01-13 13:53:05,2017-09-28 23:49:14
IS,Following the Predict with pre trained models tutorial all my prediction probabilities are 0,I followed the tutorial exactly from here I had to remove 'cudnn tune' in the json file because I was getting an error regarded it not being there Everything else is exactly the same as the tutorial and I ended up with the output looking like this probability 0 000000 class n15102894 knothole probability 0 000000 class n03209359 disk clutch probability 0 000000 class n03212811 distillery still probability 0 000000 class n03212114 disposal electric pig garbage disposal probability 0 000000 class n03211789 display window shop window shopwindow show window I tried the tutorial Predict and Extract Features with Pre trained Models ipynb and again got top 1 accuracy of 0 after removing all the offending cudnn tune sections What could I be doing wrong Any help is greatly appreciated,,"kevinthesun,kevinthesun,kevinthesun,szha",2016-12-02 00:59:24,2017-09-28 23:49:15
IS,can mxnet use hdf5 data except rec data,Now I have a hdf5 data and I want to use mxnet to train So can mxnet use hdf5 data except rec data,,"piiswrong,szha",2016-08-27 08:07:09,2017-09-28 23:49:16
IS,how to use mxnet symbol sgd update,mx symbol adam update cyfunction make atomic symbol function locals creator at 0x7f1c6cd1fa10 mx symbol adam update lr 1 name 'as' 17 19 10 home user jwhu2 mxnet dmlc core include dmlc logging h 300 17 19 10 src core symbolic cc 295 Not enough argument to call operator adam update Stack trace returned 10 entries bt 0 home user jwhu2 mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f1c7b710cac bt 1 home user jwhu2 mxnet python mxnet lib libmxnet so ZN4nnvm6Symbol7ComposeERKN4dmlc10array viewIPKS0 EERKSt13unordered mapISsS4 St4hashISsESt8equal toISsESaISt4pairIKSsS4 EEERSE 0x690 0x7f1c7ce89fa0 bt 2 home user jwhu2 mxnet python mxnet lib libmxnet so NNSymbolCompose 0x1bd 0x7f1c7ce7538d bt 3 home user jwhu2 mxnet python mxnet cy2 symbol so 0xe2e9 0x7f1c6cd5a2e9 bt 4 home user jwhu2 mxnet python mxnet cy2 symbol so 0x11fc1 0x7f1c6cd5dfc1 bt 5 python PyEval EvalFrameEx 0x98d 0x5244dd bt 6 python 0x567d14 bt 7 python PyRun InteractiveOneFlags 0x18c 0x465a2d bt 8 python PyRun InteractiveLoopFlags 0xaa 0x465b49 bt 9 python PyRun AnyFileExFlags 0x37 0x4661fe Traceback most recent call last File stdin line 1 in module File mxnet cython symbol pyx line 167 in symbol make atomic symbol function creator mxnet cython symbol cpp 3751 File mxnet cython base pyi line 36 in symbol CALL mxnet cython symbol cpp 1624 mxnet base MXNetError 17 19 10 src core symbolic cc 295 Not enough argument to call operator adam update Stack trace returned 10 entries bt 0 home user jwhu2 mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f1c7b710cac bt 1 home user jwhu2 mxnet python mxnet lib libmxnet so ZN4nnvm6Symbol7ComposeERKN4dmlc10array viewIPKS0 EERKSt13unordered mapISsS4 St4hashISsESt8equal toISsESaISt4pairIKSsS4 EEERSE 0x690 0x7f1c7ce89fa0 bt 2 home user jwhu2 mxnet python mxnet lib libmxnet so NNSymbolCompose 0x1bd 0x7f1c7ce7538d bt 3 home user jwhu2 mxnet python mxnet cy2 symbol so 0xe2e9 0x7f1c6cd5a2e9 bt 4 home user jwhu2 mxnet python mxnet cy2 symbol so 0x11fc1 0x7f1c6cd5dfc1 bt 5 python PyEval EvalFrameEx 0x98d 0x5244dd bt 6 python 0x567d14 bt 7 python PyRun InteractiveOneFlags 0x18c 0x465a2d bt 8 python PyRun InteractiveLoopFlags 0xaa 0x465b49 bt 9 python PyRun AnyFileExFlags 0x37 0x4661fe,,"piiswrong,szha",2017-02-17 09:22:01,2017-09-28 23:49:17
IS,some questions about the param num update in optimizer py,Hi I find there are some problems in updating the param num update in optimizer py For example In the first iter the first grad gets the lr when the num update is 0 L264 and then it updates the num update to 1 so the next grad and others will get the lr when the num update is 1 In the second iter the num update of the first grad is 1 and others will be 2 This may lead to different grads get different lr It does not matter if we use MultiFactorScheduler to get lr L85 However It does matter if we use other schedulers such as poly scheduler L20 I think the function update count should be changed to L136 if index not in self index update count self index update count index self begin num update self num update self index update count index self index update count index 1 and there should be changed to L264 self update count index lr self get lr index wd self get wd index,,"piiswrong,szha",2017-02-19 06:00:57,2017-09-28 23:49:18
IS,Documentation of generated functions need to be clarified,In mxnet we use cython or via ctypes on windows to generate functions for NDArray and Symbol However the generated documentation is misleading Take mxnet symbol mean as an example the documentation mxnet symbol mean says mxnet symbol mean args kwargs Parameters data NDArray Source input axis Shape tuple optional default Empty or unsigned or tuple The axes to perform the reduction If left empty a global reduction will be performed keepdims boolean optional default False If true the axis which is reduced is left in the result as dimension with size one name string optional Name of the resulting symbol Returns symbol The result symbol Return type Symbol Notice that the parameter data is type is NDArray However when we pass a ndarray as argument to this function an error will be raised python In 1 a ndarray array 1 2 3 4 In 2 type a Out 2 mxnet ndarray NDArray In 3 symbol mean a TypeError Traceback most recent call last ipython input 7 37f4ed75ee14 in module 1 symbol mean a Anaconda lib site packages mxnet 0 9 3 py3 5 egg mxnet ctypes symbol py in creator args kwargs 189 hint func name lower 190 name NameManager current get name hint 191 s compose args name name symbol kwargs 192 return s 193 Anaconda lib site packages mxnet 0 9 3 py3 5 egg mxnet symbol py in compose self args kwargs 235 for arg in args 236 if not isinstance arg Symbol 237 raise TypeError 'Compose expect Symbol as arguments' 238 for val in kwargs values 239 if not isinstance val Symbol TypeError Compose expect Symbol as arguments Notice that when apply mxnet symbol mean to a ndarray a TypeError is raised According to the documentation however the program is valid The root of this issue is that we have registered all functions from MXListAllOpNames both into mxnet ndarray and mxnet symbol modules when these two modules are imported regardless what there argument types actual are A possible solution for this trouble is do some analysis in function make ndarray function and make atomic symbol function rather than register all operators and provide misleading documentation,,"piiswrong,szha",2017-02-19 05:00:22,2017-09-28 23:49:19
IS,run image segmentation py error,i built the fcn 8s model successfully But i run image segmentation py i meet an error check failed x ctx default ctx Input array is in cpu 0 while binding with ctx gpu 0 All arguments must be in global context gpu 0 unless group2ctx is specified for cross device graph,,szha,2017-02-19 15:04:54,2017-09-28 23:49:20
IS,cross correlation convolution of two NDArray,I used the trained model get two different size of feature map from two different sized image input And now I want to do the cross correlation operation or convolution of those two feature map But I did not find such an API in document How can I do that,,"sxjscience,szha",2017-02-20 03:32:38,2017-09-28 23:49:21
IS,bugs in example numpy ops custom softmax py,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System windows 7 python3 6 Compiler vc2013 Package used Python R Scala Julia Python3 6 MXNet version 0 9 3 Error Message 12 14 49 E Projects mxnet src io iter mnist cc 91 MNISTIter load 60000 images shuffle 1 shape 100 784 12 14 50 E Projects mxnet src io iter mnist cc 91 MNISTIter load 10000 images shuffle 1 shape 100 784 Traceback most recent call last File D wzy mxnet master example numpy ops custom softmax py line 69 in module batch end callback mx callback Speedometer 100 100 File D Python36 x64 mxnet model py line 745 in fit self init params dict data provide data data provide label File D Python36 x64 mxnet model py line 485 in init params arg shapes aux shapes self symbol infer shape input shapes File D Python36 x64 mxnet symbol py line 453 in infer shape return self infer shape impl False args kwargs File D Python36 x64 mxnet symbol py line 513 in infer shape impl ctypes byref complete OSError exception access violation writing 0x0000000000000000 Exception ignored in bound method Symbol del of mxnet symbol Symbol object at 0x00000000057DC710 Traceback most recent call last File D Python36 x64 mxnet symbol py line 106 in del OSError exception access violation writing 0x0000000000000000 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 just run python custom softmax py What have you tried to solve it 1 I found that I can not do save or bind on this mlp model such as mlp save 'x json' It works only on mlp mx symbol softmax data fc3 name isoftmax' 2 I write a simple code to use this custom operator The program crashed when I tried to do any computation or saving on the model Only build in softmax works,,szha,2017-02-20 04:30:23,2017-09-28 23:49:22
IS,problem installing with python ubuntu 14 04,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler version 4 8 4 Ubuntu 4 8 4 2ubuntu1 14 04 3 Package used Python R Scala Julia Python 2 7 6 MXNet version 0 9 1 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error from src operator tensor broadcast reduce op value cc 6 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X long unsigned int Y int src operator tensor elemwise binary broadcast op h 24 3 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X unsigned int Y int src operator tensor broadcast reduce op h 151 5 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare src operator tensor elemwise binary broadcast op basic cc 12 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 12 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 12 1 note if you use fpermissive G will accept your code src operator tensor elemwise binary op extended cc 11 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 11 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 11 1 note if you use fpermissive G will accept your code src operator tensor elemwise binary broadcast op basic cc 17 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 17 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 28 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 28 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 33 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 33 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 44 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 44 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 48 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 48 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 59 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 59 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 16 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 16 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 27 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 27 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op basic cc 63 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op basic cc 63 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 32 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 32 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 43 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 43 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 48 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 48 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 59 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 59 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary op extended cc 64 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary op extended cc 64 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op logic cc 13 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op logic cc 13 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op logic cc 13 1 note if you use fpermissive G will accept your code src operator tensor elemwise binary broadcast op logic cc 16 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op logic cc 16 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op logic cc 19 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op logic cc 19 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op logic cc 22 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op logic cc 22 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op logic cc 25 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op logic cc 25 1 note is an alternate spelling for Insert whitespace between and src operator tensor elemwise binary broadcast op logic cc 28 1 error cannot begin a template argument list fpermissive src operator tensor elemwise binary broadcast op logic cc 28 1 note is an alternate spelling for Insert whitespace between and In file included from home mt08 mxnet dmlc core include dmlc registry h 13 0 from include mxnet operator util h 18 from src operator tensor elemwise unary op h 9 from src operator tensor elemwise binary broadcast op basic cc 6 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X long unsigned int Y int src operator tensor elemwise binary broadcast op h 24 3 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare In file included from home mt08 mxnet dmlc core include dmlc registry h 13 0 from include mxnet operator util h 18 from src operator tensor elemwise unary op h 9 from src operator tensor elemwise binary op basic cc 6 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X long unsigned int Y int src operator tensor elemwise op common h 77 3 required from bool mxnet op ElemwiseShape const nnvm NodeAttrs std vector nnvm TShape std vector nnvm TShape with int n in 2 int n out 1 src operator tensor elemwise binary op basic cc 11 1 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare In file included from home mt08 mxnet dmlc core include dmlc registry h 13 0 from include mxnet operator util h 18 from src operator tensor elemwise unary op h 9 from src operator tensor elemwise binary broadcast op logic cc 6 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X long unsigned int Y int src operator tensor elemwise binary broadcast op h 24 3 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare In file included from home mt08 mxnet dmlc core include dmlc registry h 13 0 from include mxnet operator util h 18 from src operator tensor elemwise unary op h 9 from src operator tensor elemwise binary op extended cc 6 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X long unsigned int Y int src operator tensor elemwise op common h 77 3 required from bool mxnet op ElemwiseShape const nnvm NodeAttrs std vector nnvm TShape std vector nnvm TShape with int n in 2 int n out 1 src operator tensor elemwise binary op extended cc 11 1 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X int Y unsigned int home mt08 mxnet nnvm include nnvm tuple h 411 5 required from mshadow Shape ndim nnvm TShape get const with int dim 2 src operator tensor broadcast reduce op h 271 3 required from void mxnet op ReduceAxesCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu reducer mshadow red sum src operator tensor broadcast reduce op value cc 18 79 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare make build src operator tensor broadcast reduce op index o Error 1 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X int Y unsigned int home mt08 mxnet nnvm include nnvm tuple h 411 5 required from mshadow Shape ndim nnvm TShape get const with int dim 2 src operator tensor elemwise binary broadcast op h 113 74 required from void mxnet op BinaryBroadcastComputeImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float OP mxnet op mshadow op eq mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 133 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mxnet op mshadow op eq src operator tensor elemwise binary broadcast op logic cc 14 81 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X unsigned int Y int home mt08 mxnet mshadow mshadow extension broadcast with axis h 197 3 required from mshadow expr BroadcastWithMultiAxesExp SrcExp DType mshadow expr ExpInfo E kDim mshadow expr broadcast to const mshadow expr Exp SrcExp DType etype const TShape with SrcExp mshadow Tensor mshadow cpu 2 float DType float int etype 0 TShape nnvm TShape src operator tensor elemwise binary broadcast op h 118 3 required from void mxnet op BinaryBroadcastComputeImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float OP mxnet op mshadow op eq mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 133 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mxnet op mshadow op eq src operator tensor elemwise binary broadcast op logic cc 14 81 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X int Y unsigned int home mt08 mxnet nnvm include nnvm tuple h 411 5 required from mshadow Shape ndim nnvm TShape get const with int dim 2 src operator tensor elemwise binary broadcast op h 113 74 required from void mxnet op BinaryBroadcastComputeImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float OP mxnet op mshadow op power mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 133 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mxnet op mshadow op power src operator tensor elemwise binary broadcast op extended cc 13 84 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X unsigned int Y int home mt08 mxnet mshadow mshadow extension broadcast with axis h 197 3 required from mshadow expr BroadcastWithMultiAxesExp SrcExp DType mshadow expr ExpInfo E kDim mshadow expr broadcast to const mshadow expr Exp SrcExp DType etype const TShape with SrcExp mshadow Tensor mshadow cpu 2 float DType float int etype 0 TShape nnvm TShape src operator tensor elemwise binary broadcast op h 118 3 required from void mxnet op BinaryBroadcastComputeImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float OP mxnet op mshadow op power mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 133 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mxnet op mshadow op power src operator tensor elemwise binary broadcast op extended cc 13 84 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X int Y unsigned int home mt08 mxnet nnvm include nnvm tuple h 411 5 required from mshadow Shape ndim nnvm TShape get const with int dim 2 src operator tensor elemwise binary broadcast op h 113 74 required from void mxnet op BinaryBroadcastComputeImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float OP mshadow op plus mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 133 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mshadow op plus src operator tensor elemwise binary broadcast op basic cc 14 84 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X unsigned int Y int home mt08 mxnet mshadow mshadow extension broadcast with axis h 197 3 required from mshadow expr BroadcastWithMultiAxesExp SrcExp DType mshadow expr ExpInfo E kDim mshadow expr broadcast to const mshadow expr Exp SrcExp DType etype const TShape with SrcExp mshadow Tensor mshadow cpu 2 float DType float int etype 0 TShape nnvm TShape src operator tensor elemwise binary broadcast op h 118 3 required from void mxnet op BinaryBroadcastComputeImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float OP mshadow op plus mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 133 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mshadow op plus src operator tensor elemwise binary broadcast op basic cc 14 84 required from here home mt08 mxnet dmlc core include dmlc logging h 94 1 warning comparison between signed and unsigned integer expressions Wsign compare make build src operator tensor elemwise binary op basic o Error 1 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck NE const X const Y with X unsigned int Y int home mt08 mxnet mshadow mshadow tensor cpu inl h 216 3 required from void mshadow MapReduceKeepLowest mshadow TRValue R mshadow cpu 1 DType const mshadow expr Exp E DType etype DType with Saver mshadow sv plusto Reducer mshadow red sum R mshadow Tensor mshadow cpu 1 float DType float E mshadow Tensor mshadow cpu 2 float int etype 0 home mt08 mxnet mshadow mshadow extension reduceto1d h 99 5 required from static void mshadow expr ExpComplexEngine SV mshadow Tensor xpu 1 DType mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 DType Eval mshadow Tensor xpu 1 DType const mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 with SV mshadow sv plusto Device mshadow cpu DType float SrcExp mshadow Tensor mshadow cpu 2 float Reducer mshadow red sum home mt08 mxnet mshadow mshadow expr engine inl h 461 5 required from static void mshadow expr ExpEngine Saver RValue DType Eval RV const mshadow expr Exp E DType 7 with E mshadow expr ReduceTo1DExp mshadow Tensor mshadow cpu 2 float float mshadow red sum 1 SV mshadow sv plusto RV mshadow Tensor mshadow cpu 1 float DType float home mt08 mxnet mshadow mshadow expression h 175 5 required from Container mshadow expr RValueExp Container DType operator const mshadow expr Exp E DType etype with E mshadow expr ReduceTo1DExp mshadow Tensor mshadow cpu 2 float float mshadow red sum 1 int etype 7 Container mshadow Tensor mshadow cpu 1 float DType float src operator tensor elemwise binary broadcast op h 185 5 required from void mxnet op ReduceToAssign mshadow Tensor xpu 2 DType mxnet OpReqType const SrcExp with Reducer mshadow red sum xpu mshadow cpu SrcExp mshadow Tensor mshadow cpu 2 float DType float src operator tensor broadcast reduce op h 271 3 required from void mxnet op ReduceAxesCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu reducer mshadow red sum src operator tensor broadcast reduce op value cc 18 79 required from here home mt08 mxnet dmlc core include dmlc logging h 95 1 warning comparison between signed and unsigned integer expressions Wsign compare make build src operator tensor elemwise binary op extended o Error 1 make build src operator tensor elemwise binary broadcast op logic o Error 1 make build src operator tensor broadcast reduce op value o Error 1 home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck NE const X const Y with X unsigned int Y int home mt08 mxnet mshadow mshadow tensor cpu inl h 216 3 required from void mshadow MapReduceKeepLowest mshadow TRValue R mshadow cpu 1 DType const mshadow expr Exp E DType etype DType with Saver mshadow sv plusto Reducer mshadow red sum R mshadow Tensor mshadow cpu 1 float DType float E mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 int etype 3 home mt08 mxnet mshadow mshadow extension reduceto1d h 99 5 required from static void mshadow expr ExpComplexEngine SV mshadow Tensor xpu 1 DType mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 DType Eval mshadow Tensor xpu 1 DType const mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 with SV mshadow sv plusto Device mshadow cpu DType float SrcExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 Reducer mshadow red sum home mt08 mxnet mshadow mshadow expr engine inl h 461 5 required from static void mshadow expr ExpEngine Saver RValue DType Eval RV const mshadow expr Exp E DType 7 with E mshadow expr ReduceTo1DExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 float mshadow red sum 1 SV mshadow sv plusto RV mshadow Tensor mshadow cpu 1 float DType float home mt08 mxnet mshadow mshadow expression h 175 5 required from Container mshadow expr RValueExp Container DType operator const mshadow expr Exp E DType etype with E mshadow expr ReduceTo1DExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 float mshadow red sum 1 int etype 7 Container mshadow Tensor mshadow cpu 1 float DType float src operator tensor elemwise binary broadcast op h 185 5 required from void mxnet op ReduceToAssign mshadow Tensor xpu 2 DType mxnet OpReqType const SrcExp with Reducer mshadow red sum xpu mshadow cpu SrcExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 DType float src operator tensor elemwise binary broadcast op h 259 3 required from void mxnet op BinaryBroadcastBackwardUseInImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float LOP mxnet op mshadow op power grad ROP mxnet op mshadow op power rgrad mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 277 5 required from void mxnet op BinaryBroadcastBackwardUseIn const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu LOP mxnet op mshadow op power grad ROP mxnet op mshadow op power rgrad src operator tensor elemwise binary broadcast op extended cc 25 87 required from here home mt08 mxnet dmlc core include dmlc logging h 95 1 warning comparison between signed and unsigned integer expressions Wsign compare home mt08 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck NE const X const Y with X unsigned int Y int home mt08 mxnet mshadow mshadow tensor cpu inl h 216 3 required from void mshadow MapReduceKeepLowest mshadow TRValue R mshadow cpu 1 DType const mshadow expr Exp E DType etype DType with Saver mshadow sv plusto Reducer mshadow red sum R mshadow Tensor mshadow cpu 1 float DType float E mshadow expr UnaryMapExp mxnet op mshadow op identity mshadow Tensor mshadow cpu 2 float float 1 int etype 1 home mt08 mxnet mshadow mshadow extension reduceto1d h 99 5 required from static void mshadow expr ExpComplexEngine SV mshadow Tensor xpu 1 DType mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 DType Eval mshadow Tensor xpu 1 DType const mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 with SV mshadow sv plusto Device mshadow cpu DType float SrcExp mshadow expr UnaryMapExp mxnet op mshadow op identity mshadow Tensor mshadow cpu 2 float float 1 Reducer mshadow red sum home mt08 mxnet mshadow mshadow expr engine inl h 461 5 required from static void mshadow expr ExpEngine Saver RValue DType Eval RV const mshadow expr Exp E DType 7 with E mshadow expr ReduceTo1DExp mshadow expr UnaryMapExp mxnet op mshadow op identity mshadow Tensor mshadow cpu 2 float float 1 float mshadow red sum 1 SV mshadow sv plusto RV mshadow Tensor mshadow cpu 1 float DType float home mt08 mxnet mshadow mshadow expression h 175 5 required from Container mshadow expr RValueExp Container DType operator const mshadow expr Exp E DType etype with E mshadow expr ReduceTo1DExp mshadow expr UnaryMapExp mxnet op mshadow op identity mshadow Tensor mshadow cpu 2 float float 1 float mshadow red sum 1 int etype 7 Container mshadow Tensor mshadow cpu 1 float DType float src operator tensor elemwise binary broadcast op h 185 5 required from void mxnet op ReduceToAssign mshadow Tensor xpu 2 DType mxnet OpReqType const SrcExp with Reducer mshadow red sum xpu mshadow cpu SrcExp mshadow expr UnaryMapExp mxnet op mshadow op identity mshadow Tensor mshadow cpu 2 float float 1 DType float src operator tensor elemwise binary broadcast op h 210 3 required from void mxnet op BinaryBroadcastBackwardUseNoneImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float LOP mxnet op mshadow op identity ROP mxnet op mshadow op identity mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 226 5 required from void mxnet op BinaryBroadcastBackwardUseNone const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu LOP mxnet op mshadow op identity ROP mxnet op mshadow op identity src operator tensor elemwise binary broadcast op basic cc 26 86 required from here home mt08 mxnet dmlc core include dmlc logging h 95 1 warning comparison between signed and unsigned integer expressions Wsign compare make build src operator tensor elemwise binary broadcast op basic o Error 1 make build src operator tensor elemwise binary broadcast op extended o Error 1 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 using version 0 8 same problem 2 gcc version change 4 7 to 4 8 3,,szha,2017-01-16 02:27:17,2017-09-28 23:49:23
IS,Execution not exiting while using distributed training,When using the launch py script for training the execution is not exiting after the training is done even for the simple MNIST example I have tried on multiple clusters lot many times including AWS linux machine cluster Can anyone please help me understand why could this be happening,,phunterlau,2017-02-20 16:46:26,2017-09-28 23:57:47
IS,Memory consumption of RNNs in version v0 9,Hi I noticed that the memory consumption is considerably higher in version v0 9 compared to v0 8 This is probably due to the move to nnvm and not the same memory optimizations being applied This can easily be reproduced with the rnn example in mxnet example rnn lstm bucketing py with larger hidden size Namely I set the following parameters With this I get a GPU memory consumption of v0 9 3 6 GB v0 8 2 3 GB v0 9 was e1cafffdbbfb805d6ca5a8bad83bd6509e057e73 v0 8 was 67bee197702581c0c8ed9585dc24143c4cbccb8e Tobi,,"tdomhan,piiswrong,tdomhan,tdomhan,eric-haibin-lin,tdomhan,tdomhan,eric-haibin-lin,tdomhan,phunterlau",2017-01-24 17:18:08,2017-09-28 23:57:48
IS,Does mxnet amalgamated library have memory effective forward operations,I'm trying to export mxnet according to Amalgamation manual and run prediction on iOS and Android with neural net models The neural nets I create might have large size and memory consuming so I want to ask following 1 Does the amalgamated library have memory optimization mechanism in forward calculation like re allocating memory regions of already calculated layers' parameter variables for next layer variables 2 Does it supports GPU calculation on mobile devices If not is there any plans for GPU support in the future,,phunterlau,2017-02-21 01:18:39,2017-09-28 23:57:49
IS,how can i mask a array,Environment info Operating System centos Compiler Package used Python R Scala Julia MXNet version 0 9 3 the newest version gives element mask is removed Please use src mask reshape mask size 1 1 1 directly as binary ops now support broadcasting how can i realize the function for example I have a matrix A shape 32 256 and a mask maxtrix shape 32 how can i do the broadcast mul function I try A mx sym broadcast mul A mx sym Reshape data mask shape 1 256 but error,,"piiswrong,phunterlau",2017-02-20 17:49:21,2017-09-28 23:57:51
IS,How can i get maximum of a matrix and use as a scalar,I want to get a maximum of a input maxtrix and us it as a scalar First I try da mx sym Variable wouldata' max mx sym max da arg shape out shape aux shape max infer shape data 256 12 print out shape I got 1L when i use it in my network da mx sym Variable wouldata' max mx sym max mx sym Variable woulda' I use max 0 as the maximum i got max cannot be use as a scalar Symbol ComposeKeyword argument name label length not found I try max mx nd array mx sym max mx sym Variable woulda' error too source array must be array like object what should i do,,phunterlau,2017-02-21 05:51:38,2017-09-28 23:57:53
IS,Where is the definition of Reducer,May i ask where is the definition of Reducer,,phunterlau,2017-02-21 06:46:28,2017-09-28 23:57:54
IS,Python Import amalgamation python mxnet predict API and MXNet Causing Core Dump,Imports caused core dump It happened when I import both of mxnet predict and mxnet coding utf 8 import sys os curr path os path dirname os path abspath os path expanduser file sys path append home liang frankwang mxnet amalgamation python sys path append home liang frankwang mxnet python from mxnet predict import Predictor load ndarray file import mxnet as mx import numpy as np import cv2 import os Detailed stack trace 15 48 36 home liang frankwang mxnet dmlc core include dmlc logging h 300 15 48 36 home liang frankwang mxnet dmlc core include dmlc any h 264 Check failed type ptype info typeid T The stored type mismatch stored N4nnvm5OpMapISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEE requested N4nnvm5OpMapISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEE Stack trace returned 55 entries bt 0 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7f7425dcf0e9 bt 1 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT iENKUlPN4dmlc3anyEE clESJ 0x1ef 0x7f7425e7b03f bt 2 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT i 0x11e 0x7f7425dd2e3e bt 3 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so 0x575f76 0x7f7425cd7f76 bt 4 lib64 ld linux x86 64 so 2 0xf3a3 0x7f746d2473a3 bt 5 lib64 ld linux x86 64 so 2 0x13ab6 0x7f746d24bab6 bt 6 lib64 ld linux x86 64 so 2 0xf1b4 0x7f746d2471b4 bt 7 lib64 ld linux x86 64 so 2 0x131ab 0x7f746d24b1ab bt 8 lib64 libdl so 2 0x102b 0x7f746ca1f02b bt 9 lib64 ld linux x86 64 so 2 0xf1b4 0x7f746d2471b4 bt 10 lib64 libdl so 2 0x162d 0x7f746ca1f62d bt 11 lib64 libdl so 2 dlopen 0x31 0x7f746ca1f0c1 bt 12 home liang anaconda2 envs frankwang cv lib python2 7 lib dynload ctypes so 0x10b2c 0x7f7465816b2c bt 13 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8942 0x7f746cf3b5a2 bt 14 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 15 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x797e1 0x7f746ceb77e1 bt 16 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 17 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x5c54f 0x7f746ce9a54f bt 18 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 19 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xb6910 0x7f746cef4910 bt 20 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xad328 0x7f746ceeb328 bt 21 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 22 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x6a67 0x7f746cf396c7 bt 23 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 24 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8596 0x7f746cf3b1f6 bt 25 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 26 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f746cf3c2e2 bt 27 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ExecCodeModuleEx 0xc2 0x7f746cf4e122 bt 28 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x11286e 0x7f746cf5086e bt 29 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113621 0x7f746cf51621 bt 30 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113894 0x7f746cf51894 bt 31 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ImportModuleLevel 0x1f0 0x7f746cf51ec0 bt 32 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xf387f 0x7f746cf3187f bt 33 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 34 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval CallObjectWithKeywords 0x43 0x7f746cf31d63 bt 35 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x3d96 0x7f746cf369f6 bt 36 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 37 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f746cf3c2e2 bt 38 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ExecCodeModuleEx 0xc2 0x7f746cf4e122 bt 39 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x11286e 0x7f746cf5086e bt 40 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x1130ca 0x7f746cf510ca bt 41 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113621 0x7f746cf51621 bt 42 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113894 0x7f746cf51894 bt 43 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ImportModuleLevel 0x1f0 0x7f746cf51ec0 bt 44 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xf387f 0x7f746cf3187f bt 45 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 46 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval CallObjectWithKeywords 0x43 0x7f746cf31d63 bt 47 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x3d96 0x7f746cf369f6 bt 48 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 49 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f746cf3c2e2 bt 50 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun FileExFlags 0xb0 0x7f746cf5c960 bt 51 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun SimpleFileExFlags 0xef 0x7f746cf5cb3f bt 52 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 Py Main 0xca4 0x7f746cf72484 bt 53 lib64 libc so 6 libc start main 0xf5 0x7f746c178b15 bt 54 python 0x400649 terminate called after throwing an instance of wouldmlc Error' what 15 48 36 home liang frankwang mxnet dmlc core include dmlc any h 264 Check failed type ptype info typeid T The stored type mismatch stored N4nnvm5OpMapISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEE requested N4nnvm5OpMapISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEE Stack trace returned 55 entries bt 0 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7f7425dcf0e9 bt 1 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT iENKUlPN4dmlc3anyEE clESJ 0x1ef 0x7f7425e7b03f bt 2 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT i 0x11e 0x7f7425dd2e3e bt 3 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so 0x575f76 0x7f7425cd7f76 bt 4 lib64 ld linux x86 64 so 2 0xf3a3 0x7f746d2473a3 bt 5 lib64 ld linux x86 64 so 2 0x13ab6 0x7f746d24bab6 bt 6 lib64 ld linux x86 64 so 2 0xf1b4 0x7f746d2471b4 bt 7 lib64 ld linux x86 64 so 2 0x131ab 0x7f746d24b1ab bt 8 lib64 libdl so 2 0x102b 0x7f746ca1f02b bt 9 lib64 ld linux x86 64 so 2 0xf1b4 0x7f746d2471b4 bt 10 lib64 libdl so 2 0x162d 0x7f746ca1f62d bt 11 lib64 libdl so 2 dlopen 0x31 0x7f746ca1f0c1 bt 12 home liang anaconda2 envs frankwang cv lib python2 7 lib dynload ctypes so 0x10b2c 0x7f7465816b2c bt 13 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8942 0x7f746cf3b5a2 bt 14 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 15 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x797e1 0x7f746ceb77e1 bt 16 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 17 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x5c54f 0x7f746ce9a54f bt 18 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 19 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xb6910 0x7f746cef4910 bt 20 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xad328 0x7f746ceeb328 bt 21 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 22 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x6a67 0x7f746cf396c7 bt 23 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 24 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8596 0x7f746cf3b1f6 bt 25 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 26 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f746cf3c2e2 bt 27 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ExecCodeModuleEx 0xc2 0x7f746cf4e122 bt 28 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x11286e 0x7f746cf5086e bt 29 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113621 0x7f746cf51621 bt 30 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113894 0x7f746cf51894 bt 31 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ImportModuleLevel 0x1f0 0x7f746cf51ec0 bt 32 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xf387f 0x7f746cf3187f bt 33 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 34 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval CallObjectWithKeywords 0x43 0x7f746cf31d63 bt 35 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x3d96 0x7f746cf369f6 bt 36 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 37 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f746cf3c2e2 bt 38 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ExecCodeModuleEx 0xc2 0x7f746cf4e122 bt 39 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x11286e 0x7f746cf5086e bt 40 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x1130ca 0x7f746cf510ca bt 41 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113621 0x7f746cf51621 bt 42 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113894 0x7f746cf51894 bt 43 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ImportModuleLevel 0x1f0 0x7f746cf51ec0 bt 44 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xf387f 0x7f746cf3187f bt 45 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f746ce87dc3 bt 46 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval CallObjectWithKeywords 0x43 0x7f746cf31d63 bt 47 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x3d96 0x7f746cf369f6 bt 48 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f746cf3c1ce bt 49 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f746cf3c2e2 bt 50 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun FileExFlags 0xb0 0x7f746cf5c960 bt 51 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun SimpleFileExFlags 0xef 0x7f746cf5cb3f bt 52 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 Py Main 0xca4 0x7f746cf72484 bt 53 lib64 libc so 6 libc start main 0xf5 0x7f746c178b15 bt 54 python 0x400649 frankwang cv liang ctum2e1302005 recaptcha python ocr predict 30 CHLang py frankwang cv liang ctum2e1302005 recaptcha python ocr predict 30 CHLang py frankwang cv liang ctum2e1302005 recaptcha python ocr predict 30 CHLang py 15 49 51 home liang frankwang mxnet dmlc core include dmlc logging h 300 15 49 51 home liang frankwang mxnet nnvm include nnvm op h 448 Check failed p second plevel Attribute FListInputNames of operator broadcast power is already registered with same plevel 10 Stack trace returned 44 entries bt 0 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7f8997cd70e9 bt 1 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT iENKUlPN4dmlc3anyEE clESJ 0x71e 0x7f8997d8356e bt 2 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT i 0x11e 0x7f8997cdae3e bt 3 home liang frankwang mxnet amalgamation python lib libmxnet so 0x575f76 0x7f89603dcf76 bt 4 lib64 ld linux x86 64 so 2 0xf3a3 0x7f89afe8b3a3 bt 5 lib64 ld linux x86 64 so 2 0x13ab6 0x7f89afe8fab6 bt 6 lib64 ld linux x86 64 so 2 0xf1b4 0x7f89afe8b1b4 bt 7 lib64 ld linux x86 64 so 2 0x131ab 0x7f89afe8f1ab bt 8 lib64 libdl so 2 0x102b 0x7f89af66302b bt 9 lib64 ld linux x86 64 so 2 0xf1b4 0x7f89afe8b1b4 bt 10 lib64 libdl so 2 0x162d 0x7f89af66362d bt 11 lib64 libdl so 2 dlopen 0x31 0x7f89af6630c1 bt 12 home liang anaconda2 envs frankwang cv lib python2 7 lib dynload ctypes so 0x10b2c 0x7f89a845ab2c bt 13 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8942 0x7f89afb7f5a2 bt 14 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 15 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x797e1 0x7f89afafb7e1 bt 16 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 17 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x5c54f 0x7f89afade54f bt 18 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 19 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xb6910 0x7f89afb38910 bt 20 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xad328 0x7f89afb2f328 bt 21 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 22 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x6a67 0x7f89afb7d6c7 bt 23 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x86c3 0x7f89afb7f323 bt 24 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 25 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8596 0x7f89afb7f1f6 bt 26 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 27 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f89afb802e2 bt 28 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ExecCodeModuleEx 0xc2 0x7f89afb92122 bt 29 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x11286e 0x7f89afb9486e bt 30 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113621 0x7f89afb95621 bt 31 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113894 0x7f89afb95894 bt 32 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ImportModuleLevel 0x1f0 0x7f89afb95ec0 bt 33 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xf387f 0x7f89afb7587f bt 34 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 35 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval CallObjectWithKeywords 0x43 0x7f89afb75d63 bt 36 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x3d96 0x7f89afb7a9f6 bt 37 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 38 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f89afb802e2 bt 39 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun FileExFlags 0xb0 0x7f89afba0960 bt 40 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun SimpleFileExFlags 0xef 0x7f89afba0b3f bt 41 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 Py Main 0xca4 0x7f89afbb6484 bt 42 lib64 libc so 6 libc start main 0xf5 0x7f89aedbcb15 bt 43 python 0x400649 terminate called after throwing an instance of wouldmlc Error' what 15 49 51 home liang frankwang mxnet nnvm include nnvm op h 448 Check failed p second plevel Attribute FListInputNames of operator broadcast power is already registered with same plevel 10 Stack trace returned 44 entries bt 0 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7f8997cd70e9 bt 1 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT iENKUlPN4dmlc3anyEE clESJ 0x71e 0x7f8997d8356e bt 2 home liang anaconda2 envs frankwang cv lib python2 7 site packages mxnet 0 9 3 py2 7 linux x86 64 egg mxnet libmxnet so ZN4nnvm2Op8set attrISt8functionIFSt6vectorISsSaISsEERKNS 9NodeAttrsEEEEERS0 RKSsRKT i 0x11e 0x7f8997cdae3e bt 3 home liang frankwang mxnet amalgamation python lib libmxnet so 0x575f76 0x7f89603dcf76 bt 4 lib64 ld linux x86 64 so 2 0xf3a3 0x7f89afe8b3a3 bt 5 lib64 ld linux x86 64 so 2 0x13ab6 0x7f89afe8fab6 bt 6 lib64 ld linux x86 64 so 2 0xf1b4 0x7f89afe8b1b4 bt 7 lib64 ld linux x86 64 so 2 0x131ab 0x7f89afe8f1ab bt 8 lib64 libdl so 2 0x102b 0x7f89af66302b bt 9 lib64 ld linux x86 64 so 2 0xf1b4 0x7f89afe8b1b4 bt 10 lib64 libdl so 2 0x162d 0x7f89af66362d bt 11 lib64 libdl so 2 dlopen 0x31 0x7f89af6630c1 bt 12 home liang anaconda2 envs frankwang cv lib python2 7 lib dynload ctypes so 0x10b2c 0x7f89a845ab2c bt 13 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8942 0x7f89afb7f5a2 bt 14 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 15 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x797e1 0x7f89afafb7e1 bt 16 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 17 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x5c54f 0x7f89afade54f bt 18 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 19 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xb6910 0x7f89afb38910 bt 20 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xad328 0x7f89afb2f328 bt 21 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 22 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x6a67 0x7f89afb7d6c7 bt 23 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x86c3 0x7f89afb7f323 bt 24 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 25 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8596 0x7f89afb7f1f6 bt 26 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 27 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f89afb802e2 bt 28 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ExecCodeModuleEx 0xc2 0x7f89afb92122 bt 29 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x11286e 0x7f89afb9486e bt 30 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113621 0x7f89afb95621 bt 31 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0x113894 0x7f89afb95894 bt 32 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyImport ImportModuleLevel 0x1f0 0x7f89afb95ec0 bt 33 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 0xf387f 0x7f89afb7587f bt 34 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f89afacbdc3 bt 35 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval CallObjectWithKeywords 0x43 0x7f89afb75d63 bt 36 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x3d96 0x7f89afb7a9f6 bt 37 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f89afb801ce bt 38 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyEval EvalCode 0x32 0x7f89afb802e2 bt 39 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun FileExFlags 0xb0 0x7f89afba0960 bt 40 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 PyRun SimpleFileExFlags 0xef 0x7f89afba0b3f bt 41 home liang anaconda2 envs frankwang cv bin lib libpython2 7 so 1 0 Py Main 0xca4 0x7f89afbb6484 bt 42 lib64 libc so 6 libc start main 0xf5 0x7f89aedbcb15 bt 43 python 0x400649 Any ideas My Environment CentOS 7 3 OpenCV 2 4 9 1 CUDA 7 5 Anaconda Python 2 7,,"piiswrong,phunterlau",2017-02-20 07:57:32,2017-09-28 23:57:55
IS,How to calculate the gradient of symbol,Environment info Operating System Ubuntu 14 04 Package used Python R Scala Julia Python MXNet version If you are using python package please provide Python version and distribution Python 2 7 I want to calculate the gradient of symbol what I am supposed to do,,"Godricly,Godricly,phunterlau",2017-02-21 03:20:54,2017-09-28 23:57:57
IS,How to set the req for a custom operator is invisible outputs,I was testing a custom operator from here and the example code runs perfectly But when I embedded it in my own network the program just crashed and displayed a message like Check failed req lsoftmax enum kWeightNorm kWriteTo 0 vs 1 I am wondering how can I set the req for this specific output to kWriteTo Notice that lsoftmax enum kWeightNorm is actually an invisible output since there are three OpOutputs here and NumVisibleOutputs is set to 1,,"nicklhy,nicklhy,luoyetx,nicklhy,luoyetx,luoyetx,nicklhy,luoyetx,phunterlau",2017-02-21 02:02:34,2017-09-28 23:57:59
IS,The optimizer object may not properly initialized,Recently I run the tutorial Handwritten Digit Recognition if I change the optimizer option to a optimizer object as following Is this an intended design,,"piiswrong,phunterlau",2017-02-22 07:18:07,2017-09-28 23:58:01
IS,mxnet do not work,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntun16 04 lts Compiler Package used Python R Scala Julia Python MXNet version newest version Or if installed from source NO MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 13 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 when I use mxnet at beginning it works well but suddenly it does not work at all what confused me is that I can import it But all modules are not be used I have reinstall it but it occurs again 2 3 What have you tried to solve it 1 reinstal 2 3,,phunterlau,2017-02-21 14:11:02,2017-09-28 23:58:02
IS,The example warpctc ocr predict py has a bug it does not provide init states values and run the ocr predict py several times and will get different results,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu Compiler gcc Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Run the predict py several time and get different results 2 It does not provide init states values for the lstm 3 What have you tried to solve it 1 The init states values should be provided 2 first init states could be a var of class little changes in def init ocr self self init states init c init h all shapes wouldata' batch size 41 64 self init states 'label' batch size num label print all shapes 3 Provide the init state values init state arrays mx nd zeros x 1 for x in self init states init state arrays np zeros batch size num hidden dtype float32 init state dict for x in self init states init state dict x 0 init state arrays self predictor forward data img1 init state dict,,phunterlau,2017-02-17 12:47:05,2017-09-28 23:58:05
IS,Any plan to implement bi lstm unroll in BaseRNNCell,Hi I found bi lstm sort example do not make full use of LSTMCell and SequentialRNNCell I guess it is related to the lack of bi lstm unroll implementation in BaseRNNCell Is anyone working on this K,,phunterlau,2017-02-23 02:27:29,2017-09-28 23:58:08
IS,How to set the environment variable KMP AFFINITY if I have 24 physical CPUs,Also setting the following two environment variables may help KMP AFFINITY granularity fine compact 1 0 if there are two physical CPUs,,phunterlau,2017-02-23 05:30:16,2017-09-28 23:58:09
IS,it takes several minutes to launch gpu,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 Compiler i7 Package used Python R Scala Julia Python MXNet version lastest Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 12 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I test the example python example image classification train mnist py network lenet gpus 0 it takes several minutes to launch gpu Minimum reproducible example if you are using your own code please provide a short script that reproduces the error when I run mtcnn demo py it negerates issus src operator cudnn convolution inl h 55 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable time 0 560523033142 init done opengl support available how should I solve the problem Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,phunterlau,2017-02-23 08:45:59,2017-09-28 23:58:11
IS,How to compress the params file,Is there solution to compress the params file,,phunterlau,2017-02-23 09:29:36,2017-09-28 23:58:12
IS,Error when use mxnet mod Module predict function,When I use the image classification example to train a simple mlp model it works fine Then I use the pre trained model to test on a new data set but as I use the module predict method I got the following error mxnet base MXNetError 20 09 46 src operator tensor matrix op inl h 772 Check failed param begin i shp i param end i shp i param begin i param end i The full stack trace is as following Traceback most recent call last File home tx eva 11 PycharmProjects recognize front xray test predict py line 50 in module predict args File home tx eva 11 PycharmProjects recognize front xray test predict py line 20 in predict mod predict eval data data num batch 1 File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet module base module py line 291 in predict self forward eval batch is train False File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet module module py line 474 in forward self exec group forward data batch is train File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet module executor group py line 345 in forward load data data batch self data arrays self data layouts File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet module executor group py line 42 in load data load general batch data targets major axis File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet module executor group py line 33 in load general d dst copy nd crop d src begin tuple begin end tuple end File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet ctypes ndarray py line 131 in generic ndarray function c array ctypes c char p c str str i for i in kwargs values File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError Any hint on how to solve this would be helpful I call the method like this mod predict eval data data num batch 1 where data is a DataIter,,phunterlau,2017-02-23 12:27:33,2017-09-28 23:58:13
PR,Add warning for advanced indexing,,,piiswrong,2017-09-28 20:26:42,2017-09-29 02:48:23
IS,scalapkg fails to build,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OSX Sierra Compiler Package used Python R Scala Julia Scala MXNet version Or if installed from source HEAD MXNet commit hash git rev parse HEAD 339270b5c74a6fece333e1aa7566e333a3e8549c If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message INFO Scanning for projects ERROR ERROR Some problems were encountered while processing the POMs ERROR wouldependencies dependency artifactId' for ml dmlc mxnet libmxnet init scala platform libtype with value 'libmxnet init scala platform ' does not match a valid id pattern line 77 column 19 ERROR The build could not read 1 project Help 1 ERROR ERROR The project ml dmlc mxnet mxnet macros 2 11 0 1 2 SNAPSHOT Users alexy src mxnet scala package macros pom xml has 1 error ERROR wouldependencies dependency artifactId' for ml dmlc mxnet libmxnet init scala platform libtype with value 'libmxnet init scala platform ' does not match a valid id pattern line 77 column 19 ERROR ERROR To see the full stack trace of the errors re run Maven with the e switch ERROR Re run Maven using the X switch to enable full debug logging ERROR ERROR For more information about the errors and possible solutions please read the following articles ERROR Help 1 make scalapkg Error 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 build shared library 2 make scalapkg What have you tried to solve it 1 grok how the parameters need to be set,,"Ldpe2G,Ldpe2G,CodingCat,szha",2017-01-23 01:55:42,2017-09-29 02:53:16
IS,training a model in python and running in in c,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"Piyush3dB,szha",2017-02-23 08:14:48,2017-09-29 02:53:17
IS,Negative indexing as input to mx sym Embedding crashes Python,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu Package used Python R Scala Julia Python Or if installed from source Installed from source for the new RNN interface version 0 9 4 MXNet commit hash git rev parse HEAD 6a0db691f156748b6091b54c0f5c3128764d1a80 If you are using python package please provide Python version and distribution 2 7 and 3 4 both Error Message Please paste the full error message including stack trace No error message the python kernel just crashes Minimum reproducible example import mxnet as mx import numpy as np vocab size 1000 embedding size 50 word vecs np random uniform 1 0 1 0 vocab size 1 embedding size x np array 1 1 2 3 4 nd sample data mx nd array x nd word vecs mx nd array word vecs embedding data mx sym Variable 'embedding data' embeddings mx sym Embedding data embedding data input dim vocab size 1 Add 1 for padding output dim embedding size name embedding print embeddings list arguments embeddings list outputs print embeddings infer shape embedding data nd sample data shape embedding weight word vecs shape Output 'embedding data' 'embedding weight' 'embedding output' 5L 1001L 50L 5L 50L inputs embedding data nd sample data embedding weight nd word vecs executor embeddings bind ctx mx cpu args inputs executor forward Kernel crashes at this point Steps to reproduce The same code pasted above is also available in a short jupyter notebook Mxnet Negative Index Embedding ipynb zip What have you tried to solve it There is a simple workaround I am using for now Fix padding to not be negative indexing and instead be word vec shape 0 1 The fix for now can just be a more graceful failure if not support for negative indexing,,"rishita,piiswrong,ap-hynninen,szha",2017-02-22 19:49:44,2017-09-29 02:53:18
IS,How to add dropout layer in new RNN API,I tried this code but failed Thanks in advance,,szha,2017-02-24 02:03:47,2017-09-29 02:53:20
IS,Locally connected layer convolution layer without weights shared,I want to build a locally connected layer just like a convolution layer without weights shared with mxnet any ideas,,szha,2017-02-21 13:37:44,2017-09-29 02:53:21
IS,cannot declare pointer to void,Environment info Operating System CentOS 7 3 1611 Compiler gcc 4 8 5 Package used Python R Scala Julia Anaconda 2 3 0 MXNet version Latest git pull v0 9 2 67 gdec952a Or if installed from source MXNet commit hash git rev parse HEAD dec952a9b65af10a4d5014c0930fac0b91aa7e4e If you are using python package please provide Python version and distribution Anaconda 2 7 13 If you are using R package please provide R sessionInfo n a Error Message Please paste the full error message including stack trace Bug when installing the python interface python setup py install snip building 'mxnet cy2 ndarray' extension creating build temp linux x86 64 2 7 creating build temp linux x86 64 2 7 mxnet creating build temp linux x86 64 2 7 mxnet cython gcc pthread fno strict aliasing g O2 DNDEBUG g fwrapv O3 Wall Wstrict prototypes fPIC I include I nnvm include I opt anaconda v2 3 0 caffe app include python2 7 c mxnet cython ndarray cpp o build temp linux x86 64 2 7 mxnet cython ndarray o cc1plus warning command line option Wstrict prototypes is valid for C ObjC but not for C enabled by default mxnet cython ndarray cpp In function PyObject pyx pf 7ndarray 22 make ndarray function generic ndarray function PyObject PyObject PyObject mxnet cython ndarray cpp 2512 36 error cannot declare pointer to void pyx t 7ndarray NDArrayHandle pyx t 15 error command 'gcc' failed with exit status 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error python setup py install Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 n a Compilation problem python interface 2 3 What have you tried to solve it 1 n a 2 3,,"piiswrong,piiswrong,szha",2017-02-02 15:56:23,2017-09-29 02:53:22
IS,how to train resnet in MP use mxnet,any suggestion,,szha,2017-02-24 09:39:48,2017-09-29 02:53:23
IS,How can I apply scalar multiplication on a specific dimension of a symbol,How can I apply scalar multiplication on a specific dimension of a symbol but without slice the tensor For example I got the symbol Conv1 with size 128 64 32 32 and variable c with size 64 How can I apply c i scalar multiplication with Conv1 128 i 32 32 That is apply the variable c scalar multiplication on axis 1 with Conv1 I know 'full group' 1x1 group convolution is result equivalent however it is too slow Thanks for reading this,,"piiswrong,szha",2017-02-23 09:34:53,2017-09-29 02:53:25
IS,Installing MXNET in the user space,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux REDHAT Compiler gcc 4 9 3 Package used Python R Scala Julia Python MXNet version latest Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide anaconda2 4 1 1 Python version and distribution 2 7 11 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace amalik hpc1 python python setup py install user running install running bdist egg running egg info writing requirements to mxnet egg info requires txt writing mxnet egg info PKG INFO writing top level names to mxnet egg info top level txt writing dependency links to mxnet egg info dependency links txt reading manifest file 'mxnet egg info SOURCES txt' writing manifest file 'mxnet egg info SOURCES txt' installing library code to build bdist linux x86 64 egg running install lib running build py running build ext building 'mxnet cy2 ndarray' extension gcc pthread fno strict aliasing g O2 DNDEBUG g fwrapv O3 Wall Wstrict prototypes fPIC I include I nnvm include I software anaconda2 include python2 7 c mxnet cython ndarray cpp o build temp linux x86 64 2 7 mxnet cython ndarray o cc1plus warning command line option Wstrict prototypes is valid for C ObjC but not for C mxnet cython ndarray cpp In function PyObject pyx pf 7ndarray 22 make ndarray function generic ndarray function PyObject PyObject PyObject mxnet cython ndarray cpp 2512 36 error cannot declare pointer to void pyx t 7ndarray NDArrayHandle pyx t 15 error command 'gcc' failed with exit status 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 I am trying to install the final building 2 3,,"piiswrong,szha",2017-02-23 21:10:53,2017-09-29 02:53:26
IS,RCNN training does not work with the latest master branch,RPNLogLoss 0 693147 and RCNNLogLoss 3 044522 do not change during training process Environment info Operating System Ubuntu 16 04 Compiler gcc 4 9 2 CUDA 8 0 44 CuDNN v5 1 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 0aeddf985e43629e3ee130b57ca1f490c0a4efa0 Python version and distribution Python 2 7 9 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 run script vgg voc07 sh 0 in example rcnn,,"piiswrong,precedenceguo,piiswrong,precedenceguo,piiswrong,precedenceguo,precedenceguo,precedenceguo,precedenceguo,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,sbodenstein,piiswrong,szha",2017-02-18 12:30:56,2017-09-29 02:53:27
IS,fcn FCN8s VGG16 symbol json file not correct no param and backward source id,tornadomeet my output symbol file like this and this file can not load by mx model load checkpoint,,szha,2017-01-14 12:40:18,2017-09-29 02:53:29
IS,Display feature maps,How Can I display features maps from a loaded model and a given image,,"piiswrong,szha",2017-02-25 21:19:08,2017-09-29 02:53:30
IS,I hope mxnet have two version for runtime version and devle version and this for runtime it has a fast speed,,,szha,2017-02-27 02:10:15,2017-09-29 02:53:31
IS,distributed issues,hi I'm working on distributed training following instructions in the tutorial tools launch py n 2 python train minist py kv store dist sync here is the error Traceback most recent call last File tools launch py line 88 in module main File tools launch py line 69 in main raise RuntimeError 'Unknown submission cluster type s' args cluster RuntimeError Unknown submission cluster type ssh any thoughts,,szha,2016-11-09 01:42:17,2017-09-29 02:53:32
IS,import mxnet as mx error cannot import name 'rmsprop update',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 Compiler Package used Python R Scala Julia Python MXNet version 0 9 4 Python version and distribution Pyhton version 3 6 0 Anaconda 4 3 0 Error Message Traceback most recent call last File stdin line 1 in module File D mxnet master python mxnet init py line 23 in module from import optimizer File D mxnet master python mxnet optimizer py line 6 in module from ndarray import sgd update sgd mom update adam update rmsprop update ImportError cannot import name 'rmsprop update',,szha,2017-02-26 14:14:23,2017-09-29 02:53:33
IS,Newbie question how to update model to correct wrong prediction,Hello Being new to MXNet and to Deep Learning I'm making my hello world network based on the digit recognition For now I achieved create and train a model with train and test datas provided in the tutorial save it load it back and make predictions using my datas images Next step and last step for this overview would be to improve my model by correcting the wrong predictions but I can not find out how to do it Is there a way to do it or the only solution I have would be to had this new images to the train datas which I got from the tutorial and go back from start create model train save and load it back to make prediction Some of the code I used I will published the whole thing as soon as I achieved the last part Initial training Thanks a lot for any help and or links that can help Regards,,szha,2017-02-25 00:17:23,2017-09-29 02:53:35
IS,source array must be array like object,Hi I got following error when try some demo code Is that normal Best Regards Haria,,szha,2017-02-28 07:52:29,2017-09-29 02:53:36
IS,Is there any rules when use MXNET CUDNN AUTOTUNE DEFAULT,some times when I do not set MXNET CUDNN AUTOTUNE DEFAULT 0 It will train but when compute the Training set is Accuracy it may give error like this 10 25 56 mxnet dmlc core include dmlc logging h 300 10 25 56 src operator cudnn convolution inl h 517 Check failed cudnnFindConvolutionForwardAlgorithm s dnn handle in desc filter desc conv desc out desc kMaxAlgos nalgo fwd algo CUDNN STATUS SUCCESS 2 vs 0 when I set the iteration times smaller from 3k to 500 error was gone It is there any unstable factor in MXNET CUDNN AUTOTUNE DEFAULT just like buckets length is too long,,"piiswrong,szha",2017-02-24 03:02:19,2017-09-29 02:53:37
IS,I want to implement CNN LSTM CTC with C instead of Python but an error arises out,I have implemented model with Python sucessfully but an error arises out in C param symbol json 278523 bytes param 0020 params 38367630 bytes 15 32 30 E OpenSource mxnet0 9 3 mxnet src nnvm legacy json util cc 153 Load ing symbol saved by previous version v0 9 1 Attempting to upgrade 15 32 39 E OpenSource mxnet0 9 3 mxnet nnvm include dmlc logging h 300 15 3 2 39 e opensource mxnet0 9 3 mxnet src operator softmax output inl h 307 Ch eck failed in type i dtype 4 vs 0 This layer requires uniform type E xpected 0 v s given 4 at label Assertion failed pred hnd file E OpenSource mxnet0 9 3 mxnet example image cl assification predict cpp image classification predict cc line 227 here is my code std string json file param symbol json std string param file param 0020 params BufferFile json data json file BufferFile param data param file Parameters int dev type 1 1 cpu 2 gpu int dev id 0 arbitrary mx uint num input nodes 10 1 for feedforward const char input key 10 data label forward l0 init c forward l1 init c forward l0 init h forward l1 init h backward l0 init c backward l1 init c backward l0 init h backward l1 init h const char input keys input key Image size and channels int width 64 int height 256 int channels 1 int num hidden 256 const mx uint input shape indptr 11 0 4 6 8 10 12 14 16 18 20 22 const mx uint input shape data 22 1 static cast mx uint channels static cast mx uint width static cast mx uint height 1 static cast mx uint 32 32 is the sequence length 1 static cast mx uint num hidden 1 static cast mx uint num hidden 1 static cast mx uint num hidden 1 static cast mx uint num hidden 1 static cast mx uint num hidden 1 static cast mx uint num hidden 1 static cast mx uint num hidden 1 static cast mx uint num hidden PredictorHandle pred hnd 0 if json data GetLength 0 param data GetLength 0 return 1 Create Predictor MXPredCreate const char json data GetBuffer const char param data GetBuffer static cast size t param data GetLength dev type dev id num input nodes input keys input shape indptr input shape data pred hnd assert pred hnd How can I fix it,,"Godricly,szha",2017-02-15 07:43:13,2017-09-29 02:53:38
IS,how to create a BucketingModule in c,we can create a predictor to forward in c like this Create Predictor MXPredCreate const char json data GetBuffer const char param data GetBuffer static cast size t param data GetLength dev type dev id num input nodes input keys input shape indptr input shape data pred hnd How can I create a BucketingModule in c just like realizing the model in python model mx mod BucketingModule sym gen sym gen default bucket key data val default bucket key context contexts any one knows about it,,szha,2017-02-28 08:51:31,2017-09-29 02:53:40
IS,can not find ImageRecordIter,Whien I reading the train imagenet py in image classification I can not find mx io ImageRecordIter in file io py and I could not run the train imagenet py I want to konw if the is ImageRecordIter deleted in the new version I must write it myself or there any ways to solve it Thanks Environment info Operating System ubuntun16 04 Compiler GCC 5 4 0 20160609 Package used Python R Scala Julia Python MXNet version 0 9 4 Python version and distribution 2 7 12 Error Message Please paste the full error message including stack trace 09 07 37 src io iter image recordio cc 221 ImageRecordIOParser None use 1 threads for decoding 09 07 37 include dmlc logging h 300 09 07 37 src io local filesys cc 66 LocalFileSystem GetPathInfo None Error No such file or directory Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io15LocalFileSystem11GetPathInfoERKNS0 3URIE 0x411 0x7f1be948b861 bt 1 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io14InputSplitBase17InitInputFileInfoERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x959 0x7f1be94b1229 bt 2 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io14InputSplitBase4InitEPNS0 10FileSystemEPKcm 0x6d 0x7f1be94b288d bt 3 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc10InputSplit6CreateEPKcjjS2 0x46f 0x7f1be947e4ef bt 4 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io19ImageRecordIOParserIfE4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESA ESaISB EE 0x903 0x7f1be90a1b93 bt 5 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io15ImageRecordIterIfE4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESA ESaISB EE 0x97 0x7f1be90a2127 bt 6 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io18ImageNormalizeIter4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x88 0x7f1be909f158 bt 7 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io11BatchLoader4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x2a5 0x7f1be9078835 bt 8 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io14PrefetcherIter4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x104 0x7f1be9078994 bt 9 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so MXDataIterCreateIter 0x511 0x7f1be9421461 Traceback most recent call last File train imagenet py line 40 in module fit fit args sym data get rec iter File home eric mxnet example image classification common fit py line 106 in fit train val data loader args kv File home eric mxnet example image classification common data py line 127 in get rec iter part index rank File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet io py line 667 in creator ctypes byref iter handle File usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 09 07 37 src io local filesys cc 66 LocalFileSystem GetPathInfo None Error No such file or directory Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io15LocalFileSystem11GetPathInfoERKNS0 3URIE 0x411 0x7f1be948b861 bt 1 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io14InputSplitBase17InitInputFileInfoERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x959 0x7f1be94b1229 bt 2 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc2io14InputSplitBase4InitEPNS0 10FileSystemEPKcm 0x6d 0x7f1be94b288d bt 3 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN4dmlc10InputSplit6CreateEPKcjjS2 0x46f 0x7f1be947e4ef bt 4 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io19ImageRecordIOParserIfE4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESA ESaISB EE 0x903 0x7f1be90a1b93 bt 5 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io15ImageRecordIterIfE4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESA ESaISB EE 0x97 0x7f1be90a2127 bt 6 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io18ImageNormalizeIter4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x88 0x7f1be909f158 bt 7 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io11BatchLoader4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x2a5 0x7f1be9078835 bt 8 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so ZN5mxnet2io14PrefetcherIter4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x104 0x7f1be9078994 bt 9 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 linux x86 64 egg mxnet libmxnet so MXDataIterCreateIter 0x511 0x7f1be9421461 terminate called after throwing an instance of istd system error' what Operation not permitted 1 python train imagenet py,,szha,2017-02-28 01:13:08,2017-09-29 02:53:41
IS,reading writing network state in C,How can network state and weights can be saved to and loaded from the file using C API Is there an example There seems to be nothing useful in C wrapper for this,,szha,2017-02-28 20:21:58,2017-09-29 02:53:42
IS,Segmentation fault abrupt crash when training,Hello guys I ran the 'train end2end py' module without modification found within the rcnn example that was provided with the mxnet package Each time I ran the training the training would abruptly stop before the completion of even the first node stating isegmentation fault core dumped ' and with the exact same error I used gdb to traceback the error provided below but I am not familiar with c Could anyone kindly assist me on this error Environment info Operating System Ubuntu 16 04 LTS Compiler cmake Package used Python R Scala Julia Python MXNet version 0 93 If you are using python package please provide Python version and distribution 2 7 13 Error Message Thread 20 python received signal SIGSEGV Segmentation fault Switching to Thread 0x7fff8affd700 LWP 15392 0x00007fffe5a97028 in std Function handler void mxnet RunContext mxnet engine CallbackOnComplete mxnet Engine PushSync std function void mxnet RunContext mxnet Context std vector mxnet engine Var std allocator mxnet engine Var const std vector mxnet engine Var std allocator mxnet engine Var const mxnet FnProperty int char const lambda mxnet RunContext mxnet engine CallbackOnComplete 1 M invoke std Any data const mxnet RunContext mxnet engine CallbackOnComplete functor args 0 args 1 at usr include c 5 functional 1869 1869 M invoke const Any data functor ArgTypes args,,"piiswrong,szha",2017-02-28 20:23:13,2017-09-29 02:53:43
IS,the speed of MXNDArraySyncCopyToCPU,I am trying to run code on cpu using mxnet I got a 256 D mx ndrray but when I try to use asnumpy to transform it to numpy array it takes a long time about 60s It is a bug Here is my code,,"piiswrong,zihaolucky,zihaolucky,zihaolucky,tqchen,Godricly,szha",2016-11-09 01:35:10,2017-09-29 02:53:44
IS,Define a new operator,Now I want define a new operator Simplicity I use the your convolution is operator after doingthis DMLC REGISTER PARAMETER myconvolutionParam MXNET REGISTER OP PROPERTY myconvolution myconvolutionProp add argument I executive program again but there is a fault AttributeError 'module' object has no attribute 'myconvolution' How can I solve this problem Thanks,,szha,2017-03-01 05:06:23,2017-09-29 02:53:46
IS,mxnet notebook example failed,The example for linear regression failed with the error below Is there anyone has idea about that Many thanks ZeroDivisionError Traceback most recent call last ipython input 14 60be205bd7b1 in module 32 optimizer params 'learning rate' 0 00001 'momentum' 0 9 33 num epoch 1000 34 batch end callback mx callback Speedometer batch size 5 35 mxnet callback pyc in call self param 114 if self init 115 if count self frequent 0 116 speed self frequent self batch size time time self tic 117 if param eval metric is not None 118 name value param eval metric get name value,,szha,2017-03-01 10:25:01,2017-09-29 02:53:47
IS,in mxnet io api python module hml how to use the loaded module,in mxnet io api python module I want to use sym set params arg params aux params because i do not want to use the previously defined model to directly predict want to use the loaded sym to predict But it has error in 5192 sym is the the loaded module mod is the defined model by code To load the saved module parameters call the load checkpoint function,,szha,2017-03-01 16:51:52,2017-09-29 02:53:48
IS,Can not get right result using mx io ImageRecordIter following official tutorial,I installed MXNET in Z mxnet so according to official tutorial of image io I set MXNET HOME as MXNET HOME 'Z mxnet' After that I download the caltech101 and unpack it under Z mxnet data caltech data 101 ObjectCategories Then I run two commands following tutorial as os system 'python s tools im2rec py list 1 recursive 1 shuffle 1 test ratio 0 2 data caltech data 101 ObjectCategories' MXNET HOME os system python s tools im2rec py num thread 4 pass through 1 data caltech data 101 ObjectCategories MXNET HOME However the generated caltech lst and caltech rec are two empty files Ignoring this strange thing I continue to run the last command as data iter mx io ImageRecordIter path imgrec data caltech rec the target record file data shape 3 227 227 output data shape An 227x227 region will be cropped from the original image batch size 4 number of samples per batch resize 256 resize the shorter edge to 256 before cropping you can add more augumentation options here use help mx io ImageRecordIter to see all possible choices data iter reset batch data iter next data batch data 0 for i in range 4 plt subplot 1 4 i 1 plt imshow data i asnumpy astype np uint8 transpose 1 2 0 plt show Finally I got an error which shows that Cannot find any files that matches the URI patternz data caltech rec What is the problem Environment info Operating System Windows 10 64 bit Package used Python R Scala Julia Python MXNet version 0 9 3 Python version and distribution 2 7,,szha,2017-03-01 10:40:40,2017-09-29 02:53:49
IS,SoftmaxOutput and the tutorial is confusing,I find SoftmaxOutput very confusing and becomes a pitfall for new mxnet users like me In this tutorial SoftmaxOutput makes its appearance very casually It appears only once in the tutorials and and no longer appears mentioned in other tutorials,,"piiswrong,szha",2016-11-27 17:20:18,2017-09-29 02:53:51
IS,Help when I go on training a language model errors occured,my model,,"szha,szha,szha,szha,szha,szha,szha,szha",2017-09-28 03:07:11,2017-09-29 02:59:42
IS,Speed Question for single machine and distribute training,Below is the speed output For one gpu single machine Node 0 06 49 12 INFO root Epoch 5 Batch 47 Speed 161 80 samples sec Train Perplexity 892 936346 For distribute training three workers Node 0 06 49 12 INFO root Epoch 5 Batch 47 Speed 161 80 samples sec Train Perplexity 892 936346 Node 1 06 49 12 INFO root Epoch 5 Batch 47 Speed 161 80 samples sec Train Perplexity 892 936346 Node 2 06 49 12 INFO root Epoch 5 Batch 47 Speed 161 80 samples sec Train Perplexity 892 936346 Could I say that the speed for single machine is 161 80 samples sec the speed for distribute training is 161 80 3 samples sec,,szha,2017-03-01 07:12:45,2017-09-29 03:40:31
IS,Suggestions on documents,Maybe it could be considered to create online documents for different versions of mxnet and manage a github project for people to edit it So the documents can be updated according to different versions and more people could get interested in mxnet and help contribute,,szha,2017-03-02 10:52:56,2017-09-29 03:40:32
IS,feature request mxnet io CSVIter data shape required parameter,I started using CSVIter and noticed data shape was a required parameter to determine the shape of a given row Is there a sentinel or something I can pass in to flag it as auto compute I could write my own helper function to read the file find the newline and count the commas on the first line but I would prefer if mxnet took care of it,,szha,2017-02-03 16:15:35,2017-09-29 03:40:35
IS,Backward shape inference,piiswrong currently the C API supports forward shape inference using the function MXSymbolInferShape Does the C API provide a similar function for backward shape inference to deduce the shapes of the gradient tensors when given the gradient at the output of the computation graph If not does the NNVM support this and how much work do you think it would take to expose a C API for this,,"Piyush3dB,formath,Piyush3dB,szha",2017-03-02 08:08:34,2017-09-29 03:40:36
IS,core dumped when I try to compile mxnet0 9 3 with nnpack support WHY,import mxnet libdc1394 error Failed to initialize libdc1394 01 20 36 mnt Mxnet docker mxnet93 dmlc core include dmlc logging h 300 01 20 36 src operator nnpack nnpack util h 25 nnp initialize failed status 51 Stack trace returned 48 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 egg mxnet libmxnet so ZN5mxnet2op16NNPACKInitializeC1Ev 0x2fb 0x7fb4ce55a17b bt 1 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 egg mxnet libmxnet so 0x2517d6 0x7fb4ce4ed7d6 bt 2 lib64 ld linux x86 64 so 2 0x1010a 0x7fb4d412c10a bt 3 lib64 ld linux x86 64 so 2 0x101f3 0x7fb4d412c1f3 bt 4 lib64 ld linux x86 64 so 2 0x14c30 0x7fb4d4130c30 bt 5 lib64 ld linux x86 64 so 2 0xffc4 0x7fb4d412bfc4 bt 6 lib64 ld linux x86 64 so 2 0x1437b 0x7fb4d413037b bt 7 lib x86 64 linux gnu libdl so 2 0x102b 0x7fb4d393602b bt 8 lib64 ld linux x86 64 so 2 0xffc4 0x7fb4d412bfc4 bt 9 lib x86 64 linux gnu libdl so 2 0x162d 0x7fb4d393662d bt 10 lib x86 64 linux gnu libdl so 2 dlopen 0x31 0x7fb4d39360c1 bt 11 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x741b 0x7fb4d298341b bt 12 python PyEval EvalFrameEx 0x41d 0x523f6d bt 13 python 0x568b3a bt 14 python 0x4c2604 bt 15 python 0x4d1c5c bt 16 python 0x55f6db bt 17 python PyEval EvalFrameEx 0x98d 0x5244dd bt 18 python PyEval EvalCodeEx 0x2b1 0x555551 bt 19 python PyEval EvalFrameEx 0x1a10 0x525560 bt 20 python PyEval EvalCodeEx 0x2b1 0x555551 bt 21 python PyEval EvalCode 0x32 0x5b41e2 bt 22 python PyImport ExecCodeModuleEx 0xaa 0x5b429a bt 23 python 0x5942af bt 24 python 0x55642f bt 25 python 0x556838 bt 26 python 0x556d9b bt 27 python 0x569cd8 bt 28 python PyEval CallObjectWithKeywords 0x6b 0x4c8c8b bt 29 python PyEval EvalFrameEx 0x2958 0x5264a8 bt 30 python PyEval EvalCodeEx 0x2b1 0x555551 bt 31 python PyEval EvalCode 0x32 0x5b41e2 bt 32 python PyImport ExecCodeModuleEx 0xaa 0x5b429a bt 33 python 0x5942af bt 34 python 0x465804 bt 35 python 0x55642f bt 36 python 0x556838 bt 37 python 0x556c4b bt 38 python 0x569c08 bt 39 python PyEval CallObjectWithKeywords 0x6b 0x4c8c8b bt 40 python PyEval EvalFrameEx 0x2958 0x5264a8 bt 41 python 0x567d14 bt 42 python PyRun InteractiveOneFlags 0x18c 0x465a2d bt 43 python PyRun InteractiveLoopFlags 0xaa 0x465b49 bt 44 python PyRun AnyFileExFlags 0x37 0x4661fe bt 45 python Py Main 0xb5e 0x466d92 bt 46 lib x86 64 linux gnu libc so 6 libc start main 0xf5 0x7fb4d3b5af45 bt 47 python 0x577c2e terminate called after throwing an instance of wouldmlc Error' what 01 20 36 src operator nnpack nnpack util h 25 nnp initialize failed status 51 Stack trace returned 48 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 egg mxnet libmxnet so ZN5mxnet2op16NNPACKInitializeC1Ev 0x2fb 0x7fb4ce55a17b bt 1 usr local lib python2 7 dist packages mxnet 0 9 3 py2 7 egg mxnet libmxnet so 0x2517d6 0x7fb4ce4ed7d6 bt 2 lib64 ld linux x86 64 so 2 0x1010a 0x7fb4d412c10a bt 3 lib64 ld linux x86 64 so 2 0x101f3 0x7fb4d412c1f3 bt 4 lib64 ld linux x86 64 so 2 0x14c30 0x7fb4d4130c30 bt 5 lib64 ld linux x86 64 so 2 0xffc4 0x7fb4d412bfc4 bt 6 lib64 ld linux x86 64 so 2 0x1437b 0x7fb4d413037b bt 7 lib x86 64 linux gnu libdl so 2 0x102b 0x7fb4d393602b bt 8 lib64 ld linux x86 64 so 2 0xffc4 0x7fb4d412bfc4 bt 9 lib x86 64 linux gnu libdl so 2 0x162d 0x7fb4d393662d bt 10 lib x86 64 linux gnu libdl so 2 dlopen 0x31 0x7fb4d39360c1 bt 11 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x741b 0x7fb4d298341b bt 12 python PyEval EvalFrameEx 0x41d 0x523f6d bt 13 python 0x568b3a bt 14 python 0x4c2604 bt 15 python 0x4d1c5c bt 16 python 0x55f6db bt 17 python PyEval EvalFrameEx 0x98d 0x5244dd bt 18 python PyEval EvalCodeEx 0x2b1 0x555551 bt 19 python PyEval EvalFrameEx 0x1a10 0x525560 bt 20 python PyEval EvalCodeEx 0x2b1 0x555551 bt 21 python PyEval EvalCode 0x32 0x5b41e2 bt 22 python PyImport ExecCodeModuleEx 0xaa 0x5b429a bt 23 python 0x5942af bt 24 python 0x55642f bt 25 python 0x556838 bt 26 python 0x556d9b bt 27 python 0x569cd8 bt 28 python PyEval CallObjectWithKeywords 0x6b 0x4c8c8b bt 29 python PyEval EvalFrameEx 0x2958 0x5264a8 bt 30 python PyEval EvalCodeEx 0x2b1 0x555551 bt 31 python PyEval EvalCode 0x32 0x5b41e2 bt 32 python PyImport ExecCodeModuleEx 0xaa 0x5b429a bt 33 python 0x5942af bt 34 python 0x465804 bt 35 python 0x55642f bt 36 python 0x556838 bt 37 python 0x556c4b bt 38 python 0x569c08 bt 39 python PyEval CallObjectWithKeywords 0x6b 0x4c8c8b bt 40 python PyEval EvalFrameEx 0x2958 0x5264a8 bt 41 python 0x567d14 bt 42 python PyRun InteractiveOneFlags 0x18c 0x465a2d bt 43 python PyRun InteractiveLoopFlags 0xaa 0x465b49 bt 44 python PyRun AnyFileExFlags 0x37 0x4661fe bt 45 python Py Main 0xb5e 0x466d92 bt 46 lib x86 64 linux gnu libc so 6 libc start main 0xf5 0x7fb4d3b5af45 bt 47 python 0x577c2e Aborted core dumped,,szha,2017-03-03 01:24:30,2017-09-29 03:40:37
IS,Segfault when calling MXSymbolGetName,In we implemented a wrapper for MXSymbolGetName On MXNet b11d3a2 this segfaults for mx Group nodes as follows cc,,szha,2017-03-03 12:39:54,2017-09-29 03:40:39
IS,Issue in running mxnet on windows,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 Compiler 64 bit Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD latest verion If you are using python package please provide Python version and distribution python 2 7 13 Error Message Please paste the full error message including stack trace C Python27 python exe C Users Avrahim PycharmProjects CDL cdl py Traceback most recent call last File C Users Avrahim PycharmProjects CDL cdl py line 2 in module import mxnet as mx File C Python27 lib site packages mxnet init py line 7 in module from base import MXNetError File C Python27 lib site packages mxnet base py line 45 in module LIB load lib File C Python27 lib site packages mxnet base py line 36 in load lib lib path libinfo find lib path File C Python27 lib site packages mxnet libinfo py line 37 in find lib path 'List of candidates n' str ' n' join dll path RuntimeError Cannot find the files List of candidates C Python27 lib site packages mxnet libmxnet dll C Python27 lib site packages mxnet lib libmxnet dll C Python27 lib site packages mxnet build Release libmxnet dll C Python27 lib site packages mxnet build libmxnet dll C Python27 lib site packages mxnet build Release libmxnet dll C Python27 lib site packages mxnet windows x64 Release libmxnet dll Process finished with exit code 1,,"sxjscience,sxjscience,tornadomeet,szha",2017-03-03 07:21:43,2017-09-29 03:40:40
IS,in mxnet0 9 4 I also find a problem the model load is so so so so so slow the train is also so so so slow the mxnet0 9 3 is faster I test my lstm python,window 10 python27 mxnet0 9 3 mxnet0 9 4,,"tornadomeet,szha",2017-03-05 02:23:56,2017-09-29 03:40:41
IS,I think USE INTEL PATH should be removed config mk or explained more,If USE INTEL PATH is set to the correct path of full MKL version it prevents MxNet from compiling when USE MKL2017 1 and USE BLAS mkl since the full MKL hides MKLML Unless the full MKL have some advantages for MxNet over or plus MKLML I think all of the referrals to the full MKL should be removed Otherwise they confuse both mind and compiler In addition I think the expression of For USE BLAS mkl only was forgotten since MKLML ROOT and USE BLAS mkl works well anymore,,szha,2017-03-05 16:41:29,2017-09-29 03:40:42
IS,AttributeError 'module' object has no attribute 'Proposal',when i run bash script additional deps sh I get the following warning,,szha,2017-02-22 12:50:39,2017-09-29 03:40:43
IS,python gpu test rtc py test fails,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu AWS DL Ubuntu AMI Compiler gcc version 4 8 4 Package used Python R Scala Julia python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD a23608fcd07e348ce4818b5cde4a95feb5068b3b If you are using python package please provide Python version and distribution Python 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python tests python gpu test rtc py 2 3 What have you tried to solve it 1 Have not attempted it yet,,"eric-haibin-lin,szha",2017-03-05 23:24:54,2017-09-29 03:40:45
IS,mod predict gives more columns than expected,I am using MXNet on IRIS dataset which has 4 features and it classifies the flowers as isetosa' haversicolor' 'virginica' My training data has 89 rows My label data is a row vector of 89 columns I encoded the flower names into number 0 1 2 as it seems mx io NDArrayIter does not accept numpy ndarray with string values Then I tried to predict using re mod predict test iter I get a result which has the shape 14 10 Why am I getting 10 columns when I have only 3 labels and how do I map these results to my labels The result of predict is shown below 0 11760861 0 12082944 0 1207106 0 09154381 0 09155304 0 09155869 0 09154817 0 09155204 0 09154914 0 09154641 0 1176083 0 12082954 0 12071151 0 09154379 0 09155323 0 09155825 0 0915481 0 09155164 0 09154923 0 09154641 0 11760829 0 1208293 0 12071083 0 09154385 0 09155313 0 09155875 0 09154838 0 09155186 0 09154932 0 09154625 0 11760861 0 12082901 0 12071037 0 09154388 0 09155303 0 09155875 0 09154829 0 09155209 0 09154959 0 09154641 0 11760896 0 12082863 0 12070955 0 09154405 0 09155299 0 09155875 0 09154839 0 09155225 0 09154996 0 09154646 0 1176089 0 1208287 0 1207095 0 09154407 0 09155297 0 09155882 0 09154844 0 09155232 0 09154989 0 0915464 0 11760896 0 12082864 0 12070941 0 09154408 0 09155297 0 09155882 0 09154844 0 09155234 0 09154993 0 09154642 0 1176088 0 12082874 0 12070983 0 09154399 0 09155302 0 09155872 0 09154837 0 09155215 0 09154984 0 09154641 0 11760852 0 12082904 0 12071032 0 09154394 0 09155304 0 09155876 0 09154835 0 09155209 0 09154959 0 09154631 0 11760963 0 12082832 0 12070873 0 09154428 0 09155257 0 09155893 0 09154856 0 09155177 0 09155051 0 09154671 0 11760966 0 12082829 0 12070868 0 09154429 0 09155258 0 09155892 0 09154858 0 0915518 0 09155052 0 09154672 0 11760949 0 1208282 0 12070852 0 09154446 0 09155259 0 09155893 0 09154854 0 09155205 0 0915506 0 09154666 0 11760952 0 12082817 0 12070853 0 0915444 0 09155261 0 09155891 0 09154853 0 09155206 0 09155057 0 09154668 0 1176096 0 1208283 0 12070892 0 09154423 0 09155267 0 09155882 0 09154859 0 09155172 0 09155044 0 09154676,,szha,2017-03-06 06:58:14,2017-09-29 03:40:46
IS,How can I create rec data from my numpy arrays,I get quite a large data set 100G to train a model Each data item is a 2 256 64 numpy array contains float number Looks much like an image with two channels Discussion about rec are all about converting images to rec Anyone tell me how to convert my numpy array to rec format,,szha,2017-03-02 03:17:27,2017-09-29 03:40:47
IS,CMake Warning that OpenBLAS HOME not used Cannot open include file 'cblas h' No such file or directory,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you I have not easily been able to install MXNet on Windows Environment info Operating System Windows Server 2016 Compiler Visual Studio 2014 Package used Python R Scala Julia Or if installed from source MXNet commit hash git rev parse HEAD 1fd940e944b88249f0b3ae926df4d1061a89ec9b Error Message Please paste the full error message including stack trace mxnet vc14 C Users Administrator mxnet cmake Wno dev DUSE CUDA 0 DUSE CUDNN 0 DBLAS Open DOpenBLAS HOME C ProgramData Miniconda2 envs mxnet vc14 Library DOpenBLAS INCLUDE DIR C ProgramData Miniconda2 envs mxnet vc14 Library include DOpenBLAS LIB C ProgramData Miniconda2 envs mxnet vc14 Library lib DUSE PROFILER 1 DCMAKE BUILD TYPE Release H Bbuild Building for Visual Studio 14 2015 The C compiler identification is MSVC 19 0 24215 1 The CXX compiler identification is MSVC 19 0 24215 1 Check for working C compiler C Program Files x86 Microsoft Visual Studio 14 0 VC bin cl exe Check for working C compiler C Program Files x86 Microsoft Visual Studio 14 0 VC bin cl exe works Detecting C compiler ABI info Checking if C linker supports verbose Checking if C linker supports verbose yes Detecting C compiler ABI info done Check for working CXX compiler C Program Files x86 Microsoft Visual Studio 14 0 VC bin cl exe Check for working CXX compiler C Program Files x86 Microsoft Visual Studio 14 0 VC bin cl exe works Detecting CXX compiler ABI info Checking if CXX linker supports verbose Checking if CXX linker supports verbose yes Detecting CXX compiler ABI info done Detecting CXX compile features Detecting CXX compile features done CMAKE MODULE PATH C Users Administrator mxnet cmake Modules OpenBLAS INCLUDE DIR SYSTEM OpenCV Disabled Try OpenMP C flag openmp Performing Test OpenMP FLAG DETECTED Performing Test OpenMP FLAG DETECTED Success Try OpenMP CXX flag openmp Performing Test OpenMP FLAG DETECTED Performing Test OpenMP FLAG DETECTED Success Found OpenMP openmp Found PythonInterp C ProgramData Miniconda2 envs mxnet vc14 python exe found version 2 7 10 Configuring done Generating done CMake Warning Manually specified variables were not used by the project OpenBLAS HOME msbuild build mxnet sln t Build m 6 c users administrator mxnet mshadow mshadow base h 136 fatal error C 1083 Cannot open include file 'cblas h' No such file or directory com piling source file C Users Administrator mxnet src c api c api ndarray c c C Users Administrator mxnet mxnet vcxproj 6 c users administrator mxnet mshadow mshadow base h 136 fatal error C 1083 Cannot open include file 'cblas h' No such file or directory com piling source file C Users Administrator mxnet src c api c api executor cc C Users Administrator mxnet mxnet vcxproj Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Followed for the most part 2 Set use opencv off in CMakeList txt mxnet option USE OPENCV Build with OpenCV support OFF 3 Run CMake cmake Wno dev DUSE CUDA 0 DUSE CUDNN 0 DBLAS Open DOpenBLAS HOME C ProgramData Miniconda2 envs mxnet vc14 Library DOpenBLAS INCLUDE DIR C ProgramData Miniconda2 envs mxnet vc14 Library include DOpenBLAS LIB C ProgramData Miniconda2 envs mxnet vc14 Library lib DUSE PROFILER 1 DCMAKE BUILD TYPE Release H Bbuild 4 Run build msbuild build mxnet sln t Build m What have you tried to solve it 1 Explicitly add include directory in CMakeLists txt include directories C ProgramData Miniconda envs mxnet vc14 Library include,,"lxn2,lxn2,yajiedesign,lxn2,lxn2,yajiedesign,szha",2017-03-01 23:36:57,2017-09-29 03:40:49
IS,cpu number is not the more the better,As a convex function 4 is the best is not it,,"sbodenstein,szha",2017-02-08 02:04:53,2017-09-29 03:40:50
IS,Repeating results while training,Dear all I was running a image regression problem based on these code train mx io NDArrayIter X train y train batch size shuffle True label name 'lin reg label' val mx io NDArrayIter X test y test batch size Lenet Symbol Configuration data mx sym Variable wouldata' net label mx sym Variable isoftmax label' first conv conv1 mx symbol Convolution data data kernel 3 3 num filter 20 tanh1 mx symbol Activation data conv1 act type tanh pool1 mx symbol Pooling data tanh1 pool type max kernel 2 2 stride 2 2 second conv conv2 mx symbol Convolution data pool1 kernel 5 5 num filter 50 tanh2 mx symbol Activation data conv2 act type tanh pool2 mx symbol Pooling data tanh2 pool type max kernel 2 2 stride 2 2 first fullc flatten mx symbol Flatten data pool1 fc1 mx symbol FullyConnected data flatten num hidden 100 tanh3 mx symbol Activation data fc1 act type tanh second fullc fc2 mx symbol FullyConnected data tanh3 num hidden 1 loss lenet mx symbol SoftmaxOutput data fc2 name isoftmax' loss mx symbol MAERegressionOutput data fc2 label net label name 'loss' net in mx symbol Variable wouldata' net in flat mx symbol Flatten data net in net label mx symbol Variable isoftmax label' fc1 mx symbol FullyConnected data net in flat name 'fc1' num hidden 512 fc1 act mx symbol Activation data fc1 name 'fc1 act' act type isigmoid' fc2 mx symbol FullyConnected data fc1 act name 'fc2' num hidden 256 fc2 act mx symbol Activation data fc2 name 'fc2 act' act type isigmoid' fc3 mx symbol FullyConnected data fc2 act name 'fc3' num hidden 256 net out mx symbol FullyConnected data fc3 name 'net out' num hidden 1 loss mx symbol LogisticRegressionOutput data fc3 label net label name 'loss' logging basicConfig level logging INFO logger logging getLogger name model mx module Module symbol lenet data names wouldata' label names isoftmax label' logger logger model bind data shapes train provide data label shapes train provide label model init params initializer mx init Xavier rnd type 'gaussian' factor type in magnitude 2 model fit train data train eval data val eval metric 'rmse' optimizer isgd' optimizer params 'learning rate' 0 001 'momentum' 0 5 num epoch train epoch batch end callback mx callback Speedometer batch size frequent 100 The problem is that the result in Epoch 0 and Epoch 1 are the same What could cause this problem Many thanks WARNING main Already binded ignoring bind INFO root Epoch 0 Batch 100 Speed 853 11 samples sec Train rmse 0 954174 INFO root Epoch 0 Batch 200 Speed 918 76 samples sec Train rmse 0 953405 INFO root Epoch 0 Batch 300 Speed 904 48 samples sec Train rmse 0 954708 INFO root Epoch 0 Batch 400 Speed 900 66 samples sec Train rmse 0 951832 INFO root Epoch 0 Batch 500 Speed 875 52 samples sec Train rmse 0 954197 INFO root Epoch 0 Batch 600 Speed 822 84 samples sec Train rmse 0 953868 INFO root Epoch 0 Batch 700 Speed 868 63 samples sec Train rmse 0 953722 INFO root Epoch 0 Batch 800 Speed 882 28 samples sec Train rmse 0 954360 INFO root Epoch 0 Batch 900 Speed 871 47 samples sec Train rmse 0 953438 INFO root Epoch 0 Batch 1000 Speed 888 41 samples sec Train rmse 0 952074 INFO root Epoch 0 Batch 1100 Speed 876 72 samples sec Train rmse 0 954426 INFO root Epoch 0 Batch 1200 Speed 872 66 samples sec Train rmse 0 954826 INFO root Epoch 0 Batch 1300 Speed 855 40 samples sec Train rmse 0 952572 INFO root Epoch 0 Batch 1400 Speed 823 90 samples sec Train rmse 0 956657 INFO root Epoch 0 Batch 1500 Speed 843 67 samples sec Train rmse 0 952269 INFO root Epoch 0 Batch 1600 Speed 853 12 samples sec Train rmse 0 952531 INFO root Epoch 0 Batch 1700 Speed 864 87 samples sec Train rmse 0 953331 INFO root Epoch 0 Batch 1800 Speed 861 15 samples sec Train rmse 0 952314 INFO root Epoch 0 Batch 1900 Speed 857 00 samples sec Train rmse 0 954161 INFO root Epoch 0 Batch 2000 Speed 863 47 samples sec Train rmse 0 953831 INFO root Epoch 0 Batch 2100 Speed 842 78 samples sec Train rmse 0 954865 INFO root Epoch 0 Batch 2200 Speed 856 31 samples sec Train rmse 0 955545 INFO root Epoch 0 Batch 2300 Speed 853 57 samples sec Train rmse 0 951860 INFO root Epoch 0 Batch 2400 Speed 853 34 samples sec Train rmse 0 952450 INFO main Epoch 0 Train rmse 0 953217 INFO main Epoch 0 Time cost 358 616 INFO main Epoch 0 Validation rmse 0 907619 INFO root Epoch 1 Batch 100 Speed 885 21 samples sec Train rmse 0 954174 INFO root Epoch 1 Batch 200 Speed 855 40 samples sec Train rmse 0 953405 INFO root Epoch 1 Batch 300 Speed 835 95 samples sec Train rmse 0 954708 INFO root Epoch 1 Batch 400 Speed 847 92 samples sec Train rmse 0 951832 INFO root Epoch 1 Batch 500 Speed 857 69 samples sec Train rmse 0 954197 INFO root Epoch 1 Batch 600 Speed 856 08 samples sec Train rmse 0 953868 INFO root Epoch 1 Batch 700 Speed 843 45 samples sec Train rmse 0 953722 INFO root Epoch 1 Batch 800 Speed 852 43 samples sec Train rmse 0 954360 INFO root Epoch 1 Batch 900 Speed 848 82 samples sec Train rmse 0 953438 INFO root Epoch 1 Batch 1000 Speed 836 39 samples sec Train rmse 0 952074 INFO root Epoch 1 Batch 1100 Speed 845 90 samples sec Train rmse 0 954426 INFO root Epoch 1 Batch 1200 Speed 824 33 samples sec Train rmse 0 954826 INFO root Epoch 1 Batch 1300 Speed 845 67 samples sec Train rmse 0 952572 INFO root Epoch 1 Batch 1400 Speed 845 23 samples sec Train rmse 0 956657 INFO root Epoch 1 Batch 1500 Speed 852 43 samples sec Train rmse 0 952269 INFO root Epoch 1 Batch 1600 Speed 845 90 samples sec Train rmse 0 952531 INFO root Epoch 1 Batch 1700 Speed 812 19 samples sec Train rmse 0 953331 INFO root Epoch 1 Batch 1800 Speed 868 63 samples sec Train rmse 0 952314 INFO root Epoch 1 Batch 1900 Speed 854 48 samples sec Train rmse 0 954161 INFO root Epoch 1 Batch 2000 Speed 814 05 samples sec Train rmse 0 953831 INFO root Epoch 1 Batch 2100 Speed 881 55 samples sec Train rmse 0 954865 INFO root Epoch 1 Batch 2200 Speed 899 14 samples sec Train rmse 0 955545 INFO root Epoch 1 Batch 2300 Speed 888 65 samples sec Train rmse 0 951860 INFO root Epoch 1 Batch 2400 Speed 845 23 samples sec Train rmse 0 952450 INFO main Epoch 1 Train rmse 0 953217 INFO main Epoch 1 Time cost 364 212 INFO main Epoch 1 Validation rmse 0 907619,,szha,2017-03-06 14:18:24,2017-09-29 03:40:51
IS,In mxnet or offical mxnet imagenet pretrained model When processing images the data layout is batchSize x channel x width x height or batchSize x channel x height xwidth,in tutorials predict imagenet I see the predict imagenet the pre trained model forward image shap is as above channel height width but I know in the mxnet pre trained model for example vgg imagenet pretrained model and the image shape is batch size channel width height but this is and in the offical mxnet readme When processing images MXNet assumes the data layout is batchSize x channel x width x height,,szha,2017-03-06 15:53:53,2017-09-29 03:40:52
IS,Good training and validation accuracy poor testing accuracy,Hello During training neural network for classification 3 classes I encountered very strange behavior of training validation and testing errors All data is stored in single csv file and is splitted before every training randomly into 3 parts training valiadation used for early stopping testing in ratio 70 20 10 During training training accuracy goes up to 99 99 and validation accuracy to 86 My problem begins when trained model is used on testing data because it has only 44 accuracy When I tried train different model xgboost validation and testing errors were roughly the same as the should be in my opinion Any ideas what could cause this behavior EDIT Dataset has 288 1 attributes and 1 6M rows Code Operating System Windows 10 x64 Compiler x64 Package used Python R Scala Julia R MXNet version 0 9 4 R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 w64 mingw32 x64 64 bit Running under Windows 8 x64 build 9200 locale 1 LC COLLATE Czech Czech Republic 1250 LC CTYPE Czech Czech Republic 1250 3 LC MONETARY Czech Czech Republic 1250 LC NUMERIC C 5 LC TIME Czech Czech Republic 1250 attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 caret 6 0 73 lattice 0 20 34 mxnet 0 9 4 dplyr 0 5 0 5 data table 1 10 4 ggplot2 2 2 1 loaded via a namespace and not attached 1 Rcpp 0 12 9 nloptr 1 0 4 plyr 1 8 4 iterators 1 0 8 5 tools 3 3 2 digest 0 6 12 lme4 1 1 12 jsonlite 1 3 9 tibble 1 2 gtable 0 2 0 nlme 3 1 128 mgcv 1 8 15 13 Matrix 1 2 7 1 foreach 1 4 3 DBI 0 5 1 parallel 3 3 2 17 SparseM 1 74 stringr 1 2 0 MatrixModels 0 4 1 htmlwidgets 0 8 21 stats4 3 3 2 grid 3 3 2 nnet 7 3 12 R6 2 2 0 25 minqa 1 2 4 reshape2 1 4 2 car 2 1 4 magrittr 1 5 29 scales 0 4 1 codetools 0 2 15 ModelMetrics 1 1 0 htmltools 0 3 5 33 MASS 7 3 45 splines 3 3 2 assertthat 0 1 pbkrtest 0 4 6 37 colorspace 1 3 2 quantreg 5 29 stringi 1 1 2 visNetwork 1 0 3 41 lazyeval 0 2 0 munsell 0 4 3,,"jeremiedb,szha",2017-03-03 14:41:15,2017-09-29 03:40:53
IS,How to use sequential module for a frozen part and trainable part,Hi I am new to Mxnet and I want to use sequential module for this specific task Suppose I have a feature extraction network which is fixed and takes no gradient In other words I use this as grad req 'null' and is train False The second network is trainable with several layers I saw there is a argument fixed param names Does it mean the parameters only get lr 0 If yes it will be not very sufficient because there is no need for the frozen part to calculate gradients and so on Any suggestions really appreciate it,,szha,2017-03-07 00:00:19,2017-09-29 03:40:55
IS,the module which save BucketingModule from different buckets can load but can not use,AttributeError 'BucketingModule' object has no attribute 'tojson' TypeError bind got an unexpected keyword argument wouldata shapes' I do not want to redefine the module I would like to use the saved model to predict I want to separate model training and model prediction Because the model is trained with Python I would like to model predictions using C or other languages It is not easy to redefine the model with C or C to call the loaded parameters,,"Godricly,Godricly,szha",2017-03-01 02:41:01,2017-09-29 03:40:56
IS,mxnet example training cannot get expected result after 0 5 0,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux idc01 rank gpu 01 2 6 32 431 el6 x86 64 1 SMP Sun Nov 10 22 19 54 EST 2013 x86 64 x86 64 x86 64 GNU Linux Compiler gcc 4 8 5 Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 10 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error I am trying the new mxnet to do the inception bn full test I was tried successful in mxnet 0 5 0 but I found still not work in mxnet 0 9 4 even try to set fix gamma false And I found the train acc is very strange which vary unstable when doing test in 0 5 0 it increase very stable Also I also doing a simple test in mnist I turn the mnist data to images then processing then to rec using alexnet to training the network It does not work at all So I guess is there something wrong with the code in train imagenet py or etc or sth bug exist,,"piiswrong,szha",2017-03-02 10:19:48,2017-09-29 03:40:57
IS,Unknown initialization pattern when seting the initializer for a specific variable,I tried to create a forgetting gate in my network thus I created a symbol However I got the error as below ValueError Unknown initialization pattern for unit13 fgt Default initialization is now limited to weight bias gamma 1 0 and beta 0 0 Please use mx sym Variable init mx init to set initialization pattern I do not really understand the error information and try to solve this by changing the initializer but it does not work,,"sxjscience,sxjscience,szha",2017-03-06 09:54:56,2017-09-29 03:40:58
IS,we need an mxnet symbol run bind forward,We need an mxnet symbol run function This would combine bind and forward Say I want to run import mxnet symbol as S A real some nonsense B real some other nonsense a S Variable b S Variable c S dot a b Now if I want to execute c on A Real and B real I need to run d c bind e d forward Instead I we ought to be able to run f c run This would combine the functionality of bind and forward It makes a lot of sense especially in the interactive interpreter where someone is trying out lots of ideas and needs to be able to see what pops out of various expressions without writing too much code This makes sense in the common case where someone has no need to keep d lying around For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"zackchase,piiswrong,antinucleon,szha",2017-03-07 02:42:38,2017-09-29 03:40:59
IS,How to set a constant array in a symbol ndarray seems not to work mxnet version is 0 9 4,a simple sym code like this import mxnet as mx data mx sym Variable name wouldata' shape 4 3 20 20 b sub mx sym broadcast sub lhs data rhs mx nd array 1 2 3 reshape 1 3 1 1 arg shapes out shapes aux shapes b sub infer shape print type out shapes type arg shapes type aux shapes it can run but all shapes are NoneType but when i change b sub to b sub mx sym broadcast sub lhs data rhs mx sym ones 1 3 1 1 everything works fine Is this normal,,szha,2017-03-04 14:26:52,2017-09-29 03:41:01
IS,API Discuss The p argument in symbol RNN is quite confusing,It is ok to put p in dropout like sym dropout p 0 5 but it is quite confusing to put an alone argument p in the context of RNN sym RNN p 0 5,,"ZihengJiang,szha",2017-03-07 05:28:31,2017-09-29 03:41:02
IS,Why no mxnet symbol relu or sigmoid,I have noticed that in mxnet symbol we have defined mxnet symbol tanh mnet symbol LeakyReLU However we do not have mxnet symbol ReLU or mxnet symbol sigmoid defined These functions are defined but only within the I think this is bad for a couple reasons This appears disorganized Like functions should be grouped in like folders This makes our library overly married to neural network notation We could imagine other times when someone might want to access the sigmoid or hinge functions but would not think of them as activation functions I think both ReLU better to spell relu and sigmoid should both be available in mxnet symbol For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"zackchase,piiswrong,mli,ZihengJiang,szha",2017-03-06 18:59:24,2017-09-29 03:41:03
IS,API Discuss Support for partial execution,As you can see in the code snippet above c and d is enough to calculate the result of e which means we only need to execute a part of graph But this usage is not supported in MXNet yet IMHO an API for partial execution maybe not like above would be necessary also helpful Image that you want to debug a part of model,,"ZihengJiang,piiswrong,ZihengJiang,piiswrong,szha",2017-03-07 05:23:30,2017-09-29 03:41:04
IS,Backward raise gpu error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu15 10 Compiler gcc 4 9 x Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source yes MXNet commit hash git rev parse HEAD be38c5b84030a63d0ab51f19737f99a75a7feb23 If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Error happens in LSTMCell when modal does backward the method forward works well by the way it works well in CPU except GPU Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Debug trace to exector backward None 2 3,,szha,2017-03-07 13:52:19,2017-09-29 03:41:05
IS,Unable to install mxnet with matlab on win10,This should probably be read more as an issue with the documentation rather than a technical obstacle While clearly not being sufficiently computer savvy to make my way through this installation I do consider myself quite technically minded and I am probably representative of a pretty large potential user group Matlab on Windows is a pretty common setup after all The pre built packages do not contain the matlab component If it is possible to combine the matlab component from a new version with a pre built old version then that is not mentioned anywhere and it seems pretty unlikely anyway That leaves building mxnet on windows by hand Going to the guide building on windows an immediate problem seems to be that it was written a while ago I have been unable to find a version of VS community 2013 older than the Visual C Compiler Nov 2013 CTP When the installed version is newer than the patch the patch wo not install Presumably this means that the patch is not needed For the record I have used VS 2013 downloaded from That link in turn was taken from Finally I am instructed to use CMake to create a Visual Studio solution in build There is no build directory in my mxnet 0 9 3 folder What am I missing Operating System Windows 10 Compiler Visual Studio 2013 Community Edition Patch 4 Package used Python R Scala Julia Matlab MXNet version 0 9 3,,szha,2017-03-07 15:24:58,2017-09-29 03:41:21
IS,well I have solved this issue but the crude contributor locked the issue,8065,,,2017-09-29 04:00:33,2017-09-29 04:04:22
IS,Be respectful to other people original title well I have solved this issue but the crude contributor locked the issue,8065,,szha,2017-09-29 04:00:35,2017-09-29 06:09:42
IS,How to run autoencoder,Environment info mac os X EI CapItan Hi I am a fresher on mxnet and I'm sorry that my english is not good I installed the mxnet on my mac and I can run example image classification Now I want to run autoencoder but I got a error message ImportError No module named sklearn datasets When I use pip install sklearn the system told me to installed it like this Requirement already satisfied scikit learn in usr local lib python2 7 site packages from sklearn I have set the path to environment but it still can not work who can tell me how to resolve this problem thank you very much,,yajiedesign,2017-03-07 15:49:42,2017-09-29 06:34:01
IS,NDArrayIter seems to crash when taking large numpy arrays as features labels,I had trouble loading my data sets as numpy arrays they were approx 39 000 00 by 340 to pass into my NDArrayIter function and initially thought it may have been some issues in my network network was not sized correctly but then realized the issue was with NDArrayIter First ticket filed for it was Debugged and failed method is at I have a feeling there must be some type of data overflow and the copy does not seem to work on c side,,"Azure-Vani,yajiedesign",2017-02-21 00:43:42,2017-09-29 06:34:10
IS,Scala Symbol is not serializable,Environment info Operating System debian testing Compiler Package used Python R Scala Julia Scala 2 11 Spark 2 0 1 MXNet version Or if installed from source from master 2017 01 16 MXNet commit hash git rev parse HEAD Error Message 2017 01 17 16 20 38 890 ERROR executor Executor Logging scala logError 91 Exception in task 0 0 in stage 1 0 TID 1 java io NotSerializableException ml dmlc mxnet Symbol at java io ObjectOutputStream writeObject0 ObjectOutputStream java 1184 at java io ObjectOutputStream defaultWriteFields ObjectOutputStream java 1548 at java io ObjectOutputStream writeSerialData ObjectOutputStream java 1509 at java io ObjectOutputStream writeOrdinaryObject ObjectOutputStream java 1432 at java io ObjectOutputStream writeObject0 ObjectOutputStream java 1178 at java io ObjectOutputStream writeObject ObjectOutputStream java 348 at ml dmlc mxnet JavaSerializer serialize Serializer scala 48 at ml dmlc mxnet KVStore setOptimizer KVStore scala 187 at ml dmlc mxnet Model anonfun trainMultiDevice 4 apply Model scala 259 at ml dmlc mxnet Model anonfun trainMultiDevice 4 apply Model scala 259 at scala Option foreach Option scala 257 at ml dmlc mxnet Model trainMultiDevice Model scala 259 at ml dmlc mxnet FeedForward fit FeedForward scala 347 at ml dmlc mxnet FeedForward fit FeedForward scala 286 at ml dmlc mxnet FeedForward fit FeedForward scala 294 at ml dmlc mxnet FeedForward fit FeedForward scala 300 at ml dmlc mxnet spark MXNet anonfun 1 apply MXNet scala 168 at ml dmlc mxnet spark MXNet anonfun 1 apply MXNet scala 127 at org apache spark rdd RDD anonfun mapPartitions 1 anonfun apply 23 apply RDD scala 785 at org apache spark rdd RDD anonfun mapPartitions 1 anonfun apply 23 apply RDD scala 785 at org apache spark rdd MapPartitionsRDD compute MapPartitionsRDD scala 38 at org apache spark rdd RDD computeOrReadCheckpoint RDD scala 319 at org apache spark rdd RDD anonfun 8 apply RDD scala 332 at org apache spark rdd RDD anonfun 8 apply RDD scala 330 at org apache spark storage BlockManager anonfun doPutIterator 1 apply BlockManager scala 935 at org apache spark storage BlockManager anonfun doPutIterator 1 apply BlockManager scala 926 at org apache spark storage BlockManager doPut BlockManager scala 866 at org apache spark storage BlockManager doPutIterator BlockManager scala 926 at org apache spark storage BlockManager getOrElseUpdate BlockManager scala 670 at org apache spark rdd RDD getOrCompute RDD scala 330 at org apache spark rdd RDD iterator RDD scala 281 at org apache spark scheduler ResultTask runTask ResultTask scala 70 at org apache spark scheduler Task run Task scala 86 at org apache spark executor Executor TaskRunner run Executor scala 274 at java util concurrent ThreadPoolExecutor runWorker ThreadPoolExecutor java 1142 at java util concurrent ThreadPoolExecutor Worker run ThreadPoolExecutor java 617 at java lang Thread run Thread java 745 Minimum reproducible example Steps to reproduce run mxnet scala package spark bin run mnist example sh example What have you tried to solve it it seems that the commit Scala Bucketing API Support which adds Symbol to Optimizer causes the problem 1 since symbol is not used right now just remove it from Optimizer temporarily 2 or convert symbol to json in Optimizer,,"yzhliu,Roshrini,qiyuangong,yajiedesign",2017-01-17 09:18:23,2017-09-29 06:34:13
IS,Loss and output layers in MXNet,I'm confused about the design of output symbols in MXNet Consider the SoftmaxOutput symbol as an example When creating the SoftmaxOutput symbol it takes only the previous layer as input Then on the forward pass is simply applies softmax and outputs e x i sum j e x j which make perfect sense But on the backward pass it computes the gradient with respect the log loss using both the previous layer and the label as input This is confusing because 1 the forward backward pass of a node in a computational graph should typically be based on the same computation 2 It is not clear when how the layer get wired up to the labels Does the model module automatically pass the labels to the last layer of a computational graph 3 With this design the computational graph never actually outputs the loss If I want to access the loss how is one supposed to do it 4 The MakeLoss symbol does not work this way It causes the graph to output the actual loss Thank you for explaining the design,,"piiswrong,fhieber,yajiedesign,szha",2017-02-22 22:02:44,2017-09-29 06:34:17
IS,Error with VGG model loading,Dear all The vgg models in the data center cannot be loaded correctly by current mxnet The error messages are as below MXNetError Traceback most recent call last ipython input 8 e2cb7ab98622 in module 1 import mxnet as mx 2 sym arg params aux params mx model load checkpoint 'vgg16' 0 virtualenvs HDR local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet model pyc in load checkpoint prefix epoch 371 parameters will be loaded from prefix epoch params 372 373 symbol sym load ' s symbol json' prefix 374 save dict nd load ' s 04d params' prefix epoch local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet symbol pyc in load fname 1067 raise TypeError 'fname need to be string' 1068 handle SymbolHandle 1069 check call LIB MXSymbolCreateFromFile c str fname ctypes byref handle 1070 return Symbol handle 1071 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 if sys version info 0 3 MXNetError 20 05 12 src io local filesys cc 154 Check failed allow null LocalFileSystem fail to open vgg16 symbol json,,yajiedesign,2017-02-13 20:14:45,2017-09-29 06:34:20
IS,How to generate synset txt files,We can use im2rec according to the lst file to generate rec files But how could we generate synset txt files And if we use NDArrayIter how to find the relation between the sequence of the prediction and label file Many thanks,,"Piyush3dB,Piyush3dB,statist-bhfz,yajiedesign",2017-02-27 17:29:55,2017-09-29 06:34:23
IS,Using different batch sizes for training and development sets,I followed the tutorial for creating a custom data iterator and it gives an unintuitive error when I use different batch sizes for training and development data Specifically the attached code runs fine when I set both trn batch size and dev batch size to the same value e g 128 on lines 62 63 however it gives an error when they are set to different values Is there way of getting around this without zero padding to the same batch size Thanks test py zip,,"Piyush3dB,yajiedesign",2017-03-07 23:00:36,2017-09-29 06:34:27
IS,MXNET metric function,I use MXNET for semantic image segmentation with binary classes Since my images primary represents background I am trying to change the sum metric function For this purpose I use mxnet metric F1 However I problems running mxnet metric F1 mod fit dataiter eval metric mx metric F1 here I apply binary classification metric batch end callback mx callback Speedometer dataiter batch size 1 epoch end callback mx callback do checkpoint to model kvstore args kvstore optimizer isgd' optimizer params optimizer params initializer initializer arg params net args aux params net auxs allow missing args from epoch 0 begin epoch args from epoch num epoch args stop epoch Unfortunately the response is Traceback most recent call last File issegm voc py line 932 in module 2017 03 09 07 21 11 655 Host Labels 169 train impl args model specs logger File issegm voc py line 675 in train impl num epoch args stop epoch File mxnet python mxnet module base module py line 412 in fit self update metric eval metric data batch label File mxnet python mxnet module module py line 556 in update metric self exec group update metric eval metric labels File mxnet python mxnet module executor group py line 470 in update metric eval metric update labels slice texec outputs File mxnet python mxnet metric py line 200 in update if y pred 1 and y true 1 ValueError The truth value of an array with more than one element is ambiguous Use a any or a all I do not understand why this is not working The pred and label values which are as I can see quite fixed and not modifiable,,yajiedesign,2017-03-09 06:15:59,2017-09-29 06:34:30
IS,Train and validation accuracy become always 0 00000,Hi I tried to fit own images with simple CNN today I was git cloned latest version of MXNet But accuracy of train and validation data are always zero INFO root Epoch 0 Batch 10 Speed 30 93 samples sec Train accuracy 0 000000 INFO root Epoch 0 Batch 20 Speed 28 97 samples sec Train accuracy 0 000000 INFO root Epoch 0 Batch 30 Speed 28 99 samples sec Train accuracy 0 000000 I think this issue come from my misunderstandings about MXNet I can only know Spped and Train accuracy during training But I want a lot of information to debug this How can I get more detailed information during training A part of code of input data is val data iter mx io ImageRecordIter path imgrec 'imgs validation DDD rec' data shape data shape batch size batch size shuffle False rand crop False train data iter mx io ImageRecordIter path imgrec 'imgs train DDD rec' data shape data shape batch size batch size shuffle False rand crop False ' My CNN is data mx symbol Variable wouldata' conv1 mx sym Convolution data data kernel 3 3 num filter 32 act1 mx sym Activation data conv1 act type relu pool1 mx sym Pooling data act1 pool type max kernel 2 2 stride 2 2 conv2 mx sym Convolution data pool1 kernel 3 3 num filter 64 act2 mx sym Activation data conv2 act type relu pool2 mx sym Pooling data act2 pool type max kernel 2 2 stride 2 2 flatten mx sym Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 512 act3 mx sym Activation data fc1 act type relu fc2 mx sym FullyConnected data act3 num hidden 4 net mx sym SoftmaxOutput data fc2 name isoftmax' Now do fitting mod mx mod Module net mod fit train data iter eval data val data iter optimizer params 'learning rate' 0 01 'momentum' 0 9 num epoch n epoch epoch end callback checkpoint batch end callback batch end callback,,yajiedesign,2017-02-11 10:57:49,2017-09-29 06:34:33
IS,How to train and test alternately,Hello first of all thanks for this wonderful project I have an MXNET model that I am using to train and test alternately In my model I have a set of outputs and a set of losses one of the losses the top level loss is a function of others as in the dummy code snippet below During training I want to log the set of losses and during testing I want to retrieve the set of outputs I only want to optimize against the top level loss the intermediate ones are for logging purpose I think I need to use a mxnet symbol Group to collect all those outputs The trouble is in my understanding forward backward requires all symbols in the group to be losses so I am having to wrap my auxiliary outputs by doing for example slice0 out mx symbol MakeLoss mx symbol BlockGrad slice0 This looks slightly unelegant so I am wondering if you could share best practices here in case I missed the correct way of doing it Thanks in advance,,yajiedesign,2017-03-09 14:19:36,2017-09-29 06:34:36
IS,where is optimizer h,In src optimizer sgd inl h the file is included as mxnet optimizer h But it does not exist Can it be deleted sgd inl h is not used by other files,,yajiedesign,2017-03-08 08:56:14,2017-09-29 06:34:40
IS,DataIter does not have provide data and provide label,I see many comments mentioning that DataIter gives provide data and provide label which do not exist in the current version L89 These comments should be modified to avoid confusion,,yajiedesign,2017-03-10 19:14:40,2017-09-29 06:34:43
IS,anyone who can tell or give me the symbol file of model vgg like this,when I dowload the params from and who can give me the symbol file like this def get symbol num classes kwargs data mx symbol Variable name data conv1 1 mx symbol Convolution data data kernel 3 3 pad 1 1 num filter 64 workspace 2048 name conv1 1 relu1 1 mx symbol Activation data conv1 1 act type relu name relu1 1 conv1 2 mx symbol Convolution data relu1 1 kernel 3 3 pad 1 1 num filter 64 workspace 2048 name conv1 2 relu1 2 mx symbol Activation data conv1 2 act type relu name relu1 2 pool1 mx symbol Pooling data relu1 2 pool type max kernel 2 2 stride 2 2 name pool1 group 2 conv2 1 mx symbol Convolution data pool1 kernel 3 3 pad 1 1 num filter 128 workspace 2048 name conv2 1 relu2 1 mx symbol Activation data conv2 1 act type relu name relu2 1 conv2 2 mx symbol Convolution data relu2 1 kernel 3 3 pad 1 1 num filter 128 workspace 2048 name conv2 2 relu2 2 mx symbol Activation data conv2 2 act type relu name relu2 2 pool2 mx symbol Pooling data relu2 2 pool type max kernel 2 2 stride 2 2 name pool2 group 3 conv3 1 mx symbol Convolution data pool2 kernel 3 3 pad 1 1 num filter 256 workspace 2048 name conv3 1 relu3 1 mx symbol Activation data conv3 1 act type relu name relu3 1 conv3 2 mx symbol Convolution data relu3 1 kernel 3 3 pad 1 1 num filter 256 workspace 2048 name conv3 2 relu3 2 mx symbol Activation data conv3 2 act type relu name relu3 2 conv3 3 mx symbol Convolution data relu3 2 kernel 3 3 pad 1 1 num filter 256 workspace 2048 name conv3 3 relu3 3 mx symbol Activation data conv3 3 act type relu name relu3 3 pool3 mx symbol Pooling data relu3 3 pool type max kernel 2 2 stride 2 2 name pool3 group 4 conv4 1 mx symbol Convolution data pool3 kernel 3 3 pad 1 1 num filter 512 workspace 2048 name conv4 1 relu4 1 mx symbol Activation data conv4 1 act type relu name relu4 1 conv4 2 mx symbol Convolution data relu4 1 kernel 3 3 pad 1 1 num filter 512 workspace 2048 name conv4 2 relu4 2 mx symbol Activation data conv4 2 act type relu name relu4 2 conv4 3 mx symbol Convolution data relu4 2 kernel 3 3 pad 1 1 num filter 512 workspace 2048 name conv4 3 relu4 3 mx symbol Activation data conv4 3 act type relu name relu4 3 pool4 mx symbol Pooling data relu4 3 pool type max kernel 2 2 stride 2 2 name pool4 group 5 conv5 1 mx symbol Convolution data pool4 kernel 3 3 pad 1 1 num filter 512 workspace 2048 name conv5 1 relu5 1 mx symbol Activation data conv5 1 act type relu name relu5 1 conv5 2 mx symbol Convolution data relu5 1 kernel 3 3 pad 1 1 num filter 512 workspace 2048 name conv5 2 relu5 2 mx symbol Activation data conv5 2 act type relu name relu5 2 conv5 3 mx symbol Convolution data relu5 2 kernel 3 3 pad 1 1 num filter 512 workspace 2048 name conv5 3 relu5 3 mx symbol Activation data conv5 3 act type relu name relu5 3 group 6 flatten mx symbol Flatten data relu5 3 name flatten fc6 mx symbol FullyConnected data flatten num hidden 4096 name fc6 relu6 mx symbol Activation data fc6 act type relu name relu6 drop6 mx symbol Dropout data relu6 p 0 5 name drop6 group 7 fc7 mx symbol FullyConnected data drop6 num hidden 4096 name fc7 relu7 mx symbol Activation data fc7 act type relu name relu7 drop7 mx symbol Dropout data relu7 p 0 5 name drop7 output fc8 mx symbol FullyConnected data drop7 num hidden num classes name fc8 softmax mx symbol SoftmaxOutput data fc8 name isoftmax' return softmax Because my file is sth different,,yajiedesign,2017-03-11 08:30:24,2017-09-29 06:34:46
IS,pooled storage manager h 80 cudaMalloc failed out of memory,I set batch 1 and out of memory,,yajiedesign,2017-03-11 12:04:02,2017-09-29 06:34:50
IS,cudnn convolution inl h 446 Check failed cudnnSetTensorNdDescriptor CUDNN STATUS SUCCESS 9 vs 0,ubuntu16 0 4 python2 7 mxnet0 9 3,,yajiedesign,2017-03-11 14:36:18,2017-09-29 06:34:53
IS,cudaMalloc failed out of memory,Environment info Operating System CentOS Linux release 7 3 1611 MXNet version 0 9 4 Python version and distribution Python 2 7 13 Anaconda 4 3 0 64 bit Error Message where hi iter and lo iter are ImageRecordIter for high resolution and lo resolution images with size 96 96 3 and 24 24 3 respectively The full code is attached here code zip Steps to reproduce I ran my code on a Tesla K80 GPU and observed that the GPU memory usage were increasing rapidly Finally it raised a cudaMalloc failed out of memory error in 609 th iteration This error can be reproduced by running the following command python test py data path home chenxu data train rec num iter 1000000 batch size 128 gpus 0 lr 0 0001 frequency 1000 save frequency 100000,,yajiedesign,2017-03-11 14:26:40,2017-09-29 06:34:56
IS,mx io NDArrayIter batch size for evaluation data,I tried a dozen of time before I get the conclusion that the training data and evaluation data shall use the same batch size What if the evaluation data size is smaller than the batch size for training I read some of the document and figured out that batch size is related to pre allocation of memory thus the prediction is conducted on one batch at a time I am not sure if I am on the right channel,,yajiedesign,2017-03-11 15:43:44,2017-09-29 06:35:00
IS,NDArray Adding to discontinuous indices,I have the following case Assume that I have have an NDArray object A with the shape D dim where dim is any arbitrary shape and D is the minibatch size I have another NDArray object B with the shape d dim where d D Now I have a mapping between the axis 0 indices of B and A such that entry B i is mapped to A j where j i My aim is to add each B i to A j The naivest thing I could to is to iterate on B is axis0 and do for each B i the operation A j B i But each such addition operation is costly To reduce the it I keep the track of each consequent j entries in the A object and do addition as A j1 j2 B i1 i2 While this lowers the number of additions still I have to do multiple ones I wonder if such an operation is possible currently Let is say I know all the j indices j1 j2 jd In numpy it is possible to do operations by using the index array as A j1 j2 jd B Trying the similar operation results in ValueError NDArray only support continuous slicing on axis 0 with MxNet NDArray My purpose is to execute this addition in a single command Is this possible with the current MxNet API,,yajiedesign,2017-03-11 19:15:11,2017-09-29 06:35:06
IS,Executor API for Python Not supported anymore,In the latest version of the Mxnet I do not see any documentation regarding the Executor object I used to utilize it as a low level calculation tool for my custom architecture Is the new Module API replacing it Is it still safe to use Executor class and its methods still And I strongly believe that you need to put the Executor object back into docs,,yajiedesign,2017-03-10 20:26:41,2017-09-29 06:35:09
IS,Docker build on raspbian fails,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Raspbian Compiler g 4 8 Package used Python R Scala Julia Pythno MXNet version latest Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace g std c 11 c DMSHADOW FORCE STREAM Wall O3 I mxnet mshadow I mxnet dmlc core include fPIC I mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs DMSHADOW USE SSE 0 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary broadcast op basic cc o build src operator tensor elemwise binary broadcast op basic o mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X int Y unsigned int mxnet nnvm include nnvm tuple h 446 5 required from mshadow Shape ndim nnvm TShape get const with int dim 5 src operator tensor broadcast reduce inl h 172 71 required from void mxnet op broadcast BinaryBroadcastComputeImpl mshadow Stream mshadow cpu mxnet OpReqType const mxnet TBlob const mxnet TBlob const mxnet TBlob with DType float OP mxnet op mshadow op power src operator tensor elemwise binary broadcast op h 116 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mxnet op mshadow op power src operator tensor elemwise binary broadcast op extended cc 17 84 required from here mxnet dmlc core include dmlc logging h 94 24 warning comparison between signed and unsigned integer expressions Wsign compare DEFINE CHECK FUNC EQ mxnet dmlc core include dmlc logging h 76 11 note in definition of macro DEFINE CHECK FUNC if x op y return LogCheckError In file included from mxnet dmlc core include dmlc registry h 13 0 from include mxnet operator util h 18 from src operator tensor elemwise unary op h 9 from src operator tensor elemwise binary broadcast op basic cc 6 mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X unsigned int Y int src operator tensor elemwise unary op h 79 3 required from here mxnet dmlc core include dmlc logging h 94 24 warning comparison between signed and unsigned integer expressions Wsign compare DEFINE CHECK FUNC EQ mxnet dmlc core include dmlc logging h 76 11 note in definition of macro DEFINE CHECK FUNC if x op y return LogCheckError mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck EQ const X const Y with X int Y unsigned int mxnet nnvm include nnvm tuple h 446 5 required from mshadow Shape ndim nnvm TShape get const with int dim 5 src operator tensor broadcast reduce inl h 172 71 required from void mxnet op broadcast BinaryBroadcastComputeImpl mshadow Stream mshadow cpu mxnet OpReqType const mxnet TBlob const mxnet TBlob const mxnet TBlob with DType float OP mshadow op plus src operator tensor elemwise binary broadcast op h 116 5 required from void mxnet op BinaryBroadcastCompute const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu OP mshadow op plus src operator tensor elemwise binary broadcast op basic cc 17 84 required from here mxnet dmlc core include dmlc logging h 94 24 warning comparison between signed and unsigned integer expressions Wsign compare DEFINE CHECK FUNC EQ mxnet dmlc core include dmlc logging h 76 11 note in definition of macro DEFINE CHECK FUNC if x op y return LogCheckError mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck NE const X const Y with X unsigned int Y int mxnet mshadow mshadow tensor cpu inl h 213 3 required from void mshadow MapReduceKeepLowest mshadow TRValue R mshadow cpu 1 DType const mshadow expr Exp E DType etype DType with Saver mshadow sv plusto Reducer mshadow red sum R mshadow Tensor mshadow cpu 1 float DType float E mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op right mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 int etype 3 mxnet mshadow mshadow extension reduceto1d h 99 63 required from static void mshadow expr ExpComplexEngine SV mshadow Tensor xpu 1 DType mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 DType Eval mshadow Tensor xpu 1 DType const mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 with SV mshadow sv plusto Device mshadow cpu DType float SrcExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op right mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 Reducer mshadow red sum mxnet mshadow mshadow expr engine inl h 461 72 required from static void mshadow expr ExpEngine Saver RValue DType Eval RV const mshadow expr Exp E DType 7 with E mshadow expr ReduceTo1DExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op right mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 float mshadow red sum 1 SV mshadow sv plusto RV mshadow Tensor mshadow cpu 1 float DType float mxnet mshadow mshadow expression h 175 78 required from Container mshadow expr RValueExp Container DType operator const mshadow expr Exp E DType etype with E mshadow expr ReduceTo1DExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op right mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 float mshadow red sum 1 int etype 7 Container mshadow Tensor mshadow cpu 1 float DType float src operator tensor elemwise binary broadcast op h 163 5 required from void mxnet op ReduceToAssign mshadow Tensor xpu 2 DType mxnet OpReqType const SrcExp with Reducer mshadow red sum xpu mshadow cpu SrcExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op right mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 DType float src operator tensor elemwise binary broadcast op h 224 79 required from void mxnet op BinaryBroadcastBackwardUseInImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float LOP mxnet op mshadow op right ROP mxnet op mshadow op left mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 241 5 required from void mxnet op BinaryBroadcastBackwardUseIn const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu LOP mxnet op mshadow op right ROP mxnet op mshadow op left src operator tensor elemwise binary broadcast op basic cc 74 80 required from here mxnet dmlc core include dmlc logging h 95 24 warning comparison between signed and unsigned integer expressions Wsign compare DEFINE CHECK FUNC NE mxnet dmlc core include dmlc logging h 76 11 note in definition of macro DEFINE CHECK FUNC if x op y return LogCheckError mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck NE const X const Y with X unsigned int Y int mxnet mshadow mshadow tensor cpu inl h 213 3 required from void mshadow MapReduceKeepLowest mshadow TRValue R mshadow cpu 1 DType const mshadow expr Exp E DType etype DType with Saver mshadow sv plusto Reducer mshadow red sum R mshadow Tensor mshadow cpu 1 float DType float E mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 int etype 3 mxnet mshadow mshadow extension reduceto1d h 99 63 required from static void mshadow expr ExpComplexEngine SV mshadow Tensor xpu 1 DType mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 DType Eval mshadow Tensor xpu 1 DType const mshadow expr ReduceTo1DExp SrcExp DType Reducer 1 with SV mshadow sv plusto Device mshadow cpu DType float SrcExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 Reducer mshadow red sum mxnet mshadow mshadow expr engine inl h 461 72 required from static void mshadow expr ExpEngine Saver RValue DType Eval RV const mshadow expr Exp E DType 7 with E mshadow expr ReduceTo1DExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 float mshadow red sum 1 SV mshadow sv plusto RV mshadow Tensor mshadow cpu 1 float DType float mxnet mshadow mshadow expression h 175 78 required from Container mshadow expr RValueExp Container DType operator const mshadow expr Exp E DType etype with E mshadow expr ReduceTo1DExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 float mshadow red sum 1 int etype 7 Container mshadow Tensor mshadow cpu 1 float DType float src operator tensor elemwise binary broadcast op h 163 5 required from void mxnet op ReduceToAssign mshadow Tensor xpu 2 DType mxnet OpReqType const SrcExp with Reducer mshadow red sum xpu mshadow cpu SrcExp mshadow expr BinaryMapExp mshadow op mul mshadow Tensor mshadow cpu 2 float mshadow expr BinaryMapExp mxnet op mshadow op power grad mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float mshadow expr MakeTensorExp mshadow expr BroadcastWithMultiAxesExp mshadow Tensor mshadow cpu 2 float float 2 mshadow Tensor mshadow cpu 2 float 2 float float 3 float 3 DType float src operator tensor elemwise binary broadcast op h 224 79 required from void mxnet op BinaryBroadcastBackwardUseInImpl const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const TShape const TShape const TShape with xpu mshadow cpu int ndim 2 DType float LOP mxnet op mshadow op power grad ROP mxnet op mshadow op power rgrad mxnet TShape nnvm TShape src operator tensor elemwise binary broadcast op h 241 5 required from void mxnet op BinaryBroadcastBackwardUseIn const nnvm NodeAttrs const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob with xpu mshadow cpu LOP mxnet op mshadow op power grad ROP mxnet op mshadow op power rgrad src operator tensor elemwise binary broadcast op extended cc 29 87 required from here mxnet dmlc core include dmlc logging h 95 24 warning comparison between signed and unsigned integer expressions Wsign compare DEFINE CHECK FUNC NE mxnet dmlc core include dmlc logging h 76 11 note in definition of macro DEFINE CHECK FUNC if x op y return LogCheckError Minimum reproducible example if you are using your own code please provide a short script that reproduces the error I am compiling a Dockerfile on raspbian Dockerfile content FROM resin rpi raspbian latest Install build essential git wget and other dependencies RUN apt get update apt get install y git cmake build essential g 4 8 c 4 8 liblapack libblas libopencv libopenblas python dev python numpy python setuptools wget python3 pip nginx supervisor python pip python opencv Clone MXNet repo and move into it RUN cd git clone recursive cd mxnet Copy config mk cp make config mk config mk Set OpenBLAS sed i is USE BLAS atlas USE BLAS openblas g' config mk Make make j nproc Install Python package RUN cd mxnet python python setup py install Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 install docker on pi3 2 docker build t mxnet 3 What have you tried to solve it 1 changed g 2 tried debian and it works 3,,yajiedesign,2017-03-12 06:06:05,2017-09-29 06:35:12
IS,magic python functions in ndarray py,I have been reading the code of mxnet for some further understanding of the platform In optimizer py I saw following code from ndarray import sgd update sgd mom update adam update rmsprop update rmspropalex update However the python implementations of these functions are absent in ndarray py I am sure these functions are written in c but these should at least some cython wraps for them but I only get NNVM REGISTER OP sgd mom update when scanning the code using ack,,"ZihengJiang,yajiedesign",2017-03-12 13:00:39,2017-09-29 06:35:16
IS,Spark Model Training with multiple executors,This is more of a feature request that I may even be willing to work on in my spare time However I first wanted to see if anyone is either working on this or had any specific plans Basically right now it seems like you can only use one executor for training an mxnet model It would be nice if mxnet could do model parameter averaging across spark partitions over each iteration of training or multiple This would require each partition to have access to an mxnet model then apply the training on the subset of data partition and after an iteration s is completed we could then average the weights and biases for the central core model We then would pass the updated model to each of the partitions for another iteration or series of iterations and repeat I know there are other deep learning libraries that have something similar and I wanted to see where you guys were on this,,"dmmiller612,piiswrong,yzhliu,dmmiller612,yzhliu,dmmiller612,yzhliu,yajiedesign",2017-03-10 15:22:59,2017-09-29 06:35:20
IS,DISCUSSION CI Test Deploy,We recently moved some tests into a jenkins cluster with GPU test enabled This issue discusses future improvements for both CI test and deploy What to test The tests can be categorized into four aspects 1 Sanity check such as lint which checks the code style 2 Unit test test individual functions and classes such as testing operators 3 Integration test test the correctness of multi modules For example training a CNN model on mnist or predicting on a pre trained model We can reuse the codes on folder example 4 Performance test Since we care about system performance we should benchmark the performance from time to time We have test codes for each category somewhat but still away from 100 test coverage I roughly list some issues here 1 lint We have roughly 40K lines python codes but only python mxnet is checked And we have put too many global pylint disable on the beginning of each file 5307 is trying to improve it 2 unittest We should enable docstring test for python so a same piece of codes can serve as both document and unittest Also consider to enable pair test with other libraries such as caffe or torch 3 integration performance test There are several test py scripts on example and tools But they are not been automatically tested How to test Each category has different time cost 1 1min 2 10min 30min 3 1h 4 1h Therefore we can run category 1 2 for every pull request but let 3 4 run periodically such every night Besides we need to test on various environments including hardware such as CPU GPU OSs such as linux mac os x windows android and libraries such as CUDA 7 5 CUDA 8 MKL DNN It is unlikely we can cover all combinations so we should focus on common cases a Linux Ubuntu 14 04 16 04 Centos with CPU openblas MKLDNN and GPU CUDA 7 5 CUDA 8 b Mac os x with CPU c Windows with CPU We can cover a with jenkins and docker on AWS b is running on travis while c is running on appveyor We should move both travis and appveyor tasks into jenkins as well if we figured out how to do it Ly is working on the latter A sample unittest build contains several subtasks 1 start a docker instance with the desired environment set up 1min 2 compile 3min 10min 3 c unittest 1min 4 python unittest 5min 5 scala unittest 5min 6 r unittest 5min 7 We can run these subtasks in parallel if a single build takes more than 30min Deploy We should provide pre build library so that a user can use it directly instead of building from scratch Currently we host pip build on travis which supports mac os x cpu linux cpu gpu We should add windows and android supports in the future,,"mli,mli,kevinthesun,eric-haibin-lin,yajiedesign",2017-03-09 22:14:00,2017-09-29 06:35:23
IS,Combining items within batch during forward pass,Is there any way in Mxnet to concat for example 3 consecutive images from same batch on forward pass right after one of layers and then pass them thru several more layers as combined input to get one output I e for initial batch size of 300 on the inputs there will be 100 final outputs for same batch,,yajiedesign,2017-03-12 20:09:56,2017-09-29 06:35:26
IS,Cross Entropy Loss Function,Hi all I am trying to run distributed version on image classification python example and I am getting the output Also I am trying to measure the loss function for each 20 epochs However I could not do it Can anybody give me a suggestion of who to report loss function after each 20 epochs Sincerely,,yajiedesign,2017-03-12 20:44:58,2017-09-29 06:35:30
IS,Access true label,I am a beginner and try to use a pre trained resnet I have a problem of accessing the ground truth label I looked at some tutorial and still got quite confused sym arg params aux params mx model load checkpoint aresnet 152' 0 all layers sym get internals net all layers 'flatten0' ' output' net mx symbol FullyConnected data net num hidden num classes name 'fc1' net mx symbol SoftmaxOutput data net name isoftmax' This softmaxoutput layer does not seem to take the true label as a argument which is described as a non optional parameter on the website How the loss is calculated when label is not passed to the softmax output layer I looked at the input list of symbol sym the pretrained resnet model and found softmax label is in there I want to scale the loss contribution of each sample based on their true labels How can I access the labels under this circumstances And How can I access the loss it seems the loss is computed in the softmaxoutput layer but not outputted For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-03-11 22:52:57,2017-09-29 06:35:33
IS,Using MKL causes C layer blow up while running lstm bucketing example,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you WARNING discarded 89 sentences longer than the largest bucket WARNING discarded 4 sentences longer than the largest bucket 01 06 12 home ubuntu mxnet dmlc core include dmlc logging h 300 01 06 12 src operator mkl mkl concat inl h 196 Check failed e E SUCCESS 1 vs 0 Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fa00e11cc1c bt 1 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op11MKLConcatOpIN7mshadow3cpuEfE7ForwardERKNS 9OpContextERKSt 6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0xc10 0x7fa00ecfd950 bt 2 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xec2092 0x7fa00ed9a092 bt 3 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x8c 0x7fa00ed5531c bt 4 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice1 3PushToExecuteEPNS2 8OprBlockEbENKUlvE clEvEUlvE E9 M invokeERKSt9 Any data 0x2e 0x7fa00ed57bbe bt 5 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fa006208c80 bt 6 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fa01cadf6ba bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fa01c81582d 01 06 12 home ubuntu mxnet dmlc core include dmlc logging h 300 01 06 12 src engine threaded engine h 336 01 06 12 src operator mkl mkl concat i nl h 196 Check failed e E SUCCESS 1 vs 0 Stack trace returned 8 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7fa00e11cc1c bt 1 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op11MKLConcatOpIN7mshadow3cpuEfE7ForwardERKNS 9OpContextERKSt 6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0xc10 0x7fa00ecfd950 bt 2 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xec2092 0x7fa00ed9a092 bt 3 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x8c 0x7fa00ed5531c bt 4 usr local lib python2 7 dist packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice1 3PushToExecuteEPNS2 8OprBlockEbENKUlvE clEvEUlvE E9 M invokeERKSt9 Any data 0x2e 0x7fa00ed57bbe bt 5 usr lib x86 64 linux gnu libstdc so 6 0xb8c80 0x7fa006208c80 bt 6 lib x86 64 linux gnu libpthread so 0 0x76ba 0x7fa01cadf6ba bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7fa01c81582d Environment info Operating System LInux Ubuntu 16 04 Compiler gcc 4 8 Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD 55bb4cd2e06c24b46664ac708150e2283e9695c3 If you are using python package please provide Python version and distribution python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error mxnet example rnn python lstm bucketing py Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Compile with USE BLAS mkl 2 run mxnet example rnn python lstm bucketing py 3 What have you tried to solve it 1 2 3,,"sergeykolychev,piiswrong,glingyan,sergeykolychev,glingyan,piiswrong,sergeykolychev,piiswrong,sergeykolychev,sergeykolychev,glingyan,sergeykolychev,glingyan,glingyan,sergeykolychev,glingyan,sergeykolychev,glingyan,sergeykolychev,yajiedesign",2017-03-09 01:08:03,2017-09-29 06:35:36
IS,What factor impact the speed xxx samples sec in training process,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 5 Compiler gcc 4 8 4 Package used Python R Scala Julia MXNet version Or if installed from source installed by git clone mxnet recursive MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 13 anaconda 4 3 0 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3 Build environment nvidia 375 26 cuda 8 0 gcc 4 8 4 ubundu 14 04 5 cudnn 5 1 First we implement DenseNet BC introduced by Gao Huang Zhuang Liu Kilian Q Weinberger Laurens van der Maaten in the paper Densely Connected Convolutional Networks with MXNet More detailed information can be found in However we do not know what factor impact the speed xxx samples sec in training process In this figure we deploy neural network DenseNet BC on VGG Face data num classes 2613 num examples 1483368 with batch size 128 image However in the following figure we deploy the same neural network and same training procedure on MS Celeb data num classes 79051 num examples 4513005 with batch size 96 image Is this the reason of data cause this large difference on speed If is not what factor impact the speed xxx samples sec in training process,,yajiedesign,2017-03-13 08:06:07,2017-09-29 06:35:40
IS,Why do not use TBlob directly in Operator,I am wondering why do not use TBlob directly in Operator is Forward and Backward What is the advantage of using Tensor compared to TBlob If Tensor is better than TBlob then why do not pass Tensor to Forward and Backward directly,,yajiedesign,2017-03-13 10:02:44,2017-09-29 06:35:43
IS,Passing data from numpy trough the intermediate API and getting back the cross entropy,There are quite a few questions which are unclear of how to do First I want to initalize the model without any data given binded This seems quite difficult and could not find an example Secondly I want to be able to pass numpy data to the forward and backward methods of the intermediate API rather than using the fit method or alternatively is there anyway to get a sensible callback which to return the cross entorpy the time taken for the iteration My code currently looks like this The main goal is to use this in a benchmark facility so any suggestions of how to do this properly in MXNet are welcome,,yajiedesign,2017-03-01 00:37:25,2017-09-29 06:35:46
IS,key stride not found when using to graphviz with Convolutional layers in Julia API,From the Julia API we can not access the attr stride for Convolution layers I reported this issue in the Julia API repo suggested this is may be a bug in the core mxnet API issuecomment 285790634 Environment info Operating System ubuntu 14 04 Compiler gcc Ubuntu 4 8 4 2ubuntu1 14 04 3 4 8 4 Package used Python R Scala Julia Julia Version 0 5 0 2016 09 19 18 14 UTC Official release x86 64 pc linux gnu MXNet version julia Pkg status MXNet 0 2 1 e599d51d Error Message What have you tried to solve it Obviously removing the access to the attr in L72 L74 makes the error go away but then there is no stride info in the graph Apparently the python api does the same thing as Julia is L252,,"facundoq,yajiedesign",2017-03-13 18:36:17,2017-09-29 06:35:50
IS,LSTM MXNet perplexity score,Hello I am using Mxnet R package and walking through the following tutorial I see that the lstm that was trained is used to generate text However is there a way to use the lstm model to evaluate test text When using language models we can use perplexity to evaluate test data Lower the perplexity closer the language model to test data Higher the perplexity the farther away the language model is to test data Is there an equivalent score that can be generated using LSTM Thanks Aarthi Environment info Operating System MacOS Compiler Package used Python R Scala Julia R R sessionInfo R version 3 3 2 2016 10 31 Platform x86 64 apple darwin13 4 0 64 bit Running under macOS Sierra 10 12 3 locale 1 en US UTF 8 en US UTF 8 en US UTF 8 C en US UTF 8 en US UTF 8 attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 mxnet 0 9 4 h2o 3 10 2 2 ggplot2 2 2 1 plyr 1 8 4 loaded via a namespace and not attached 1 Rcpp 0 12 7 knitr 1 15 1 magrittr 1 5 devtools 1 12 0 5 munsell 0 4 3 colorspace 1 2 7 R6 2 2 0 stringr 1 1 0 9 httr 1 2 1 visNetwork 1 0 3 tools 3 3 2 drat 0 1 2 13 grid 3 3 2 gtable 0 2 0 git2r 0 18 0 withr 1 0 2 17 htmltools 0 3 5 lazyeval 0 2 0 assertthat 0 1 digest 0 6 10 21 tibble 1 2 codetools 0 2 15 htmlwidgets 0 7 bitops 1 0 6 25 RCurl 1 95 4 8 curl 2 2 memoise 1 0 0 labeling 0 3 29 stringi 1 1 2 scales 0 4 1 jsonlite 1 1 Error Message No error message,,yajiedesign,2017-03-13 19:36:06,2017-09-29 06:35:54
IS,Which mxnet version am i using,I am a newbie with MXNET How do i check the version My git branch says its master,,"wangg12,yajiedesign",2017-03-11 19:07:11,2017-09-29 06:35:57
IS,How to write a new custom layer with learned parameters,Anyone know how to write a new custom layer with learned param Is there any tutorial or example that create new layers with learned parameters,,yajiedesign,2017-03-12 15:48:49,2017-09-29 06:36:04
IS,unable to compile mxnet after latest pull,I am unable to compile mxnet after the latest pull on Monday March 13 2017 I am trying to do make clean make Environment info Operating System Mac OS X El Capitan version 10 11 5 Compiler Configured with prefix Applications Xcode app Contents Developer usr with gxx include dir Applications Xcode app Contents Developer Platforms MacOSX platform Developer SDKs MacOSX10 12 sdk usr include c 4 2 1 Apple LLVM version 8 0 0 clang 800 0 38 Target x86 64 apple darwin15 5 0 Thread model posix InstalledDir Applications Xcode app Contents Developer Toolchains XcodeDefault xctoolchain usr bin Package used Python R Scala Julia Python 2 7 MXNet commit hash git rev parse HEAD b446aef95d738ce0ce1bbe621fa9fc41b549ef6c If you are using python package please provide Python version and distribution Python 2 7 12 Anaconda 4 2 0 x86 64 Error Message rm rf python mxnet so python mxnet cpp rm f r build lib bin R package NAMESPACE R package man R package R mxnet generated R R package inst R package src o R package src so mxnet tar gz cd Users Forough mxnet dmlc core Applications Xcode app Contents Developer usr bin make clean cd rm f line split o recordio split o input split base o io o local filesys o data o recordio o config o libdmlc a test filesys test test dataiter test test iostream test test recordio test test split read test test stream read test test split test test libsvm parser test test split repeat read test test strtonum test test logging test test parameter test test registry test test csv parser test test unittest dmlc unittest test unittest unittest any o test unittest unittest array view o test unittest unittest config o test unittest unittest json o test unittest unittest logging o test unittest unittest main o test unittest unittest optional o test unittest unittest serializer o test unittest unittest threaditer o src src include dmlc test Users Forough mxnet cd Users Forough mxnet ps lite Applications Xcode app Contents Developer usr bin make clean cd rm rf build tests test connection tests test kv app tests test simple app tests d find src name pb ch delete Users Forough mxnet cd Users Forough mxnet nnvm Applications Xcode app Contents Developer usr bin make clean cd rm f rf build lib bin o o o cli test Users Forough mxnet g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator mkl mkl cppwrapper cc o build src operator mkl mkl cppwrapper o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator mkl mkl memory cc o build src operator mkl mkl memory o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator nnpack nnpack util cc o build src operator nnpack nnpack util o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor broadcast reduce op index cc o build src operator tensor broadcast reduce op index o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor broadcast reduce op value cc o build src operator tensor broadcast reduce op value o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor control flow op cc o build src operator tensor control flow op o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary broadcast op basic cc o build src operator tensor elemwise binary broadcast op basic o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary broadcast op extended cc o build src operator tensor elemwise binary broadcast op extended o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary broadcast op logic cc o build src operator tensor elemwise binary broadcast op logic o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary op basic cc o build src operator tensor elemwise binary op basic o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary op extended cc o build src operator tensor elemwise binary op extended o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary op logic cc o build src operator tensor elemwise binary op logic o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary scalar op basic cc o build src operator tensor elemwise binary scalar op basic o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary scalar op extended cc o build src operator tensor elemwise binary scalar op extended o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise binary scalar op logic cc o build src operator tensor elemwise binary scalar op logic o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise sum cc o build src operator tensor elemwise sum o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor elemwise unary op cc o build src operator tensor elemwise unary op o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor indexing op cc o build src operator tensor indexing op o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor init op cc o build src operator tensor init op o g std c 11 c DMSHADOW FORCE STREAM Wall O3 I Users Forough mxnet mshadow I Users Forough mxnet dmlc core include fPIC I Users Forough mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 DMSHADOW USE CUDA 0 DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include I usr local opt openblas include DMXNET USE NVRTC 0 MMD c src operator tensor matrix op cc o build src operator tensor matrix op o In file included from src operator tensor matrix op cc 7 In file included from src operator tensor matrix op inl h 9 In file included from include mxnet operator util h 24 In file included from include mxnet base h 16 Users Forough mxnet nnvm include nnvm tuple h 35 3 error static assert failed Tuple only support simple data type like int static assert std is pod ValueType value src operator tensor matrix op inl h 673 37 note in instantiation of template class 'nnvm Tuple dmlc optional int ' requested here nnvm Tuple dmlc optional int begin end In file included from src operator tensor matrix op cc 7 In file included from src operator tensor matrix op inl h 14 In file included from src operator tensor elemwise op common h 18 src operator tensor operator common h 229 11 error no matching member function for call to 'Init' param Init attrs dict src operator tensor matrix op cc 209 18 note in instantiation of function template specialization 'mxnet op ParamParser mxnet op SliceParam ' requested here set attr parser ParamParser SliceParam Users Forough mxnet dmlc core include dmlc parameter h 127 15 note candidate function with Container std 1 unordered map std 1 basic string char std 1 basic string char std 1 hash std 1 basic string char std 1 equal to std 1 basic string char std 1 allocator std 1 pair const std 1 basic string char std 1 basic string char not viable no known conversion from 'mxnet op SliceParam' to wouldmlc Parameter mxnet op SliceParam ' for object argument inline void Init const Container kwargs In file included from src operator tensor matrix op cc 7 In file included from src operator tensor matrix op inl h 9 In file included from include mxnet operator util h 18 In file included from Users Forough mxnet dmlc core include dmlc registry h 14 In file included from Users Forough mxnet dmlc core include dmlc parameter h 22 In file included from Users Forough mxnet dmlc core include dmlc json h 29 Users Forough mxnet dmlc core include dmlc any h 179 21 error no viable conversion from 'mxnet op SliceParam' to wouldmlc any' this construct std forward T other Users Forough mxnet dmlc core include dmlc any h 229 3 note in instantiation of function template specialization wouldmlc any any mxnet op SliceParam ' requested here any std forward T other swap this src operator tensor operator common h 241 17 note in instantiation of function template specialization wouldmlc any operator mxnet op SliceParam ' requested here attrs parsed std move param src operator tensor matrix op cc 209 18 note in instantiation of function template specialization 'mxnet op ParamParser mxnet op SliceParam ' requested here set attr parser ParamParser SliceParam Users Forough mxnet dmlc core include dmlc any h 176 13 note candidate constructor with T mxnet op SliceParam inline any any T other Users Forough mxnet dmlc core include dmlc any h 192 13 note candidate constructor not viable no known conversion from 'mxnet op SliceParam' to wouldmlc any ' for 1st argument inline any any any other Users Forough mxnet dmlc core include dmlc any h 196 13 note candidate constructor not viable no known conversion from 'mxnet op SliceParam' to 'const dmlc any ' for 1st argument inline any any const any other Users Forough mxnet dmlc core include dmlc any h 163 31 note passing argument to parameter 'other' here inline void construct any other 3 errors generated make build src operator tensor matrix op o Error 1 Minimum reproducible example make clean make,,yajiedesign,2017-03-13 21:54:35,2017-09-29 06:36:07
IS,predict API for dynamic batch,Can I use the python mxnet predict of Amalgamation for dynamic batch The API shows I must specify the input data shape when I create the Predictor But it not make sense when using it in ocr task because I do not know how many words image i can split out from a big image,,yajiedesign,2017-03-13 03:25:44,2017-09-29 06:36:13
IS,when i use mx version is 0 9 4 train faster rcnn i found it is very slowly,when i use mx version is 0 9 4 train faster rcnn i found it is running slow what is the reason ENVIRONMENT ubuntu14 04 GTX1080 CUDA 8 0,,"sergeykolychev,yajiedesign",2017-03-14 09:43:26,2017-09-29 06:36:17
IS,how to save checkpoint at the end of some minibatch during training,It is easy to save checkpoint at each epoch callback by define parameter epoch end callback But how to save checkpoint at the end of some minibatch during training instead of saving ckpt at the end of each epoch,,yajiedesign,2017-03-15 08:47:13,2017-09-29 06:36:21
IS,mxnet base MXNetError 20 49 24 src c api c api ndarray cc 274 Operator ones cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered,MXNet OSX install OK running CPU ok but GPU with the following errors import mxnet as mx a mx nd ones 2 3 mx gpu with the followings traces Stack trace returned 4 entries bt 0 0 libmxnet so 0x0000000107abc005 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x000000010805c9cb MXImperativeInvoke 13723 bt 2 2 ctypes cpython 35m darwin so 0x00000001007de077 ffi call unix64 79 bt 3 3 python 0x00007fff5bffe990 progname 140730441914608,,"piiswrong,yajiedesign",2017-03-14 12:51:42,2017-09-29 06:36:24
IS,Caffe importer fails to resnet 50,Running convert model py on the ResNet 50 model from here produces an error importing the BatchNorm layers I do not have Caffe installed so am using the pure protobuf version of the importer Environment info OSX MXNet version 0 9 4 dea87660c9d3b55ccd28a812116c93ca9e5032c2 Protobuf version converting layer conv1 wmat shape 64 3 7 7 bias shape 64 Traceback most recent call last File convert model py line 162 in module main File convert model py line 158 in main convert model args prototxt args caffemodel args save model name File convert model py line 127 in convert model mean mean reshape aux shape dic mean name AttributeError 'RepeatedScalarFieldContainer' object has no attribute areshape',,"sbodenstein,mli,yajiedesign",2017-03-15 16:32:01,2017-09-29 06:36:27
IS,Error with for loop on symbols,Hi I am trying to implement an RNN in MXNet as I have no GPU at my disposal I sadly cannot use mxnet symbol rnn I use a for loop for implementation and everything works fine until exactly the 7th iteration self past horizon 1 7 Then MXNet throws the following error message 18 12 07 D Program Files x86 Jenkins workspace mxnet mxnet dmlc core include dmlc logging h 300 18 12 07 d program files x86 jenkins workspace mxnet mxnet src operator tensor matrix op inl h 905 CropAssign only supports kWriteTo 18 12 07 D Program Files x86 Jenkins workspace mxnet mxnet dmlc core include dmlc logging h 300 18 12 07 d program files x86 jenkins workspace mxnet mxnet src engine threaded engine h 336 18 12 07 d program files x86 jenkins workspace mxnet mxnet src operator tensor matrix op inl h 905 CropAssign only supports kWriteTo An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging The error apparently occurs during the backward operation Here is my code def getgru self data num hidden name hidden state mx sym zeros shape self batch size num hidden 1 ones mx sym ones shape self batch size num hidden 1 W z mx sym Variable 'W z' shape self batch size num hidden num hidden init mx init Uniform 1 W r mx sym Variable 'W r' shape self batch size num hidden num hidden init mx init Uniform 1 W mx sym Variable 'W' shape self batch size num hidden num hidden init mx init Uniform 1 U z mx sym Variable 'U z' shape self batch size num hidden num hidden init mx init Uniform 1 U r mx sym Variable 'U r' shape self batch size num hidden num hidden init mx init Uniform 1 U mx sym Variable 'U' shape self batch size num hidden num hidden init mx init Uniform 1 outputs mx sym Variable 'outputs' data mx sym swapaxes data data dim1 1 dim2 2 for i in range self past horizon 1 cur data mx sym slice data data begin 0 0 i end self batch size num hidden i 1 reset gate mx sym Activation mx sym batch dot W r cur data mx sym batch dot U r hidden state act type isigmoid' update gate mx sym Activation mx sym batch dot W z cur data mx sym batch dot U z hidden state act type isigmoid' if i 0 outputs cur data continue hidden state mx sym tanh mx sym batch dot W cur data mx sym batch dot U reset gate hidden state update ones update gate mx sym slice outputs begin 0 0 i 1 end self batch size num hidden i update gate hidden state outputs mx sym concat outputs update dim 2 output mx sym swapaxes data outputs dim1 1 dim2 2 return output I have tried out various types of input data from which it is clear that this is not a memory issue I can say with certainty that the problem lies with the getgru function because the remaining NN works perfectly when I skip the GRU I have varied the other parameters num hidden batch size and only past horizon produces the error Therefore my working hypothesis is that the computation of gradients through the loop only works for a certain number of steps Could you help me out here by suggesting a solution or an alternative Thank you so much,,"piiswrong,piiswrong,yajiedesign",2017-03-15 17:20:23,2017-09-29 06:36:30
IS,Training faster rcnn use resnet,when i used resnet trained faster rcnn it had a good trained result but the test is poor cmd bash example rcnn script resnet voc07 sh 0 After trained i test the dataset that used trained i found that the result is error all of the result is wrong who can help me Or tell me your idea thanks,,yajiedesign,2017-03-16 01:54:37,2017-09-29 06:36:33
IS,How to load model in the tutorial Text Classification Using a Convolutional Neural Network,There is no model loading and test code in the tutorial and the way it saves the model seems different with others Now I think the API used in the tutorial is kind of outdated The nametuple mechanism can be replaced by mod Module maybe Need help thanks,,yajiedesign,2017-03-15 11:51:10,2017-09-29 06:36:36
IS,How can I turn down the memory optimization option,It seems that the macro definition of NNVM EXEC ENABLE INPLACE has been removed in the 0 93 version How can I turn down the memory optimization option,,yajiedesign,2017-03-16 08:51:48,2017-09-29 06:36:40
IS,Why the speed of same exe in win7 is much slower than in win10,it cost about 150ms image in win7 while about 50ms image in win10,,yajiedesign,2017-03-16 11:13:13,2017-09-29 06:36:43
IS,Predicting with mx io ImageRecordIter object returns different size vector each time is run,Hi I'm training a CNN using mx io ImageRecordIter objects as input during training and also for predicting purposes However I have come across a weird behavior when calling the predict function Each time is run this function returns a vector of predicted values always less than or equal to the original number of images in the test image iterator After a while trying to solve this problem I have realized that every time predict is called the size of the returned vector is always increased by the same amount until it reaches or exceeds the original size and then starts again For example if the test image iterator contains 194 images predict calls initially return 132 then 138 144 all the way to 192 and then starts again from 132 Another discovery was that during execution time running predict more than once did not change the vector size in the same way Although it is still the wrong size different from the original one consecutive calls to predict return the same size for a while then for a short period of time it increases in the same amount it did in the first place and then returns the initial wrong size For example it prints 132 for a while then 138 144 150 156 and then 132 again for a while It does not always reach 156 or more sometimes it pops a small size like 144 and then starts returning 132 again However if I do Sys sleep 0 01 after each predict run during execution time apparently it allows enough time for the program or whatever being is responsible for this to advance in the returned size which means that it does not get stuck in 132 but goes all the way from 132 to 192 the limit for steps of size 6 each I came up with this solution after realizing that when the execution was paused in debug mode using browser with Rstudio the vector sizes changed in a constant way after each call of the predict function The following is my code for creating the image iterator pretty standard I'm also including the workaround I'm currently using to face this problem As a side note I'm using Ubuntu 16 04 with CUDA support MXNET version is 0 9 3 but this has also happened to me in version 0 7 If you need any other information please do not hesitate to ask,,yajiedesign,2017-03-16 12:27:37,2017-09-29 06:36:46
IS,Arithmetic operations on symbols with single element rhs,Environment info OS Windows 7 6 1 7601 SP2 MXNet installed from Python version and distribution 3 5 3 packaged by conda forge default Feb 9 2017 15 12 38 MSC v 1900 64 bit AMD64 Error Message,,"piiswrong,yajiedesign",2017-03-15 23:32:59,2017-09-29 06:36:51
IS,Half precision support for optimizers other than SGD,I would like to have some parameters of a network to be half precision to save memory I am running into inconsistent behavior of different optimizers when using half precision float16 variables for a toy network of y softmax w x with x float32 and w float16 cast to float32 the Adam optimizer sometimes produces nan weights scores whereas SGD seems to work fine Run example below with x dtype 'float32' and w dtype 'float16' Other optimizers such as adagrad adadelta do not seem to support float16 at all leading to a attr check fail Environment info Operating System MacOS MXNet version 8b6c8598f8f15a765dd9695c92ce251cd9eda747 Steps to reproduce x dtype w dtype optimizer outcome float32 float16 adam nan sometimes float32 float16 adagrad adadelta check fail float32 float32 ok float16 float16 FC float16 only supported by CuDNN What is the current support of optimizers for these scenarios If not supported at all is Adam simply missing a check like AdaGrad AdaDelta,,"fhieber,fhieber,yajiedesign",2017-03-14 07:45:33,2017-09-29 06:36:54
IS,can mxnet support a sparse matrix,such as 0 1 1 9 1 19 1 21 1 24 1 34 1 36 1 39 1 42 1 53 1 56 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 106 1 117 1 122 1 1 3 1 9 1 19 1 21 1 30 1 34 1 36 1 40 1 41 1 53 1 58 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 106 1 118 1 124 1 0 1 1 9 1 20 1 21 1 24 1 34 1 36 1 39 1 41 1 53 1 56 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 106 1 117 1 122 1 0 3 1 9 1 19 1 21 1 24 1 34 1 36 1 39 1 51 1 53 1 56 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 106 1 116 1 122 1 0 4 1 7 1 11 1 22 1 29 1 34 1 36 1 40 1 41 1 53 1 58 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 105 1 119 1 124 1 0 3 1 10 1 20 1 21 1 23 1 34 1 37 1 40 1 42 1 54 1 55 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 106 1 118 1 126 1 1 3 1 9 1 11 1 21 1 30 1 34 1 36 1 40 1 51 1 53 1 58 1 65 1 69 1 77 1 86 1 88 1 92 1 95 1 102 1 106 1 117 1 124 1,,yajiedesign,2017-03-14 06:28:12,2017-09-29 06:37:00
IS,how to limit threads number in windows,When training with cpu mode openmp will create multi threads but toward some very small net model the performance of using multi threads is not better or even worse In linux by set the enviorment variable OMP NUM THREADS 1 the threads number can be limited but in windows it does not work I have tried some other enviorment varable OPENBLAS NUM THREADS MXNET CPU WORKER NTHREADS MXNET CPU PRIORITY NTHREADS but the result is same dose any one have some suggestion,,yajiedesign,2017-03-17 02:51:59,2017-09-29 06:37:04
IS,one hot usage does not match documentation,The new one hot usage does not match the usage described by the documentation Documentation claims Making the indices argument an ndarray does not help Also the old onehot encode function has been deprecated making this somewhat problematic,,"piiswrong,yajiedesign",2017-03-16 22:21:02,2017-09-29 06:37:07
IS,How does BatchNorm exactly work in MxNet,It was not immediately clear to me how the BatchNorm operator works from the documentation In the original paper in the training stage the operator uses the batch statistics for normalization and accumulate those as moving averages which are to be used in the inference stage The MxNet BatchNorm operator has a use global stats flag which adjusts if I understand correctly that behavior If set to true it uses the global statistics from the auxillary arrays and if set to false it uses batch statistics Now my question is how does setting is train to True False in the forward pass affacts the behavior of the BatchNorm combined with the use global stats flag For example does setting use global stats to False would override is train flag and cause the operator to use batch statistics everytime Or is is train not effective for BatchNorm at all,,"zhreshold,leezu,yajiedesign",2016-11-17 12:40:16,2017-09-29 06:37:10
IS,Make it easier to save test and validation error,I think it would be nice to make easier to get the test and validation error when using the Module API Right now you have to go through a convoluted process of using eval end callback and batch end callback and collapsing the batches so they match epochs Please correct me if there is an easier way to do this The goal would be to have a boolean toogle for the Module API which will record the train and validation error or have an specific callback that does this,,"larroy,larroy,yajiedesign",2017-03-16 16:53:51,2017-09-29 06:37:14
IS,rename mx metric Torch to mx metric Custom,Minor refactor of mx metric Torch to rename it to custom metric When you provide a custom loss function often you will want a custom evaluation metric Torch is not a very good name For example when using MakeLoss to minimize log likelihood,,"larroy,yajiedesign",2017-03-17 10:20:23,2017-09-29 06:37:17
IS,About model parameters sharing between multithreads for image prediction,I trained a image classification model and do prediction by the c api according to the example in mxnet example cpp image classification The problem is when I launch multithreads to do the prediction I must create a new model for each thread and that is very memory consuming So how can I share one model between all the predict thread is there any simple way to do that The following are the main codes for prediction Models path for the model BufferFile json data symbol json BufferFile param data model params const mx uint input shape indptr 2 0 4 trained width trained height channel num const mx uint input shape data 4 1 static cast mx uint channels static cast mx uint width static cast mx uint height PredictorHandle out 0 alias for void Create Predictor MXPredCreate const char json data GetBuffer const char param data GetBuffer static cast size t param data GetLength dev type dev id num input nodes input keys input shape indptr input shape data out MXPredSetInput out data image data data image size Do Predict Forward MXPredForward out Get Output Result MXPredGetOutputShape out output index shape shape len MXPredGetOutput out output index data 0 size,,"indhub,yajiedesign",2017-01-12 08:15:18,2017-09-29 06:37:20
IS,Distributed Training Tuning frequency of parameter synchronization,Hi In distributed training is there a knob to tune the frequency of parameter synchronization Can I synchronize every 10 batches instead of one Need this for experimental purposes Thanks,,"piiswrong,piiswrong,yajiedesign",2017-03-17 01:37:36,2017-09-29 06:37:23
IS,Refactor isoftmax label' to 'label',softmax label is not really a good default name many times you are not doing softmax I think 'label' would be a better default name This proposed refactor would change the default values of isoftmax label' to 'label',,"larroy,piiswrong,yajiedesign",2017-03-17 10:56:49,2017-09-29 06:37:26
IS,follow mnist tutorial but cannot get result,Hi I'm a beginner of mxnet I follow the mnist tutorial but get some Error here are errors I wanna know is the tutorial out of date I just followed the tutorial I'm also confused about the input of mxnet Whatever I wanna train or predict I have to use data iterator I cannot directly use np ndarray or mx ndarray as the input just like the feeder of tensorflow Last question in python api how the model can connect the input data with the symbol placeholder just using data names and label names Thank you all,,yajiedesign,2017-03-17 14:03:16,2017-09-29 06:37:29
IS,recommend to use 0 as default value for mxnet rnn BucketSentenceIter is variable invalid label instead of 1,In file mxnet python mxnet rnn io py L86 line 86 the default value of invalid label is defined 1 however if we use this default value cudnn lstm bucketing py cannot run refers to the document of mxnet symbol Embedding mxnet symbol Embedding which requires all the input values should be integers in the range 0 input dim setting the default value of invalid label to be 1 is very dangerous since many nlp applications apply RNN and mxnet symbol Embedding layer together I recommend to use 0 as default value,,"piiswrong,ap-hynninen,yajiedesign",2017-03-17 14:29:13,2017-09-29 06:37:32
IS,How to share params in a network,Hi If I have a network I hope some layers in this network can share params how can I do Can I just get them the same name,,"piiswrong,yajiedesign",2017-03-15 13:09:52,2017-09-29 06:37:35
IS,fixed param in module,Hi The current fixed param is only compatible with params fixed at early stage For example fixing A here,,"Godricly,kevinthesun,ZiyueHuang,Godricly",2017-09-20 07:28:23,2017-09-29 09:52:49
IS,rcnn fix param list prefix checking,The code logic here is confusing It is checking whether the prefix is a part of name other than the prefix of it link L60 L65,,"Godricly,precedenceguo,Godricly",2017-09-11 08:26:54,2017-09-29 09:52:59
IS,out of memory error on windows,Environment info Operating System Windows Server 2016 MXNet version 0 11 0 Or if installed from source Install with pip install mxnet cu80 Python version and distribution 2 7 run python train mnist py in examples folder would cause out of memory error which is weird since the memory is sufficient enough 17 45 39 G deeplearn mxnet dmlc core include dmlc logging h 308 17 45 39 g deeplearn mxnet src storage pinned memory storage h 54 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA out of memory 17 45 39 G deeplearn mxnet dmlc core include dmlc logging h 308 17 45 39 g deeplearn mxnet src engine threaded engine h 347 17 45 39 g deeplearn mxnet src storage pinned memory storage h 54 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA out of memory An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,"zhreshold,zhreshold,yajiedesign",2017-09-26 09:49:54,2017-09-29 14:23:11
IS,A wrong comment In python mxnet io py,In python mxnet io py DataIter class the comment is def getdata self Get data of current batch Returns data NDArray The data of current batch pass def getlabel self Get label of current batch Returns label NDArray The label of current batch pass When I try to write a custom DataIter I return a NDArray object in these function then a Shape inconsistent error will occur like MXNetError Shape inconsistent Provided 100 600 inferred shape 100 it spend my whole afternoon to found the solution return NDArray a list of NDArray poor me,,yajiedesign,2017-03-03 10:37:28,2017-09-29 15:40:06
IS,How to load mx mod BucketingModule training models for lstm 005 json,,,yajiedesign,2017-03-18 11:56:48,2017-09-29 15:40:10
IS,Dependence of training accuracy on number of CPU nodes,I am training MNIST classifier using the mnist example provided in MXNET repository I came to know that convergence is not guaranteed when using asynchronous update of weights from the documentation at following link Sync vs Async synchronous vs asynchronous With the multi CPU node cluster that I am training MNIST classifier on I tried once with async update and the other time with sync update of weights with 5 nodes of t2 micro instances of AWS The training starts with a good accuracy of 80 while using wouldist sync' and converges However it starts and even arrives at only around 10 12 accuracy at the end of 20 epochs while using wouldist async' updation of weights On the other side with lower number of cluster nodes like 4 CPU nodes both of them are leading to convergence I am unable to understand clearly what could be the reason of dependence of number of CPU nodes in cluster for distributed training convergence Can anyone please help me understand about this MXNET version 0 9 3,,"qiyuangong,yajiedesign",2017-03-07 06:48:23,2017-09-29 15:40:13
IS,I have error in executing python loss example,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia Python Error home facultemi mxnet python mxnet module base module py 64 UserWarning Data provided by label shapes do not match names specified by label names vs isoftmax label' warnings warn msg Traceback most recent call last File home facultemi mxnet example module python loss py line 60 in module num epoch n epoch File home facultemi mxnet python mxnet module base module py line 471 in fit self forward backward data batch File home facultemi mxnet python mxnet module base module py line 195 in forward backward self backward File home facultemi mxnet python mxnet module sequential module py line 338 in backward module backward out grads out grads File home facultemi mxnet python mxnet module python module py line 305 in backward self backward impl File home facultemi mxnet python mxnet module python module py line 319 in backward impl grad self grad func self scores self labels IndexError only integers slices ellipsis numpy newaxis None and integer or boolean arrays are valid indices,,yajiedesign,2017-03-19 10:17:51,2017-09-29 15:40:17
IS,How to train ssd model for one class against background,I have only one class to detect When I train the model only with this class it predicts thousands of detection boxes so that I guess it tries to capture all ground truth boxes with random guesses of predicted boxes How can I force the model to consider background as well,,"howard0su,yajiedesign",2016-12-21 13:26:56,2017-09-29 15:40:21
IS,Can I speak chinese here,,,"tornadomeet,ysh329,leezu,yajiedesign",2017-03-17 01:58:11,2017-09-29 15:40:24
IS,Linear Regression Model Comparison Btwn TF Theano Caffe MXNet,Background Currently I'm running some linear regression tests for a non thesis paper I'm still naive in the ML realm so I'm relatively sure I may have misinterpreted something I would like to best represent each framework appropriately to ensure a consistent evaluation approach I have managed to get a simple example using airfoil self noise data Self Noise The specific data file I use for my standard pandas read in format is at airfoil self noise csv Inside the script you will notice that all inputs are pushed into a z score format and this is done across all 4 frameworks For Tensorflow since it is iterating and does not allow an 'num epoch' variable I assumed the following would work 5000 Epochs Iterations Epochs Samples Batch Size 5000 Epochs 58710 Iterations Model Used The Framework Set up is as follows MLP 3 Layers 5 Inputs 1 Output Layer 1 25 Hidden ReLu Activation Layer 2 5 Hidden ReLu Activation Layer 3 Final Output No Activation Batch Size 128 Learning Rate 0 001 Num Epochs 5000 Optimizer Adam Metric Mean Squared Error Train Test Split 80 20 I have highlighted my network creations for each of the four frameworks below Theano L20 L42 TF L69 L104 MXNet L10 L29 and Caffe L101 L113 The Issue As for the specific question in mind I'm seeing relatively different R 2 values specifically for MXNet Comparing my Predicted versus ideal values inside excel R 2 for MXNet 0 6463 R 2 for TF 0 8797 R 2 for Theano 0 845 R 2 for Caffe 0 5979 If anyone could point out issues with the MXNet code that would be highly appreciative Caffe seems to act the same as MXNet So maybe this is not an issue Cheers Louis Environment info Operating System Windows Docker Containers Package used Python R Scala Julia Python MXNet version Python version and distribution Python 2 7 6,,"sxjscience,yajiedesign",2017-03-18 21:02:59,2017-09-29 15:40:27
IS,Linux Install MXNet,In Non root user after I configured the config mk then make j12 But I get the following class of error In file included from home jlu guan gcc 4 9 4 gcc compile include c 4 9 4 random 42 0 from home jlu guan gcc 4 9 4 gcc compile include c 4 9 4 bits stl algo h 66 from home jlu guan gcc 4 9 4 gcc compile include c 4 9 4 algorithm 62 from home jlu guan MXNet dmlc core include dmlc json h 15 from src operator mkl operator common h 11 from src operator mkl mkl memory cc 22 home jlu guan gcc 4 9 4 gcc compile include c 4 9 4 limits 1558 7 internal compiler error Illegal instruction min GLIBCXX USE NOEXCEPT return FLT MIN In file included from home jlu guan MXNet dmlc core include dmlc parameter h 12 0 from home jlu guan MXNet dmlc core include dmlc registry h 14 from include mxnet operator util h 18 from src operator tensor broadcast reduce op h 9 from src operator tensor broadcast reduce op value cc 6 home jlu guan gcc 4 9 4 gcc compile include c 4 9 4 limits 1558 7 internal compiler error Illegal instruction min GLIBCXX USE NOEXCEPT return FLT MIN Please submit a full bug report with preprocessed source if appropriate Please include the complete backtrace with any bug report See for instructions I have no idea what it means What is to be done about this,,yajiedesign,2017-03-19 15:36:05,2017-09-29 15:40:30
IS,When I run ssd example on cpu computer I get Check failed temp size num negative 0 vs 3 error error,Environment info Operating System centos 7 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 install mxnet by offical instruction 2 download dataset by ssd readme text 3 cd home q mxnet example ssd and run python train py What have you tried to solve it I searched by google and someone faced this error also but nobody share their solutions I'm a newer for deep learn and mxnet only have cpu machine maybe I should buy a computer with N card Thanks for your reply,,yajiedesign,2017-03-16 12:03:55,2017-09-29 15:40:33
IS,Distributed deep learning problem on yarn,,,"qiyuangong,qiyuangong,formath,formath,qiyuangong,yajiedesign",2017-02-27 02:46:55,2017-09-29 15:40:40
IS,Installation problem on Mac 10 11 6 El Capitan,Have anyone been successful compiling and installing mxnet on Mac 10 11 6 El Capitan OSX using CUDA 8 0 libraries Been facing a lot of issues 1 openmp not supported tried installing latest clang and it results in nvcc error saying 40000 not supported 2 changed clang to gcc4 8 and 6 2 and nvcc complains nvcc fatal GNU C C compiler is no longer supported as a host compiler on Mac OS X Appreciate any advice Patrick,,yajiedesign,2017-03-20 06:36:08,2017-09-29 15:40:43
IS,Distributed Training How does mxnet worker and ps find each other,For example if I run the instruction below from the link above Which code or API could explain the mechanism about how the machines find each other It seems that they just run in SSH way tools launch py n 2 launcher ssh H hosts python train mnist py network lenet kv store dist sync Thanks,,"formath,formath,formath,formath,formath,qiyuangong,qiyuangong,yajiedesign",2017-02-03 08:03:56,2017-09-29 15:40:47
IS,CUDA unknown error,Hey guys I am trying to use the mxnet lib in R and python using my old laptop OS Ubuntu 14 04LTE Graphics lspci grep VGA The error is Start training with 1 devices 14 38 03 home wilfried mxnet dmlc core include dmlc logging h 300 14 38 03 src storage storage cc 37 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Stack trace returned 10 entries bt 0 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f0c8419d5dc bt 1 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so ZN5mxnet11StorageImpl14ActivateDeviceENS 7ContextE 0x2f3 0x7f0c84d8f9c3 bt 2 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so ZN5mxnet11StorageImpl5AllocEmNS 7ContextE 0x4a 0x7f0c84d8beba bt 3 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so MXNDArrayCreate 0x58d 0x7f0c849ef63d bt 4 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs mxnet so ZN5mxnet1R7NDArray5EmptyERKN4Rcpp9DimensionERKNS2 6VectorILi19ENS2 15PreserveStorageEEE 0xdd 0x7f0c67e78d4d bt 5 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs mxnet so ZN4Rcpp12CppFunction2INS 4XPtrIN5mxnet1R6NDBlobENS 15PreserveStorageEXadL ZNS 25standard delete finalizerIS4 EEvPT EELb0EEERKNS 9DimensionERKNS 6VectorILi19ES5 EEEclEPP7SEXPREC 0xd2 0x7f0c67e856e2 bt 6 home wilfried R x86 64 pc linux gnu library 3 3 Rcpp libs Rcpp so Z23InternalFunction invokeP7SEXPREC 0xd1 0x7f0c91703d51 bt 7 usr lib R lib libR so 0xce3c1 0x7f0cb3ae03c1 bt 8 usr lib R lib libR so Rf eval 0x6fb 0x7f0cb3b255ab bt 9 usr lib R lib libR so 0x1158b0 0x7f0cb3b278b0 Fehler in mx nd internal empty array shape ctx 14 38 03 src storage storage cc 37 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Stack trace returned 10 entries bt 0 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f0c8419d5dc bt 1 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so ZN5mxnet11StorageImpl14ActivateDeviceENS 7ContextE 0x2f3 0x7f0c84d8f9c3 bt 2 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so ZN5mxnet11StorageImpl5AllocEmNS 7ContextE 0x4a 0x7f0c84d8beba bt 3 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs libmxnet so MXNDArrayCreate 0x58d 0x7f0c849ef63d bt 4 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs mxnet so ZN5mxnet1R7NDArray5EmptyERKN4Rcpp9DimensionERKNS2 6VectorILi19ENS2 15PreserveStorageEEE 0xdd 0x7f0c67e78d4d bt 5 home wilfried R x86 64 pc linux gnu library 3 3 mxnet libs mxnet so ZN4Rc Unfortunately I am right now out of ideas I already checked all paths and permissions I installed cuda to usr local cuda bin Any ideas All the best from Austria Will,,yajiedesign,2017-03-20 13:44:16,2017-09-29 15:40:50
IS,How to see C function invocations behind the SWIG interface mxnet,I need to do some system integration in C has to be Given the above caffe code What is the python wrapper used by mxnet swig Similarly it will be interesting to know whether I can do the same from python to c in mxnet or other python binding code if it is easier to do it than caffe Here is a sister problem in tensorflow 4 7945,,yajiedesign,2017-03-18 01:32:59,2017-09-29 15:40:54
IS,Segfault when changing the name of a variable other madness,There are some serious issues with how the names of variables are implemented 1 Names are attributes 2 Names can be manually set e g a set attr name a 3 Names do not show up in the attributes returned by list attr I understand that having the name appear as an attribute makes the recursive call attr dict a little bit less pretty but otherwise anyone trying to understand what these functions do will be confused For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"zackchase,yajiedesign",2017-03-20 22:36:09,2017-09-29 15:40:57
IS,set attr needs to be public or list attr needs to be private,Right now set attr is a private function But list attr is public So basically if the user can not set the attributes then list attr will always return They should either both be public or both be private in symbol py For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"zackchase,piiswrong,tqchen,piiswrong,yajiedesign",2017-03-20 20:43:51,2017-09-29 15:41:00
IS,Where is the definition of the mx symbol Convolution,Like mx symbol Convolution mx symbol Pooling etc Where is the definition of those operators I can not find it please help Thanks,,"luoyetx,Godricly,ysh329,yajiedesign",2017-03-01 01:11:48,2017-09-29 15:41:07
IS,question about random crop in ImageRecordIter,If I set this rand crop augmentation parameter in ImageRecordIter to be true does that mean the ImageRecordIter will always crop a random part from each of my images and feed it to my model I also notice there are two parameters called max crop size and min crop size I believe that these two parameters will ensure the sizes of the random cropped parts are in min crop size max crop size But what does the default setting mean Does it mean it will crop any size that is smaller than the original if I do not set max crop size and min crop size For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-03-21 18:33:59,2017-09-29 15:41:11
IS,error 'class nnvm Symbol' has no member named 'GetChildren',src c api c api symbolic cc 156 44 error 'class nnvm Symbol' has no member named 'GetChildren' I did get pull in my mxnet folder and then bash install mxnet ubuntu python sh environment Linux ubuntu1604 4 4 0 64 generic 85 Ubuntu SMP Mon Feb 20 11 50 30 UTC 2017 x86 64 x86 64 x86 64 GNU Linux g Ubuntu 5 4 0 6ubuntu1 16 04 4 5 4 0 20160609 Copyright C 2015 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE gcc Ubuntu 5 4 0 6ubuntu1 16 04 4 5 4 0 20160609 Copyright C 2015 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE usr local cuda 8 0 bin nvcc nvcc NVIDIA R Cuda compiler driver Copyright c 2005 2016 NVIDIA Corporation Built on Tue Jan 10 13 22 03 CST 2017 Cuda compilation tools release 8 0 V8 0 61 cudnn define CUDNN MAJOR 5 define CUDNN MINOR 1 define CUDNN PATCHLEVEL 5 define CUDNN VERSION CUDNN MAJOR 1000 CUDNN MINOR 100 CUDNN PATCHLEVEL include driver types h pkg config modversion opencv 2 4 9 1 MXNet version eeeab1c4c617defe2ba5b7ba4aa8588d9a7f98db python V Python 2 7 12 Tue Feb 28 08 27 50 2017 NVIDIA SMI 375 39 Driver Version 375 39,,"eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,yajiedesign",2017-02-28 13:26:50,2017-09-29 15:41:14
IS,How to dynamically set k in mxnet symbol topk,I have a setting in which I want to use the mxnet symbol topk operator and and get the top k largest elements in a vector during the feedforward calculation The problem is it is not known to me what the k should be in advance k is generated as the result of a prior operator in the symbolic network Current specification of the topk operator takes k as an hardcoded parameter in the compile time Is there a workaround to this such that I can give k dynamically,,"sxjscience,sxjscience,yajiedesign",2017-03-20 14:33:23,2017-09-29 15:41:18
IS,mxnet how to make prediction using a trained RNN model,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Centos 6 6 Compiler gcc 4 8 5 Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 Anaconda 4 2 0 64 bit If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Traceback most recent call last File lstm bucketing py line 138 in module x model predict eval data data val num batch 10 File home mypath software try mxnet mxnet python mxnet module base module py line 342 in predict outputs out 0 out shape 0 pad copy for out in self get outputs TypeError unsupported operand type s for 'long' and 'NoneType' Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import numpy as np import mxnet as mx import argparse parser argparse ArgumentParser description Train RNN on Penn Tree Bank formatter class argparse ArgumentDefaultsHelpFormatter parser add argument ' num layers' type int default 2 help 'number of stacked RNN layers' parser add argument ' num hidden' type int default 200 help 'hidden layer size' parser add argument ' num embed' type int default 200 help 'embedding layer size' parser add argument ' gpus' type str help 'list of gpus to run e g 0 or 0 2 5 empty means using cpu ' 'Increase batch size when using multiple gpus for best performance ' parser add argument ' kv store' type str default wouldevice' help 'key value store type' parser add argument ' num epochs' type int default 25 help 'max num of epochs' parser add argument ' lr' type float default 0 01 help 'initial learning rate' parser add argument ' optimizer' type str default isgd' help 'the optimizer type' parser add argument ' mom' type float default 0 0 help 'momentum for sgd' parser add argument ' wd' type float default 0 00001 help 'weight decay for sgd' parser add argument ' batch size' type int default 32 help 'the batch size ' parser add argument ' disp batches' type int default 50 help ishow progress for every n batches' def tokenize text fname vocab None invalid label 1 start label 0 lines open fname readlines lines filter None i split ' ' for i in lines sentences vocab mx rnn encode sentences lines vocab vocab invalid label invalid label start label start label num sent len sentences num vocab len vocab return sentences vocab if name 'main' import logging head ' asctime 15s message s' logging basicConfig level logging DEBUG format head args parser parse args buckets buckets 10 20 30 40 50 60 start label 1 invalid label 0 train sent vocab tokenize text data ptb train txt start label start label invalid label invalid label val sent tokenize text data ptb test txt vocab vocab start label start label invalid label invalid label data train mx rnn BucketSentenceIter train sent args batch size buckets buckets invalid label invalid label data val mx rnn BucketSentenceIter val sent args batch size buckets buckets invalid label invalid label stack mx rnn SequentialRNNCell for i in range args num layers stack add mx rnn LSTMCell num hidden args num hidden prefix 'lstm l d ' i def sym gen seq len data mx sym Variable wouldata' label mx sym Variable isoftmax label' embed mx sym Embedding data data input dim len vocab output dim args num embed name 'embed' stack reset outputs states stack unroll seq len inputs embed merge outputs True pred mx sym Reshape outputs shape 1 args num hidden pred mx sym FullyConnected data pred num hidden len vocab name 'pred' label mx sym Reshape label shape 1 pred mx sym SoftmaxOutput data pred label label name isoftmax' return pred wouldata' isoftmax label' if args gpus contexts mx gpu int i for i in args gpus split ' ' else contexts mx cpu 0 model mx mod BucketingModule sym gen sym gen default bucket key data train default bucket key context contexts model fit train data data train eval data data val eval metric mx metric Perplexity invalid label kvstore args kv store optimizer args optimizer optimizer params 'learning rate' args lr 'momentum' args mom 'wd' args wd initializer mx init Xavier factor type in magnitude 2 34 num epoch args num epochs batch end callback mx callback Speedometer args batch size args disp batches epoch end callback mx callback do checkpoint obama x model predict eval data data val num batch 10 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python lstm bucketing py num epochs 1 2 3 What have you tried to solve it 1 I tried the example of RNN located here The training finishes successfully and I hope to use it to do next word prediction 2 I just read the source code for base module py and find that it has a function predict which can be used to make prediction Therefore I tried to call the predict function after the training So I run x model predict eval data data val num batch 10 But error happens and it says outputs out 0 out shape 0 pad copy for out in self get outputs TypeError unsupported operand type s for 'long' and 'NoneType' What is wrong How to use a trained RNN to make predictions BTW the tutorial for character prediction does not work When I run it I received error saying XXX function has been deprecated and then the process terminated Thank you all for helping me,,"FCInter,Cloudyrie,Cloudyrie,yajiedesign",2017-03-08 16:48:29,2017-09-29 15:41:21
IS,python example gan dcgan py error,Operating System ubuntu 16 04 I just operated the example And i turn down the batch size the error remains train iter mx io NDArrayIter X train batch size batch size Pdb continue terminate called after throwing an instance of istd bad alloc' what std bad alloc the error comes out after loading data My gpu is GTX1060 6G Who can help me,,"ysh329,yajiedesign",2016-12-20 04:34:13,2017-09-29 15:41:25
IS,support Executor for online inference,piiswrong haibin lin Hi does mxnet has any plan to develop an executor for online inference Current executor needs to bind for every batch when input varies like fully cnn which will impact the efficiency Adding an executor with extra infer shape pass in excutor is forward function will eliminate the bind process,,"Godricly,eric-haibin-lin,kevinthesun,piiswrong,ZihengJiang,yajiedesign",2017-03-22 09:12:28,2017-09-29 15:41:41
IS,Getting Nan and inf in the output of MLP,Hi all I am using EC2 Ubuntu and python When I ran the train mnist py on distributed environment 3 workers and 1 server I got the nan and inf as output from activation layer and fully connected layer I show a sample of my output below I hope someone has an answer INFO root Batch 1131 relu1 backward data nan INFO root Batch 1131 fc1 backward data nan INFO root Batch 1131 fc3 backward weight nan INFO root Batch 1131 fc1 backward bias nan INFO root Batch 1131 fc3 backward bias nan INFO root Batch 1131 data 0 32695 INFO root Batch 1131 fc1 backward weight nan INFO root Batch 1131 relu2 backward data nan INFO root Batch 1131 fc1 backward bias nan INFO root Batch 1131 fc1 weight nan INFO root Batch 1131 fc2 backward data nan INFO root Batch 1131 data 0 334415 INFO root Batch 1131 fc1 bias nan INFO root Batch 1131 fc2 backward weight nan INFO root Batch 1131 fc2 weight nan INFO root Batch 1131 fc1 weight nan INFO root Batch 1131 fc2 backward bias nan INFO root Batch 1131 fc1 bias nan INFO root Batch 1131 fc2 bias nan INFO root Batch 1131 relu1 backward data nan INFO root Batch 1131 fc3 weight nan INFO root Batch 1131 fc2 weight nan INFO root Batch 1131 fc1 backward data nan INFO root Batch 1131 fc3 bias nan INFO root Batch 1131 fc2 bias nan INFO root Batch 1131 fc1 backward weight nan INFO root Batch 1131 softmax label 5 30585 INFO root Batch 1131 fc1 backward bias nan INFO root Batch 1131 fc3 weight nan INFO root Batch 1131 data 0 337408 INFO root Batch 1131 fc3 bias nan INFO root Batch 1131 fc1 weight nan INFO root Batch 1131 softmax label 5 24767 INFO root Batch 1131 fc1 bias nan INFO root Batch 1131 fc2 weight nan INFO root Batch 1131 fc2 bias nan INFO root Batch 1131 fc3 weight nan INFO root Batch 1131 fc3 bias nan INFO root Batch 1131 softmax label 5 40555,,"ysh329,yajiedesign",2017-03-21 15:16:12,2017-09-29 15:41:45
IS,BatchNorm is not support for 3D only support 2D,1,,yajiedesign,2017-03-23 10:10:45,2017-09-29 15:41:48
IS,Multi label output with Softmax,Is it possible to make a Softmax prob output for a dataset of the following structure data shape 1 3 Car model Color Manufacturer label shape 1 5 Part1 Part2 Part3 Part4 Part5 So I want to have the probability of a part given a car model That is a vector of probabilities of each part given some features of a car That is a the output of the network should be a matrix with size datapoints X width of label shape This is the code so far I'm getting dimension issues const batch size 5 data provider mx CSVIter data csv CSV DATA PATH data shape 1 3 label csv CSV LABEL PATH label shape 1 5 batch size batch size eval provider mx CSVIter data csv CSV EVAL DATA PATH data shape 1 3 label csv CSV EVAL LABEL PATH label shape 1 5 batch size batch size data mx Variable data label mx Variable softmax label Network design fc1 mx FullyConnected data name fc1 num hidden 50 bn1 mx BatchNorm fc1 act1 mx Activation bn1 name relu1 act type relu fc2 mx FullyConnected act1 name fc2 num hidden 5 softmax mx SoftmaxOutput fc2 label name softmax model mx FeedForward softmax context mx cpu optimizer mx SGD lr learning rate momentum momentum weight decay weight decay mx fit model optimizer data provider n epoch N EPOCH eval data eval provider initializer mx UniformInitializer Output is something like MXNet mx MXError Error in operator softmax Shape inconsistent Provided 5 5418 1 inferred shape 5 I also tried Reshape fc2 mx FullyConnected act1 name fc2 num hidden 100 shape mx Reshape fc2 shape batch size 5 3 softmax mx SoftmaxOutput shape label name softmax And the output logging h 300 14 43 57 src operator tensor matrix op inl h 137 Check failed oshape Size dshape Size 75012210 vs 27090 Target shape size is different to source Target 2769 5418 5 Source 5 5418 nvm the actual shape output,,yajiedesign,2017-03-21 14:51:00,2017-09-29 15:41:52
IS,MxNet using GPU cluster,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Redhat Linux Compiler gcc Package used Python R Scala Julia python 2 7 11 MXNet version Or if installed from source installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 11 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace mxnet amalik node02 image classification tools launch py n 2 launcher ssh H hosts python train mnist py network lenet kv store dist sync Traceback most recent call last File train mnist py line 5 in module import argparse ImportError No module named argparse Traceback most recent call last File train mnist py line 5 in module Traceback most recent call last File train mnist py line 5 in module Traceback most recent call last File train mnist py line 5 in module import argparse import argparse import argparse ImportError No module named argparse ImportError No module named argparse ImportError No module named argparse Exception in thread Thread 4 Traceback most recent call last File software anaconda2 envs mxnet lib python2 7 threading py line 801 in bootstrap inner self run File software anaconda2 envs mxnet lib python2 7 threading py line 754 in run self target self args self kwargs File lfs1 home amalik mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File software anaconda2 envs mxnet lib python2 7 subprocess py line 186 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no node02 p 22 'export LD LIBRARY PATH software cuda 7 5 lib64 software cuda 7 5 lib software gcc 6 1 0 libexec software gcc 6 1 0 lib64 software gcc 6 1 0 lib usr lib64 software anaconda2 lib software openblas lib software opencv lib export DMLC SERVER ID 1 export DMLC WORKER ID 0 export DMLC PS ROOT URI 10 20 19 2 export DMLC ROLE worker export DMLC PS ROOT PORT 9091 export DMLC NUM WORKER 2 export DMLC NUM SERVER 2 cd lfs1 home amalik mxnet example image classification python train mnist py network lenet kv store dist sync'' returned non zero exit status 1 Exception in thread Thread 3 Traceback most recent call last File software anaconda2 envs mxnet lib python2 7 threading py line 801 in bootstrap inner self run File software anaconda2 envs mxnet lib python2 7 threading py line 754 in run self target self args self kwargs File lfs1 home amalik mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File software anaconda2 envs mxnet lib python2 7 subprocess py line 186 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no node03 p 22 'export LD LIBRARY PATH software cuda 7 5 lib64 software cuda 7 5 lib software gcc 6 1 0 libexec software gcc 6 1 0 lib64 software gcc 6 1 0 lib usr lib64 software anaconda2 lib software openblas lib software opencv lib export DMLC SERVER ID 1 export DMLC PS ROOT URI 10 20 19 2 export DMLC ROLE server export DMLC PS ROOT PORT 9091 export DMLC NUM WORKER 2 export DMLC NUM SERVER 2 cd lfs1 home amalik mxnet example image classification python train mnist py network lenet kv store dist sync'' returned non zero exit status 1 Exception in thread Thread 2 Traceback most recent call last File software anaconda2 envs mxnet lib python2 7 threading py line 801 in bootstrap inner self run File software anaconda2 envs mxnet lib python2 7 threading py line 754 in run self target self args self kwargs File lfs1 home amalik mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File software anaconda2 envs mxnet lib python2 7 subprocess py line 186 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no node02 p 22 'export LD LIBRARY PATH software cuda 7 5 lib64 software cuda 7 5 lib software gcc 6 1 0 libexec software gcc 6 1 0 lib64 software gcc 6 1 0 lib usr lib64 software anaconda2 lib software openblas lib software opencv lib export DMLC SERVER ID 0 export DMLC PS ROOT URI 10 20 19 2 export DMLC ROLE server export DMLC PS ROOT PORT 9091 export DMLC NUM WORKER 2 export DMLC NUM SERVER 2 cd lfs1 home amalik mxnet example image classification python train mnist py network lenet kv store dist sync'' returned non zero exit status 1 Exception in thread Thread 5 Traceback most recent call last File software anaconda2 envs mxnet lib python2 7 threading py line 801 in bootstrap inner self run File software anaconda2 envs mxnet lib python2 7 threading py line 754 in run self target self args self kwargs File lfs1 home amalik mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File software anaconda2 envs mxnet lib python2 7 subprocess py line 186 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no node03 p 22 'export LD LIBRARY PATH software cuda 7 5 lib64 software cuda 7 5 lib software gcc 6 1 0 libexec software gcc 6 1 0 lib64 software gcc 6 1 0 lib usr lib64 software anaconda2 lib software openblas lib software opencv lib export DMLC SERVER ID 1 export DMLC WORKER ID 1 export DMLC PS ROOT URI 10 20 19 2 export DMLC ROLE worker export DMLC PS ROOT PORT 9091 export DMLC NUM WORKER 2 export DMLC NUM SERVER 2 cd lfs1 home amalik mxnet example image classification python train mnist py network lenet kv store dist sync'' returned non zero exit status 1 13 13 53 lfs1 home amalik mxnet dmlc core include dmlc logging h 300 13 13 53 src kvstore kvstore cc 37 compile with USE DIST KVSTORE 1 to use dist sync Stack trace returned 10 entries bt 0 home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x2b2c9c489769 bt 1 home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet7KVStore6CreateEPKc 0x681 0x2b2c9ce3eec1 bt 2 home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so MXKVStoreCreate 0xd 0x2b2c9cd4006d bt 3 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so ffi call unix64 0x4c 0x2b2c938bc57c bt 4 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so ffi call 0x1f5 0x2b2c938bbcd5 bt 5 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so ctypes callproc 0x3e6 0x2b2c938b3376 bt 6 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so 0x9db3 0x2b2c938aadb3 bt 7 lfs1 software anaconda2 envs mxnet bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x2b2c8b813e93 bt 8 lfs1 software anaconda2 envs mxnet bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x715d 0x2b2c8b8c680d bt 9 lfs1 software anaconda2 envs mxnet bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x2b2c8b8c8c3e Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File lfs1 home amalik mxnet example image classification common fit py line 97 in fit kv mx kvstore create args kv store File home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet kvstore py line 403 in create ctypes byref handle File home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet base py line 77 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 13 13 53 src kvstore kvstore cc 37 compile with USE DIST KVSTORE 1 to use dist sync Stack trace returned 10 entries bt 0 home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x2b2c9c489769 bt 1 home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet7KVStore6CreateEPKc 0x681 0x2b2c9ce3eec1 bt 2 home amalik local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so MXKVStoreCreate 0xd 0x2b2c9cd4006d bt 3 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so ffi call unix64 0x4c 0x2b2c938bc57c bt 4 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so ffi call 0x1f5 0x2b2c938bbcd5 bt 5 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so ctypes callproc 0x3e6 0x2b2c938b3376 bt 6 software anaconda2 envs mxnet lib python2 7 lib dynload ctypes so 0x9db3 0x2b2c938aadb3 bt 7 lfs1 software anaconda2 envs mxnet bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x2b2c8b813e93 bt 8 lfs1 software anaconda2 envs mxnet bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x715d 0x2b2c8b8c680d bt 9 lfs1 software anaconda2 envs mxnet bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x2b2c8b8c8c3e Exception in thread Thread 1 Traceback most recent call last File software anaconda2 envs mxnet lib python2 7 threading py line 801 in bootstrap inner self run File software anaconda2 envs mxnet lib python2 7 threading py line 754 in run self target self args self kwargs File lfs1 home amalik mxnet tools dmlc core tracker dmlc tracker tracker py line 363 in lambda target lambda subprocess check call self cmd env env shell True args File software anaconda2 envs mxnet lib python2 7 subprocess py line 186 in check call raise CalledProcessError retcode cmd CalledProcessError Command 'python train mnist py network lenet kv store dist sync' returned non zero exit status 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"piiswrong,mli,mli,yajiedesign",2017-03-07 18:16:24,2017-09-29 15:41:55
IS,Consideration on applying mask on RNN,at time step t the lstm network receive previous cell c t 1 and hidden state h t 1 and other inputs then it computes c t and h t suppose mask 0 out of sequence keep previous cell and hidden state we can perform c t 1 1 mask c t mask h t 1 1 mask h t mask then we can get what we want the same formulation can be applied when mask 1 does this operation slow,,yajiedesign,2017-03-23 17:00:35,2017-09-29 15:42:00
IS,How to truncate backpropagation in LSTM,I want to use LSTM to implement a classification task on 20 news group dataset as described in the paper Semi Supervised Sequence Learning the authors truncated backpropagation up to 400 timesteps from the end of the sequence and clipped the cell outputs and gradient see section 4 How can I implement this feature using MXNet,,"piiswrong,yajiedesign",2017-03-23 07:33:48,2017-09-29 15:42:04
IS,Any value in mx model FeedForward create array batch size yields to Check failed from shape to shape operands shape mismatchfrom shape 20 1 to shape 20,Environment info Operating System Linux Fedora 23 cat proc version Linux version 4 8 13 100 fc23 x86 64 mockbuild bkernel02 phx2 fedoraproject org gcc version 5 3 1 20160406 Red Hat 5 3 1 6 GCC 1 SMP Fri Dec 9 14 51 40 UTC 2016 Compiler gcc version 5 3 1 20160406 Package used Python R Scala Julia R version 3 3 1 MXNet version packageVersion mxnet 1 0 9 4 If you are using R package please provide R sessionInfo sessionInfo R version 3 3 1 2016 06 21 Platform x86 64 redhat linux gnu 64 bit Running under Fedora 23 Workstation Edition locale 1 LC CTYPE es MX UTF 8 LC NUMERIC C LC TIME es MX UTF 8 LC COLLATE es MX UTF 8 5 LC MONETARY es MX UTF 8 LC MESSAGES es MX UTF 8 LC PAPER es MX UTF 8 LC NAME C 9 LC ADDRESS C LC TELEPHONE C LC MEASUREMENT es MX UTF 8 LC IDENTIFICATION C attached base packages 1 stats graphics grDevices utils datasets methods base other attached packages 1 mxnet 0 9 4 loaded via a namespace and not attached 1 igraph 1 0 1 Rcpp 0 12 10 rstudioapi 0 6 magrittr 1 5 munsell 0 4 3 colorspace 1 3 2 7 R6 2 2 0 brew 1 0 6 stringr 1 2 0 plyr 1 8 4 dplyr 0 5 0 visNetwork 1 0 3 13 Rook 1 1 1 tools 3 3 1 grid 3 3 1 gtable 0 2 0 DBI 0 6 influenceR 0 1 0 19 DiagrammeR 0 9 0 htmltools 0 3 5 lazyeval 0 2 0 digest 0 6 12 assertthat 0 1 tibble 1 2 25 gridExtra 2 2 1 RColorBrewer 1 1 2 ggplot2 2 2 1 codetools 0 2 15 htmlwidgets 0 8 viridis 0 3 4 31 rgexf 0 15 3 stringi 1 1 3 scales 0 4 1 XML 3 98 1 5 jsonlite 1 3 Error Message Start training with 1 devices 01 23 17 root mxnet dmlc core include dmlc logging h 300 01 23 17 src ndarray ndarray cc 239 Check failed from shape to shape operands shape mismatchfrom shape 20 1 to shape 20 Stack trace returned 10 entries bt 0 usr lib64 R library mxnet libs libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7f598889a179 bt 1 usr lib64 R library mxnet libs libmxnet so ZN5mxnet10CopyFromToERKNS 7NDArrayEPS0 i 0x418 0x7f5988ffb4e8 bt 2 usr lib64 R library mxnet libs libmxnet so 0x9b373a 0x7f5988f9473a bt 3 usr lib64 R library mxnet libs libmxnet so MXImperativeInvoke 0xda2 0x7f59892c3a62 bt 4 usr lib64 R library mxnet libs mxnet so ZN5mxnet1R7NDArray10CopyFromToERKS1 PS1 0xa6 0x7f597b62bed6 bt 5 usr lib64 R library mxnet libs mxnet so ZN5mxnet1R8Executor11UpdateArrayEPKcRKN4Rcpp6VectorILi19ENS4 15PreserveStorageEEEPS7 bb 0x40a 0x7f597b5efa1a bt 6 usr lib64 R library mxnet libs mxnet so ZN4Rcpp10CppMethod3IN5mxnet1R8ExecutorEvRKNS 6VectorILi19ENS 15PreserveStorageEEEbbEclEPS3 PP7SEXPREC 0x77 0x7f597b5f8de7 bt 7 usr lib64 R library mxnet libs mxnet so ZN4Rcpp6class IN5mxnet1R8ExecutorEE11invoke voidEP7SEXPRECS6 PS6 i 0xa6 0x7f597b5fb316 bt 8 usr lib64 R library Rcpp libs Rcpp so Z22CppMethod invoke voidP7SEXPREC 0x1c1 0x7f598c1f9221 bt 9 usr lib64 R lib libR so 0xd40c1 0x7f59b7c720c1 Error in exec update arg arrays arg arrays match name skip null 01 23 17 src ndarray ndarray cc 239 Check failed from shape to shape operands shape mismatchfrom shape 20 1 to shape 20 Stack trace returned 10 entries bt 0 usr lib64 R library mxnet libs libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7f598889a179 bt 1 usr lib64 R library mxnet libs libmxnet so ZN5mxnet10CopyFromToERKNS 7NDArrayEPS0 i 0x418 0x7f5988ffb4e8 bt 2 usr lib64 R library mxnet libs libmxnet so 0x9b373a 0x7f5988f9473a bt 3 usr lib64 R library mxnet libs libmxnet so MXImperativeInvoke 0xda2 0x7f59892c3a62 bt 4 usr lib64 R library mxnet libs mxnet so ZN5mxnet1R7NDArray10CopyFromToERKS1 PS1 0xa6 0x7f597b62bed6 bt 5 usr lib64 R library mxnet libs mxnet so ZN5mxnet1R8Executor11UpdateArrayEPKcRKN4Rcpp6VectorILi19ENS4 15PreserveStorageEEEPS7 bb 0x40a 0x7f597b5efa1a bt 6 usr lib64 R library mxnet libs mxnet so ZN4Rcpp10CppMethod3IN5mxnet1R8ExecutorEvRKNS 6VectorILi19ENS 15PreserveStorageEEEbbEcl Minimum reproducible example model mx model FeedForward create NN model X trainInput y trainOutput ctx mx cpu num round 30 array batch size 20 learning rate 0 05 momentum 0 9 wd 0 00001 eval metric mx metric accuracy epoch end callback mx callback log train metric 100 01 23 17 root mxnet dmlc core include dmlc logging h 300 01 23 17 src ndarray ndarray cc 239 Check failed from shape to shape operands shape mismatchfrom shape 20 1 to shape 20 Please notice that the mismatchfrom shape 20 1 to shape 20 corresponds to the array batch size 20 Steps to reproduce usr bin env Rscript rm list ls setwd home master datasets require mxnet width 28 height 28 num channels 1 train read csv train csv test read csv test csv train data matrix train test data matrix test trainInput t train c 1 ncol train testInput test c 1 ncol test trainOutput t train ncol train testOutput test ncol test dim trainInput c width height num channels ncol trainInput data mx symbol Variable wouldata' conv 1 mx symbol Convolution data data kernel c 5 5 num filter 20 tanh 1 mx symbol Activation data conv 1 act type tanh pool 1 mx symbol Pooling data tanh 1 pool type max kernel c 2 2 stride c 2 2 conv 2 mx symbol Convolution data pool 1 kernel c 5 5 num filter 50 tanh 2 mx symbol Activation data conv 2 act type tanh pool 2 mx symbol Pooling data tanh 2 pool type max kernel c 2 2 stride c 2 2 flatData mx symbol Flatten data pool 2 fcl 1 mx symbol FullyConnected data flatData num hidden 500 tanh 3 mx symbol Activation data fcl 1 act type tanh fcl 2 mx symbol FullyConnected data tanh 3 num hidden 2 NN model mx symbol SoftmaxOutput data fcl 2 mx set seed 100 model mx model FeedForward create NN model X trainInput y trainOutput ctx mx cpu num round 30 array batch size 20 learning rate 0 05 momentum 0 9 wd 0 00001 eval metric mx metric accuracy epoch end callback mx callback log train metric 100 What have you tried to solve it No it seems that is an internal C file error Thanks,,yajiedesign,2017-03-23 07:52:46,2017-09-29 15:42:14
IS,A3C dont have acceleration effect,I run the A3C as it in the mxnet sample after fix bug with same parameters However in aws with 1cpu 32 cores I run it with 1cores 1worker 4cores 4worker 16cores 16worker I did not see any improment anyone has campare the results under different situation any advice,,"piiswrong,yajiedesign",2017-03-23 10:56:41,2017-09-29 15:42:18
IS,A issue about Distributed Training with train mnist,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 Compiler gcc version 4 8 4 Ubuntu 4 8 4 2ubuntu1 14 04 3 Package used Python R Scala Julia python MXNet version V0 7 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 6 Error Message Please paste the full error message including stack trace train mnist can be laucned in distributed cpus but it seemed it can not work normally because the programm do not use the cpu and memory and the logging message can not be seen Minimum reproducible example I use train mnist to test Distributed Training according to the guide provided at the steps include 1 There are two machines in total each of which has gpus and use ssh service However the ssh users are diffrent with these two machines 2 I installed NFS on the two machines and ensure each of them can access each other However the mxnet folder on these two machines are different I used soft link to ensure that one machine can access the mxnet folder of another machine with the same folder path of itself 3 I run the command python tools launch py n 2 launcher ssh H hosts python train mnist py network lenet kv store dist sync I run this command on machine A with a hosts file in which I wrote another machine is IP addr like user 10 46 169 115 On machine A I was required to input the ssh password of another machine Although almost every time I must input the password serveral times the programm fortunately can run finally 4 I used ps ef on these two machines and saw python train mnist py running on the two machines But it seemed that python train mnist py was blocked and can not run normally with the cpu and memory usage equalling zero In addition I also can not see the logging messages when training as local version who can tell me what is wrong,,yajiedesign,2017-03-23 07:11:09,2017-09-29 15:42:21
IS,MXNET distributed training segmentation fault,I built a cluster of 1 server and 3 workers I followed the MXnet installation guide However when I executed tools launch py n 2 H hosts sync dst dir tmp mxnet python train mnist py network lenet kv store dist sync Assertion failed data NULL size 0 src msg cpp 97 bash line 1 2902 Aborted core dumped python train mnist py network lenet kv store dist sync Exception in thread Thread 6 Traceback most recent call last File usr lib python2 7 threading py line 810 in bootstrap inner self run File usr lib python2 7 threading py line 763 in run self target self args self kwargs File home ubuntu mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File usr lib python2 7 subprocess py line 540 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no ip p 22 'export DMLC SERVER ID 2 export DMLC WORKER ID 1 export DMLC PS ROOT URI ip export DMLC ROLE worker export DMLC PS ROOT PORT 9098 export DMLC NUM WORKER 3 export DMLC NUM SERVER 3 cd tmp mxnet python train mnist py network lenet kv store dist sync'' returned non zero exit status 134 any help,,"dleen,qiyuangong,dleen,yajiedesign",2017-03-10 22:38:51,2017-09-29 15:42:25
IS,Train accuracy nan my parameter setting error,I'm training a DeepID model Datum are home yuanshuai sdcard download caltech 256 caltech 256 60 train rec and home yuanshuai sdcard download caltech 256 caltech 256 60 val rec from Code is edited based on mxnet example image classification train cifar10 py I suspect that there is a problem with the parameter settings see Key Code resulting in the division calculation is wrong Besides I also have questions below 1 What does num examples refer in parser set defaults see Key Code number of train examples or test ones 2 What does image shape refer in parser set defaults see Key Code How do I know image shape of rec data format Data is from Environment info Operating System Jetson TX1 Ubuntu16 04 LTS 64bit 4G RAM MXNet 0 94 Problem Message,,"ysh329,yajiedesign",2017-03-22 08:27:51,2017-09-29 15:42:35
IS,example ssd error Check failed op nullptr Operator MultiBoxPrior is not registered,I use commit e4f73f1f4e76397992c4b0a33c139d52b4b7af0e,,"zhreshold,yajiedesign",2017-03-24 02:00:18,2017-09-29 15:42:39
IS,SliceChannel only supports 'float32' data type,It seems like SliceChannel only supports 'float32' data type Is it really the case Since this is such a generic operator I would expect it to support every data type I can work around this problem by casting before and after SliceChannel but I hope there is a better way On a side note When throwing the data type error in include mxnet operator h 267 it should be better to also print type names 'int32' 'float64' instead of type indices 0 1 2 3 For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 5 LTS Compiler gcc Ubuntu 4 8 4 2ubuntu1 14 04 3 4 8 4 Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD dc47e06f6f05adf08050fb62b8e9d855f94422bf If you are using python package please provide Python version and distribution Python 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace,,"bikestra,piiswrong,reminisce,reminisce,yajiedesign",2017-03-21 05:08:30,2017-09-29 15:42:43
IS,Something wrong when I ran the example of image classification,I just installed mxnet in my Banana Pi and wanted to test it While I ran the example in image classification it did not work I cannot find the reason for my first try with mxnet Does someone know what happens When I ran the code without gup python train cifar10 py network resnet num layers 110 batch size 128 gpus I got ValueError invalid literal for int with base 10 ' xe2 x80 x98 xe2 x80 x99' While I ran the example python train mnist py network mlp I got IOError CRC check failed 0x6d9b6e54 0x85c4e58dL Environment info Operating System debian 8 in Banana Pi Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 7907e45 Python version and distribution 2 7 9 Error Message root armbian mxnet example image classification python train cifar10 py network resnet num layers 110 batch size 128 gpus INFO root start with arguments Namespace batch size 128 benchmark 0 data nthreads 4 data train wouldata cifar10 train rec' data val wouldata cifar10 val rec' disp batches 20 gpus ' xe2 x80 x98 xe2 x80 x99' image shape '3 28 28' kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epochs '200 250' max random aspect ratio 0 max random h 36 max random l 50 max random rotate angle 0 max random s 50 max random scale 1 max random shear ratio 0 min random scale 1 model prefix None mom 0 9 network aresnet' num classes 10 num epochs 300 num examples 50000 num layers 110 optimizer isgd' pad size 4 random crop 1 random mirror 1 rgb mean '123 68 116 779 103 939' test io 0 top k 0 wd 0 0001 22 44 45 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 train rec use 1 threads for decoding 22 44 47 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 val rec use 1 threads for decoding Traceback most recent call last File train cifar10 py line 53 in module fit fit args sym data get rec iter File root mxnet example image classification common fit py line 131 in fit mx gpu int i for i in args gpus split ' ' ValueError invalid literal for int with base 10 ' xe2 x80 x98 xe2 x80 x99' root armbian mxnet example image classification python train mnist py network mlp INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldevice' load epoch None lr 0 1 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File root mxnet example image classification common fit py line 103 in fit train val data loader args kv File train mnist py line 41 in get mnist iter 't10k labels idx1 ubyte gz' 't10k images idx3 ubyte gz' File train mnist py line 24 in read data image np fromstring fimg read dtype np uint8 reshape len label rows cols File usr lib python2 7 gzip py line 261 in read self read readsize File usr lib python2 7 gzip py line 315 in read self read eof File usr lib python2 7 gzip py line 354 in read eof hex self crc IOError CRC check failed 0x6d9b6e54 0x85c4e58dL,,"piiswrong,mli,ysh329,yajiedesign",2016-12-09 15:10:45,2017-09-29 15:42:47
IS,the synset txt in pretrained resnext 101 model is wrong,the synset txt in resnext is wrong order i generate a new synset txt by using the train lst,,"piiswrong,tornadomeet,terrychenism,tornadomeet,terrychenism,yajiedesign",2017-03-24 15:01:34,2017-09-29 15:42:51
IS,some questions about random,Hi if I have a network I hope I can get the same result every training how can I do I find just use mx random seed seed can not help it just help get the same init params for the network every training but still get different grads every training I have checked my input data iter it can get the same input data every training,,yajiedesign,2017-03-24 08:39:32,2017-09-29 15:42:56
IS,compile error fatal error c1001,when compile the source file I meet an error 3 E zhx2 mxnet master mxnet master nnvm src pass order mutation cc 114 fatal error C1001 3 f dd vctools compiler cxxfe sl p1 c trees h 884 3 3 Visual C 3 My environment is windows 7 64bit vs2013 i have installed the visual c compiler nov 2013 ctp and replace the original vs2013 files cuda7 5 cudnn v3 v4 v5 i tried all the version opencv3 0 openblas how can i solve this problem,,"piiswrong,yajiedesign",2017-01-18 01:44:04,2017-09-29 15:42:59
IS,Why the features extracted from mxnet cpp Windows API different from that extracted from mxnet Linux,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 professional Compiler Microsoft visual studio 2013 Package used Python R Scala Julia MXNet version mxnet cpp Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error MXNet Windows code include iostream include vector include string include mxnet cpp MxNetCpp h include fstream include map using namespace std using namespace mxnet cpp include opencv2 opencv hpp using namespace cv const int K 4 topK The global context change them if necessary Context global ctx kGPU 0 Context global ctx kCPU 0 class FeatureExtractor private the mean image get from the pretrained model NDArray mean img the following two maps store all the paramters need by the model map string NDArray args map map string NDArray aux map Symbol net int channel Executor executor Get the feature layer we want to extract void GetFeatureSymbol string json net use the following to check all the layers' names net Symbol Load json net GetInternals for const auto layer name net ListOutputs cout layer name endl net Symbol Load json net GetInternals global pool output Fill the trained paramters into the model a k a net executor void LoadParameters string params map string NDArray paramters NDArray Load params 0 paramters for const auto k paramters if k first substr 0 4 aux auto name k first substr 4 k first size 4 aux map name k second Copy global ctx if k first substr 0 4 arg auto name k first substr 4 k first size 4 args map name k second Copy global ctx WaitAll is need when we copy data between GPU and the main memory NDArray WaitAll void GetMeanImg string mean mean img NDArray Shape 1 channel 299 299 global ctx false mean img SyncCopyFromCPU NDArray LoadToMap mean mean img GetData 1 channel 299 299 NDArray WaitAll public FeatureExtractor prepare the model fill the pretrained parameters get the mean image channel 3 GetFeatureSymbol model Inception v3c json LoadParameters model Inception v3c 20 params GetMeanImg model meanc bin FeatureExtractor int c string json net string params string mean channel c GetFeatureSymbol json net LoadParameters params GetMeanImg mean vector float Extract NDArray data Normalize the pictures data mean img data Slice 0 1 mean img data Slice 1 2 mean img args map data data bind the excutor executor net SimpleBind global ctx args map map string NDArray map string OpReqType aux map executor Forward false print out the features auto array executor outputs 0 Copy Context kCPU 0 NDArray WaitAll vector float feat cout array Size endl 2048 for int i 0 i array Size i feat push back array At 0 i cout array At 0 i return feat read images and store them the NDArray format that MXNet cpp can handle NDArray Mat2Array string filename int channel string file name list filename std vector float array for auto t file name list cv Mat mat cv imread t resize pictures to 224 224 according to the pretrained model cv resize mat mat cv Size 299 299 for int c 0 c channel c for int i 0 i 299 i for int j 0 j 299 j array push back static cast float mat data i 299 j channel c NDArray ret Shape 1 channel 299 299 global ctx false ret SyncCopyFromCPU array data 1 channel 299 299 NDArray WaitAll return ret int main string filename testc jpg auto data Mat2Array filename 3 FeatureExtractor fe 3 inception v3c json inception v3c 20 params meanc bin vector float feat fe Extract data return 0 mxnet Linux code import mxnet as mx import logging import numpy as np import scipy io as sio import time logger logging getLogger logger setLevel logging DEBUG batch size 1 test dataiter mx io ImageRecordIter path imgrec testc rec mean img rgbd meanc bin rand crop False rand mirror False data shape 3 299 299 batch size batch size round batch False preprocess threads 1 prefix models inception v3 inception v3c num epoch 20 model mx model FeedForward load prefix num epoch ctx mx gpu internals model symbol get internals fea symbol internals global pool output feature extractor mx model FeedForward ctx mx gpu symbol fea symbol arg params model arg params aux params model aux params allow extra params True global pooling feature feature extractor predict test dataiter Unfortunately the feat has the same 2048 dim as global pooling feature but has completely different content as it making the performance much worse in Windows than on Linux Does the mxnet Windows api has bugs,,"piiswrong,yajiedesign",2016-12-09 13:36:11,2017-09-29 15:43:04
IS,Do we need to decay the learning rate when using RMSProp or Nesterov,I'm trying to train the Inception BN model on the ILSVRC12 dataset and I have some questions on the training configurations I have heard that decaying the learning rate is necessary to get a good model accuracy So how about using adaptive learning rate tuning algorithms such as RMSProp Nesterov AdaDelta or AdaGrad Do they work well in this case Also when we use those adaptive learning rate tuning algorithms do we still need to decay the learning rate Thanks,,yajiedesign,2017-03-27 18:24:54,2017-09-29 15:43:08
IS,How do I use the python training model in the background to get the predict results to the web front end in C and RNN,,,"kevinthesun,yajiedesign",2017-03-27 14:12:20,2017-09-29 15:43:11
IS,3d pooling problem,Hi all I compiled the latest mxnet with cudnn 5 0 3d conv is ok but 3d pooling is not Below is an example code The right output from 3d pooling should be no larger than 1 0 However I often get results 3 40282e 38 which seems to me that cudnn has not return calculated output Any advice,,"reminisce,yajiedesign",2017-01-05 06:34:44,2017-09-29 15:43:21
IS,Big kernel size makes cudnnPoolingForward crash,Test code Key words src operator cudnn pooling inl h 74 Check failed cudnnPoolingForward s dnn handle pooling desc alpha in desc data dptr beta out desc out dptr CUDNN STATUS SUCCESS Looks like cudnn does not support pooling with too big kernel size,,"piiswrong,reminisce,yajiedesign",2016-12-26 07:00:19,2017-09-29 15:43:24
IS,cudnn result not consistent with cpu is,I compiled mxnet with cudnn and used the example in example notebooks predict with pretrained model ipynb I printed global pooling feature sum to check the consistency betwwen cudnn and cpu The result differs significantly After removing the cudnn pooling operator two results became the same I wonder whether there are bugs in cudnn is pooling function I tested cudnn v3 and v5 and both had the same behavior Please check this,,"piiswrong,winstywang,neodooth,reminisce,yajiedesign",2016-11-02 03:19:40,2017-09-29 15:43:28
IS,How to implement early stopping,I have seen this question around but no answer has emerged By early stopping i mean stopping the training when the validation error does not decrease in a given number of epochs I ask because the validation set should be used to control overfitting and without implementing early stopping this is not possible In the attachment you can see the training curve in blue and the validation curve in red you can clearly see that the model has overfit The training error goes down while the validation error rises The plot is 1 acc just to make it easier to see the overfitting attachment,,yajiedesign,2016-11-05 23:41:10,2017-09-29 15:43:32
IS,Example kaggle ndsb1 'Namespace' object has no attribute 'model prefix',C ProgramData Anaconda2 python exe D Python code MXnet example kaggle ndsb1 train dsb py Namespace batch size 64 clip gradient 5 0 data dir wouldata48 ' gpus None kv store 'local' load epoch None log dir ' tmp ' log file None lr 0 01 lr factor 1 lr factor epoch 15 network wouldsb' num classes 121 num epochs 100 num examples 20000 save model prefix ' models sample net' 2017 03 28 09 47 44 496 Node 0 start with arguments Namespace batch size 64 clip gradient 5 0 data dir wouldata48 ' gpus None kv store 'local' load epoch None log dir ' tmp ' log file None lr 0 01 lr factor 1 lr factor epoch 15 network wouldsb' num classes 121 num epochs 100 num examples 20000 save model prefix ' models sample net' Traceback most recent call last File D Python code MXnet example kaggle ndsb1 train dsb py line 87 in module train model fit args net get iterator File D Python code MXnet example kaggle ndsb1 train model py line 29 in fit model prefix args model prefix AttributeError 'Namespace' object has no attribute 'model prefix',,yajiedesign,2017-03-28 01:59:17,2017-09-29 15:43:36
IS,example rcnn mistake KeyError 'width',Environment info Operating System Ubuntu16 04 2 Compiler gcc5 4 0 Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source yes Error Message Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 changed to 0 9 1 2 3 What have you tried to solve it 1 NO 2 3,,"piiswrong,precedenceguo,yajiedesign",2017-03-28 03:06:48,2017-09-29 15:43:39
IS,How can i adjust the sync frequency when use distributed learning,while using launcher under local mode how can i adjust the parameter sync frequency it seems each worker have one parameter sever and every sever can communicate with each other can i adjust it to 1master n worker mode,,yajiedesign,2017-03-28 09:42:27,2017-09-29 15:43:42
IS,symbol bind error,when i use test py to test my model i got the error now i have to use cpu it is too slow i train my model with train model fit Is it save model correctly in version 0 9 3 i mean is it save model with GPU format and when i use train model fit cmd shows that mxnet model FeedForward has been deprecated Please use mxnet mod Module instead now i have to use cpu it is too slow Environment info Operating System unbuntu 14 Package used Python R Scala Julia Python MXNet version 0 9 3 Python version and distribution If you are using R package please provide R sessionInfo Error Message 1 Check failed x ctx default ctx Input array is in cpu 0 while binding with ctx gpu 0 All arguments must be in global context gpu 0 unless group2ctx is specified for cross device graph 2 warning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead Minimum reproducible example i use lfw py to test i changged 105 to feed mx model FeedForward load args model prefix args epoch ctx model args feed arg params model auxs feed aux params symbol googlenet get Steps to reproduce What have you tried to solve it 1 read 5061 2 3,,yajiedesign,2017-03-24 03:41:31,2017-09-29 15:43:45
IS,running lazy evaluation code example outputs Illegal instruction,Environment info Operating System CentOS Linux release 7 2 1511 Compiler gcc version 4 8 5 20150623 Red Hat 4 8 5 4 GCC Package used Python R Scala Julia Python MXNet version mxnet cu80 0 9 3a3 CUDA version cuda 8 0 61 Nvidia driver 367 57 GPU Grid K1 Python version and distribution CPython 2 7 5 Error Message time for all computations are pushed into the backend engine 0 001749 sec Illegal instruction Minimum reproducible example the following code is copied from mxnet tutorial Steps to reproduce just run the code above What have you tried to solve it I'm not sure what is going on What maybe the problem,,"yajiedesign,eric-haibin-lin",2017-03-28 09:25:57,2017-09-29 15:43:49
IS,issue in example warpctc toy ctc py,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler make 3 81 gcc 5 4 1 cuda version v8 0 44 cudnn 5 05 Package used Python MXNet version install from source MXNet commit hash git rev parse HEAD 93d00403b9d175812fdf6c749d5c4e587fd2782f If you are using python package please provide Python version and distribution Python 3 5 2 Anaconda custom 64 bit Error Message Steps to reproduce cd example warpctc python toy ctc py What have you tried to solve it 1 change the config mk and make clean make python setup py install,,"liueryun,yajiedesign",2017-03-16 14:51:17,2017-09-29 15:43:52
IS,Any route to make auto complete of my PyCharm or any other IDE work again,Newbie to mxnet It is important to have auto complete when learning writing my own code However some of the symbols in mxnet just cannot be recoganised by PyCharm I do not know if this is an issue with PyCharm or other editors also have this problem image,,yajiedesign,2017-03-28 07:14:42,2017-09-29 15:43:56
IS,cpp package lenet example throws exception,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler gcc 5 2 Package used Python R Scala Julia cpp package MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 5918bd1008239f67c493f42a8c7d6190c8c6a5fc If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 set USE CPP PACKAGE 1 in config mk or set USE CPP PACKAGE to ON in CMakeLists txt 2 make 3 build cpp package example lenet What have you tried to solve it 1 Checked problem in debugger Stack is args map data data array Slice 0 batch size Copy ctx dev inline NDArray NDArray Slice mx uint begin mx uint end const begin 0 end 42 NDArrayHandle handle CHECK EQ MXNDArraySlice GetHandle begin end handle 0 int MXNDArraySlice NDArrayHandle handle mx uint slice begin mx uint slice end NDArrayHandle out NDArray ptr new NDArray API BEGIN ptr static cast NDArray handle Slice slice begin slice end inline NDArray Slice index t begin index t end const NDArray ret this CHECK is none NDArray is not initialized CHECK GE shape 0 end Slice end index out of range size t length shape ProdShape 1 shape ndim ret offset begin length ret shape 0 end begin return ret The CHECK GE assertion fires slice 0 1 28 28,,"cjolivier01,cjolivier01,yajiedesign",2017-03-29 21:36:08,2017-09-29 15:44:00
IS,the train accuracy is upon 0 98 while the validation accuracy is around 0 02 sometimes with version 0 9 4,it is so instable of V0 9 4,,yajiedesign,2017-03-28 10:04:41,2017-09-29 15:44:03
IS,support spark 2 1 0,In scala package spark module I have changed spark version to 2 1 0 And I tried building with make scalapkg but compilation errors ocurred like this ERROR home alm mxnet scala package spark src main scala ml dmlc mxnet spark transformer MXNet scala 34 error class MXNet needs to be abstract since method train in class Predictor of type dataset org apache spark sql Dataset ml dmlc mxnet spark transformer MXNetModelWrap is not defined INFO Note that org apache spark sql Dataset does not match org apache spark sql DataFrame their type parameters differ INFO class MXNet extends Predictor Vector MXNet MXNetModelWrap INFO ERROR home alm mxnet scala package spark src main scala ml dmlc mxnet spark transformer MXNet scala 43 error method train overrides nothing INFO Note the super classes of class MXNet contain the following non final members named train INFO protected def train dataset org apache spark sql Dataset ml dmlc mxnet spark transformer MXNetModelWrap INFO override def train dataset DataFrame MXNetModelWrap INFO ERROR two errors found Is there any plan to support spark 2 1 0 Kidong Lee,,yajiedesign,2017-03-30 01:59:03,2017-09-29 15:44:07
IS,fcn xs can not finish training task with 8s model,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu14 04 Compiler g 5 4 1 Package used Python R Scala Julia python MXNet version 0 9 3 Or if installed from source yes MXNet commit hash git rev parse HEAD 3a48185324a47d988ba6cef3bfbdcaecd30793a0 If you are using python package please provide Python version and distribution Python2 7 6 If you are using R package please provide R sessionInfo Error Message INFO root Epoch 32 Batch 1640 Speed 5 64 samples sec Train accuracy 0 761236 Error in python' double free or corruption out 0x00007f64c4000020 run fcnxs sh line 13 27997 Aborted core dumped python u fcn xs py model fcn8s prefix home andrew ExtraSpace fcn xs model pascal FCN16s VGG16 epoch 27 init type fcnxs Minimum reproducible example Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error I just use run fcnxs sh step by step following the tutorial if you are using your own code please provide a short script that reproduces the error I have finish the training task with fcn 32s and fcn 16s model after 50 epoch iteration When I train fcn 8s model after 32 epoch problem occured What have you tried to solve it I do not know what could do,,yajiedesign,2017-03-29 04:04:43,2017-09-29 15:44:10
IS,mx io ImageRecordIter confused me does this function is related to time,I use the latest mxnet code 0 11 and I want to study the usage of the mx io ImageRecordIter I use a simple image dataset which includes 16 images This small dataset is downloaded from I convert this image database as testim rec If I run the following code import mxnet as mx import matplotlib pyplot as plt import time import numpy as np def get iterators datapath batch size data shape 3 448 448 dataiter mx io ImageRecordIter path imgrec datapath batch size batch size data shape data shape return dataiter batch size 6 datapath wouldata testim rec' dataiter get iterators datapath batch size time sleep 2 dataiter reset number 0 for batch in dataiter data batch data 0 for i in range batch size plt subplot 3 batch size i 1 number batch size plt imshow data i asnumpy astype np uint8 transpose 1 2 0 number number 1 plt show The result will be figure 1 If I run the following code import mxnet as mx import matplotlib pyplot as plt import time import numpy as np def get iterators datapath batch size data shape 3 448 448 dataiter mx io ImageRecordIter path imgrec datapath batch size batch size data shape data shape return dataiter batch size 6 datapath wouldata testim rec' dataiter get iterators datapath batch size time sleep 2 dataiter reset number 0 for batch in dataiter data batch data 0 for i in range batch size plt subplot 3 batch size i 1 number batch size plt imshow data i asnumpy astype np uint8 transpose 1 2 0 number number 1 plt show The result will be figure 2 The second code only sleep two seconds before dataiter reset The two iteration results are not same Does the iterator is related to the time Or I write wrong codes,,"ptrendx,ptrendx,ptrendx,ptrendx,ptrendx,ptrendx",2017-09-02 09:05:43,2017-09-29 16:49:15
IS,GPUDeviceStorage is not used why,I found that GPUDeviceStorage is not used however CPUDeviceStorage is used why is that,,"ptrendx,ptrendx,ptrendx",2017-08-16 09:17:32,2017-09-29 16:49:20
IS,benchmark score py is broken,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 5 LTS Compiler gcc Ubuntu 4 8 4 2ubuntu1 14 04 3 4 8 4 Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 84ce29bcbbe0b17240d1976f49871eab0560238c If you are using python package please provide Python version and distribution Python 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error mxnet example image classification python benchmark score py usr lib python2 7 dist packages numpy oldnumeric init py 11 ModuleDeprecationWarning The oldnumeric module will be dropped in Numpy 1 9 warnings warn msg ModuleDeprecationWarning INFO root network alexnet INFO root device cpu 0 Traceback most recent call last File benchmark score py line 63 in module speed score network net dev d batch size b num batches 10 File benchmark score py line 27 in score sym data shape get symbol network batch size File benchmark score py line 22 in get symbol num layers num layers TypeError get symbol takes exactly 2 arguments 1 given Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Install the latest MXNet 2 Run the benchmark score py 3 What have you tried to solve it 1 2 3,,"ptrendx,ptrendx,ptrendx",2017-07-25 05:15:10,2017-09-29 16:49:24
IS,float16 has no performance improvement,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System RHEL 7 3 Compiler GCC 4 8 5 Package used Python R Scala Julia Python MXNet version 0 11 1 Or if installed from source Yes MXNet commit hash git rev parse HEAD ef16cc28635d0c1ad8889232f7461c2d41fa61c7 If you are using python package please provide Python version and distribution 2 7 5 If you are using R package please provide R sessionInfo Error Message no error message Steps to reproduce I was training resnet50 using train imagenet py in mxnet example image classification folder I compiled mxnet with CUDA 9 CUDNN 7 0 and run it on four V100 GPUs The command I used to run with the default Float32 is python train imagenet py network resnet num layers 50 gpus 0 1 2 3 num examples 1281167 num epochs 1 data train cm shared scratch database mxnet ilsvrc12 train rec data val cm shared scratch database mxnet ilsvrc12 val rec kv store device To use Float16 I just added the option dtype float16 The training speed is 1036 15 samples sec with float32 and 1034 27 samples sec for float16 So why there is no performance improvement at all What have you tried to solve it I tried nvidia branch of Caffe and got close to 2x speedup with FP16 compared to FP32 Regards Rengan,,"piiswrong,ptrendx,ptrendx,ptrendx,ptrendx,ptrendx,ptrendx,ptrendx,ptrendx",2017-09-06 20:25:14,2017-09-29 16:49:28
IS,the usage for cpu multi cores with local kv store,when use local kv store should I adjust batch to batch size worker num it seems mnist use the same batch size with lenet has no speed up at all I want to figure out how i can achieve speed up using local kv store as limited to resource 1cpu with 32 cores,,szha,2017-03-30 02:46:21,2017-09-29 16:49:49
IS,problem about kv store,1st situation 1cpu 32cores kv mx kvstore create local use launcher in example python launcher py cpus 0 n 4 2rd situation 2cpu kv mx kvstore create local use launcher in example python launcher py cpus 0 1 n 4 r will the 2 situation use kv I use the 1st for fast train but did not have effect,,szha,2017-03-30 06:50:22,2017-09-29 16:49:50
IS,NNPACK compiling error since 5519,since line 64 in file src operator pooling inl h has been modified to template typename xpu typename DType To enable NNPACK 3 lines in src oprator nnpack nnpack pooling inl h should be updated old template typename xpu typename Reducer typename DType template typename xpu typename DType old class NNPACKPoolingOp public PoolingOp xpu Reducer DType class NNPACKPoolingOp public PoolingOp xpu DType private PoolingParam param public explicit NNPACKPoolingOp PoolingParam p old PoolingOp xpu Reducer DType p PoolingOp xpu DType p this param p 1 line in src oprator pooling cc should be updated line 53 return new NNPACKPoolingOp cpu mshadow red maximum float param return new NNPACKPoolingOp cpu float param,,"reminisce,reminisce,reminisce,piiswrong,szha",2017-03-30 04:11:58,2017-09-29 16:49:52
IS,Faster R CNN decrease memory usage,I tried the Faster R CNN example with the Pascal VOC data and ran out of memory just as I expected with my GTX 770 with 2GB since the README says 4GB are needed although for Fast R CNN which is more memory intensive Will decreasing the number of training images or adjusting any other parameters help to reduce the memory usage I am only interested on training with a relatively small dataset With CNTK is Fast not faster R CNN implementation I was able to train on a 350 image dataset by decreasing the number of ROIs Is there something similiar in this implementation,,"piiswrong,piiswrong,szha",2017-03-29 19:36:42,2017-09-29 16:49:53
IS,Fix broken links on mxnet io,shows the 27 broken links on mxnet io,,"madjam,szha",2017-03-27 23:20:03,2017-09-29 16:49:54
IS,Broken Links,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Broken Link Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Broken links Click the below link 2 3 It shows 404 What have you tried to solve it 1 2 3,,szha,2017-03-29 09:14:19,2017-09-29 16:49:55
IS,BatchNorm Error Gamma parameter,Environment info Operating System Ubuntu14 04 Compiler Package used Python R Scala Julia Python MXNet version 0 9 3 release python2 7 3 If you are using R package please provide Error Message Please paste the full error message including stack trace When I get gamma parameter from model I train the gamma parameter in all layer are the same value and The features I calculate manual can only match the first feature Mxnet calculate is there exists a bugs can anyone tell me sdkk1n 6 more evidence I can refer different gamma value from the the batch node output It seems that when I load parameter and model from disks the Mxnets knows the right gamma value but when i convert the gamma to numpy something goes wrong how can i check parameter in disks I want know what is really save in disk can anyone tell me how,,"piiswrong,szha",2017-03-30 09:02:43,2017-09-29 16:49:56
IS,about Frequently Asked Questions,In faq No increase in speed when using more than one GPU or computer Check the following Does your neural network already run fast such as 1000 example sec or 10 batches sec If yes it is unlikely to speed up any further by adding more resources because of the communication overhead If neural network alreading run fast will training become slower or get larger train validation error,,"tornadomeet,szha",2017-03-31 06:47:34,2017-09-29 16:50:01
IS,Installation error local installation,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Error Message Please paste the full error message including stack trace usr bin ld cannot find lopenblas collect2 error ld returned 1 exit status make lib libmxnet so Error 1 I am trying to do a local installation What have you tried to solve it 1 Installed openblas locally set the LD LIBRARY PATH in the profile so everytime i login it is executed 2 Set the argument ADD LDFLAGS I path to library 3 execute make PREFIX path to local installation,,szha,2017-04-01 02:58:21,2017-09-29 16:50:02
IS,low speedup ratio when using multi GPUs and multi machines,Environment info Operating System centos 7 1 Cuda 7 5 MXNet version v0 9 Python versio v2 7 Error Message I am trying to use multiple GPUs and multiple machines for training an image classification task using resnet network but the performance is not as expected I have summed up some data as a few tables below multiple GPUs in a single machine a batch size per GPU 32 kv store num of gpu batch size speed speedup ratio local 1 32 95 381 1 000 local 2 64 76 490 0 802 local 4 128 142 664 1 496 b batch size per GPU 64 kv store num of gpu batch size speed speedup ratio local 1 64 98 701 1 000 local 2 128 112 307 1 138 local 4 256 217 973 2 208 2 machines c batch size per GPU 64 kv store num of gpu batch size speed speedup ratio dist sync 1 64 73 082 1 000 dist sync 2 128 138 605 1 897 dist sync 4 256 291 866 3 994 My problem 1 I think the performance and the number of GPUs is a linear relationship But when I increase the number of GPUs from 1 to 2 with kv store local as in tables a and b the performance is not improved but worse when batch size is 32 The number of GPUs from 1 to 4 the performance is slightly better 2 My result in tables b and c is inconsistent with if there are n machines and we use batch size b then dist sync behaviors equally to local with batch size n b multi devices md Minimum reproducible example single machine,,"piiswrong,mli,szha",2017-02-10 09:13:56,2017-09-29 16:50:03
IS,Faster rcnn test had error python test py gpu 0,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu14 04 Compiler Package used Python R Scala Julia MXNet version 0 94 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 Error Message Please paste the full error message including stack trace Namespace dataset 'PascalVOC' dataset path wouldata VOCdevkit' epoch 10 gpu 0 has rpn True image set '2007 test' network 'vgg' prefix 'model e2e' proposal 'rpn' root path wouldata' shuffle False thresh 0 001 vis False 'ANCHOR RATIOS' 0 5 1 2 'ANCHOR SCALES' 8 16 32 'FIXED PARAMS' 'conv1' 'conv2' 'FIXED PARAMS SHARED' 'conv1' 'conv2' 'conv3' 'conv4' 'conv5' 'IMAGE STRIDE' 0 'NUM ANCHORS' 9 'NUM CLASSES' 2 'PIXEL MEANS' array 103 939 116 779 123 68 'RCNN FEAT STRIDE' 16 'RPN FEAT STRIDE' 16 'SCALES' 600 1000 'TEST' 'BATCH IMAGES' 1 'CXX PROPOSAL' True 'HAS RPN' True 'NMS' 0 3 'PROPOSAL MIN SIZE' 16 'PROPOSAL NMS THRESH' 0 7 'PROPOSAL POST NMS TOP N' 2000 'PROPOSAL PRE NMS TOP N' 20000 'RPN MIN SIZE' 16 'RPN NMS THRESH' 0 7 'RPN POST NMS TOP N' 300 'RPN PRE NMS TOP N' 6000 'TRAIN' 'ASPECT GROUPING' True 'BATCH IMAGES' 2 'BATCH ROIS' 128 'BBOX MEANS' 0 0 0 0 0 0 0 0 'BBOX NORMALIZATION PRECOMPUTED' False 'BBOX REGRESSION THRESH' 0 5 'BBOX STDS' 0 1 0 1 0 2 0 2 'BBOX WEIGHTS' array 1 1 1 1 'BG THRESH HI' 0 5 'BG THRESH LO' 0 0 'CXX PROPOSAL' True 'END2END' False 'FG FRACTION' 0 25 'FG THRESH' 0 5 'RPN BATCH SIZE' 256 'RPN BBOX WEIGHTS' 1 0 1 0 1 0 1 0 'RPN CLOBBER POSITIVES' False 'RPN FG FRACTION' 0 5 'RPN MIN SIZE' 16 'RPN NEGATIVE OVERLAP' 0 3 'RPN NMS THRESH' 0 7 'RPN POSITIVE OVERLAP' 0 7 'RPN POSITIVE WEIGHT' 1 0 'RPN POST NMS TOP N' 2000 'RPN PRE NMS TOP N' 12000 num images 41 voc 2007 test gt roidb loaded from data cache voc 2007 test gt roidb pkl home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet module base module py 64 UserWarning Data provided by label shapes do not match names specified by label names vs 'cls prob label' warnings warn msg 15 13 05 src operator cudnn convolution inl h 55 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable 15 13 11 home root1 new software mxnet dmlc core include dmlc logging h 300 15 13 11 src operator proposal cu 496 Check failed error cudaSuccess 77 vs 0 an illegal memory access was encountered Stack trace returned 8 entries bt 0 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f832ab6264c bt 1 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op13ProposalGPUOpIN7mshadow3gpuEE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x176e 0x7f832c1e7d2e bt 2 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xf0dc9f 0x7f832b41dc9f bt 3 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x8c 0x7f832b40913c bt 4 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f832b40c650 bt 5 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7f831dedba60 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f8338d10184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f8338a3d37d 15 13 11 home root1 new software mxnet dmlc core include dmlc logging h 300 15 13 11 src engine threaded engine h 336 15 13 11 src operator proposal cu 496 Check failed error cudaSuccess 77 vs 0 an illegal memory access was encountered Stack trace returned 8 entries bt 0 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f832ab6264c bt 1 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op13ProposalGPUOpIN7mshadow3gpuEE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x176e 0x7f832c1e7d2e bt 2 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xf0dc9f 0x7f832b41dc9f bt 3 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x8c 0x7f832b40913c bt 4 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f832b40c650 bt 5 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7f831dedba60 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f8338d10184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f8338a3d37d An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f832ab6264c bt 1 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x371 0x7f832b409421 bt 2 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f832b40c650 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7f831dedba60 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f8338d10184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f8338a3d37d terminate called after throwing an instance of wouldmlc Error' what 15 13 11 src engine threaded engine h 336 15 13 11 src operator proposal cu 496 Check failed error cudaSuccess 77 vs 0 an illegal memory access was encountered Stack trace returned 8 entries bt 0 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f832ab6264c bt 1 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet2op13ProposalGPUOpIN7mshadow3gpuEE7ForwardERKNS 9OpContextERKSt6vectorINS 5TBlobESaIS9 EERKS8 INS 9OpReqTypeESaISE EESD SD 0x176e 0x7f832c1e7d2e bt 2 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so 0xf0dc9f 0x7f832b41dc9f bt 3 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x8c 0x7f832b40913c bt 4 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f832b40c650 bt 5 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7f831dedba60 bt 6 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f8338d10184 bt 7 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f8338a3d37d An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f832ab6264c bt 1 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x371 0x7f832b409421 bt 2 home root1 local lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f832b40c650 bt 3 usr lib x86 64 linux gnu libstdc so 6 0xb1a60 0x7f831dedba60 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f8338d10184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f8338a3d37d Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python test py gpu 0 2 3 What have you tried to solve it 1 run the cmd python test py gpu 0 sometime it work 2 3,,szha,2017-04-01 07:21:16,2017-09-29 16:50:05
IS,Can we use MXNET to do unsupervised learning,Can we use MXNET to do unsupervised learning such as clustering Where shall I find the examples and API Thanks,,"piiswrong,szha",2016-11-01 22:52:33,2017-09-29 16:50:06
IS,mx model FeedForward predict has something wrong,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version 0 93 0 94 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace I use iris dataset to do some tests I build some neural network based on mxnet After training I do some prediction with the test dataset it seems I get the best good performance 'ce' 'acc' 0 0025108862668275834 1 0 But I compare the original labels and the prediction only 50 of them is right And I use the trained model to predict some new samples the prediction are also very bad I am confused for these result So I made some codes in Tensorflow to do the same things The Tensorflow codes works very well Minimum reproducible example if you are using your own code please provide a short script that reproduces the error encoding utf 8 import mxnet as mx import logging import numpy as np mx random seed 1 mlp net mx sym Variable wouldata' net mx sym FullyConnected net name 'fc1' num hidden 10 net mx sym Activation net name arelu1' act type relu net mx sym FullyConnected net name 'fc2' num hidden 3 net mx sym SoftmaxOutput net name isoftmax' We use utils function in sklearn to get iris dataset in pickle from sklearn import datasets from sklearn utils import shuffle from sklearn preprocessing import StandardScaler iris datasets load iris Normalize data scalar StandardScaler scalared data scalar fit transform iris data shuffle data X y shuffle scalared data iris target X y scalared data iris target split index 90 split index2 100 batch size split index2 split index train data np concatenate X split index X split index2 axis 0 astype 'float32' train label np append y split index y split index2 val data X split index split index2 astype 'float32' val label y split index split index2 Build iterator train iter mx io NDArrayIter data train data label train label batch size batch size shuffle True val iter mx io NDArrayIter data val data label val label batch size batch size shuffle True logging basicConfig level logging INFO epoch num 20 lr 0 07 devs mx cpu mod mx model FeedForward ctx devs symbol net num epoch epoch num learning rate lr momentum 0 9 optimizer isgd' mod fit X train iter eval metric 'acc' eval data val iter print 'Finished training' import time time sleep 1 y mod predict val iter num batch 1 reset False The evaluation labels 2 1 1 0 0 1 0 0 0 2 for b in val label batch size print b print The prediction labels 0 1 2 0 1 0 0 0 1 2 Only 50 of them is right for a in y max np argmax a print max print But the score is 'ce' 'acc' 0 0025108862668275834 1 0 What is happen print 'ce' 'acc' mod score val iter 'ce' 'acc' num batch 1 mod save iris epoch epoch num def get predict result a result mod predict mx io NDArrayIter data a label None batch size 1 print ishape of predict s' result shape result for a in result max np argmax a print max The lables should be 1 0 2 1 new samples np array 5 5 2 3 4 0 1 3 5 0 3 3 1 4 0 2 5 8 2 7 5 1 1 9 5 5 2 6 4 4 1 2 dtype float My Model seems have a very good result 'ce' 'acc' 0 0024675801396369934 1 0 But it is not true The lables should be 1 0 2 1 But I got the prediction labels as 2 1 2 2 get predict result new samples Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Run the codes you can see the printing information 2 3 What have you tried to solve it 1 2 3,,szha,2017-04-01 12:25:16,2017-09-29 16:50:07
IS,Why CPU utilization is so high with only Softmax Layer and reasonable with MLP,Hi all Nowadays I am using MXNet and I noticed very strange behavior in MXNet I am using provided example on the GitHub called Train mnist py I am working on AWS environment local and distributed Machine Specification Ubuntu instance m4 xlarge Model m4 xlarge vCPU 4 Mem GiB 16 SSD Storage GB EBS only Dedicated EBS Bandwidth Mbps 750 I modified the MLP py for the following def get symbol num classes 10 kwargs data mx symbol Variable wouldata' flatten mx sym Flatten data data fc3 mx symbol FullyConnected data flatten name 'fc3' num hidden num classes mlp mx symbol SoftmaxOutput data fc3 name isoftmax' return mlp My CPU utilization for single or distributed systems is almost 80 so high comparing to other framework Then I run the same code with out modification a simple multilayer perceptron import mxnet as mx def get symbol num classes 10 kwargs data mx symbol Variable wouldata' flatten mx sym Flatten data data fc1 mx symbol FullyConnected data flatten name 'fc1' num hidden 512 act1 mx symbol Activation data fc1 name arelu1' act type relu fc2 mx symbol FullyConnected data act1 name 'fc2' num hidden 256 act2 mx symbol Activation data fc2 name arelu2' act type relu fc3 mx symbol FullyConnected data act2 name 'fc3' num hidden num classes mlp mx symbol SoftmaxOutput data fc3 name isoftmax' return mlp Here the CPU utilization is about 40 for single machine or distributed version My question is why its so high for Softmax layer alone and low with network layers softmax layer I am wondering if this is a common phenomenon in MXNet or I am having something not right Sincerely,,"piiswrong,piiswrong,szha",2017-03-31 22:56:22,2017-09-29 16:50:09
IS,Difference between CXX PROPOSAL and custom proposal in example rcnn,I do not quite know about the difference between CXX PROPOSAL and custom proposal in symbol resnet py,,szha,2017-04-01 16:16:55,2017-09-29 16:50:10
IS,Parameter Passing problem of custom operator in python,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu Compiler Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 12 When I try to pass a parameter to my operator based on CustomProp it automatically casts the value to a string type And that is one output shape that I got 10L 1 '888888888888888888888888888888888888888888888888' '8888888888888888888888888888888888888888888888888888888888888888' Hope you can update soon,,"piiswrong,szha",2017-04-01 08:53:52,2017-09-29 16:50:12
IS,Error compiling docs,Running the new command to compile docs make html I get the following error Error Message,,"zackchase,mli,szha",2017-04-01 22:23:56,2017-09-29 16:50:13
IS,Can suse11 supports mxnet,,,szha,2017-04-02 02:08:21,2017-09-29 16:50:14
IS,Python testing nosetests and if name main,As far as I understand the only supported way to run all python test cases is with nosetests Nevertheless most if not all python test cases run some tests when started as python test X py Unfortunately the two ways to start the tests are inconsistent as the if name main part of the test X py file is not guaranteed to run all tests run by nosetests For example when running python test operator gpu py L645 the method test upsampling bilinear with type L377 is not run though it is run by nosetests This is confusing and I therefore propose to change the current behaviour I think there are two possible approaches Make sure that if name main runs all relevant test cases Do not run any test cases in if name main but print an error message requesting the user to run nosetests The first option seems inferior as it requires constant care when adding new test cases while option two is a one time change What do you think,,"leezu,szha",2017-04-02 05:27:13,2017-09-29 16:50:15
IS,python train mnist py gpus 0 report error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 04 Compiler Package used Python R Scala Julia python MXNet version 0 95 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 2 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2017 04 03 22 25 47 2 3,,"piiswrong,szha",2017-04-03 14:29:17,2017-09-29 16:50:16
IS,definition conflict of slice,After I updating mxnet to version 0 9 5 it came an error it seems that there is a function named slice in mxnet conficting with python built in class slice Does anyone have the same problem as me,,"piiswrong,szha",2017-04-03 04:13:07,2017-09-29 16:50:18
IS,Amalgamation for android failed on Dropout layer,Environment Info Operating System Ubuntu 14 04 64bit Desktop Compiler arm linux androideabi clang MXNet version v0 9 3 NDK vesion android ndk r13b Error Discription error message org dmlc mxnet MxnetException value 4 14277e 314 for Parameter p exceed bound 4 14277e 314 4 14277e 314 in operator Dropout name dropout0 p 0 5 it is thrown out by these lines in file amalgamation mxnet predict all cc perhaps passed from file dmlc core include dmlc parameter h It will happen for any network carrying dropout layer including NIN and inception network from Mxnet Zoo Where does the number 4 14277e 314 comes from It does not make sense for v 4 14277e 314 in this case is not exceed the bound of 4 14277e 314 4 14277e 314 Compiling Steps My compiling process is listed in,,"piiswrong,sbodenstein,szha",2017-02-14 10:05:06,2017-09-29 16:50:19
IS,How MXNet synchronize training done,Hi all I am curious about how synchronous training done in MXNet Can anybody point me where to read Is there any kind of queue,,szha,2017-04-03 23:56:56,2017-09-29 16:50:21
IS,How to load and data non image for logistic regression,Hi all I am trying to write a logistic regression code for distributed machine I have my data label and vector of features example label features 0 00 1 0 1 1 0 1 0 002 My general structure as the following Define the network symbol equivalent to logistic regression net mx symbol Variable wouldata' net mx symbol FullyConnected data net name 'fc1' num hidden 1 net mx symbol LinearRegressionOutput data net name 'logistic regression' Load data I do not know how to do loading data for my data csv How to do iteration model mx model FeedForward create symbol net X num epoch 20 learning rate 0 1 Make prediction model predict Can anybody who familiar with MXNet give me some guidance Sincerely,,szha,2017-04-02 01:12:25,2017-09-29 16:50:22
PR,Fix faq breadcrumb,Fixed Faq breadcrumb to FAQ literal with all camel case Fixed broken links,,thinksanky,2017-09-29 04:35:02,2017-09-29 16:58:41
PR,Update the threshold for diff,piiswrong krishnamurthy Sometimes the test fails due to flaky value in assert difference eg arange FAILED 1 14497025E 5 was not less than or equal to 1 0E 5 NDArraySuite scala 173,,"gautamkmr,szha",2017-09-29 05:49:21,2017-09-29 17:21:33
PR,Perl Fix for Perl GPU CI build,,,"sergeykolychev,sergeykolychev,piiswrong,sergeykolychev",2017-09-28 23:42:27,2017-09-29 17:23:33
PR,Fix broken link in the tutorial of adding operators,,,reminisce,2017-09-29 17:22:45,2017-09-29 17:24:10
PR,upadte the python version for nosetest,rahul003,,"gautamkmr,gautamkmr,gautamkmr,gautamkmr",2017-09-28 23:22:04,2017-09-29 17:26:52
PR,variational dropout cell,This PR implements the variational dropout in RNN described in in gluon rnn API,,"szha,yzhang87,szha,yzhang87,szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,szha,szha,szha,szha,reminisce,reminisce,szha,szha,szha,szha,szha,szha",2017-07-16 21:31:22,2017-09-29 17:35:28
PR,DO NOT MERGE This is just to test the new changes made in PR 8034,,,mbaijal,2017-09-27 00:08:49,2017-09-29 17:39:28
PR,Fixed a bug in image det aug default cc,A pair of brackets is missing when computing 1 new scale new scale which would cause an arithmetic error,,"xioryu,piiswrong,zhreshold",2017-09-29 10:52:28,2017-09-29 17:39:52
PR,Fix softmax cross entropy list input names,1 Add FListInputNames attribute to softmax cross entropy operator 2 Add softmax cross entropy unit test,,"kevinthesun,geoalgo,kevinthesun,eric-haibin-lin",2017-06-20 21:31:19,2017-09-29 17:59:15
IS,Unknown error from using module predict,I am working on using a simple lenet style conv net to predict some values similar to the kaggle ndsb2 example However after I fit the model and tried to using predict a key error 0 pops out and I am not sure what it means as no information is provided As I have successfully fitted the model I attempted to run predict with the train data iterator and the same error appears so I am pretty sure it is not due to the construction of the data iterator Below is the error information KeyError Traceback most recent call last ipython input 122 06a34d3cf6f8 in module 1 con model predict eval data data val Users Sen anaconda lib python3 5 site packages mxnet 0 9 1 py3 5 macosx 10 6 x86 64 egg mxnet module base module py in predict self eval data num batch merge batches reset always output list 289 if num batch is not None and nbatch num batch 290 break 291 self forward eval batch is train False 292 pad eval batch pad 293 outputs out 0 out shape 0 pad copy for out in self get outputs Users Sen anaconda lib python3 5 site packages mxnet 0 9 1 py3 5 macosx 10 6 x86 64 egg mxnet module module py in forward self data batch is train 450 451 assert self binded and self params initialized 452 self exec group forward data batch is train 453 454 def backward self out grads None Users Sen anaconda lib python3 5 site packages mxnet 0 9 1 py3 5 macosx 10 6 x86 64 egg mxnet module executor group py in forward self data batch is train 311 312 313 load data data batch self data arrays self data layouts 314 if is train is None 315 is train self for training Users Sen anaconda lib python3 5 site packages mxnet 0 9 1 py3 5 macosx 10 6 x86 64 egg mxnet module executor group py in load data batch targets major axis 41 def load data batch targets major axis 42 Load data into sliced arrays 43 load general batch data targets major axis 44 45 Users Sen anaconda lib python3 5 site packages mxnet 0 9 1 py3 5 macosx 10 6 x86 64 egg mxnet module executor group py in load general data targets major axis 28 end axis slice idx stop 29 pylint disable no member protected access 30 if d src context d dst context 31 nd crop d src begin tuple begin end tuple end out d dst 32 else Users Sen anaconda lib python3 5 site packages mxnet 0 9 1 py3 5 macosx 10 6 x86 64 egg mxnet ndarray py in context self 462 check call LIB MXNDArrayGetContext 463 self handle ctypes byref dev typeid ctypes byref dev id 464 return Context Context devtype2str dev typeid value dev id value 465 466 KeyError 0 Environment info Operating System MacOS version 10 12 2,,"piiswrong,szha",2017-02-22 10:00:01,2017-09-29 18:18:32
IS,Amalgamation compile error for Android,Cannot build Amalgamation on my Mac 10 10 This is how I build the standalone toolchain python build tools make standalone toolchain py stl libc arch arm api 21 install dir tmp my android toolchain And I change the OPENBLAS ROOT SYS ROOT CXX CC following the instructions But when I type make ANDROID 1 to compiled I got errors like this Users echo Library Android my android toolchain include c 4 9 x cstddef 43 25 fatal error stddef h No such file or directory Then I read this thread and add this to makefile DEFS isystem Users echo Library Android my android toolchain include c 4 9 x Then the error becomes following jni mxnet predict all cc 53 36 fatal error src c api c api error cc No such file or directory I also tried Clang as I change the makefile like this export SYS ROOT Users echo Library Android my android toolchain sysroot export CXX Users echo Library Android my android toolchain bin arm linux androideabi clang export CC Users echo Library Android my android toolchain bin arm linux androideabi clang But the error remains jni mxnet predict all cc 53 10 fatal error isrc c api c api error cc' file not found Anyone can help me to compile this,,"arank,szha",2017-03-24 09:36:31,2017-09-29 18:18:33
IS,how to use two dist sync kv store in one job,I launch a distributed job using two neural network and I want both networks trained in parallel After launching error occurs Anyone can help me,,szha,2017-04-01 09:52:59,2017-09-29 18:18:34
IS,How to define a new operator to support Factorization Machine,I find it is very easy to write FM using TensorFlow but I am not familiar with mxnet how to define a new operator in mxnet to support Factorization Machine,,"formath,szha",2017-04-04 07:15:20,2017-09-29 18:18:35
IS,time series forecasting using rnn,library Quandl sh stock ex Quandl YAHOO SS 600292 type xts library xts data scale sh stock ex 1 5 feat merge na trim lag data 1 na trim lag data 2 na trim lag data 3 na trim lag data 4 na trim lag data 5 all FALSE dataset merge feat data all FALSE colnames dataset c lag 1 lag 2 lag 3 lag 4 lag 5 obj index 1 4000 training as data frame dataset index testing as data frame dataset index library mxnet train x data matrix training 6 train y training 6 test x data matrix testing 6 test y testing 6 get label function X label array 0 dim dim X d dim X 1 w dim X 2 for i in 0 w 1 for j in 1 d label i d j X i d j w d 1 return label X train label get label t train x X val label get label t test x X train list data t train x label X train label X val list data t test x label X val label batch size 5 seq len 5 num hidden 5 num embed 5 num rnn layer 1 num round 100 update period 1 learning rate 0 1 wd 0 00001 clip gradient 1 mx set seed 0 model mx rnn X train X val num rnn layer num rnn layer seq len seq len num hidden num hidden num embed num embed num label 5 batch size batch size input size 5 ctx mx cpu num round num round update period update period initializer mx init uniform 0 01 dropout 0 optimizer sgd batch norm FALSE learning rate learning rate wd wd clip gradient clip gradient It turn out an error 23 09 52 d chhong mxnet src operator reshape inl h 227 Using target shape will be deprecated 23 09 52 D chhong mxnet dmlc core include dmlc logging h 235 23 09 52 D chhong mxnet src ndarray ndarray cc 227 Check failed from shape to shape operands shape mismatch Error 23 09 52 D chhong mxnet src ndarray ndarray cc 227 Check failed from shape to shape operands shape mismatch How to understand this error message and locate the error code Is there any useful tutorial or papers about time series forecasting using rnn or lstm many thanks,,szha,2016-10-19 15:43:05,2017-09-29 18:18:37
IS,method does not work under current version 2017 1 20 of mxnet,I run python mxnet example dec dec py under version 0 7 of mxnet and can get good result of clustering accuracy 84 But when I updated mxnet to the newest version running the same code only gets accuracy of about 24 Then I reverse back to version 0 7 the accuracy can reach 84 again So it should be that something is changed in the new version resulting the failure of running dec py But I can not figure it out Did someone try this method and encounter the same issue Environment info Operating System Ubuntu 16 04 Compiler python 2 7 Package used Python R Scala Julia scikit learn MXNet version Installed from source MXNet commit hash git rev parse HEAD f2c2b52a4a00ac5c99604ecfd170066124359fe0 If you are using python package please provide Python version and distribution python 2 7 Error Message No errors but the result of running 'example dec dec py' is very very bad using the newest version of mxnet,,szha,2017-01-22 11:36:46,2017-09-29 18:18:39
IS,Resnet pre trained models,The resnet pretrained models hosted on mxnet does not come with epoch number e g resnet 18 0000 params 2016 10 21 02 15 45M It may confuse user how many epochs it was trained,,"eric-haibin-lin,szha",2017-04-06 17:44:33,2017-09-29 18:18:41
IS,scala Module API scala MatchError in updateParams while training,While training I get a scala MatchError exception in updateParams The network is built with the Module API The network is based on a pretrained VGG16 network where only the fully connected layers are replaced and trained The task is a regression Part of the layers are not to be trained listed in fixedParamNames The error arise when calling fit Environment info MXNet version latest about one week ago Error Message,,"benqua,benqua,Ldpe2G,benqua,yzhliu,benqua,benqua,yzhliu,szha",2017-04-01 12:15:44,2017-09-29 18:18:42
IS,What is bucket key in mxnet io DataBatch data label pad None index None bucket key None provide data None provide label None,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"kevinthesun,kevinthesun,szha",2017-04-05 10:32:17,2017-09-29 18:18:43
IS,gradient computation support for take and batch take,mx sym take and batch take do not seem to support gradient computation In terms of the first argument however gradients of these operators should be well defined Can the support for gradient computation of these operators be added I talked with and he said you are the most familiar with these operators Would you please take a look,,"bikestra,reminisce,bikestra,reminisce,szha",2017-03-23 22:12:28,2017-09-29 18:18:44
IS,QA What is the intention of mean image,,,szha,2017-04-07 20:53:52,2017-09-29 18:18:45
IS,gpu can not be used,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 04 Compiler Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace with mx Context mx gpu f 16 18 06 home franknan mxnet dmlc core include dmlc logging h 300 16 18 06 src c api c api ndarray cc 390 Operator ones cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered Stack trace returned 10 entries bt 0 home franknan mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f2f9b6b599c bt 1 home franknan mxnet python mxnet lib libmxnet so MXImperativeInvoke 0x8f2 0x7f2f9c29eb02 bt 2 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f2fa85ece18 bt 3 usr lib x86 64 linux gnu libffi so 6 ffi call 0x32a 0x7f2fa85ec87a bt 4 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x23f 0x7f2fa87fb3af bt 5 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11603 0x7f2fa8800603 bt 6 python PyObject Call 0x43 0x55ab249d8da3 bt 7 python PyEval EvalFrameEx 0x6092 0x55ab249f23f2 bt 8 python PyEval EvalCodeEx 0x255 0x55ab249eab85 bt 9 python PyEval EvalFrameEx 0x699d 0x55ab249f2cfd Traceback most recent call last File stdin line 2 in module File stdin line 2 in f File home franknan mxnet python mxnet ndarray py line 975 in ones return internal ones shape shape ctx ctx dtype dtype File home franknan mxnet python mxnet ctypes ndarray py line 164 in generic ndarray function c array ctypes c char p c str val for val in vals File home franknan mxnet python mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 16 18 06 src c api c api ndarray cc 390 Operator ones cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered Stack trace returned 10 entries bt 0 home franknan mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f2f9b6b599c bt 1 home franknan mxnet python mxnet lib libmxnet so MXImperativeInvoke 0x8f2 0x7f2f9c29eb02 bt 2 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f2fa85ece18 bt 3 usr lib x86 64 linux gnu libffi so 6 ffi call 0x32a 0x7f2fa85ec87a bt 4 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x23f 0x7f2fa87fb3af bt 5 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11603 0x7f2fa8800603 bt 6 python PyObject Call 0x43 0x55ab249d8da3 bt 7 python PyEval EvalFrameEx 0x6092 0x55ab249f23f2 bt 8 python PyEval EvalCodeEx 0x255 0x55ab249eab85 bt 9 python PyEval EvalFrameEx 0x699d 0x55ab249f2cfd Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 with mx Context mx gpu f 2 3 What have you tried to solve it 1 2 3,,"piiswrong,piiswrong,szha",2017-04-07 08:39:48,2017-09-29 18:18:46
IS,when I set the kvstore None Multi GPU is faster than One GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu14 04 Compiler g Package used Python R Scala Julia Python MXNet version 0 8 0 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 Problem 4 GPUs GTX1080 in one Server Multi GPUs is slower than One GPU when kvstore 'local' What have you tried to solve it when I set the kvstore None Multi GPU is faster than One GPU,,"piiswrong,piiswrong,szha",2017-04-05 12:21:43,2017-09-29 18:18:47
IS,Replace element mask,I am trying to re code the following code next c mx sym element mask next c mask name t d l d c seqidx layeridx with next c next c mask reshape mask size 1 next c shape 1 next c set attr name t d l d c seqidx layeridx I got an error AttributeError 'Symbol' object has no attribute areshape' What would be the proper way to code it Thanks,,szha,2017-04-08 20:23:44,2017-09-29 18:18:48
IS,Use multiple GPUs during predict,I am trying to extract middle layer activation maps using a trained model However when I set to 2 GPUs it always just use only 1 GPU and the other GPU is 0 From this thread it said I should set kv store to device instead of local However since I did not fit the model to new data all I used is predict function Where should I change the kv store Here are the code model load mx model FeedForward load prefix 0 ctx mx gpu 0 mx gpu 1 numpy batch size 1 layer name arelu1 2 output' arelu2 2 output' arelu3 3 output' arelu4 3 output' arelu5 3 output' arelu6 output' arelu7 output' all layers model load symbol get internals fea symbol all layers layer name 2 feature extractor mx model FeedForward ctx mx gpu 0 mx gpu 1 symbol fea symbol numpy batch size 1 arg params model load arg params aux params model load aux params allow extra params True val feature valdata vallabel feature extractor predict img return data True,,"piiswrong,szha",2017-04-09 00:43:19,2017-09-29 18:18:50
IS,cpp package OpWrapperGenerator py exists some bug,recently i try to use cpp version to write some code code and it works it seems op h is generate by OpWrapperGenerator py but there are some bug existed,,"piiswrong,lx75249,piiswrong,szha",2017-04-08 13:24:50,2017-09-29 18:18:51
IS,TypeError only integer scalar arrays can be converted to a scalar index,Im run the deep3d under mxnet latest and before the final I get this error TypeError only integer scalar arrays can be converted to a scalar index What is this How can fix it,,szha,2017-01-04 10:46:20,2017-09-29 18:18:52
IS,Creating and reading dmlc InputSplit from Python,Is it possible to use dmlc InputSplit from Python I have code Python code which packs my custome data into record files and iterators which read the packed data and convert to NDArray for training I am trying to split up the record files as described here but cannot find any uses of dmlc InputSplit for reading or writing anywhere but c Is this feature only available for code written in c,,"jmerkow,szha",2017-04-09 21:38:47,2017-09-29 18:18:53
IS,Mac Matlab 2016a Error using loadlibrary No supported compiler or SDK was found,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Mac OS X Sierra Compiler Package used Python R Scala Julia Matlab 2016a MXNet version Or if installed from source install from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-04-10 05:23:46,2017-09-29 18:18:54
IS,How can I get the value of the output of the last backword node,When mxnet process a fc network the noeds in the graph executor are just like belows 0 data 1 flatten 2 fullyconnected1 weight 3 fullyconnected1 bias 4 fullyconnected1 5 relu6 6 drop6 7 fullyconnected2 weight 8 fullyconnected2 bias 9 fullyconnected2 10 relu7 11 drop7 12 fullyconnected3 weight 13 fullyconnected3 bias 14 fullyconnected3 15 softmax label 16 softmax 17 softmax backward 18 fullyconnected3 backward 19 drop7 backward 20 relu7 backward 21 fullyconnected2 backward 22 drop6 backward 23 relu6 backward 24 fullyconnected1 backward After run the backward operations by 'RunOps true num forward nodes idx num nodes ' I can get the value of the output of the node 23 by 'op nodes 23 exec out array 0 ' But the value in 'op nodes 24 exec out array 0 ' has many 'nan',,"solin319,szha",2017-04-10 08:52:56,2017-09-29 18:18:55
IS,batch norm fails on validation batch sizes larger than 1024,I'm trying to train a network and can get my training batch size up to 1 024 000 given the small size of the inputs but validation will fail if I have a batch size greater than 1024 The large sample size is necessary because it drastically increases speed 1million samples sec vs 40k samples sec and allows it to converge faster I can not set the training size to the large number and the validation size to another number or another error is raised Here is what it looks like when I set both to 1024 and it works Is there a way that I can utilize both batch norm and large batch sizes while also using a validation set If I only have a training set they both work it only fails on the validation step Thanks,,"piiswrong,piiswrong,piiswrong,piiswrong,ap-hynninen,ap-hynninen,szha",2017-02-13 03:36:23,2017-09-29 18:18:57
IS,DataIO with mx image ImageIter file to give to path imgidx,Hello guys I have been trying to read recordIO files with mx image ImageIter using Python Python version 2 7 mxnet version 0 9 5 Windows 10 my code file read augment data py looks as follows dataiter train mx image ImageIter path imgrec imagerecords rec data shape 3 512 512 path imgidx imageindex idx batch size batch size shuffle False plot 16 images to see they are correctly loaded and check augmentation dataiter train reset batch dataiter train next data batch data 0 for i in range 16 plt subplot 4 4 i 1 plt imshow data i asnumpy astype np uint8 transpose 1 2 0 plt show The code works fine as long as shuffle False If I set set shuffle True the following error occurs Traceback most recent call last File read augment data 02 py line 84 in module batch dataiter train next File C Users hup56199 Anaconda2 envs mxnet lib site packages mxnet 0 9 5 py2 7 egg mxnet image py line 439 in next label s self next sample File C Users hup56199 Anaconda2 envs mxnet lib site packages mxnet 0 9 5 py2 7 egg mxnet image py line 412 in next sample s self imgrec read idx idx File C Users hup56199 Anaconda2 envs mxnet lib site packages mxnet 0 9 5 py2 7 egg mxnet recordio py line 156 in read idx return self read File C Users hup56199 Anaconda2 envs mxnet lib site packages mxnet 0 9 5 py2 7 egg mxnet recordio py line 93 in read ctypes byref size File C Users hup56199 Anaconda2 envs mxnet lib site packages mxnet 0 9 5 py2 7 egg mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 14 05 02 D Program Files x86 Jenkins workspace mxnet mxnet dmlc core src recordio cc 65 Check failed header 0 RecordIOWriter kMagic Invalid RecordIO File My first guess was that the index file I provide with path imgidx is not correct It has two columns first column an index and second column a label I shuffled the images bevor generating the recordio file thats why numbers in first column are not in order 456 2 123 4 45 1 Using the file name xy jpg in either column also did not work Any help would be really appreciated here Maybe it is possible to provide an example of how to use the ImageIter with shuffle True Thanks in advance,,"piiswrong,szha",2017-04-10 13:04:17,2017-09-29 18:18:58
IS,Split an existing symbol graph into 2 symbols,Say we have a symbol like AlexNet model which outputs are Now I just want to split this symbol into 2 symbols for example the first symbol bottom includes nodes from data to flatten0 output and the second symbol top ONLY includes nodes from flatten0 output to softmax output The former can be achieved by flatten alexnet get internals 'flatten0 output' but after getting node flatten0 output I cannot cut the connection between flatten0 output and previous nodes i e the second symbol top is exactly the original symbol Splitting a symbol into 2 symbols is for model parallelism usage in this way we can put the first symbol bottom into a mxnet mod Module using GPU 1 and the second symbol top into another mxnet mod Module using GPU2 This could be a method to implement model parallelism for an existing symbol but if you have another option please let me know Thanks,,"piiswrong,szha",2017-04-10 12:42:30,2017-09-29 18:18:59
IS,prediction loading an image with mx image imdecode,Hello I am trying to predict an image loading the image with the function mxnet image imdecode I used mx nd function reshape expand dims but I have an error when I try to predict I do not understand where the problem come from the reshaping seems to work I can print it little batch mx image imdecode open path image 'rb' read flag 1 to rgb 1 print little batch shape little batch mx image imresize little batch 224 224 little batch mx nd expand dims little batch axis 0 little batch mx nd reshape little batch 1 3 224 224 print little batch shape print little batch asnumpy 0 2 batch mx io DataBatch data little batch label mx nd array '1' model forward batch is train False The error is message 12 08 52 src nnvm legacy json util cc 190 Loading symbol saved by previous version v0 9 4 Attempting to upgrade 12 08 52 src nnvm legacy json util cc 198 Symbol successfully upgraded 12 09 10 src operator cudnn algoreg inl h 57 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable 12 09 12 root mxnet dmlc core include dmlc logging h 300 12 09 12 src ndarray ndarray function cu 18 Check failed to type flag from type flag 0 vs 3 Source and target must have the same data type when copying across devices Stack trace returned 9 entries bt 0 root mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f20ddc296c9 bt 1 root mxnet python mxnet lib libmxnet so ZN5mxnet7ndarray4CopyIN7mshadow3cpuENS2 3gpuEEEvRKNS 5TBlobEPS5 NS 7ContextES9 NS 10RunContextE 0xd3 0x7f20df1ca8d3 bt 2 root mxnet python mxnet lib libmxnet so 0xeb9bad 0x7f20de5b4bad bt 3 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataS1 S3 0x23 0x7f20ddc994a3 bt 4 root mxnet python mxnet lib libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x98 0x7f20de4e31f8 bt 5 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE0 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f20de4e7520 bt 6 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f20ca85fc30 bt 7 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f20fe34b184 bt 8 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f20fe07837d 12 09 12 root mxnet dmlc core include dmlc logging h 300 12 09 12 src engine threaded engine h 329 12 09 12 src ndarray ndarray function cu 18 Check failed to type flag from type flag 0 vs 3 Source and target must have the same data type when copying across devices Stack trace returned 9 entries bt 0 root mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f20ddc296c9 bt 1 root mxnet python mxnet lib libmxnet so ZN5mxnet7ndarray4CopyIN7mshadow3cpuENS2 3gpuEEEvRKNS 5TBlobEPS5 NS 7ContextES9 NS 10RunContextE 0xd3 0x7f20df1ca8d3 bt 2 root mxnet python mxnet lib libmxnet so 0xeb9bad 0x7f20de5b4bad bt 3 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataS1 S3 0x23 0x7f20ddc994a3 bt 4 root mxnet python mxnet lib libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x98 0x7f20de4e31f8 bt 5 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE0 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f20de4e7520 bt 6 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f20ca85fc30 bt 7 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f20fe34b184 bt 8 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f20fe07837d An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 root mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f20ddc296c9 bt 1 root mxnet python mxnet lib libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x4ac 0x7f20de4e360c bt 2 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE0 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f20de4e7520 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f20ca85fc30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f20fe34b184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f20fe07837d terminate called after throwing an instance of wouldmlc Error' what 12 09 12 src engine threaded engine h 329 12 09 12 src ndarray ndarray function cu 18 Check failed to type flag from type flag 0 vs 3 Source and target must have the same data type when copying across devices Stack trace returned 9 entries bt 0 root mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f20ddc296c9 bt 1 root mxnet python mxnet lib libmxnet so ZN5mxnet7ndarray4CopyIN7mshadow3cpuENS2 3gpuEEEvRKNS 5TBlobEPS5 NS 7ContextES9 NS 10RunContextE 0xd3 0x7f20df1ca8d3 bt 2 root mxnet python mxnet lib libmxnet so 0xeb9bad 0x7f20de5b4bad bt 3 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataS1 S3 0x23 0x7f20ddc994a3 bt 4 root mxnet python mxnet lib libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x98 0x7f20de4e31f8 bt 5 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE0 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f20de4e7520 bt 6 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f20ca85fc30 bt 7 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f20fe34b184 bt 8 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f20fe07837d An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging Stack trace returned 6 entries bt 0 root mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f20ddc296c9 bt 1 root mxnet python mxnet lib libmxnet so ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS 10RunContextEPNS0 8OprBlockE 0x4ac 0x7f20de4e360c bt 2 root mxnet python mxnet lib libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE0 clEvEUlvE E9 M invokeERKSt9 Any data 0x60 0x7f20de4e7520 bt 3 usr lib x86 64 linux gnu libstdc so 6 0x90c30 0x7f20ca85fc30 bt 4 lib x86 64 linux gnu libpthread so 0 0x8184 0x7f20fe34b184 bt 5 lib x86 64 linux gnu libc so 6 clone 0x6d 0x7f20fe07837d How can I solve the problem,,"piiswrong,szha",2017-04-10 11:16:11,2017-09-29 18:19:01
IS,Request about ndarray take function,In my code a function which I commonly call does the following operation Assume that we have a tensor A of shape d1 d2 dn and tensor B of shape D1 d2 dn where D1 d1 So the axis0 indices of the tensor A is mapped to a subset of axis0 indices of B Assume that this mapping is stored into a dict f What I want to do this is to pick A i and add to B f i for each axis0 index of A basically for i in 0 A shape 0 B f i A i This operation is inefficient and constitutes a bottleneck Theoretically I can solve this with the take operation where I can get the subset of B as determined by the mapping f and add A as in the example B mx ndarray zeros shape 10 4 ctx mx gpu 0 A mx ndarray ones shape 3 4 ctx mx gpu 0 f 0 3 1 4 2 6 indices mx ndarray array source array 3 4 6 ctx mx gpu 0 b subset mx ndarray take B indices b subset A The problem here is b subset is a new buffer it does not refer to the original B and when I add A on it this new buffer is modified not the original one Is it possible to add at least a flag to ndarray is take so that it enables the modification of the original array to handle the cases like above,,"reminisce,szha",2017-04-02 10:57:16,2017-09-29 18:19:02
IS,vote new navbar color,zackchase suggested to change the navbar i cannot tell the new one is much better so i would like to have a vote,,"mli,mli,mli,jspisak,eric-haibin-lin,zackchase,reminisce,Godricly,zihaolucky,zackchase,szha",2017-04-06 01:01:10,2017-09-29 18:19:03
IS,Is there anyway to delay computing of loss,Say I have a compute graph Is there anyway I can do only one forward pass in this scenario instead of two,,szha,2017-04-11 02:38:49,2017-09-29 18:19:04
IS,Failed to run distribute job with protobuf 2 6 1,Environment info Operating System Debian Compiler gcc Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 9 If you are using R package please provide R sessionInfo Error Message Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"mli,qiyuangong,szha",2017-03-08 05:52:19,2017-09-29 18:19:06
IS,Can I train the faster RCNN with multi GPU with multi machine,I found faster RCNN is supported in mxnet Can I train it on multi GPU with multi machine,,"tornadomeet,szha",2016-10-13 09:06:07,2017-09-29 18:50:06
IS,The sequential module example in mxnet example module sequential module py does not work for python3,When I run the example code at mxnet example module sequential module py with Python 3 I get the following error I did not modify the code in any way except change demo data model parallelism in line 10 to False as I do not have 4 gpus Traceback most recent call last File sequential module py line 41 in module mod seq add mod1 add mod2 take labels True auto wiring True File home sumeth miniconda3 envs mx lib python3 5 site packages mxnet module sequential module py line 64 in add for key in kwargs iterkeys AttributeError wouldict' object has no attribute 'iterkeys' However when I switched to Python 2 it runs fine Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia Python 2 and 3 MXNet version 0 9 3a3 If you are using python package please provide Python version and distribution Anaconda3 python 2 7 and 3 5,,szha,2017-04-13 00:48:58,2017-09-29 18:50:07
IS,How to correct run rcnn in windows with cython compiled,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System windows 10 x64 Compiler vs2013 update5 nvcc gcc Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution anaconda2 python2 7 anaconda3 python3 5 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace C Users Trangle Anaconda2 python exe D MyCoding PublicCode example2 rcnn test py Traceback most recent call last File D MyCoding PublicCode example2 rcnn test py line 5 in module from rcnn tools test rcnn import test rcnn File D MyCoding PublicCode example2 rcnn rcnn tools test rcnn py line 7 in module from symbol import File D MyCoding PublicCode example2 rcnn rcnn symbol init py line 1 in module from symbol vgg import File D MyCoding PublicCode example2 rcnn rcnn symbol symbol vgg py line 2 in module import proposal File D MyCoding PublicCode example2 rcnn rcnn symbol proposal py line 14 in module from rcnn processing nms import py nms wrapper cpu nms wrapper gpu nms wrapper File D MyCoding PublicCode example2 rcnn rcnn processing nms py line 3 in module from cython gpu nms import gpu nms ImportError No module named gpu nms Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 just test rcnn 2 3 What have you tried to solve it 1 how to compile the needed pyd for rcnn in windows 2 3,,"Trangle,szha",2017-04-13 06:42:10,2017-09-29 18:50:09
IS,Running error in the example code of googlenet,I run the example code of googlenet image classification on a K80 server with the newest version of MXNet 0 9 5 It seems that the error is caused by the size configuration of pooling Could you help to fix it Thanks The command I used,,szha,2017-04-06 15:56:31,2017-09-29 18:50:09
IS,Fix for GoogLeNet symbol in example image classification symbols googlenet py,When running example image classification benchmark score py with the GoogLeNet symbol it causes a crash see 4827 src operator pooling inl h 208 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 7 exceeds input 6 padded to 6 In short the error occurs because layer pool6 attempts to pool with a 7x7 kernel on a 6x6 input The cause is MXNet is default valid pooling convention which causes pool1 pool3 pool4 and pool5 to produce smaller output matrices than expected one element less per row and per column eventually resulting in above error The correct symbol definition should be,,"jmerkow,szha",2017-02-15 11:59:33,2017-09-29 18:50:10
IS,example rcnn demo py error while detecting object using gpu,code works while running on CPU For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 04 on AWS p2 8xl Compiler gcc Package used Python R Scala Julia python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace ubuntu ip 172 31 36 165 mxnet example rcnn python demo py prefix final epoch 0 image bike jpg gpu 1 12 01 58 src engine engine cc 36 MXNet start using engine NaiveEngine 12 01 59 home ubuntu mxnet dmlc core include dmlc logging h 300 12 01 59 src storage pooled storage manager h 84 cudaMalloc failed out of memory Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7ff446af368c bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet7storage23GPUPooledStorageManager5AllocEm 0x1d8 0x7ff447715948 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet11StorageImpl5AllocEmNS 7ContextE 0x57 0x7ff4477177d7 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xee6609 0x7ff447430609 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataS1 S3 0x23 0x7ff446b608c3 bt 5 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine11NaiveEngine9PushAsyncESt8functionIFvNS 10RunContextENS0 18CallbackOnCompleteEEENS 7ContextERKSt6vectorIPNS0 3VarESaISA EESE NS 10FnPropertyEiPKc 0x8c 0x7ff44735ca5c bt 6 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6Engine8PushSyncESt8functionIFvNS 10RunContextEEENS 7ContextERKSt6vectorIPNS 6engine3VarESaIS9 EESD NS 10FnPropertyEiPKc 0x124 0x7ff446b62314 bt 7 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet10CopyFromToERKNS 7NDArrayEPS0 i 0x62c 0x7ff44743943c bt 8 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xf3f8e4 0x7ff4474898e4 bt 9 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so MXImperativeInvoke 0x2cd 0x7ff447330d0d Traceback most recent call last File demo py line 142 in module main File demo py line 137 in main predictor get net symbol args prefix args epoch ctx File demo py line 36 in get net arg params aux params load param prefix epoch convert True ctx ctx process True File home ubuntu mxnet example rcnn rcnn utils load model py line 53 in load param arg params convert context arg params ctx File home ubuntu mxnet example rcnn rcnn utils load model py line 35 in convert context new params k v as in context ctx File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet ndarray py line 871 in as in context return self copyto context File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet ndarray py line 820 in copyto return internal copyto self out hret File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet ctypes ndarray py line 164 in generic ndarray function c array ctypes c char p c str val for val in vals File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 12 01 59 src storage pooled storage manager h 84 cudaMalloc failed out of memory Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7ff446af368c bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet7storage23GPUPooledStorageManager5AllocEm 0x1d8 0x7ff447715948 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet11StorageImpl5AllocEmNS 7ContextE 0x57 0x7ff4477177d7 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xee6609 0x7ff447430609 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvN5mxnet10RunContextENS0 6engine18CallbackOnCompleteEEZNS0 6Engine8PushSyncESt8functionIFvS1 EENS0 7ContextERKSt6vectorIPNS2 3VarESaISC EESG NS0 10FnPropertyEiPKcEUlS1 S3 E E9 M invokeERKSt9 Any dataS1 S3 0x23 0x7ff446b608c3 bt 5 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6engine11NaiveEngine9PushAsyncESt8functionIFvNS 10RunContextENS0 18CallbackOnCompleteEEENS 7ContextERKSt6vectorIPNS0 3VarESaISA EESE NS 10FnPropertyEiPKc 0x8c 0x7ff44735ca5c bt 6 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet6Engine8PushSyncESt8functionIFvNS 10RunContextEEENS 7ContextERKSt6vectorIPNS 6engine3VarESaIS9 EESD NS 10FnPropertyEiPKc 0x124 0x7ff446b62314 bt 7 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet10CopyFromToERKNS 7NDArrayEPS0 i 0x62c 0x7ff44743943c bt 8 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xf3f8e4 0x7ff4474898e4 bt 9 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so MXImperativeInvoke 0x2cd 0x7ff447330d0d 12 01 59 src engine naive engine cc 35 Engine shutdown Minimum reproducible example if you are using your own code please provide a short script that reproduces the error python demo py prefix final epoch 0 image bike jpg gpu 1 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 install latest mxnet download the models 2 run example python demo py prefix final epoch 0 image bike jpg gpu 1 3 What have you tried to solve it 1 2 3,,"Godricly,thatindiandude,szha",2017-04-12 18:33:12,2017-09-29 18:50:12
IS,TypeError Compose expect Symbol as arguments,I want to implement CNN BLSTM CTC with CUDNN but an error arise Traceback most recent call last File home Documents MyCode VLPR view py line 25 in module output CNN MultiBLSTM CTC CUDNN num hidden num label num lstm layer File home Documents MyCode VLPR blstm py line 325 in CNN MultiBLSTM CTC CUDNN output cell unroll num length inputs hidden merge outputs True layout 'NTC' File home mxnet python mxnet rnn rnn cell py line 637 in unroll inputs symbol Concat inputs dim 0 File home mxnet python mxnet ctypes symbol py line 191 in creator s compose args name name symbol kwargs File home mxnet python mxnet symbol py line 241 in compose raise TypeError 'Compose expect Symbol as arguments' TypeError Compose expect Symbol as arguments Here is my code How can I fix it def CNN MultiBLSTM CTC CUDNN num hidden num label num lstm layer num class 70 num length 32 data mx sym Variable wouldata' label mx sym Variable 'label' wordvec CNN data hidden mx symbol Flatten wordvec i for i in range num length cell mx rnn FusedRNNCell num hidden num layers num lstm layer mode 'lstm' bidirectional True output cell unroll num length inputs hidden merge outputs True layout 'NTC' pred mx sym Reshape output shape 1 num hidden pred mx sym FullyConnected data pred num hidden num class label mx sym Reshape data label shape 1 label mx sym Cast data label dtype 'int32' sm mx sym WarpCTC data pred label label label length num label input length num length return sm def CNN3 indata the net is similar to VGG16 each image size is 64 256 stage 1 body CNN Module data indata kernel 3 3 pad 1 1 num filter 64 stride 1 1 num module 2 BatchNorm True body mx symbol Pooling data body pool type max kernel 2 2 stride 2 2 32 128 stage 2 body CNN Module data body kernel 3 3 pad 1 1 num filter 128 stride 1 1 num module 2 BatchNorm True body mx symbol Pooling data body pool type max kernel 2 2 stride 2 2 16 64 stage 3 body CNN Module data body kernel 3 3 pad 1 1 num filter 256 stride 1 1 num module 2 BatchNorm True body mx symbol Pooling data body pool type max kernel 2 2 stride 2 2 8 32 stage 4 body CNN Module data body kernel 3 3 pad 1 1 num filter 256 stride 1 1 num module 2 BatchNorm True body mx symbol Pooling data body pool type max kernel 2 1 stride 2 1 4 32 stage 5 body CNN Module data body kernel 3 3 pad 1 1 num filter 512 stride 1 1 num module 2 BatchNorm True body mx symbol Pooling data body pool type max kernel 2 1 stride 2 1 2 32 stage 5 body CNN Module data body kernel 2 1 pad 0 0 num filter 512 stride 1 1 num module 1 BatchNorm True body mx sym SliceChannel data body num outputs 32 axis 3 squeeze axis 1 return body,,"Godricly,ysh329,szha",2017-02-28 08:18:36,2017-09-29 18:50:13
IS,Does MXNet support RDMA over Converged Ethernet ROCE,Does MXNet support RDMA over Converged Ethernet ROCE when the communication between ps and workers,,"pineking,weijianwen,szha,weijianwen,weijianwen",2017-04-13 10:59:20,2017-09-29 18:50:14
IS,Nan for each epoch,Hello I have problems regarding training of a convolutional neural network I experience NaN is in the loss for each epoch I have also tried to change the loss function but with same error I do not see any indication of NaN is in the internal network at any batches However the final loss gives NaN is The report of the training is here 2017 04 10 19 58 31 948 Host Batch 154 conv1a backward weight 0 000210175 2017 04 10 19 58 31 948 Host Batch 154 conv1a weight 0 0382393 2017 04 10 19 58 31 948 Host Batch 154 linear2 backward weight 0 201007 2017 04 10 19 58 31 949 Host Batch 154 linear2 weight 0 0153211 2017 04 10 19 58 31 949 Host Batch 154 res2a branch1 backward weight 0 000367878 2017 04 10 19 58 31 949 Host Batch 154 res2a branch1 weight 0 246445 2017 04 10 19 58 31 949 Host Batch 154 res2a branch2a backward weight 0 000476692 2017 04 10 19 58 31 949 Host Batch 154 res2a branch2a weight 0 0853167 2017 04 10 19 58 31 949 Host Batch 154 res2a branch2b1 backward weight 0 000416497 2017 04 10 19 58 31 949 Host Batch 154 res2a branch2b1 weight 0 0608358 2017 04 10 19 58 31 949 Host Batch 154 res2b1 branch2a backward weight 0 000309442 2017 04 10 19 58 31 949 Host Batch 154 res2b1 branch2a weight 0 0601303 2017 04 10 19 58 31 949 Host Batch 154 res2b1 branch2b1 backward weight 0 000337989 2017 04 10 19 58 31 949 Host Batch 154 res2b1 branch2b1 weight 0 060109 2017 04 10 19 58 31 949 Host Batch 154 res2b2 branch2a backward weight 0 000268277 2017 04 10 19 58 31 949 Host Batch 154 res2b2 branch2a weight 0 0594585 2017 04 10 19 58 31 950 Host Batch 154 res2b2 branch2b1 backward weight 0 000302767 2017 04 10 19 58 31 950 Host Batch 154 res2b2 branch2b1 weight 0 0588229 2017 04 10 19 58 31 950 Host Batch 154 res3a branch1 backward weight 0 000458686 2017 04 10 19 58 31 950 Host Batch 154 res3a branch1 weight 0 174965 2017 04 10 19 58 31 950 Host Batch 154 res3a branch2a backward weight 0 000435945 2017 04 10 19 58 31 950 Host Batch 154 res3a branch2a weight 0 0593551 2017 04 10 19 58 31 950 Host Batch 154 res3a branch2b1 backward weight 0 000483657 2017 04 10 19 58 31 950 Host Batch 154 res3a branch2b1 weight 0 0420545 2017 04 10 19 58 31 950 Host Batch 154 res3b1 branch2a backward weight 0 000396346 2017 04 10 19 58 31 950 Host Batch 154 res3b1 branch2a weight 0 0419271 2017 04 10 19 58 31 950 Host Batch 154 res3b1 branch2b1 backward weight 0 00039641 2017 04 10 19 58 31 950 Host Batch 154 res3b1 branch2b1 weight 0 0419139 2017 04 10 19 58 31 950 Host Batch 154 res3b2 branch2a backward weight 0 000321227 2017 04 10 19 58 31 950 Host Batch 154 res3b2 branch2a weight 0 0417158 2017 04 10 19 58 31 951 Host Batch 154 res3b2 branch2b1 backward weight 0 000357124 2017 04 10 19 58 31 951 Host Batch 154 res3b2 branch2b1 weight 0 0417599 2017 04 10 19 58 31 951 Host Batch 154 res4a branch1 backward weight 0 000450693 2017 04 10 19 58 31 951 Host Batch 154 res4a branch1 weight 0 125856 2017 04 10 19 58 31 951 Host Batch 154 res4a branch2a backward weight 0 000492684 2017 04 10 19 58 31 951 Host Batch 154 res4a branch2a weight 0 0417395 2017 04 10 19 58 31 951 Host Batch 154 res4a branch2b1 backward weight 0 000484657 2017 04 10 19 58 31 951 Host Batch 154 res4a branch2b1 weight 0 0296349 2017 04 10 19 58 31 951 Host Batch 154 res4b1 branch2a backward weight 0 000380861 2017 04 10 19 58 31 951 Host Batch 154 res4b1 branch2a weight 0 0295497 2017 04 10 19 58 31 951 Host Batch 154 res4b1 branch2b1 backward weight 0 000409661 2017 04 10 19 58 31 951 Host Batch 154 res4b1 branch2b1 weight 0 0295103 2017 04 10 19 58 31 951 Host Batch 154 res4b2 branch2a backward weight 0 000353395 2017 04 10 19 58 31 951 Host Batch 154 res4b2 branch2a weight 0 0295437 2017 04 10 19 58 31 951 Host Batch 154 res4b2 branch2b1 backward weight 0 000381005 2017 04 10 19 58 31 952 Host Batch 154 res4b2 branch2b1 weight 0 0295163 2017 04 10 19 58 31 952 Host Batch 154 res4b3 branch2a backward weight 0 000388173 2017 04 10 19 58 31 952 Host Batch 154 res4b3 branch2a weight 0 0295094 2017 04 10 19 58 31 952 Host Batch 154 res4b3 branch2b1 backward weight 0 000353592 2017 04 10 19 58 31 952 Host Batch 154 res4b3 branch2b1 weight 0 0294803 2017 04 10 19 58 31 952 Host Batch 154 res4b4 branch2a backward weight 0 000375338 2017 04 10 19 58 31 952 Host Batch 154 res4b4 branch2a weight 0 0295069 2017 04 10 19 58 31 952 Host Batch 154 res4b4 branch2b1 backward weight 0 000341182 2017 04 10 19 58 31 952 Host Batch 154 res4b4 branch2b1 weight 0 0294631 2017 04 10 19 58 31 952 Host Batch 154 res4b5 branch2a backward weight 0 000368075 2017 04 10 19 58 31 952 Host Batch 154 res4b5 branch2a weight 0 0294748 2017 04 10 19 58 31 952 Host Batch 154 res4b5 branch2b1 backward weight 0 000323147 2017 04 10 19 58 31 952 Host Batch 154 res4b5 branch2b1 weight 0 0294281 2017 04 10 19 58 31 953 Host Batch 154 res5a branch1 backward weight 0 000713526 2017 04 10 19 58 31 953 Host Batch 154 res5a branch1 weight 0 0883342 2017 04 10 19 58 31 953 Host Batch 154 res5a branch2a backward weight 0 000745788 2017 04 10 19 58 31 953 Host Batch 154 res5a branch2a weight 0 0295571 2017 04 10 19 58 31 953 Host Batch 154 res5a branch2b1 backward weight 0 000752134 2017 04 10 19 58 31 953 Host Batch 154 res5a branch2b1 weight 0 0294675 2017 04 10 19 58 31 953 Host Batch 154 res5b1 branch2a backward weight 0 000697756 2017 04 10 19 58 31 953 Host Batch 154 res5b1 branch2a weight 0 0208629 2017 04 10 19 58 31 953 Host Batch 154 res5b1 branch2b1 backward weight 0 000691178 2017 04 10 19 58 31 953 Host Batch 154 res5b1 branch2b1 weight 0 029506 2017 04 10 19 58 31 953 Host Batch 154 res5b2 branch2a backward weight 0 000745268 2017 04 10 19 58 31 953 Host Batch 154 res5b2 branch2a weight 0 0208363 2017 04 10 19 58 31 954 Host Batch 154 res5b2 branch2b1 backward weight 0 000626755 2017 04 10 19 58 31 954 Host Batch 154 res5b2 branch2b1 weight 0 0294602 2017 04 10 19 58 31 954 Host Batch 154 res6a branch1 backward weight 0 000802658 2017 04 10 19 58 31 954 Host Batch 154 res6a branch1 weight 0 06239 2017 04 10 19 58 31 954 Host Batch 154 res6a branch2a backward weight 0 00172652 2017 04 10 19 58 31 954 Host Batch 154 res6a branch2a weight 0 0626998 2017 04 10 19 58 31 954 Host Batch 154 res6a branch2b1 backward weight 0 00119287 2017 04 10 19 58 31 954 Host Batch 154 res6a branch2b1 weight 0 0294494 2017 04 10 19 58 31 954 Host Batch 154 res6a branch2b2 backward weight 0 000777648 2017 04 10 19 58 31 954 Host Batch 154 res6a branch2b2 weight 0 0624873 2017 04 10 19 58 31 954 Host Batch 154 res7a branch1 backward weight 0 00046971 2017 04 10 19 58 31 954 Host Batch 154 res7a branch1 weight 0 0441438 2017 04 10 19 58 31 955 Host Batch 154 res7a branch2a backward weight 0 00100658 2017 04 10 19 58 31 955 Host Batch 154 res7a branch2a weight 0 0442069 2017 04 10 19 58 31 955 Host Batch 154 res7a branch2b1 backward weight 0 000699126 2017 04 10 19 58 31 955 Host Batch 154 res7a branch2b1 weight 0 0208296 2017 04 10 19 58 31 955 Host Batch 154 res7a branch2b2 backward weight 0 000475449 2017 04 10 19 58 31 955 Host Batch 154 res7a branch2b2 weight 0 0442136 2017 04 10 19 58 31 955 Host Epoch 1 Batch 76 Speed 1 39 samples sec Train cross entropy 0 856569 2017 04 10 19 58 31 955 Host Epoch 1 Train cross entropy nan,,"kevinthesun,piiswrong,geoalgo,szha",2017-04-10 18:43:19,2017-09-29 18:50:15
IS,loading image with mx image imdecode flag 0,An error arised OpenCV Error Assertion failed scn 3 scn 4 in cv ipp cvtColor file C buildslave64 win64 amdocl master PackSlave win64 vc14 shared opencv modules imgproc src color cpp line 7341 How can I fix it Environment info Operating System Ubuntu and Windows Compiler Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 12,,"piiswrong,szha",2017-04-12 05:15:45,2017-09-29 18:50:16
IS,Autoencoder Why is the shape of args set to be num hidden num input,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System CentOS 6 6 Compiler gcc 4 8 Package used Python R Scala Julia python I'm testing the example of autoencoder here I do not understand the settings of shapes of args like follows Why is the shape of encoder args set to be num hidden num input instead of num input num hidden Suppose X is the input data item with dimension num input then to encode X we should multiply X with the transition matrix which is the encoder weight here Since X is in shape 1 num input should not the encoder weight be of shape num input num hidden Why it is set to be the transpose,,"FCInter,szha",2017-04-13 14:10:55,2017-09-29 18:50:17
IS,nan metrics during SSD training,I use ssd in the example to train a model to identify the Chinese words in a picture encountered 2 problem 1 Problem During training i encountered NaN metrics for both cross entropy loss and smooth l1 loss After setting the clip gradient parameters for optimization to 5 the NaN metrics disappeared So the NaN metrics is caused by gradient explosion 2 Problem The cross entropy metric never convergence I have 30000 training samples each sample has added some Chinese words generated randomly and the location of the words are correctly provided to the model Can any one help me on the second problem,,"zhreshold,szha",2017-04-13 10:18:48,2017-09-29 18:50:18
IS,Broadcast to with inferred size,There are a number of scenarios in which I would like to broadcast repeat tile a tensor to a size that matches a batch size determined at binding time This would mean that I would like to be able to have the size of the broadcast based on one of those shape elements However it seems those methods require an upfront constant when defining the graph which takes some of the wind out of shape inference It would be nice to have auxiliary functionality that let you specify the size by looking at an axis of the shape of some input symbol that gets ground at shape inference time,,"piiswrong,szha",2017-04-14 20:58:26,2017-09-29 18:50:19
IS,can mxnet support multiple optimizer scenario,i would like to do some tests data slice 2 parts first part used optimizer1 such as adagrad second part used optimizer2 such as adadelta and then using the same loss fuction can mxnet support this,,szha,2017-04-14 08:27:09,2017-09-29 18:50:20
IS,The arg params context of Module were changed to cpu after binded,Here is the code nd zeros x 0 shape dtype x 0 dtype creates nd array with default cpu context change to nd zeros x 0 shape dtype x 0 dtype ctx x 0 0 context can solve the problem,,szha,2017-04-15 10:50:14,2017-09-29 18:50:22
IS,faster rcnn multi gpu problem,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu16 04 Compiler gcc 5 4 Package used Python R Scala Julia Python MXNet version 0 9 1 Or if installed from source yes Problem I have met a problem in multi gpu rcnn training when i use 4 1080 the speed is no faster than 2 gpu and the terminal report the message only 4 out of 12 GPU pairs are enabled direct access The command is python train end2end gpu 0 1 2 3 Is the vgg network is reason when I use resnet the speed is increases linearly with the number of gpu and is there some parameters should set when i use python train end2end gpu 0 the speed is 5 4 samples s when i use python train end2end gpu 0 1 the speed is 8 samples s when i use python train end2end gpu 0 1 2 the speed is 7 4 samples s when i use python train end2end gpu 0 1 2 3 the speed is 6 4 samples s when i use python train end2end network resnet gpu 0 the speed is 2 3 samples s when i use python train end2end network resnet gpu 0 1 the speed is 4 0 samples s when i use python train end2end network resnet gpu 0 1 2 the speed is 5 0 samples s when i use python train end2end network resnet gpu 0 1 2 3 the speed is 5 7 samples s,,szha,2017-04-15 05:56:05,2017-09-29 18:50:23
IS,What is the shape of x and w of RNNs in cudnn,I'm developing a DL toolbox and plan to use cudnn But the shape of x and w of RNNs confused me a lot In most frameworks the shape of input tensor is minibatch timestep inputdim but in cudnn RNNsample it define the dimA of xDesc by So is that means the shape of x is timestep minibatch inputdim And the shape of W weights also confused me a lot,,"sbodenstein,szha",2017-04-10 03:47:26,2017-09-29 18:50:24
IS,scala module api no Callback to get train EvalMetric,With the scala API the default output while training with the module api is I would like to plot the training vs validation EvalMetric to see when it starts overfitting To do so I would like to write a cvs vile with the columns epoch train mse validation mse because the above output is not really usable to make a plot It is possible with setEvalEndCallbac to set a BatchEndCallback at the end of the eval stage to write the validation metric however it seems it is not possible to get the train metric with a callback function The setEpochEndCallback function takes a EpochEndCallback parameters which does not provide a way to access the EvalMetric for the training set How can I output the EvalMetric for the training set at the end of an epoch My use case plotting training vs validation error seems quite common Is there another simpler way to do it I basically just want the numbers that are output by default but in a csv format,,"benqua,Ldpe2G,benqua,Ldpe2G,benqua,yzhliu,benqua,szha",2017-04-14 13:32:00,2017-09-29 18:50:25
IS,combine two ImageRecordIter into one using NDArrayIter,I m trying to combine two ImageRecordIter into one DataIter using NDArrayIter the code def get multi input kv None return mx io NDArrayIter data wouldata ms' first rec iter kv getdata wouldata cnfd' sec rec iter kv getdata label isoftmax ms label' first rec iter kv getlabel isoftmax cnfd label' sec rec iter kv getlabel batch size cfg batch size but there are two problems I have encountered 1 the log have 4 decoding iter items but I just used two Iter so I am confused the log as follows 22 22 32 src io iter image recordio cc 220 ImageRecordIOParser home mpiNode ID MS FACE MS result selected fileList rec use 7 threads for decoding 22 22 32 src io iter image recordio cc 220 ImageRecordIOParser home mpiNode rec CNFD rec use 7 threads for decoding 22 22 33 src io iter image recordio cc 220 ImageRecordIOParser home mpiNode ID MS FACE MS result selected fileList rec use 7 threads for decoding 22 22 33 src io iter image recordio cc 220 ImageRecordIOParser home mpiNode rec CNFD rec use 7 threads for decoding 2 both the ImageRecordIter file are large 60G 10G but the mxnet will save the checkpoint every epoch and the epoch just have one batch the log are 2017 04 15 22 22 50 681 Node 0 Epoch 0 Batch 0 learning rate 0 080000 2017 04 15 22 22 50 682 Node 0 Epoch 0 Train accuracy 0 000000 2017 04 15 22 22 50 682 Node 0 Epoch 0 Train cross entropy 11 601508 2017 04 15 22 22 50 683 Node 0 Epoch 0 Time cost 5 906 2017 04 15 22 22 53 421 Node 0 Saved checkpoint to mxm log train model 0001 params 2017 04 15 22 22 53 953 Node 0 Epoch 1 Batch 0 Speed 977 96 samples sec Train accuracy 0 006250 2017 04 15 22 22 53 954 Node 0 Epoch 1 Batch 0 Speed 977 96 samples sec Train cross entropy 9 567853 2017 04 15 22 22 53 954 Node 0 Epoch 1 Batch 0 learning rate 0 076764 2017 04 15 22 22 53 954 Node 0 Epoch 1 Train accuracy nan 2017 04 15 22 22 53 954 Node 0 Epoch 1 Train cross entropy nan 2017 04 15 22 22 53 954 Node 0 Epoch 1 Time cost 0 531 2017 04 15 22 22 57 010 Node 0 Saved checkpoint to mxm log train model 0002 params 2017 04 15 22 22 57 542 Node 0 Epoch 2 Batch 0 Speed 891 83 samples sec Train accuracy 0 012500 2017 04 15 22 22 57 542 Node 0 Epoch 2 Batch 0 Speed 891 83 samples sec Train cross entropy 8 360397 2017 04 15 22 22 57 542 Node 0 Epoch 2 Batch 0 learning rate 0 073434 2017 04 15 22 22 57 542 Node 0 Epoch 2 Train accuracy nan 2017 04 15 22 22 57 542 Node 0 Epoch 2 Train cross entropy nan 2017 04 15 22 22 57 542 Node 0 Epoch 2 Time cost 0 531,,szha,2017-04-15 14:38:14,2017-09-29 18:50:26
IS,Cannot find custom operator type proposal,op Custom name rpn rois attr feat stride 16 is train False op type proposal ratios 0 5 1 2 scales 4 8 16 32 inputs 355 0 0 358 0 0 359 0 0 work in python2 7 resave json from but error in c using MXPredCreate api src operator custom custom inl h 128 Check failed registry find param op type registry end Cannot find custom operator type proposal,,szha,2017-04-17 07:55:39,2017-09-29 18:50:27
IS,AttributeError module 'mxnet' has no attribute isymbol',Environment info Operating System Ubuntu 16 04LTS Compiler g MXNet version 0 93 Using quick installation for python Python version and distribution Python 3 6 in anaconda Error Message Just a simple input import mxnet as mx a mx symbol Variable 'a' Then I got the message AttributeError module 'mxnet' has no attribute isymbol' And I find that all the attributes in my mxnet module are only doc loader name path spec and package I have tried to reinstall mxnet but still receive this message,,szha,2017-04-17 15:44:20,2017-09-29 18:50:28
IS,Which version of Ubuntu is recommended to use mxnet on gpu 14 04 or 16 04,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"sxjscience,szha",2017-04-17 03:03:36,2017-09-29 18:50:29
IS,AssertionError File mxnet ssd dataset iterator py line 134 in get batch assert len batch data 0,hi I used this model to detect food images there is error whe I want to run my predict code the error is following the cloud server is Ubuntu 16 04 thanks Traceback most recent call last File predict 500 845 py line 330 in module dump image out file image list test crop dir debug File predict 500 845 py line 155 in dump image dets detector im detect image list dir extension File i mxnet ssd detect detector py line 95 in im detect is train False File i mxnet ssd dataset iterator py line 70 in init self get batch File i mxnet ssd dataset iterator py line 134 in get batch assert len batch data 0 AssertionError,,szha,2017-04-13 06:20:14,2017-09-29 18:50:30
IS,How to make some convolution weights same in a network,,,"zhreshold,szha",2017-04-17 08:13:46,2017-09-29 18:50:31
IS,How to initialize a network weights with two loss functions,I used symbol I defined contains two loss functions When executing python train cifar10 py network snet pad size 0 it output error message below,,"ysh329,szha",2017-04-19 12:39:10,2017-09-29 18:50:35
IS,Is there a way could debug through python and c smoothly,as the title,,"eric-haibin-lin,szha",2017-04-18 09:53:26,2017-09-29 18:50:36
IS,Multi GPU batch normalization on MXNet,When we train a model with batch norm layer with multi gpus using MXNet will the mean and variance swap among the multi gpus or only calculated within one GPU Thanks,,szha,2017-04-19 02:31:07,2017-09-29 18:50:36
IS,How to save the final model in training mnist data using train mnist py,Like the title I do my experient and I want save the final model but I find the args using model prefix save the each epoch I want 200 epochs and if I save each one my hard disk is not engouh so any method I can do my job may be sb ask why i used 200 epchs I use the scipt but not the mnists data Thanks,,"ysh329,szha",2017-03-10 02:16:53,2017-09-29 22:36:18
IS,im2rec py problem with image index all 0,Hi I used the im2rec py tool to make a train lst and val lst for my data but in the lists image index are all zero command I type in terminal mxnet tools python im2rec py list True home dyc data home dyc data train recursive True And I got these lines in my train lst 0 57 000000 KJDZ0007 FYZCBXPZ 67 X00100000K0S jpg 0 59 000000 KJDZ0009 SWDJZ 59 X11000000V63 jpg 0 203 000000 neg X110000041XM jpg 0 186 000000 KJDZ0203 KHHD 85zxyh X11000003Q0J jpg 0 75 000000 KJDZ0101 JSSWXSGJSWJTYJDFP 62 X112000001PH jpg 0 37 000000 KJDZ0003 SZYXHQCKMXZ 58 X11000004OX9 jpg 0 8 000000 05 KJDZ0003 jsyhckmxz X102000001FH jpg 0 70 000000 KJDZ0101 HBZZSZYFP 7287 X11000002WWL jpg 0 120 000000 KJDZ0103 SHZZSPTFP X110000056Z8 jpg 0 104 000000 KJDZ0103 BJSGJSWJTYJDFP 21 X11000002L6J jpg 0 203 000000 neg X110000030XK jpg 0 88 000000 KJDZ0101 SXSDFSWJTYJDFP A8800011HO6E jpg 0 24 000000 18 KJDZ0202 ywhdskpz X11000002EYA jpg 0 172 000000 KJDZ0202 XJJKD 59nyyh X104000001E2 jpg 0 203 000000 neg X11000003ZQ8 jpg 0 198 000000 KJDZ0203 YXFKHD 94jnnsh X104000005B5 jpg 0 146 000000 KJDZ0202 CKLXD 88xyyh X110000058M7 jpg 0 38 000000 KJDZ0003 ZDMX 29 A880001lHZKR jpg 0 203 000000 neg X110000015VG jpg 0 144 000000 KJDZ0201 ZGYXDZJSFKPZ 58 X11000004KTV jpg 0 57 000000 KJDZ0007 FYZCBXPZ 67 X11000000998 jpg 0 168 000000 KJDZ0202 WSYXSKHD 88 X11000000HB4 jpg 0 30 000000 28 KJDZ0101 jzytyfpzk X00100000L0F jpg 0 18 000000 12 KJDZ0203 gnzfywfkhd X11000003PG8 jpg 0 141 000000 KJDZ0201 ZGJSYXDWKHZYHD 86 X10200000E8J jpg 0 182 000000 KJDZ0202 ZGYXDGKHSKTZD X00300000A2C jpg 0 78 000000 KJDZ0101 NBZZSZYFP X11000004C5N jpg 0 123 000000 KJDZ0103 XMSGJSWJTYJDFP X11000004NNH jpg 0 24 000000 18 KJDZ0202 ywhdskpz X004000001A5 jpg 0 78 000000 KJDZ0101 NBZZSZYFP X11000003ZW2 jpg 0 192 000000 KJDZ0203 SFRZTZS 87nyyh X10200000A90 jpg 0 119 000000 KJDZ0103 SDZZSZYFP X104000006T2 jpg 0 100 000000 KJDZ0101 ZJSDFSWJTYJDFP 6263 X11000004FCU jpg 0 108 000000 KJDZ0103 FJZZSPTFP X10200000C5Z jpg 0 186 000000 KJDZ0203 KHHD 85zxyh X1100000190B jpg 0 101 000000 KJDZ0101 ZJSGJSWJTYJDFP 7778 X1100000514Q jpg 0 175 000000 KJDZ0202 ZFYWHDSK 83 X1100000395L jpg 0 185 000000 KJDZ0203 KHFFHD 57zgyh X11000005GLW jpg Do I use it incorrectly,,"piiswrong,ysh329,szha",2016-11-11 13:40:29,2017-09-29 22:36:20
IS,Can the label be a float value,do a regression by network,,szha,2017-04-18 08:10:41,2017-09-29 22:36:22
IS,if it is easy to distribute a model to the online system,deep learning will be widely used on industry I think,,szha,2017-04-17 23:56:23,2017-09-29 22:36:23
IS,Dynamic Graphs,Feature Request Would be great to have dynamic unrolling of recurrent nets LSTMs GRUs etc for training long sequences Also to be able to implement Recursive Nets Multidimensional Recurrent Nets e g PixelRNN or PyramidLSTM used for images require very pretty long timesteps to parse pixel by pixel Theano scan is limited to 1D sequences and it is quite tricky to implement MDRNNs in Tensorflow Also Memory Networks Differential Neural Computer would benefit from dynamic graphs,,"piiswrong,szha",2017-04-20 20:41:38,2017-09-29 22:36:25
IS,request on updating docs and examples about rnns,I noticed from python API that symbol RNN and symbol rnn has been added to mxnet net mx sym RNN data net state size 1234 num layers 56 mode 'lstm' p 0 1 exists in python api doc but no example and further explaination i pretty much like this one because it looks very simple and easy to usee btw mx rnn LSTMCell 100 prefix 'rnn ' forget bias 1 0 exists in example but API docs contains no info about the whole mx rnn module and tutorials about rnn are still using lstm py that write by their own outdated for mxnet now,,"piiswrong,mli,piiswrong,szha",2017-04-19 18:15:39,2017-09-29 22:36:27
IS,How to implement across channel LRN in MXNet as Caffe,The LRN in MXNet only supports within channel mode,,szha,2017-04-21 10:48:46,2017-09-29 22:36:29
IS,Exponential symbol error,MXNet version installed from source0 9 5 62ecb60fb327e615db1b8b22ccaf7b0230598a29 Python version and distribution 2 7 Error Message O K in documents mxnet symbol MakeLoss it is told that exp 0 1 2 inf 1 0 707 So the question is what the hell exponent you use Minimum reproducible example if you are using your own code please provide a short script that reproduces the error exp 0 1 2 inf 1 0 707,,szha,2017-04-21 11:41:13,2017-09-29 22:36:32
IS,How to train my own data with fcn xs,I want to train my own data with fcn xs the data have 2 class include the background I know is that it should change the numclass in the fcn xs py are there any other places to change And what is the requirements for the size of images Can anybody help me I am not quite sure what should I do for training with my own data It is a little complicated for me MXNet version 0 9,,szha,2017-04-14 06:04:52,2017-09-29 22:36:35
IS,Intel MKL version,This commit requires Intel MKL 2018 beta which some features are not available in Intel MKL 2017 For example File src operator mkl mkl pooling inl h dnnAlgorithmPoolingAvgIncludePadding is only defined in Intel MKL 2018 beta And currently it was not defined Intel MKL 2017 update 2,,szha,2017-04-20 19:56:32,2017-09-29 22:36:37
IS,idea,it would be cool if mxnet would feature a deep convolutional recursive swarm of hybrid bdi and artificial neural networks it would be cool if it would be possible to feed such a thing with elastic fusion for gesture recognition using advanced computer vision algorithms,,szha,2017-04-22 19:08:11,2017-09-29 22:36:39
IS,imread read blank None image for file jpg,why do i get such error while i use im2rec py to create rec file the file is available and the path is correct i can run cv2 read to read the same file correctly in a new py file,,szha,2017-04-23 11:34:09,2017-09-29 22:36:43
IS,mxnet symbol Symbol grad not implemented,I'm going to use mxnet symbol Symbol grad to define my own loss function however this function is not implemented I found that tensorflow has this function If there is a way to implement this function in mxnet Thank you For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler g Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 11 29 30 home zxlee Downloads mxnet12 dmlc core include dmlc logging h 300 11 29 30 src c api c api symbolic cc 547 not implemented Stack trace returned 10 entries bt 0 home zxlee miniconda2 envs mxnet12 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f11b5175b0c bt 1 home zxlee miniconda2 envs mxnet12 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so MXSymbolGrad 0x4a 0x7f11b5ddcb4a bt 2 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so ffi call unix64 0x4c 0x7f11c679257c bt 3 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so ffi call 0x1f5 0x7f11c6791cd5 bt 4 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so ctypes callproc 0x3e6 0x7f11c6789376 bt 5 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so 0x9db3 0x7f11c6780db3 bt 6 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f11c7d6ae93 bt 7 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x715d 0x7f11c7e1d80d bt 8 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f11c7e1fc3e bt 9 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8b47 0x7f11c7e1f1f7 Traceback most recent call last File test mxnet2 py line 10 in module grad loss grad wrt wouldata' File home zxlee miniconda2 envs mxnet12 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet symbol py line 1076 in grad ctypes byref handle File home zxlee miniconda2 envs mxnet12 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet base py line 83 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 11 29 30 src c api c api symbolic cc 547 not implemented Stack trace returned 10 entries bt 0 home zxlee miniconda2 envs mxnet12 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f11b5175b0c bt 1 home zxlee miniconda2 envs mxnet12 lib python2 7 site packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so MXSymbolGrad 0x4a 0x7f11b5ddcb4a bt 2 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so ffi call unix64 0x4c 0x7f11c679257c bt 3 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so ffi call 0x1f5 0x7f11c6791cd5 bt 4 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so ctypes callproc 0x3e6 0x7f11c6789376 bt 5 home zxlee miniconda2 envs mxnet12 lib python2 7 lib dynload ctypes so 0x9db3 0x7f11c6780db3 bt 6 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyObject Call 0x53 0x7f11c7d6ae93 bt 7 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x715d 0x7f11c7e1d80d bt 8 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyEval EvalCodeEx 0x89e 0x7f11c7e1fc3e bt 9 home zxlee miniconda2 envs mxnet12 bin lib libpython2 7 so 1 0 PyEval EvalFrameEx 0x8b47 0x7f11c7e1f1f7 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx data mx sym Variable wouldata' weight mx sym Variable 'weight' out data weight label mx sym Variable 'weight' loss out label 2 0 loss mx sym MakeLoss loss name 'loss' grad loss grad wrt wouldata' Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-04-24 03:40:52,2017-09-29 22:36:44
IS,when import mxnet using python3 5 the error occurred with the newest version 0 9 5,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System windows 10 x64 Compiler visual c 12 0 Package used Python R Scala Julia python3 5 MXNet version 0 9 5 Or if installed from source 0 9 5 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message In 1 import mxnet UnicodeDecodeError Traceback most recent call last ipython input 1 30eb4f951ea5 in module 1 import mxnet C Users kw w Documents My3rdPartyLib MxNet python mxnet init py in module 7 from base import MXNetError 8 from import base 9 from import contrib 10 from import ndarray 11 from import name C Users kw w Documents My3rdPartyLib MxNet python mxnet contrib init py in module 8 from import ndarray as nd 9 10 from import autograd 11 from import tensorboard C Users kw w Documents My3rdPartyLib MxNet python mxnet contrib autograd py in module 8 from base import LIB check call string types 9 from base import mx uint NDArrayHandle c array 10 from ndarray import NDArray zeros like 11 from symbol import GRAD REQ MAP 12 C Users kw w Documents My3rdPartyLib MxNet python mxnet ndarray py in module 873 return self copyto context 874 875 init ndarray module NDArray mxnet 876 877 def onehot encode indices out C Users kw w Documents My3rdPartyLib MxNet python mxnet ctypes ndarray py in init ndarray module ndarray class root namespace 202 hdl OpHandle 203 check call LIB NNGetOpHandle c str name ctypes byref hdl 204 function make ndarray function hdl name 205 if function name startswith ' contrib ' 206 function name function name 9 C Users kw w Documents My3rdPartyLib MxNet python mxnet ctypes ndarray py in make ndarray function handle name 67 ret type py str ret type value if ret type value is not None else '' 68 doc str build doc func name 69 py str desc value 70 arg names 71 arg types C Users kw w Documents My3rdPartyLib MxNet python mxnet base py in lambda x 22 this function is needed for python3 23 to convert ctypes char p value back to python str 24 py str lambda x x decode 'utf 8' 25 else 26 string types basestring UnicodeDecodeError 'utf 8' codec can not decode bytes in position 55 56 invalid continuation byte Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"Trangle,piiswrong,yajiedesign,yajiedesign,Trangle,szha",2017-04-15 11:48:43,2017-09-29 22:36:46
IS,Use aux args in OP failed,I am writing an OP which will use the extra args parameter add the extra parameter shape information in OPProp is InferShape Is it bug,,szha,2017-04-25 07:41:35,2017-09-29 22:36:47
IS,Automatic type conversion not supported,Writing a program with NDArray using default type 'float32' and turns out the value overflow in the end And it seems like I need to manually change all the variables into 'float64' otherwise the compiler will throw exception While in Numpy data types will be automatically converted into the advanced one e g a float32 will be converted to float64 if it adds up with a float64 Do I miss anything here or this is what mxnet support right now,,"EvanzzzZ,piiswrong,EvanzzzZ,szha",2017-04-25 06:25:55,2017-09-29 22:36:48
IS,Missing information in the module tutorial,In the module tutorial there is some missing information that should be added in order to help the reader In the first script from data iter import SyntheticData it should be pointed out that the module data iter could be found in in order to run the first script There is a missing pointer to data ipynb as it should point to the following notebook in GitHub,,"eric-haibin-lin,szha",2017-04-25 03:47:55,2017-09-29 22:36:49
IS,Low Precision Support for Inference and Training,Opening issue to track low precision int8 fp16 training and inference progress as well as support for CUDA8 int8 fp16 acceleration support,,"arank,arank,szha",2017-04-25 23:06:50,2017-09-29 22:36:50
IS,bug in run Character level language models,bug in,,"formath,szha",2016-12-24 20:04:37,2017-09-29 22:36:51
IS,DeprecationWarning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead,there is something wrong model py 870 DeprecationWarning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead my mxnet is the latest version i am new to python and confused about this problem could you give me some advice about how to modify this Thank you so much,,"Soonhwan-Kwon,szha",2017-04-25 11:24:19,2017-09-29 22:36:52
IS,Create loss function,Hello I am about to implement a new loos function My data is quite imbalanced and thus I would like to pay more attention to the class with few samples It is regarding binary classification in image object detection The cost function I would like to implement is loss w pos y log sigma w neg 1 y log 1 sigma data mx sym Variable wouldata' label mx sym Variable 'label' out mx symbol Softmax data data name isoftmax' I dont know if 0 or 1 ce weightpos label mx sym log out weightneg 1 label mx sym log 1 out loss mx sym MakeLoss ce normalization 'batch' It fails to compile and train lease use mx sym Variable init mx init to set initialization pattern' name ValueError Unknown initialization pattern for label Default initialization is now limited to weight bias gamma 1 0 and beta 0 0 Please use mx sym Variable init mx init to set initialization pattern How do I create custom loss function,,szha,2017-04-25 16:32:39,2017-09-29 22:36:54
IS,Questions about CSVIter,piiswrong I have some questions about CSVIter If shuffle True is the data shuffled after every epoch Speed when using multiple CSVIter in different jobs When I launch a second training job it uses different CSV file the training speed drops is it normal And why,,"zihaolucky,szha",2017-04-09 13:55:25,2017-09-29 22:36:55
IS,Prefetching data using parallel threads,Does mxnet provide a method to prefetch data into a queue using multiple threads that actually run in parallel I have textual data and not images I am using python and multithreading in python is not parallel so I need to use a C based method for reading data in a parallel fashion In short does mxnet have a similar method like that of tensorflow is coordinator and queue runner,,"saurabh3949,piiswrong,szha",2017-04-26 23:38:30,2017-09-29 22:36:56
IS,Normalization is not done in SoftmaxOutput if N D label probability array is given,From L124 I found normalization property is ignored if shapes of output and label are equal Also from L171 I found the gradient is divided by s3 2 even if normalization is null But in the documentation it says null means do nothing I believe the exact behaviors of SoftmaxOutput in each condition combination of parameters should be explained in the documentation,,"jiajiechen,szha",2017-04-26 08:40:33,2017-09-29 22:36:57
IS,Better Validation Accuracy with Tensorflow than MxNet,I tried to compare the speed and the validation accuracy of a Convolutional Neural Network with 5 layers 3 convolutional layers and 2 fully connected layers on MxNet and Tensorflow For MNIST dataset It shows that with the almost same accuracy MxNet is quite faster than Tensorflow However I did this comparison with Dogs vs Cats It shows that MxNet cannot be as good as Tensorflow in accuracy I have also tried Residual blocks and again It demonstrated that the accuracy is not comparable with the accuracy that can be achieved by Tensorflow I used mx model FeedForward create for training and mx io NDArrayIter for data traitors I repeated the experiment with different sgd optimizer like Adam sgd momentom and RMSProp However could not achieve the same accuracy as Tensorflow Could you please help me to understand that where the problem might be from Is it from Mxnet functions that I used or in general MxNet in not as accurate as Tensorflow Thank you in advance for your consideration,,"zhreshold,zhreshold,Jerryzcn,zhreshold,jmerkow,zhreshold,zhreshold,szha",2017-04-10 09:21:22,2017-09-29 22:36:58
IS,Cannot reproduce a model using the model parameters,Environment info Operating System CentOS 6 6 Compiler gcc 4 8 Package used Python R Scala Julia Python MXNet version Or if installed from source from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 12 Anaconda custom 64 bit Error Message Cannot reproduce a model using the model parameters Minimum reproducible example I'm trying to reproduce the feed forward step of an autoencoder by myself using python I first train the model and save the params in files Then I implement the feed forward step and load the params Then I feed data into it If correctly implemented the feed forward step should yield the same output as that yielded by the original model in mxnet However the result I get is different I do not understand what each argument means as show in the example here I do not understand what each term means i e args args grad args mult auxs I only know that the args is used to multiply the data items I only save the args in files and load it into my own implementation I did not load other params I think this would be the reason why my implementation is different from mxnet But what these params mean Could someone tell me Thank you all for helping me,,"FCInter,szha",2017-04-28 03:46:03,2017-09-29 22:36:59
IS,Training MNIST example with VGG broken,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 4 Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 06f1c4dee7636f7db3ae22d192ec0ee1a095e100 If you are using python package please provide Python version and distribution Python2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'vgg' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 23 48 43 home ubuntu src mxnet dmlc core include dmlc logging h 304 23 48 43 src operator pooling inl h 196 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 2 exceeds input 1 padded to 1 Stack trace returned 10 entries bt 0 home ubuntu src mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f3a84c4612c bt 1 home ubuntu src mxnet python mxnet lib libmxnet so ZNK5mxnet2op11PoolingProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x883 0x7f3a858e9073 bt 2 home ubuntu src mxnet python mxnet lib libmxnet so 0xd13088 0x7f3a85724088 bt 3 home ubuntu src mxnet python mxnet lib libmxnet so 0x103babd 0x7f3a85a4cabd bt 4 home ubuntu src mxnet python mxnet lib libmxnet so 0x103d3b2 0x7f3a85a4e3b2 bt 5 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x518 0x7f3a85a38b18 bt 6 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7f3a855d12ae bt 7 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x240 0x7f3a855d40e0 bt 8 home ubuntu src mxnet python mxnet lib libmxnet so MXSymbolInferShape 0x329 0x7f3a855cbf79 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f3a88b99adc infer shape error Arguments data 64 1L 28L 28L softmax label 64 Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File home ubuntu src mxnet example image classification common fit py line 187 in fit monitor monitor File home ubuntu src mxnet python mxnet module base module py line 445 in fit for training True force rebind force rebind File home ubuntu src mxnet python mxnet module module py line 388 in bind state names self state names File home ubuntu src mxnet python mxnet module executor group py line 205 in init self bind exec data shapes label shapes shared group File home ubuntu src mxnet python mxnet module executor group py line 301 in bind exec shared group File home ubuntu src mxnet python mxnet module executor group py line 548 in bind ith exec arg shapes aux shapes self symbol infer shape input shapes File home ubuntu src mxnet python mxnet symbol py line 747 in infer shape res self infer shape impl False args kwargs File home ubuntu src mxnet python mxnet symbol py line 871 in infer shape impl ctypes byref complete File home ubuntu src mxnet python mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator pool5 23 48 43 src operator pooling inl h 196 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 2 exceeds input 1 padded to 1 Stack trace returned 10 entries bt 0 home ubuntu src mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f3a84c4612c bt 1 home ubuntu src mxnet python mxnet lib libmxnet so ZNK5mxnet2op11PoolingProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x883 0x7f3a858e9073 bt 2 home ubuntu src mxnet python mxnet lib libmxnet so 0xd13088 0x7f3a85724088 bt 3 home ubuntu src mxnet python mxnet lib libmxnet so 0x103babd 0x7f3a85a4cabd bt 4 home ubuntu src mxnet python mxnet lib libmxnet so 0x103d3b2 0x7f3a85a4e3b2 bt 5 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x518 0x7f3a85a38b18 bt 6 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7f3a855d12ae bt 7 home ubuntu src mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x240 0x7f3a855d40e0 bt 8 home ubuntu src mxnet python mxnet lib libmxnet so MXSymbolInferShape 0x329 0x7f3a855cbf79 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f3a88b99adc Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 cd src mxnet examples image classification 2 python2 7 train mnist py network vgg 3 What have you tried to solve it 1 Re compile MXNet without MKL 2 3,,"thatindiandude,szha",2017-04-27 23:50:32,2017-09-29 22:37:00
IS,How to convert the C syntax to python,I find it helpful to read the source code to find it useful functions for my application The problem is I am not sure how to write the corresponding file function in python Is there a general template that I can use to convert from c to python For example I am trying to convert a model from caffe and I see there is a function convert model,,szha,2017-04-28 11:04:11,2017-09-29 22:37:01
IS,Doc Check Dead links,There are still many dead links in doc eg In there is a dead link I suggest add a dead link checker in CI,,"antinucleon,szha",2017-04-28 17:19:56,2017-09-29 22:37:02
IS,DOC Doc is little error,I just find a little error in how to finetune the code shows as following In method fit the arg params and symbol are not used in method the new sym and new args should be arg params and symbol,,szha,2017-04-29 09:34:41,2017-09-29 22:37:28
PR,fix superresolution example,,,szha,2017-09-29 21:42:36,2017-09-29 22:42:44
IS,Discussion Revise Module API get outputs get input grads,Currently Module get outputs Module get input grads etc return a list and the user is responsible to use the correct index to get the correct output input gradient etc This is error prone and requires a tight coupling between different parts of the code i e a user can not change the order of inputs without changing all the parts of the code where he is calling the get input grads method One workaround is to use dict zip with the output names data names m outputs dict zip m output names m get outputs This is less error prone and induces less coupling i e even if the order changes the user can still access the outputs by their name It might be advisable to change the API to directly return such a dict like object However this naive workaround would break current code So instead one could base the return type on an adaptation of collections OrderedDict making values accessible both by their name and their index bringing together the advantage of access by name and staying compatible with current code What do you think,,"leezu,piiswrong,szha",2017-04-29 07:30:33,2017-09-29 23:32:42
IS,How can I crop a feature map through given coordinates like croping an image,Hi everyone I'm trying to crop a feature map from given coordinates like ROIAlign operation in resent paper Mask RCNN Is there any way to do such thing,,"piiswrong,szha",2017-04-30 10:00:10,2017-09-29 23:32:43
IS,End2End Captcha Recognition OCR Segmentation fault,When I run the End2End Captcha Recognition OCR example get a segmentation fault This is the original blog url I changed the devs from gpu to cpu for lack of hardware gpu Below is the main code Environment info Operating System virtual machine Linux Mint 18 base on Ubuntu 16 04 64bit Compiler gcc 5 4 0 Package used Python R Scala Julia Python 2 7 12 MXNet version get from git master 2017 2 23 If you are using python package please provide opencv 3 2 Error Message run in PyCharm Process finished with exit code 139 interrupted by signal 11 SIGSEGV gdb py bt Traceback most recent call first File test g py line 56 in iter img cv2 imdecode img cv2 IMREAD COLOR File home mao mxnet python mxnet model py line 236 in train multi device for data batch in train data File home mao mxnet python mxnet model py line 816 in fit sym gen self sym gen File test g py line 135 in module model fit X data train eval data data test eval metric Accuracy batch end callback mx callback Speedometer 32 50 gdb bt 0 0x0000000000000000 in 1 0x00007ffff18149ee in cv imdecode cv InputArray const int from usr lib x86 64 linux gnu libopencv highgui so 2 4 2 0x00007fffd9728f92 in pyopencv cv imdecode object object object from usr lib python2 7 dist packages cv2 so 3 0x00000000004c468a in call function oparg optimized out pp stack 0x7fffffffd440 at Python ceval c 4350 4 PyEval EvalFrameEx at Python ceval c 2987 5 0x00000000004dddca in gen send ex isra 0 lto priv at Objects genobject c 85 6 0x00000000004c4c6f in PyEval EvalFrameEx at Python ceval c 2806 7 0x00000000004c2765 in PyEval EvalCodeEx at Python ceval c 3582 8 0x00000000004ca099 in fast function nk 17 na optimized out n optimized out pp stack 0x7fffffffd7e0 func function at remote 0x7fffd4b0c488 at Python ceval c 4445 9 call function oparg optimized out pp stack 0x7fffffffd7e0 at Python ceval c 4370 10 PyEval EvalFrameEx at Python ceval c 2987 11 0x00000000004c2765 in PyEval EvalCodeEx at Python ceval c 3582 12 0x00000000004ca099 in fast function nk 4 na optimized out n optimized out pp stack 0x7fffffffd9f0 func function at remote 0x7fffd4b0d0c8 at Python ceval c 4445 13 call function oparg optimized out pp stack 0x7fffffffd9f0 Type return to continue or q return to quit at Python ceval c 4370 14 PyEval EvalFrameEx at Python ceval c 2987 15 0x00000000004c2765 in PyEval EvalCodeEx at Python ceval c 3582 16 0x00000000004c2509 in PyEval EvalCode co optimized out globals optimized out locals optimized out at Python ceval c 669 17 0x00000000004f1def in run mod lto priv at Python pythonrun c 1376 18 0x00000000004ec652 in PyRun FileExFlags at Python pythonrun c 1362 19 0x00000000004eae31 in PyRun SimpleFileExFlags at Python pythonrun c 948 20 0x000000000049e14a in Py Main at Modules main c 640 21 0x00007ffff7811830 in libc start main main 0x49dab0 main argc 2 argv 0x7fffffffde38 init optimized out fini optimized out rtld fini optimized out stack end 0x7fffffffde28 at csu libc start c 291 22 0x000000000049d9d9 in start What have you tried to solve it I find the point of interrupte is img cv2 imdecode img cv2 IMREAD COLOR So I split this part to another file only generate img from captcha module and decode it by opencv It can run well But when I run the example with mxnet decode by opencv will be segmentation fault What is wrong,,szha,2017-02-26 09:11:40,2017-09-29 23:32:44
IS,How to identify whether the content of the rec files for training are correctly generated,I train the vgg on the MS Celeb 1M finding that it doest is converge at all I'm convinced that it has something to do wtih the dataset itself because this can perform well on others datasets and I also use part of MS Celeb 1M to change the number of the classes and samples of the dataset but the result is the same I wonder who have the similare experience I in particular suspect the input data which is wrongly generated by the im2rec program But I do not know how to identify whether the rec files is correct By cafefully observing the print of the training it is curious during every epoch that it gets the same Train accuracy and Validation accuracy INFO root Epoch 1 Batch 100 Speed 1249 03 samples sec Train accuracy 0 000031 INFO root Epoch 1 Batch 200 Speed 1225 23 samples sec Train accuracy 0 000078 INFO root Epoch 1 Batch 300 Speed 1221 93 samples sec Train accuracy 0 000039 INFO root Epoch 1 Resetting Data Iterator INFO root Epoch 1 Time cost 366 774 INFO root Saved checkpoint to model msimg 0002 params INFO root Epoch 1 Validation accuracy 0 000059 INFO root Epoch 2 Batch 100 Speed 1248 00 samples sec Train accuracy 0 000031 INFO root Epoch 2 Batch 200 Speed 1250 36 samples sec Train accuracy 0 000078 INFO root Epoch 2 Batch 300 Speed 1257 64 samples sec Train accuracy 0 000039 INFO root Epoch 2 Resetting Data Iterator INFO root Epoch 2 Time cost 361 297 INFO root Saved checkpoint to model msimg 0003 params INFO root Epoch 2 Validation accuracy 0 000059 INFO root Epoch 3 Batch 100 Speed 1240 24 samples sec Train accuracy 0 000031 INFO root Epoch 3 Batch 200 Speed 1232 99 samples sec Train accuracy 0 000078 INFO root Epoch 3 Batch 300 Speed 1230 92 samples sec Train accuracy 0 000039,,szha,2017-04-30 03:40:11,2017-09-29 23:32:46
IS,converting caffe model,I converted a caffe model to mxnet model on one machine and load the model to a mxnet docker container when trying to load the model from the docker container I see the following message loading symbol saved by MXNet version 905 with lower version of MXNet v904 May cause undefined behavior Please update MXNet if you encounter any issue I think the docker image is with mxnet version 0 9 3 How to find the above maxnet version numbers like v904 v905,,szha,2017-05-01 15:00:35,2017-09-29 23:32:47
IS,How to implement gradient operators,I want to implement the improved WGAN with mxnet However the gradient penalty is a great headache It is a complex loss function which contains the data gradient For simple loss functions we can calculate the gradient of loss with respect to the output easily but how to calculate the data gradient with respect to the output It seems that tensorflow has a gradient operator just as the common convolution operators or fully connected operators With such an operator things become simple we just need to write the expression for the loss function However I do not have an idea of it Maybe mxnet can also have such an operator in the future,,szha,2017-05-01 16:59:27,2017-09-29 23:32:48
IS,MXNet threads and processes,Hi all my question is not an issue but a request for help I am interesting to reading MXNet core in depth However I kind of missing many points and it is currently difficult for me My goal is to know everything related to create kill threads and processes Can anybody here point me out for documentation and part of source code should I read Sincerely Salem,,"yuruofeifei,szha",2017-05-01 15:00:46,2017-09-29 23:32:50
IS,Question What would be the suggested way to initialize keys on server,This is not a bug I have a question I'm porting ZMQVan to InfiniBand which requires explicit buffer to be setup before data can be transmitted This means I need to know beforehand the number of keys and the size of each key Currently the key on server is initialized by worker 0 calling into KVStore Init after PostOffice Start is called where connections are set up I'm wondering what would be the suggested way to tell the server what is the number of keys and the size of each key without communicating with any worker Please advice Many thanks,,"piiswrong,szha",2017-05-02 02:34:50,2017-09-29 23:32:51
IS,Question Where does the code for Server and Scheduler diverge from that of Worker,This is not a bug I have a question I have been using this script to launch mxnet but when it seems every role calls into the FeedForward Fit method in Model py which calls train multi device and presumably starts training But server role and scheduler should not be doing actual training besides launching their ps lite instances Where does the the code for server and scheduler role diverge from that of worker is Many thanks,,"piiswrong,szha",2017-05-02 02:30:17,2017-09-29 23:32:52
IS,input type to dropout is not specified Error,When building a model in 0 9 4 and using symbol Variable to create a learnable weight one will get an Error if that same learnable weight is used in mx symbol Dropout def model learnable weight mx sym Variable 'learnable weight' init mx init Xavier factor type in magnitude 2 34 dp lw mx sym Dropout data learnable weight p 0 2 return out out model mod mx mod Module out mod fit train data data train eval metric metric num epoch 20 optimizer params 'learning rate' 0 3 'momentum' 0 9 initializer mx init Orthogonal 0 25 batch end callback mx callback Speedometer batch size 50 MXNetError Error in operator dp lw 10 37 06 src operator dropout inl h 183 input type to dropout is not specified THE FIX add dtype 'float32' as param to your sym Variable call def model learnable weight mx sym Variable 'learnable weight' init mx init Xavier factor type in magnitude 2 34 dtype 'float32' dp lw mx sym Dropout data learnable weight p 0 2 return out out model mod mx mod Module out mod fit train data data train eval metric metric num epoch 20 optimizer params 'learning rate' 0 3 'momentum' 0 9 initializer mx init Orthogonal 0 25 batch end callback mx callback Speedometer batch size 50,,"mli,piiswrong,szha",2017-05-01 16:28:21,2017-09-29 23:32:53
IS,How to decrease the memory consumption in forward is there something like MXNET BACKWARD DO MIRROR or memmonger,,,szha,2017-05-02 08:06:18,2017-09-29 23:32:54
IS,Set OpenMP Threads at Runtime,We would like to be able to set the number of OpenMP threads used by MXNet at runtime via omp set num threads One approach is to expose an api function MXNET DLL int MXSetNumOMPThreads int thread num in c api h which calls into omp set num threads Something like this is also done by libraries like WarpCTC what do you think of this Also,,"sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,szha",2017-05-02 14:18:37,2017-09-29 23:32:55
IS,Profiler outputs are titled with symbol type instead of name,I am trying to use the profiler on a net with many layer but it is hard to distinguish between them because they are all titled 'FullyConnected' Is there a way for the profiler to use the symbol name fc1 vs FullyConnected The screen shot at looks like it is using the symbol name Environment info Operating System Distributor ID Ubuntu Description Ubuntu 16 04 2 LTS Compiler gcc Ubuntu 5 4 1 2ubuntu1 16 04 5 4 1 20160904 Copyright C 2015 Free Software Foundation Inc This is free software see the source for copying conditions There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE Package used Python R Scala Julia Python 3 6 MXNet Version 3d4d845 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I am able to reproduce using What have you tried to solve it 1 Using both symbolic and all modes for profiling,,szha,2017-05-03 00:10:54,2017-09-29 23:32:58
IS,How to define other losses as metrics when training such as cross entropy loss etc,have some examples Thanks octocat,,"ysh329,szha",2017-05-03 02:40:05,2017-09-29 23:32:59
IS,Confused by type of operator embedding,Environment info Operating System CentOS release 6 6 Final Compiler gcc 4 8 2 Package used Python R Scala Julia python MXNet version 0 9 4 MXNet commit hash git rev parse HEAD 3b9d82dd909f9a0bdae3a6177db515ede5c53796 Python version and distribution 2 7 Code Why the type of embed weight is numpy int32 not numpy float32,,"eric-haibin-lin,szha",2017-02-18 12:23:48,2017-09-29 23:33:00
IS,index nreturn Symbol only accept nonnegative index,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-05-03 06:50:03,2017-09-29 23:33:01
IS,All tutorials are in python Is there tutorials in C,I did not find how to using MXNET in C I am using VS2015 and windows10 The cpppackage seems not support for MSVC,,"hjk41,II-Matto,lx75249,szha",2017-04-22 07:29:04,2017-09-29 23:33:04
IS,Problems when implementing Stale Synchronous Parallel SSP in MXNet,Hello I am now implementing Stale Synchronous Parallel SSP in MXNet However I have met a problem for a long time I mainly code in kvstore dist server h to implement SSP To briefly describe my code when the difference of the number of training iterations finished between the fastest worker and the slowest worker reaches the maximum number I set the fastest worker will be blocked to wait for the slowest worker and the push data of the fastest worker will be stored in a waitlist in the server node temporarily to be updated later However when I complete my codes and train Cifar 10 with 1 server node and 2 worker nodes only use GPU I always meet a sudden stop and find errors listed as below also the full version error message is pasted at the end of this post home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val ValueError operands could not be broadcast together with remapped shapes original remapped 10L and requested shape 96L I printed the vals size and total val in a log file found that the value of the two variants kept equal to each other for a long time from the beginning of the training but suddenly they had different values and the training stopped The last few lines of the log file that recorded the two values are listed as below vals size 15360 total val 15360 vals size 96 total val 96 vals size 48 total val 48 vals size 48 total val 48 vals size 48 total val 48 vals size 69120 total val 69120 vals size 112 total val 112 vals size 112 total val 112 vals size 17920 total val 17920 vals size 112 total val 112 vals size 80 total val 80 vals size 80 total val 80 vals size 57600 total val 57600 vals size 80 total val 80 vals size 48 total val 48 vals size 27648 total val 27648 vals size 48 total val 48 vals size 48 total val 48 vals size 32 total val 32 vals size 2048 total val 2048 vals size 32 total val 32 vals size 32 total val 32 vals size 27648 total val 27648 vals size 32 total val 32 vals size 32 total val 32 vals size 32 total val 32 vals size 32 total val 32 vals size 3072 total val 3072 vals size 32 total val 32 vals size 32 total val 32 vals size 2592 total val 2592 vals size 96 total val 96 vals size 96 total val 96 vals size 96 total val 10 So could someone please help me review my code and the error message and tell me what caused the errors Thanks a lot Environment info Operating System Ubuntu 14 04 1 LTS Compiler gcc version 4 8 2 Package used Python R Scala Julia Python Python version and distribution Python 2 7 6 Error Message 15 20 10 home xiongzi mxnet dmlc core include dmlc logging h 235 15 20 10 home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val terminate called after throwing an instance of wouldmlc Error' what 15 20 10 home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val Traceback most recent call last File ctypes callbacks c line 314 in 'calling callback function' File home xiongzi mxnet example image classification python mxnet kvstore py line 45 in updater handle updater key lhs rhs File home xiongzi mxnet example image classification python mxnet optimizer py line 820 in updater optimizer update index weight grad states index File home xiongzi mxnet example image classification python mxnet optimizer py line 305 in update mom lr grad wd weight File home xiongzi mxnet example image classification python mxnet ndarray py line 101 in add return add self other File home xiongzi mxnet example image classification python mxnet ndarray py line 658 in add None File home xiongzi mxnet example image classification python mxnet ndarray py line 629 in ufunc helper rhs rhs broadcast to lhs shape File home xiongzi mxnet example image classification python mxnet ndarray py line 354 in broadcast to raise ValueError err str ValueError operands could not be broadcast together with remapped shapes original remapped 10L and requested shape 96L 15 20 13 home xiongzi mxnet dmlc core include dmlc logging h 235 15 20 13 home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val terminate called after throwing an instance of wouldmlc Error' what 15 20 13 home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val bash line 1 5319 Aborted core dumped python train cifar10 py data dir home xiongzi mxnet data cifar10 batch size 128 lr 0 1 lr factor 94 num examples 5000 num epoch 1 gpus 0 1 cluster1 begin 0 cluster1 end 0 2 cluster2 begin 0 2 cluster2 end 0 4 cluster3 begin 0 4 cluster3 end 0 6 cluster4 begin 0 6 cluster4 end 0 8 cluster5 begin 0 8 cluster5 end 1 kv store dist ssp Exception in thread Thread 4 Traceback most recent call last File usr lib python2 7 threading py line 810 in bootstrap inner self run File usr lib python2 7 threading py line 763 in run self target self args self kwargs File home xiongzi mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File usr lib python2 7 subprocess py line 540 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no 192 168 0 3 p 22 'export LD LIBRARY PATH usr local cuda lib64 export DMLC INTERFACE eth1 export DMLC PS ROOT URI 192 168 0 2 export DMLC ROLE worker export DMLC NUM SERVER 1 export DMLC NUM WORKER 2 export DMLC PS ROOT PORT 9102 cd home xiongzi mxnet example image classification python train cifar10 py data dir home xiongzi mxnet data cifar10 batch size 128 lr 0 1 lr factor 94 num examples 5000 num epoch 1 gpus 0 1 cluster1 begin 0 cluster1 end 0 2 cluster2 begin 0 2 cluster2 end 0 4 cluster3 begin 0 4 cluster3 end 0 6 cluster4 begin 0 6 cluster4 end 0 8 cluster5 begin 0 8 cluster5 end 1 kv store dist ssp'' returned non zero exit status 134 bash line 1 27037 Aborted core dumped python train cifar10 py data dir home xiongzi mxnet data cifar10 batch size 128 lr 0 1 lr factor 94 num examples 5000 num epoch 1 gpus 0 1 cluster1 begin 0 cluster1 end 0 2 cluster2 begin 0 2 cluster2 end 0 4 cluster3 begin 0 4 cluster3 end 0 6 cluster4 begin 0 6 cluster4 end 0 8 cluster5 begin 0 8 cluster5 end 1 kv store dist ssp Exception in thread Thread 3 Traceback most recent call last File usr lib python2 7 threading py line 810 in bootstrap inner self run File usr lib python2 7 threading py line 763 in run self target self args self kwargs File home xiongzi mxnet tools dmlc core tracker dmlc tracker ssh py line 60 in run subprocess check call prog shell True File usr lib python2 7 subprocess py line 540 in check call raise CalledProcessError retcode cmd CalledProcessError Command issh o StrictHostKeyChecking no 192 168 0 2 p 22 'export LD LIBRARY PATH usr local cuda lib64 export DMLC INTERFACE eth1 export DMLC PS ROOT URI 192 168 0 2 export DMLC ROLE worker export DMLC NUM SERVER 1 export DMLC NUM WORKER 2 export DMLC PS ROOT PORT 9102 cd home xiongzi mxnet example image classification python train cifar10 py data dir home xiongzi mxnet data cifar10 batch size 128 lr 0 1 lr factor 94 num examples 5000 num epoch 1 gpus 0 1 cluster1 begin 0 cluster1 end 0 2 cluster2 begin 0 2 cluster2 end 0 4 cluster3 begin 0 4 cluster3 end 0 6 cluster4 begin 0 6 cluster4 end 0 8 cluster5 begin 0 8 cluster5 end 1 kv store dist ssp'' returned non zero exit status 134,,szha,2017-05-03 08:34:13,2017-09-29 23:33:05
IS,Feature Request Persistent RNN is,The new persistent RNN functionality in cuDNN v6 should be supported can you add that to Projects 'feature requests,,"sbodenstein,piiswrong,szha",2017-05-03 15:22:40,2017-09-29 23:33:07
IS,How python API translated to lower level,Hi all I am here struggling with reading MXNet source code I am a end user for a few months and I decided to go deep in the code I read some parts of it but I still struggle So I wanna trace the code from end user to the low level However I have not seen any clear explanation how to read it Also How API python transfer the code to the lower layer Sincerely,,"ysh329,szha",2017-05-03 14:56:19,2017-09-29 23:33:08
IS,libcblas so 3 cannot open shared object file No such file or directory,Hi all Operating System I am working on a cluster of 3 machines AWS ubuntu 14 04 and Python2 7 I am following the official site for installation and running the code for distributed training When I execute this tools launch py n 2 H hosts sync dst dir tmp mxnet python train mnist py network lenet kv store dist sync This is the error Traceback most recent call last File train mnist py line 10 in module from common import find mxnet fit File tmp mxnet common find mxnet py line 4 in module import mxnet as mx File tmp mxnet mxnet init py line 7 in module from base import MXNetError File tmp mxnet mxnet base py line 46 in module LIB load lib File tmp mxnet mxnet base py line 38 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File usr lib python2 7 ctypes init py line 365 in init self handle dlopen self name mode OSError libcblas so 3 cannot open shared object file No such file or directory Traceback most recent call last File train mnist py line 10 in module from common import find mxnet fit File tmp mxnet common find mxnet py line 4 in module import mxnet as mx File tmp mxnet mxnet init py line 7 in module from base import MXNetError File tmp mxnet mxnet base py line 46 in module LIB load lib File tmp mxnet mxnet base py line 38 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File usr lib python2 7 ctypes init py line 365 in init self handle dlopen self name mode OSError libcblas so 3 cannot open shared object file No such file or directory Any Idea,,"szha,anirudhacharya,szha,anirudhacharya,szha,anirudhacharya,anirudhacharya,anirudhacharya",2017-03-07 10:15:06,2017-09-29 23:33:10
IS,why sym SoftmaxOutput can not support other name,if i define mx symbol SoftmaxOutput data ip5 name softmax1111 it will report Symbol InferShapeKeyword argument name softmax label not found Ca not it support arbitrarily name in SoftmaxOutput,,szha,2017-05-04 08:55:39,2017-09-29 23:33:11
IS,Unable to run MXnet with yarn as a launcher,python tools launch py n 2 launcher yarn python train mnist py network lenet kv store dist sync 17 05 03 01 24 49 WARN util NativeCodeLoader Unable to load native hadoop library for your platform using builtin java classes where applicable 17 05 03 01 24 50 WARN shortcircuit DomainSocketFactory The short circuit local reads feature cannot be used because libhadoop cannot be loaded 17 05 03 01 24 50 INFO impl TimelineClientImpl Timeline service address http server X X X X 8188 ws v1 timeline 17 05 03 01 24 50 INFO client RMProxy Connecting to ResourceManager at server X X X X 8050 17 05 03 01 24 50 INFO client AHSProxy Connecting to Application History server at server X X X X 10200 17 05 03 01 24 51 INFO dmlc Client jobname DMLC nworker 2 nsever 2 python username root 17 05 03 01 24 51 INFO dmlc Client Submitting application application 1489701902537 0147 17 05 03 01 24 51 INFO impl YarnClientImpl Submitted application application 1489701902537 0147 Application application 1489701902537 0147 finished with state FINISHED at 1493774702568 Diagnostics num tasks4 finished 0 failed 4 DMLC Task 0 failed more than 3times Available queues default 17 05 03 01 25 02 INFO impl YarnClientImpl Killed application application 1489701902537 0147 Here is my config mk for mxnet Template configuration for compiling mxnet If you want to change the configuration please use the following steps Assume you are on the root directory of mxnet First copy the this file so that any local changes will be ignored by git cp make config mk Next modify the according entries and then compile by make or build in parallel with 8 threads make j8 choice of compiler export CC gcc export CXX g export NVCC nvcc whether compile with options for MXNet developer DEV 0 whether compile with debug DEBUG 0 whether compiler with profiler USE PROFILER the additional link flags you want to add ADD LDFLAGS the additional compile flags you want to add ADD CFLAGS matrix computation libraries for CPU GPU whether use CUDA during compile USE CUDA 0 add the path to CUDA library to link and compile flag if you have already add them to environment variable leave it as NONE USE CUDA PATH usr local cuda USE CUDA PATH NONE whether use CuDNN R3 library USE CUDNN 0 CUDA architecture setting going with all of them For CUDA 6 0 comment the 50 lines for compatibility CUDA ARCH gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 50 code compute 50 whether use cuda runtime compiling for writing kernels in native language i e Python USE NVRTC 0 whether use opencv during compilation you can disable it however you will not able to use imbin iterator USE OPENCV 0 use openmp for parallelization USE OPENMP 1 MKL ML Library for Intel CPU Xeon Phi Please refer to MKL README md for details MKL ML Library folder need to be root for usr local Change to User Home directory for standard user For USE BLAS mkl only MKLML ROOT usr local whether use MKL2017 library USE MKL2017 0 whether use MKL2017 experimental feature for high performance Prerequisite USE MKL2017 1 USE MKL2017 EXPERIMENTAL 0 whether use NNPACK library USE NNPACK 0 choose the version of blas you want to use can be mkl blas atlas openblas in default use atlas for linux while apple for osx UNAME S shell uname s ifeq UNAME S Darwin USE BLAS apple else USE BLAS atlas endif add path to intel library you may need it for MKL if you did not add the path to environment variable USE INTEL PATH NONE If use MKL only for BLAS choose static link automatically to allow python wrapper ifeq USE MKL2017 0 ifeq USE BLAS mkl USE STATIC MKL 1 endif else USE STATIC MKL NONE endif Settings for power and arm arch ARCH shell uname a ifneq filter ARCH armv6l armv7l powerpc64le ppc64le aarch64 USE SSE 0 else USE SSE 1 endif distributed computing whether or not to enable multi machine supporting USE DIST KVSTORE 1 whether or not allow to read and write HDFS directly If yes then hadoop is required USE HDFS 0 path to libjvm so required if USE HDFS 1 LIBJVM JAVA HOME jre lib amd64 server whether or not allow to read and write AWS S3 directly If yes then libcurl4 openssl dev is required it can be installed on Ubuntu by sudo apt get install y libcurl4 openssl dev USE S3 0 additional operators path to folders containing projects specific operators that you do not want to put in src operators EXTRA OPERATORS other features Create C interface package USE CPP PACKAGE 0 plugins whether to use caffe integration This requires installing caffe You also need to add CAFFE PATH build lib to your LD LIBRARY PATH CAFFE PATH HOME caffe MXNET PLUGINS plugin caffe caffe mk whether to use torch integration This requires installing torch You also need to add TORCH PATH install lib to your LD LIBRARY PATH TORCH PATH HOME torch MXNET PLUGINS plugin torch torch mk WARPCTC PATH HOME warp ctc MXNET PLUGINS plugin warpctc warpctc mk whether to use sframe integration This requires build sframe git github com dato code SFrame git SFRAME PATH HOME SFrame MXNET PLUGINS plugin sframe plugin mk USE BLAS atlas ADD CFLAGS I usr include openblas ADD CFLAGS I usr include openblas L usr local lib ADD LDFLAGS lopencv core lopencv imgproc lopencv imgcodecs Can someone please tell me what is wrong with my set up,,szha,2017-05-03 01:39:29,2017-09-29 23:33:12
IS,mxnet conv only support zero padding,I find in torch there are different types of padding like SpatialReflectionPadding SpatialReplicationPadding In mxnet are there similar padding methods,,"xlvector,piiswrong,sbodenstein,piiswrong,xlvector,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein,sbodenstein,matteosal,matteosal,szha",2016-10-18 04:25:17,2017-09-29 23:33:13
IS,skip exec node does not work for forward backward nodes in computational graph,Hello I m doing a synchronous distributed machine learning project where I am trying to terminate some of the slower instances workers per distributed iteration This means that out of the n workers I want to only wait for k workers say 90 to complete their gradient computations At the same time I would like to terminate the n k workers that haven t reported their gradients back to the master When the next iteration starts all n workers including the n k terminated ones will again compute gradients as if it is a normal distributed iteration I m trying to force the slow workers to go directly into the next iteration by using skip exec node L76 e g skip the back prop nodes in the computational graph Environment info Operating System ubuntu 14 04 Compiler gcc 4 8 4 Package used Python MXNet version Build from source MXNet commit hash git rev parse HEAD 974d7dad55c6a07ccae8333af1c6f064920fe9c9 Python Version python 2 7 6 Error Message Without skipping any nodes the time cost is like 'Time cost for forward backward 0 0002989768981933594' 'Time cost for update metric 0 25156497955322266' Then in my very simple example in which I just skip all the nodes in computational graph the time costs after the skip are 'Time cost for forward backward 0 0003230571746826172' 'Time cost for update metric 0 11911201477050781' My question is why skip exec node will skip some execution for update metric operation rather than forward backward execution How can I skip the forward and backward prop operation in MXNet Minimum reproducible example I only added a few lines of code in L780 everything else is the same as the original version Steps to reproduce The example I use is the image classification examples 1 cd mxnet example image classification 2 python train mnist py network lenet batch size 128,,"piiswrong,mli,mli,szha",2017-04-30 19:25:58,2017-09-29 23:33:16
IS,ps lite clarification in distributed systems and ps lite version in MXNet,Hi all In regards to parameter server I have two questions MXNet uses parameter server but I am guessing that ps lite is that exactly the same parameter server version generation that is using in dmlc MXNet Can anybody shed some light on this In ps lite the distributed systems have two costs in addition to single machine One is the data communication cost namely sending data over the network the other one is synchronization cost due to the imperfect load balance and performance variance cross machines Can anybody explain the second cost in the distributed version,,"pluskid,szha",2017-05-05 16:59:41,2017-09-29 23:33:17
IS,random forest neural networks xgboost gbdt,for the binary classification problem on some structural data I find RF is performance is amazing,,szha,2017-05-08 03:34:31,2017-09-29 23:33:18
IS,KVStoreDist Init called many times,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you When running 1 worker and 1 server with distributed learning I noticed that while the worker initializes one KVStoreDist src kvstore kvstore dist h its Init call appears to be called many times I'm wondering if this intended and why is this necessary Many thanks Environment info Operating System Ubuntu 14 04 Compiler GCC 5 2 Package used Python R Scala Julia Python MXNet version Or if installed from source Installed from source MXNet commit hash git rev parse HEAD df27b9c7c35d98f22b9b19d6c84af21f5df97400 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message 04 14 44 src kvstore kvstore dist h 63 Calling INIT 0 04 14 44 src kvstore kvstore dist h 77 Calling Barrier 0 04 14 44 src kvstore kvstore dist h 80 Ending INIT 0 04 14 44 src kvstore kvstore dist h 63 Calling INIT 0 04 14 44 src kvstore kvstore dist h 77 Calling Barrier 0 04 14 44 src kvstore kvstore dist h 80 Ending INIT 0 04 14 44 src kvstore kvstore dist h 63 Calling INIT 0 04 14 44 src kvstore kvstore dist h 77 Calling Barrier 0 04 14 44 src kvstore kvstore dist h 80 Ending INIT 0 Minimum reproducible example mxnet tools launch py H train hosts n1 s 1 python train resnet py data dir home ubuntu train imagenet data lowq256 aug level 1 depth 18 batch size 1 kv store dist sync device Steps to reproduce Just run that script What have you tried to solve it I'm not sure if this is a problem or is known so it is intended,,"eric-haibin-lin,szha",2017-05-08 04:20:46,2017-09-29 23:33:19
IS,Question about Makefile and cmake,I have a question about build system Why not just maintain one build system such as cmake I notice that the Makefile is hand written for now Does it mean that I have to build project under the source file directory if using Makefile,,"EricFisher,szha",2017-05-08 06:59:42,2017-09-29 23:33:21
IS,mxnet cuda9 0 and cudnn 7 GPU slower than CPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Linux cbw server 4 10 0 32 generic 36 16 04 1 Ubuntu SMP Wed Aug 9 09 19 02 UTC 2017 x86 64 x86 64 x86 64 GNU Linux Compiler Package used Python R Scala Julia Python MXNet version mxnet cu8 0 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 3 6 2 Anaconda Inc If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Use the tutorial in the mxnet straight dope chapter02 liner regression gluon 2 change ctx to GPU 3 the GPU training time is slower than CPU time What have you tried to solve it 1 Is it the reason that i install cuda 9 0 and cudnn 7 0,,yajiedesign,2017-09-29 13:55:19,2017-09-30 01:32:51
IS,How to put 2 networks in different gpus resnet152 in gpu0 and resnet50 in gpu1,Because of my large batch size I can not put both resnet152 and resnet50 in the same gpu How to put 2 network in different gpus I found the lstm example of model parallelism but it is hard for me to understand Who can show me an easier example Thanks,,solin319,2017-09-25 07:23:45,2017-09-30 02:03:52
PR,WIP support multi workers in the same process,x atomically update customer id test in mac with spark test in ubuntu with spark x fix mac compilation problem,,"CodingCat,piiswrong,CodingCat",2017-09-29 14:17:49,2017-09-30 04:37:38
PR,Just a test2 Do not merge,Just an empty commit Please do not Merge,,"mbaijal,eric-haibin-lin",2017-09-12 22:04:22,2017-09-30 04:39:03
PR,Refactor infer storage function for sparse operators,Background The current rule for graph executor to dispatch on FCompute FComputeEx is naive if any input output contains non default storage FComputeEx will be preferred otherwise choose FCompute Why it is bad Such dispatch rule may lead to the case where FComputeEx is registered and chosen to dispatch but it is not capable of handle specific storage types and need to fallback inside FComputeEx Ideally we want FComputeEx functions to be fallback free Since fallback requires temp resource on gpu when fallback happens inside FComputeEx the resource needs to be requested on the fly This leads to unclear responsibility since all resource requests are expected to be handled in attach exec resource pass The new interface to fix it We should incorporate dispatch logic inside infer storage type pass with the new interface below During infer storage type pass we will decide whether to dispatch for FComputeEx or FCompute based on attrs When dispatch type is kFComputeFallback extra resources are requested Changes Summary Updated infer attr pass function for graph executor which now is able to infer attribute for node entries storage type and for the node itself dispatch mode The stype of gradients are provided as inputs diff d8b5a5b027d00584737fb6486cba38b9R585 to InferStorage during Executor Bind Added default infer storage type diff 4ccbea6a44e3cc4c99195a1e7c55c09eR321 function If an operator does not have FInferStorageType attr set this infer storage type function will be called Added warning messages when storage fallback is detected during inferstorage It can be turned off via environment variable Updated the infer storage function for all sparse operators and custom operators Removed all FComputeFallback call inside FComputeEx Removed ad hot storage resource requests in cast storage Removed a kernel for SGD with dense grad and row sparse weight Disabled elemwise binary op for CSR inputs since we do not have any unit test for them We should turn it back on after the test is in place The operator should set FResourceRequest to get temp storage instead of doing that inside the operator Added registration for many unary operators on gpu They are already covered in test sparse operator when default ctx is gpu Removed some unused functions in test sparse operator py,,"eric-haibin-lin,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,eric-haibin-lin,eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-09-19 17:54:09,2017-09-30 05:55:11
IS,WarpCTC Plugin fails to compile on OSX with CUDA support,Environment info Operating System OSX Compiler Clang Apple LLVM version 8 0 0 clang 800 0 42 1 Error Message and set USE CUDNN 1 USE CUDA PATH usr local cuda USE CUDA 1,,"sbodenstein,sbodenstein,piiswrong,piiswrong,sbodenstein,yajiedesign",2017-05-07 19:32:51,2017-09-30 11:04:22
IS,Check failed vals size total val in KVWorker Val Pull,Hello I store some push requests from a worker node in a list and do not let the server node handle these push requests until a certain requirement is satisfied i e not handle the present pull request immediately block it until a time the user sets For example if a worker node is too fast and has run 10 more iterations than the slowest worker node in the cluster I try to block the handle of the present push request from this fastest worker node in the server node only handle the push requests in the waitlist and send responses to the fastest worker node if the slowest worker has catched up with it However I met an error as below 03 19 47 home xiongzi mxnet dmlc core include dmlc logging h 235 03 19 47 home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val terminate called after throwing an instance of wouldmlc Error' what 03 19 47 home xiongzi mxnet ps lite include ps kv app h 579 Check failed vals size total val The code is in mxnet ps lite include ps kv app h I feel puzzled about the code that raises the error in the function KVWorker Val Pull CHECK EQ vals size total val Could someone please explain this sentence of code for me vals size refers to the the size of values in the present pull request and the total val refers to the total size of all values of the pull request is timestamp Plz point out my mistake if I says something wrong So what is the purpose of checking whether vals size and total val are equal to each other And what may cause the error I mention above Thanks a lot,,yajiedesign,2017-05-08 08:37:00,2017-09-30 11:04:25
IS,API docs submission GridGenerator,Description Generates 2d sampling grid for bilinear sampling Parameters,,"Lyken17,piiswrong,yajiedesign",2017-05-08 02:31:20,2017-09-30 11:04:29
IS,Negative padding specification in Padding Layer negative values in TShapes,Negative padding specification in Padding Layer are officially not supported but are not catched and result in undefined behavior at least when using the python interface Negative values are overflown to large positive values in the padding spec TShape before hitting the operator Magically enough some other overflow happens during evaluation and the output is actually a cropped tensor even if this is unintended for 'constant' and 'edge' methods only Also this is maybe affecting more than just the padding layer,,"matteosal,piiswrong,yajiedesign",2017-05-05 12:44:26,2017-09-30 11:04:33
IS,Are there any examples or instruction of using data augmentation in ImageRecordIter v1,New ImageRecordIter v1 provides rich data augmentation options But i do not know how to use them Are there any examples or instruction of using data augmentation in ImageRecordIter v1,,"ysh329,yajiedesign",2017-04-30 05:49:11,2017-09-30 11:04:36
IS,API docs submission LinearRegressionOutput,Description It is used to add output layer based on linear activation to the neural network It computes and optimizes for squared loss Parameters,,"domdivakaruni,yajiedesign",2017-05-03 09:49:18,2017-09-30 11:04:39
IS,More efficient implementation of strided convolution,I need to use strided convolution to do some classification tasks But I find that strided convolution is not as efficient as expected For example convolution with stride 2 costs more time than convolution with stride 1 I do not know why Is there an efficient way to implement strided convolution,,"piiswrong,yajiedesign",2017-05-08 07:44:37,2017-09-30 11:04:44
IS,Duplicate classes in the Python API,MXNet currently has duplicates such as mxnet io ImageRecordIter and mxnet io ImageRecordIter v1 which is detrimental to both the users and developers An easy way around these problems would be to have better separation between dev and stable branches along with two versions at least of the doc stable and dev with an auto redirect to the stable version by default,,"JeanKossaifi,piiswrong,yajiedesign",2017-05-09 00:45:53,2017-09-30 11:04:48
IS,make bin im2rec Error 1,hello I use ubuntu14 04 cuda8 cudnn5 1 opencv2 4 9 When I run the command bash install mxnet ubuntu python sh it shows error this seems to be the cause of opencv how to solve this problem any advice is appreciated Thank you very much,,"Lyken17,yajiedesign",2017-03-10 08:10:59,2017-09-30 11:04:52
IS,Symbol Compose not get correct node inputs when run the line n inputs i it second outputs 0,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows Compiler Visual Studio 2013 Package used Python R Scala Julia Python MXNet version Or if installed from source 0 9 3 official release MXNet commit hash git rev parse HEAD b11d3a255 If you are using python package please provide Python version and distribution 2 7 11 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Traceback most recent call last File D users keche code mxnet example image classification fine tune py line 53 in module sym arg params args num classes args layer before fullc File D users keche code mxnet example image classification fine tune py line 18 in get fine tune model net mx symbol FullyConnected data net num hidden num classes name 'fc' File C Miniconda2 x64 lib site packages mxnet 0 9 3 py2 7 egg mxnet ctypes symbol py line 191 in creator s compose args name name symbol kwargs File C Miniconda2 x64 lib site packages mxnet 0 9 3 py2 7 egg mxnet symbol py line 250 in compose self handle name num args keys args WindowsError exception access violation reading 0x0000000000000080 Process finished with exit code 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error I add log info in the symbolic cc void Symbol Compose const array view const Symbol args for size t i args size i n req i auto it kwargs find arg names i if it kwargs end it first arg names i LG Compose here 1 1 1 1 nnvm get VariableParam it second outputs 0 node attrs parsed n inputs i it second outputs 0 nmatched LG Compose here 1 1 1 1 quit else LG Compose here 1 1 1 2 n inputs i NodeEntry CreateVariableNode DefaultVarName name arg names i 0 0 copy attribute of parent over automatically created variables n inputs i node attrs dict n attrs dict Then the error happened in the line nnvm get VariableParam it second outputs 0 node attrs parsed Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error I run the fine tune py,,yajiedesign,2017-05-09 12:57:15,2017-09-30 11:04:55
IS,Check failed type nullptr The any container is empty requested struct nnvm VariableParam,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows Server 2012 Compiler Visual Studio 2013 Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source Build from source MXNet commit hash git rev parse HEAD 1dec793e1dc2eaa5d82780ededb2986bf1f45d6a If you are using python package please provide Python version and distribution Python 2 7 11 Miniconda2 x64 If you are using R package please provide R sessionInfo Error Message 15 39 31 D code mxnet git dmlc core include dmlc logging h 300 15 39 31 D code mxnet git dmlc core include dmlc any h 261 Check failed type nullptr The any container is empty requested struct nnvm VariableParam Traceback most recent call last File D code mxnet git example image classification fine tune py line 53 in module sym arg params args num classes args layer before fullc File D code mxnet git example image classification fine tune py line 18 in get fine tune model net mx symbol FullyConnected data net num hidden num classes name 'fc' File C Miniconda2 x64 lib site packages mxnet 0 9 5 py2 7 egg mxnet ctypes symbol py line 191 in creator s compose args name name symbol kwargs File C Miniconda2 x64 lib site packages mxnet 0 9 5 py2 7 egg mxnet symbol py line 311 in compose self handle name num args keys args File C Miniconda2 x64 lib site packages mxnet 0 9 5 py2 7 egg mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 15 39 31 D code mxnet git dmlc core include dmlc any h 261 Check failed type nullptr The any container is empty requested struct nnvm VariableParam Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error example image classification fine tune py Pyhon fine tune py pretrained model mxnet git model inception bn xb load epoch 25 num classes 63 layer before fullc global pool data train Images 20170416 shuffle rec data val 0170413 pnEqual shuffle rec num examples 67000 lr step epochs 50 100 150 gpus 0 1 model prefix model inception bn new num epoch 300 top k 5 What have you tried to solve it I tried to build two versions 0 9 5 and 0 9 3 the same issue happened,,yajiedesign,2017-05-03 07:54:20,2017-09-30 11:04:59
IS,Error accnn py,Hello I try to use accnn py so as to accelerate the inception model but I get an error The model comes from python home rd Documents RD2 lib mxnet tools accnn accnn py m Inception 7 load epoch 1 save model new inception ratio 2 15 30 12 src nnvm legacy json util cc 190 Loading symbol saved by previous version v0 8 0 Attempting to upgrade 15 30 12 src nnvm legacy json util cc 198 Symbol successfully upgraded home rd Documents RD2 lib mxnet python mxnet model py 870 DeprecationWarning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead kwargs 15 30 35 home rd Documents RD2 lib mxnet dmlc core include dmlc logging h 300 15 30 33 src operator pooling inl h 196 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 8 exceeds input 5 padded to 5 Stack trace returned 10 entries bt 0 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f172cf9965c bt 1 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZNK5mxnet2op11PoolingProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x92f 0x7f172dabe30f bt 2 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0xe19a52 0x7f172d83ea52 bt 3 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0x1c6a3fc 0x7f172e68f3fc bt 4 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0x1c6b99a 0x7f172e69099a bt 5 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0x1c6ccad 0x7f172e691cad bt 6 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaIS7 EE 0x32c 0x7f172e6b6fac bt 7 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x3c9 0x7f172dbc5b79 bt 8 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EENSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x1e5 0x7f172dbc9145 bt 9 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so MXSymbolInferShape 0xdeb 0x7f172dbbf48b infer shape error Arguments data 1 3 224 224 Traceback most recent call last File home rd Documents RD2 lib mxnet tools accnn accnn py line 25 in module config 'conv params' rank selection get ranksel model args ratio File home rd Documents RD2 lib mxnet tools accnn rank selection py line 23 in get ranksel output shapes model symbol get internals infer shape data 1 3 224 224 File home rd Documents RD2 lib mxnet python mxnet symbol py line 636 in infer shape res self infer shape impl False args kwargs File home rd Documents RD2 lib mxnet python mxnet symbol py line 719 in infer shape impl ctypes byref complete File home rd Documents RD2 lib mxnet python mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator global pool 15 30 33 src operator pooling inl h 196 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 8 exceeds input 5 padded to 5 Stack trace returned 10 entries bt 0 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f172cf9965c bt 1 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZNK5mxnet2op11PoolingProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x92f 0x7f172dabe30f bt 2 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0xe19a52 0x7f172d83ea52 bt 3 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0x1c6a3fc 0x7f172e68f3fc bt 4 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0x1c6b99a 0x7f172e69099a bt 5 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so 0x1c6ccad 0x7f172e691cad bt 6 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaIS7 EE 0x32c 0x7f172e6b6fac bt 7 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x3c9 0x7f172dbc5b79 bt 8 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EENSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x1e5 0x7f172dbc9145 bt 9 home rd Documents RD2 lib mxnet python mxnet lib libmxnet so MXSymbolInferShape 0xdeb 0x7f172dbbf48b how can I solve this problem,,yajiedesign,2017-05-09 13:35:57,2017-09-30 11:05:02
IS,New logo choices,Here are some delicious logo design contributions thanks to musho what do you think 03 mxnet visual03 04 mxnet visual04 05a mxnet visual05 a 05b mxnet visual05 b,,"domdivakaruni,pluskid,mli,eric-haibin-lin,madjam,CodingCat,phunterlau,antinucleon,ZihengJiang,arank,sandeep-krishnamurthy,ysh329,yzhliu,terrychenism,CNevd,piiswrong,ysh329,Godricly,nswamy,piiswrong,bhavinthaker,pracheer,yajiedesign",2017-05-04 19:12:45,2017-09-30 11:05:05
IS,Cross Entropy does not change when running LSTM speech timit demo,I am new to MXnet and have just installed the newest MXnet in my Ubuntu 14 04 5 LTS server by using bash install mxnet ubuntu python sh command I have tried some matrix computation demo by using both CPU and GPU after installation It proved that the installation was successful As a Kaldi user my goal is to try using mxnet to train acoustic models So I decided to try the Speech LSTM Demo The kaldi part of timit was already successful and after modifing all configurations related to mxnet I ran the run timit sh script and the configuration file that I used was default timit cfg Then I got the following error report I think it is a big problem and I am still struggling with it Can anyone give me some advice about how to debug this problem Thanks very much,,yajiedesign,2017-05-09 09:27:05,2017-09-30 11:05:08
IS,how can i print any weight arrays using monitor,there is one example code using monitor return mx nd norm d np sqrt d size but it print the L2 of weight if i would like to the weight array how can i change the code i tried return d asnumpy it report assert isinstance v list list,,yajiedesign,2017-05-10 13:53:49,2017-09-30 11:05:12
IS,Take up too much GPU memory,Neural style python2 7 ubuntu with GPU0 when training is about 1 8G GPU memory took up with the default parameters So what things can I do to reduce the GPU memory,,"sxjscience,yajiedesign",2017-03-20 11:57:30,2017-09-30 11:05:15
IS,High memory consumption using NDArray interface,Here are two codes for comparison It only consumes 400MB memory Way smaller than the previous one and also faster It seems that all the intermediate memory spaces used for the operator are somehow created rather than doing inplace operation The question is when the space will be allocated The is a mutable operation so they should be serialized during computation Is this because our is not correctly implemented The mxnet version is the latest version on nnvm branch This is related to the RNN performance of memory consumption of MinPy Please help,,"jermainewang,piiswrong,jermainewang,jermainewang,piiswrong,jermainewang,jermainewang,piiswrong,hotpxl,piiswrong,yajiedesign",2016-12-20 00:22:07,2017-09-30 11:05:26
IS,Is there any convenient way to print the Tensor xpu dim struct content in the operator forward function,Hello I want to debug the operator forward and backward function in operator class to see what it means And I write a function in that class to print out the data in Tensor When I run mxnet with this code invoke the function like this print info tensor data However The program will generate the 'Segment fault' error when execute the std cout tensor dptr j Waiting for your help thanks,,"Godricly,piiswrong,yajiedesign",2016-08-29 02:09:50,2017-09-30 11:05:30
IS,ToolkitError Error applying callback Error code 1,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OSX 10 11 Compiler Unknown Package used Python R Scala Julia Python MXNet version 0 7 0 If you are using python package please provide Python version and distribution 2 7 13 Error Message Please paste the full error message including stack trace ToolkitError Traceback most recent call last ipython input 39 e494156d0f91 in module 1 mx model mx model FeedForward create symbol softmax X dataiter num epoch num epoch learning rate 0 1 wd 0 0001 momentum 0 9 eval metric mx metric Accuracy Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet model pyc in create symbol X y ctx num epoch epoch size optimizer initializer eval data eval metric epoch end callback batch end callback kvstore logger work load list eval batch end callback kwargs 912 logger logger 913 work load list work load list 914 eval batch end callback eval batch end callback 915 return model 916 Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet model pyc in fit self X y eval data eval metric epoch end callback batch end callback kvstore logger work load list monitor eval batch end callback 793 logger logger work load list work load list monitor monitor 794 eval batch end callback eval batch end callback 795 sym gen self sym gen 796 797 Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet model pyc in train multi device symbol ctx arg names param names aux names arg params aux params begin epoch end epoch epoch size optimizer kvstore update on kvstore train data eval data eval metric epoch end callback batch end callback logger work load list monitor eval batch end callback sym gen 220 while True 221 do reset True 222 for data batch in train data 223 executor manager load data batch data batch 224 Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet io pyc in next self 71 The data of next batch 72 73 if self iter next 74 return DataBatch data self getdata label self getlabel 75 pad self getpad index self getindex Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet io pyc in iter next self 733 734 def iter next self 735 ret super self class self iter next 736 Postprocess normalize by mean scale 737 self data ndarray self data ndarray self rgb mask self scale Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet io pyc in iter next self 606 self pad end self data size 607 end self data size 608 self copy start end 609 if self pad 0 610 bias self batch size self pad Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet io pyc in copy self start end bias 594 595 def copy self start end bias 0 596 copy from sframe self data sframe self data ndarray start end self field length bias 597 if self label field is not None 598 copy from sarray self label sframe self label ndarray start end 1 bias Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab mxnet ndarray pyc in copy from sframe sf arr start end field length bias 96 callback handle SFrameCallBackHandle arr handle value bias end start 97 ctypes cast c field length arr ctypes POINTER ctypes c ulonglong 98 gl extensions sframe callback sf addr ctypes addressof callback handle start end 99 100 Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab extensions pyc in lambda args kwargs 166 167 def make injected function fn arguments 168 return lambda args kwargs run toolkit function fn arguments args kwargs 169 170 def class instance from name class name arg kwarg Users songshuang anaconda2 envs gl env lib python2 7 site packages graphlab extensions pyc in run toolkit function fnname arguments args kwargs 155 if ret 0 True 156 if len ret 1 0 157 raise ToolkitError ret 1 158 else 159 raise ToolkitError Toolkit failed with unknown error ToolkitError Error applying callback Error code 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error mx model mx model FeedForward create symbol softmax X dataiter num epoch num epoch learning rate 0 1 wd 0 0001 momentum 0 9 eval metric mx metric Accuracy,,yajiedesign,2017-05-10 17:27:40,2017-09-30 11:05:34
IS,Using ignore labels in SoftmaxOutput with multi output true,I have a confusion regarding using SoftmaxOutput for multi label classification with ignore labels It seems that if the labels are provided as a 1 hot array the ignore labels are not honoured and gradients are still compute for those x y locations For example Lets say we have a image segmentation problem with two labels S Sky 0 and G Ground 1 and C Car 2 and ignore label 255 You can construct the labels using either 1ofK encoding or using a single array to hold all labels Method1 Label 0 1 1 1 1 255 255 0 0 0 0 0 0 Label 1 0 0 0 0 255 255 0 0 1 1 1 1 Label 2 0 0 0 0 255 255 1 1 0 0 0 0 Method2 Label 0 0 0 0 255 255 2 2 1 1 1 1 1 And say your prediction will look something like this Pred 0 0 9 0 9 0 9 0 9 0 9 0 9 0 0 0 0 0 0 Pred 1 0 0 0 0 0 0 0 0 0 9 0 9 0 9 0 9 Pred 2 0 0 0 0 0 0 0 9 0 9 0 0 0 0 When we use Method1 with SoftmaxLabel with multi output true the ignore labels are NOT honoured in the gradients computed by backward However they work as expected with Method2 Why does ignore label work differently for method1 and method2 Sample code to reproduce this is attached import mxnet as mx import numpy as np def test softmax workspace 256 import logging head ' asctime 15s message s' logging basicConfig level logging DEBUG format head ignore label 255 data mx sym Variable name wouldata' input data to softmax joint labels mx sym Variable name 'labs' sym mx sym SoftmaxOutput data data label joint labels multi output True use ignore True ignore label ignore label name isoftmax' l1 np array 1 1 1 1 255 255 0 0 0 0 0 0 0 0 0 0 255 255 0 0 1 1 1 1 0 0 0 0 255 255 1 1 0 0 0 0 l2 np array 0 0 0 0 255 255 2 2 1 1 1 1 d1 np array 0 9 0 9 0 9 0 9 0 9 0 9 0 0 0 0 0 0 0 0 0 0 0 9 0 9 0 0 0 9 0 9 0 9 0 9 0 0 0 0 0 9 0 9 0 9 0 9 0 0 0 0 d1 np expand dims d1 axis 0 l1 np expand dims l1 axis 0 l2 np expand dims l2 axis 0 print would1 ' format d1 shape print 'l1 ' format l1 shape print 'l2 ' format l2 shape print would1 ' format d1 print 'l1 ' format l1 print 'l2 ' format l2 d mx nd array d1 lmx1 mx nd array l1 lmx2 mx nd array l2 dx mx nd empty d1 shape e1 sym bind ctx mx cpu args wouldata' d 'labs' lmx1 args grad wouldata' dx e1 forward is train True e1 backward out grads mx nd ones dx shape print ' n e1 grad arrays 0 ' format e1 grad arrays 0 asnumpy e2 sym bind ctx mx cpu args wouldata' d 'labs' lmx2 args grad wouldata' dx e2 forward is train True e2 backward out grads mx nd ones dx shape print ' n e2 grad arrays 0 ' format e1 grad arrays 0 asnumpy if name ' main ' test softmax The returned output is d1 1 3 3 4 l1 1 3 3 4 l2 1 3 4 d1 0 9 0 9 0 9 0 9 0 9 0 9 0 0 0 0 0 0 0 0 0 0 0 9 0 9 0 0 0 9 0 9 0 9 0 9 0 0 0 0 0 9 0 9 0 9 0 9 0 0 0 0 l1 1 1 1 1 255 255 0 0 0 0 0 0 0 0 0 0 255 255 0 0 1 1 1 1 0 0 0 0 255 255 1 1 0 0 0 0 l2 0 0 0 0 255 255 2 2 1 1 1 1 e1 grad arrays 0 4 48470414e 01 4 48470414e 01 4 48470414e 01 4 48470414e 01 2 54666672e 02 2 54666672e 02 2 24235207e 01 2 24235207e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 54666672e 02 2 54666672e 02 2 24235207e 01 2 24235207e 01 4 48470414e 01 4 48470414e 01 4 48470414e 01 4 48470414e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 54666672e 02 2 54666672e 02 4 48470414e 01 4 48470414e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 2 24235192e 01 e2 grad arrays 0 0 03737254 0 03737254 0 03737254 0 03737254 0 0 0 01868627 0 01868627 0 01868627 0 01868627 0 01868627 0 01868627 0 01868627 0 01868627 0 01868627 0 01868627 0 0 0 01868627 0 01868627 0 03737254 0 03737254 0 03737254 0 03737254 0 01868627 0 01868627 0 01868627 0 01868627 0 0 0 03737254 0 03737254 0 01868627 0 01868627 0 01868627 0 01868627 Note that the e2 grad has 0 is where the ignore label is set where as e1 does not honour the ignore label Can someone explain what I am doing wrong here,,yajiedesign,2017-05-11 05:54:12,2017-09-30 11:05:40
IS,core dump when predict using multithreading,I want to predict using multithreading every thread load the same model to predict simultaneously because one machine has four card But I find that single thread is ok multi thread will core The core happen in nnvm Op UpdateAttrMap function detailed information listed below is there anybody knows what happend btw If I invoke MXPredCreatePartialOut serially by mutex that is ok Environment info Operating System CentOS 6 3 Compiler gcc 4 8 2 Package used Python R Scala Julia c MXNet version 0 9 3 Or if installed from source yes MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 0 0x00007f1a42c8c3f7 in raise from opt compiler gcc 4 8 2 lib libc so 6 1 0x00007f1a42c8d7d8 in abort from opt compiler gcc 4 8 2 lib libc so 6 2 0x00007f1a42cca554 in libc message from opt compiler gcc 4 8 2 lib libc so 6 3 0x00007f1a42ccfdbe in malloc printerr from opt compiler gcc 4 8 2 lib libc so 6 4 0x00007f1a42cd0a97 in int free from opt compiler gcc 4 8 2 lib libc so 6 5 0x00007f1a4b443ef9 in std Function base Base manager mxnet op lambda nnvm NodeAttrs const 4 M destroy at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 functional 1926 6 0x00007f1a4b44379e in std Function base Base manager mxnet op lambda nnvm NodeAttrs const 4 M manager at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 functional 1950 7 0x00007f1a4a8c11f9 in std Function base Function base at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 functional 2030 8 0x00007f1a4b1fcd50 in std function unsigned int nnvm NodeAttrs const function at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 functional 2174 9 0x00007f1a4b4487a2 in std pair std function unsigned int nnvm NodeAttrs const int pair at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl pair h 96 10 0x00007f1a4b456caa in void std Destroy std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl construct h 93 11 0x00007f1a4b4557c4 in void std Destroy aux false destroy std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl construct h 103 12 0x00007f1a4b45371b in void std Destroy std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl construct h 126 13 0x00007f1a4b450899 in void std Destroy std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int std pair std function unsigned int nnvm NodeAttrs const int std allocator std pair std function unsigned int nnvm NodeAttrs const int at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl construct h 151 14 0x00007f1a4b453cd8 in std vector std pair std function unsigned int nnvm NodeAttrs const int std allocator std pair std function unsigned int nnvm NodeAttrs const int M fill insert gnu cxx normal iterator std pair std function unsigned int nnvm NodeAttrs const int std vector std pair std function unsigned int nnvm NodeAttrs const int std allocator std pair std function unsigned int nnvm NodeAttrs const int unsigned long std pair std function unsigned int nnvm NodeAttrs const int const at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits vector tcc 517 15 0x00007f1a4b450b60 in std vector std pair std function unsigned int nnvm NodeAttrs const int std allocator std pair std function unsigned int nnvm NodeAttrs const int insert gnu cxx normal iterator std pair std function unsigned int nnvm NodeAttrs const int std vector std pair std function unsigned int nnvm NodeAttrs const int std allocator std pair std function unsigned int nnvm NodeAttrs const int unsigned long std pair std function unsigned int nnvm NodeAttrs const int const at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl vector h 1024 16 0x00007f1a4b44bee7 in std vector std pair std function unsigned int nnvm NodeAttrs const int std allocator std pair std function unsigned int nnvm NodeAttrs const int resize unsigned long std pair std function unsigned int nnvm NodeAttrs const int const at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 bits stl vector h 687 17 0x00007f1a4b448b31 in nnvm Op nnvm Op set attr std function unsigned int nnvm NodeAttrs const std basic string char std char traits char std allocator char const std function unsigned int nnvm NodeAttrs const const int lambda dmlc any 1 operator dmlc any const at home img mxnet mxnet nnvm include nnvm op h 444 18 0x00007f1a4b450c86 in std Function handler void dmlc any nnvm Op nnvm Op set attr std function unsi Type return to continue or q return to quit gned int nnvm NodeAttrs const std basic string char std char traits char std allocator char const std function unsigned int nnvm NodeAttrs const const int lambda dmlc any 1 M invoke std Any data const dmlc any at home opt gcc 4 8 2 bpkg r2 gcc 4 8 2 bpkg r2 include c 4 8 2 functional 2071 19 0x00007f1a4d263317 in nnvm Op UpdateAttrMap std basic string char std char traits char std allocator char const std function void dmlc any from lib libmxnet so 20 0x00007f1a4b448f7c in nnvm Op nnvm Op set attr std function unsigned int nnvm NodeAttrs const std basic string char std char traits char std allocator char const std function unsigned int nnvm NodeAttrs const const int at home img mxnet mxnet nnvm include nnvm op h 426 21 0x00007f1a4b5f0565 in mxnet op RegisterLegacyOpProp at src nnvm legacy op util cc 330 22 0x00007f1a4b48d874 in MXListAllOpNames at src c api c api symbolic cc 57 23 0x00007f1a4b4a61b6 in MXPredCreatePartialOut at src c api c predict api cc 88,,yajiedesign,2017-04-20 10:09:27,2017-09-30 11:05:43
IS,Warp CTC error on GPU cuda memcpy or memset failed,My env is Ubuntu 14 04 CUDA 8 0 cuDNN 5 1 Torch 7 0 GTX960 When I compile warp ctc everything is normal and passed warp ctc build test gpu I rebuild mxnet successfully except it shows warning nvcc warning The 'compute 20' ism 20' and ism 21' architectures are deprecated and may be removed in a future release Use Wno deprecated gpu targets to suppress warning Whichever I execute example image classification train mnist py on mx context cpu 0 or mx context gpu 0 it works normal But when I execute example warpctc toy ctc py on mx context gpu 0 an error occurred And if I use cpu it is OK How to solve this problem THX,,yajiedesign,2016-11-14 03:17:46,2017-09-30 11:05:46
IS,problem in example image classification predict cpp image classification predict cc,I'm using the MXNET in C API refer to the example in example image classification predict cpp image classification predict cc I have one question in the GetImageFile function The code as follow,,yajiedesign,2017-05-11 13:01:38,2017-09-30 11:05:50
IS,Any plan to enable multiprocessing feature of NDArray,Hi Is there any plan to implement a method that could share NDArray or model using multiprocessing Many tasks in deep reinforcement learning needs shared model to run in parallel and due to training and evaluation often happen on line like train a batch size of 20 then inference using parameter sever method multi machine is rather slow small batch size frequent parameter communication A reference could be Many Thanks,,"piiswrong,piiswrong,piiswrong,yajiedesign",2017-05-10 09:41:59,2017-09-30 11:05:52
IS,Is optimization a blocking operation on parameter server,Assuming that I have 1 parameter server node in dist async mode and multiple worker nodes I am trying to do most of the computation on the server using a custom updater function Essentially I am trying to implement Word2Vec using async SGD using the parameter server My updater method receives some indices from the workers and then it performs several matrix operations on the server itself to update the weights Will these operations be blocking If yes then is it true that I am not achieving any parallelism out of it,,"saurabh3949,yajiedesign",2017-05-11 21:11:25,2017-09-30 11:05:56
IS,tools im2rec cc does not handle error cases,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace im2rec cc throws exception in the cases where a file from lst is not an image or does not exist It would be better if it handled the exception instead like in im2rec py Minimum reproducible example if you are using your own code please provide a short script that reproduces the error bin im2rec image lst image root dir output bin resize 256 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-05-11 22:49:37,2017-09-30 11:05:59
IS,Distributed MXNet fails to bind symbols,Environment info Operating System Ubuntu 16 04 1 LTS Compiler Package used Python MXNet version mxnet 0 9 5 MXNet commit hash 232d8372c668b07ed7c7e58718fc529567139b00 If you are using python package please provide Python version and distribution Python 2 7 6 Error Message When trying to train ResNet on CIFAR10 my validation accuracy was very strange the system would essentially converge at 85 accuracy in the expected period of time then within 20 or so iterations the model would jump up to 92 accuracy and stay there barely changing accuracy between iterations I discovered that no matter which type of optimizer I selected there was no change to the way the model would train So I was searching through the code to find the root of this problem and I found that the program was getting stuck at this line of code L623 Digging a little deeper it turns out that this function call L1350 never returns I'm not even sure how the model is able to train without proceeding past this line but I put print statements on either side of it and only the first prints I have experienced the same problem running any of the models included in the zoo and have even reduced the model down to a single convolution layer FC softmax and it still occurs It only occurs when there is a convolution layer in the model Any idea what the problem could be I can not dig any deeper into the code as it eventually reaches some pre compiled library Minimum reproducible example This problem occurs when running ResNet from the model zoo in a distributed setting occurs with both kv store dist async and dist sync Steps to reproduce This problem occurs by running the following command python tools launch py n 4 launcher ssh H hosts python train cifar10 py network resnet kv store dist async gpus 0,,yajiedesign,2017-05-11 03:46:47,2017-09-30 11:06:03
IS,mxnet base MXNetError on gpus,I'm trying to run mxnet on an multi label problem using self defined MakeLoss the code runs fine on cpu but fails on gpus I cannot find any related information online Environment info Operating System Centos Compiler gcc 5 3 0 Package used Python R Scala Julia python 3 6 MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message 21 55 20 home ubuntu mxnet distro mxnet build dmlc core include dmlc logging h 304 21 55 20 home ubuntu mxnet distro mxnet build mshadow mshadow cuda tensor gpu inl cuh 118 Check failed err cudaSuccess 11 vs 0 Name MapPlanLargeKernel ErrStr invalid argument Stack trace returned 10 entries bt 0 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0x184dfc 0x2b4cd8067dfc bt 1 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0x17e3b2e 0x2b4cd96c6b2e bt 2 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0x17f4a7a 0x2b4cd96d7a7a bt 3 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb45646 0x2b4cd8a28646 bt 4 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb201f7 0x2b4cd8a031f7 bt 5 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb2431c 0x2b4cd8a0731c bt 6 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb24c4d 0x2b4cd8a07c4d bt 7 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb47472 0x2b4cd8a2a472 bt 8 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so MXExecutorForward 0x15 0x2b4cd89ca475 bt 9 mnt home yangshao linuxbrew lib libffi so 6 ffi call unix64 0x4c 0x2b4c97755ff6 Traceback most recent call last File mlp py line 178 in module num epoch 3 File mnt home yangshao mxnet lib python3 6 site packages mxnet module base module py line 472 in fit self forward backward data batch File mnt home yangshao mxnet lib python3 6 site packages mxnet module base module py line 193 in forward backward self forward data batch is train True File mnt home yangshao mxnet lib python3 6 site packages mxnet module module py line 538 in forward self exec group forward data batch is train File mnt home yangshao mxnet lib python3 6 site packages mxnet module executor group py line 379 in forward exec forward is train is train File mnt home yangshao mxnet lib python3 6 site packages mxnet executor py line 124 in forward ctypes c int int is train File mnt home yangshao mxnet lib python3 6 site packages mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 21 55 20 home ubuntu mxnet distro mxnet build mshadow mshadow cuda tensor gpu inl cuh 118 Check failed err cudaSuccess 11 vs 0 Name MapPlanLargeKernel ErrStr invalid argument Stack trace returned 10 entries bt 0 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0x184dfc 0x2b4cd8067dfc bt 1 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0x17e3b2e 0x2b4cd96c6b2e bt 2 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0x17f4a7a 0x2b4cd96d7a7a bt 3 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb45646 0x2b4cd8a28646 bt 4 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb201f7 0x2b4cd8a031f7 bt 5 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb2431c 0x2b4cd8a0731c bt 6 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb24c4d 0x2b4cd8a07c4d bt 7 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so 0xb47472 0x2b4cd8a2a472 bt 8 mnt home yangshao mxnet lib python3 6 site packages mxnet libmxnet so MXExecutorForward 0x15 0x2b4cd89ca475 bt 9 mnt home yangshao linuxbrew lib libffi so 6 ffi call unix64 0x4c 0x2b4c97755ff6 21 55 21 src engine naive engine cc 35 Engine shutdown Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-05-12 02:00:51,2017-09-30 11:06:06
IS,Parameter Server,Hi all How to read parameter server code Do you guys have documentary files what is the last generation of the parameter server I read the paper for generation 3rd and I found it interesting,,yajiedesign,2017-05-12 21:28:30,2017-09-30 11:06:12
IS,gradient problem in 0 9 5 see 5734,I have given the reproduction code in 5734 please have a look,,yajiedesign,2017-05-13 12:05:46,2017-09-30 11:06:15
IS,suggest add a condition statement to allow label to be None,before L522 should add because sometime we do not need labels for training,,yajiedesign,2017-05-13 13:02:16,2017-09-30 11:06:18
IS,how to train own dataset with fcn xs,Hi I have worked with FCN models for image segmentation using FCN XS my dataset is about UAV remote sensing images and I labeled them Thedataset have 4 class contains background all images are spilted into 500 500 I gernerate own train lst and val lst i change 21 to 4 in fcn xs py But when i run scripts python u fcn xs py model fcn32s prefix VGG FC ILSVRC 16 layers epoch 74 init type vgg16 i meet error File home hjy trian 0 7 0 fcn xs solver py line 66 in fit label shape 1 label shape 2 self ctx ValueError cannot reshape array of size 750000 into shape 1 250000 How to solve it,,"Godricly,yajiedesign",2017-03-11 15:36:28,2017-09-30 11:06:22
IS,How to use SpacialTransformer or Gridgenerator and bilinearsampler Are there any examples,,,yajiedesign,2017-05-14 07:45:19,2017-09-30 11:06:25
IS,use MXSymbolListAtomicSymbolCreators and out size is zero,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos7 Compiler g GCC 4 8 5 20150623 Package used Python R Scala Julia c package MXNet version Or if installed from source MXNet commit hash git rev parse HEAD Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 the output AtomicSymbolCreator array is zero 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-05-14 08:24:41,2017-09-30 11:06:28
IS,How to get the total number of keys in kvstore,Hi I want to get the total number of keys in kvstore In addition to directly counting the different keys in mxnet src kvstore kvstore dist srever h is there any existing function that I can use And could someone please tell me how the keys are organized and distributed on multiple server nodes or which file I should read Thank you,,yajiedesign,2017-05-14 17:34:57,2017-09-30 11:06:32
IS,Query Compile Flags,Is there anyway to check compile flags of mxnet with python interface The way i'm trying to use is to locate libmxnet so and check linkage with ldd so that I can know whether cudnn is enabled along with which version of cudnn i'm using Things become more complicated when there are multiple instance installed in the system such as locations at local lib python2 7 dist packages usr local lib python2 7 dist packages and multiple version of the library might available in these directories I'm wondering is there any more friendly way to do this Or what additional work should be done to enable this feature,,"liangfu,yajiedesign",2017-05-15 01:58:57,2017-09-30 11:06:35
IS,SoftmaxOutput how to get probabilities,It looks like softmax output by default returns 1 for the class with highest probability and 0 for the rest Is there a way to get true probability to understand the confidence of classification We followed the LeNet example in CPP wrapper,,"jiajiechen,yajiedesign",2017-05-09 13:43:43,2017-09-30 11:06:38
IS,mkl mkl pooling inl h 185 error C2065 wouldnnAlgorithmPoolingAvgIncludePadding' undeclared identifier,When I compile mxnet on windows 10 using VS2015,,"piiswrong,yajiedesign",2017-04-22 06:56:45,2017-09-30 11:06:42
IS,Is there a conflict between anaconda and mxnet,I had installed anaconda2 python2 7 in Ubuntu16 04 Then I compile and installe mxnet But when I use anaconda python interpret execute scripts such as,,"piiswrong,ysh329,ysh329,yajiedesign",2016-12-02 07:19:35,2017-09-30 11:06:45
IS,RNN data iterator for the general inputs,The current design of bucketing iterator could only handle the case of input vector size 1 Of course this is a general use case for the seq 2 seq or language modeling But it could not be used for more general input shape In summary it would be good if we could have a new iterator that can handle input vector of size n,,yajiedesign,2017-05-16 04:00:05,2017-09-30 11:06:48
IS,Different feature values in caffemodel and mxnet model,Environment info Operating System Windows Compiler Microsoft Visual Studio VC14 Package used Python R Scala Julia Python C MXNet version 0 9 5 Python version and distribution 2 7 First of all I am a newbie in the mxnet There have been two issues when I tried to incorporate mxnet in my project I used to work in caffe before 1 So for testing I downloaded pretrained caffemodel and deploy prototxt of Resnet 50 from and converted the model into mxnet ndarray using mxnet tools caffe converter convert model py Then I have extracted feature of 2048 dimension from pool5 layer in both caffe and mxnet using both the python and C But the values of the feature vector are way too different I am providing the python and c code which I have used to extract the feature May I know why is there so much difference in values in the feature vector I have tried with pretrained vgg face model too but there also I keep getting different values in caffe and mxnet 2 Also a strange thing noticed when I wanted to convert a resnet 50 caffemodel that I have trained in caffe with my own datasets The caffemodel works perfectly well in extracting features in caffe But when i tried to convert it into mxnet there were no issues in conversion but I keep getting same feature vector for every image even I checked with blank image When I digged into the network outputs I saw after two three layer of convolution and batch normalization layers the outputs become similar for any input Can you point me into any direction where I should look into to fix this error feature extract mxnet cpp txt feature extract mxnet python txt,,yajiedesign,2017-05-16 09:44:10,2017-09-30 11:06:51
IS,404 error for the installation guide,Hello the link to the installation guide seems to be broken This page is linked both in github and the website,,"chetkhatri,yajiedesign",2017-05-11 12:53:48,2017-09-30 11:06:54
IS,CSharp Operator RNN is not registered,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System win 10 Compiler vs2015 Package used Python R Scala Julia CSharp work like c MXNet commit hash git rev parse HEAD 38f7c5584016e92ba1e0ee1b00ea6632740f67ce Error Message Failed loading Op fused rnn0rnn of type RNN 08 55 26 G deeplearn mxnet nnvm src core op cc 55 Check failed op nullptr Operator RNN is not registered Minimum reproducible example Steps to reproduce 1 call MXSymbolCreateFromFile with Pinvoke 2 some error but python load same checkpoint work fine What have you tried to solve it,,"yajiedesign,yajiedesign",2017-05-17 01:01:57,2017-09-30 11:06:58
IS,Getting compile with USE DIST KVSTORE 1 to use dist sync even after setting it to 1,I followed the instructions here to install MXNet for Python I was able to successfully run train a MLP on mnist by running python train mnist py I then copied the config mk from the make directory to the top level MXNet installation directory Edited it to set USE DIST KVSTORE 1 then ran the commands Edit I'm running MXNet on a Ubuntu 14 04 machine,,"miguelgfierro,mli,yajiedesign",2016-10-25 06:10:50,2017-09-30 11:07:01
IS,Install mxnet into anaconda is python,I'm trying to use minpy which requires mxnet in anaconda is jupyter notebook minpy can be installed using anaconda is pip successfully However there is an issue on installing mxnet I noticed using anaconda is pip install mxnet does not work opencv not found although opencv pc clearly exists and the path is correctly set So I wonder how can I install mxnet into anaconda is python instead of system is default python by changing some settings I figured out I can disable using opencv by changing config mk file I used a similar way to get both minpy and mxnet work on system is default python Thank you,,"zihaolucky,zihaolucky,zihaolucky,zihaolucky,zihaolucky,zihaolucky,yajiedesign",2016-10-04 10:14:44,2017-09-30 11:07:04
IS,Problems about training speed,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler Package used Python R Scala Julia Python MXNet version 0 9 4 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 6 If you are using R package please provide R sessionInfo Error Message I have a problem about the speed when I train a model the speed is lower and lower during the training process I use 4 gpus TITAN X and here is the training speed Could anyone give me some help thanks screenshot from 2017 04 22 15 43 00 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"eric-haibin-lin,piiswrong,yajiedesign",2017-04-22 07:46:16,2017-09-30 11:07:08
IS,problem with data iterator for multiple inputs and labels,I am trying to use a dataset input as well as labels of shape 1000 x 45 Using this in a straightforward way does not work in order to have a multidimensional label you need a dictionary of many labels So I create the dictionaries with keys wouldata1' to wouldata45' and a single value each as well as 'label1' to 'label45' and a single value each When I try to 'bind' mod bind data shapes data iter provide data label shapes data iter provide label I get an error ValueError Data provided by data shapes do not match names specified by data names DataDesc data42 1 1L type 'numpy float32' NCHW DataDesc data43 1 1L type 'numpy float32' NCHW DataDesc data40 1 1L type 'numpy float32' NCHW DataDesc data41 1 1L type 'numpy float32' NCHW DataDesc data44 1 1L type 'numpy float32' NCHW DataDesc data28 1 1L type 'numpy float32' NCHW DataDesc data29 1 1L type 'numpy float32' NCHW DataDesc data20 1 1L type 'numpy float32' NCHW DataDesc data21 1 1L type 'numpy float32' NCHW DataDesc data22 1 1L type 'numpy float32' NCHW DataDesc data23 1 1L type 'numpy float32' NCHW DataDesc data24 1 1L type 'numpy float32' NCHW DataDesc data25 1 1L type 'numpy float32' NCHW DataDesc data26 1 1L type 'numpy float32' NCHW DataDesc data27 1 1L type 'numpy float32' NCHW DataDesc data9 1 1L type 'numpy float32' NCHW DataDesc data8 1 1L type 'numpy float32' NCHW DataDesc data5 1 1L type 'numpy float32' NCHW DataDesc data4 1 1L type 'numpy float32' NCHW DataDesc data7 1 1L type 'numpy float32' NCHW DataDesc data6 1 1L type 'numpy float32' NCHW DataDesc data1 1 1L type 'numpy float32' NCHW DataDesc data0 1 1L type 'numpy float32' NCHW DataDesc data3 1 1L type 'numpy float32' NCHW DataDesc data2 1 1L type 'numpy float32' NCHW DataDesc data37 1 1L type 'numpy float32' NCHW DataDesc data36 1 1L type 'numpy float32' NCHW DataDesc data35 1 1L type 'numpy float32' NCHW DataDesc data34 1 1L type 'numpy float32' NCHW DataDesc data33 1 1L type 'numpy float32' NCHW DataDesc data32 1 1L type 'numpy float32' NCHW DataDesc data31 1 1L type 'numpy float32' NCHW DataDesc data30 1 1L type 'numpy float32' NCHW DataDesc data39 1 1L type 'numpy float32' NCHW DataDesc data38 1 1L type 'numpy float32' NCHW DataDesc data19 1 1L type 'numpy float32' NCHW DataDesc data18 1 1L type 'numpy float32' NCHW DataDesc data15 1 1L type 'numpy float32' NCHW DataDesc data14 1 1L type 'numpy float32' NCHW DataDesc data17 1 1L type 'numpy float32' NCHW DataDesc data16 1 1L type 'numpy float32' NCHW DataDesc data11 1 1L type 'numpy float32' NCHW DataDesc data10 1 1L type 'numpy float32' NCHW DataDesc data13 1 1L type 'numpy float32' NCHW DataDesc data12 1 1L type 'numpy float32' NCHW vs wouldata',,yajiedesign,2017-05-17 12:04:16,2017-09-30 11:07:11
IS,How to implement Early Stopping in MXnet,Hi I want to do some scheduling in the training process 1 halve the learning rate when the validation accuracy does not increase 2 stop the training when the learning rate is small enough So I need to get the accuracy and learning rate history but I can not find something like a monitor holding these information Can anyone help,,"piiswrong,piiswrong,yajiedesign",2017-02-10 07:40:36,2017-09-30 11:07:14
IS,Distributed MXNet Import Error,Environment info Operating System Ubuntu 16 04 1 LTS Python version and distribution python 2 7 MXNet version 0 9 3 Official Release Version Downloaded from But nnvm dmlc core ps lite and mshadow are not in the release version So I clone the newest version of these four packages to their directories Error Message The compilation procedure of distributed MXNet USE DIST KVSTORE 1 is passed but after the compilation and sudo python setup py install the error occurred when import mxnet import mxnet libprotobuf FATAL google protobuf stubs common cc 61 This program requires version 2 6 0 of the Protocol Buffer runtime library but the installed version is 2 5 0 Please update your library If you compiled the program yourself make sure that your headers are from the same version of Protocol Buffers as your link time library Version verification failed in build mir pkdHET mir 0 21 0 16 04 20160330 obj x86 64 linux gnu src protobuf mir protobuf pb cc terminate called after throwing an instance of 'google protobuf FatalException' what This program requires version 2 6 0 of the Protocol Buffer runtime library but the installed version is 2 5 0 Please update your library If you compiled the program yourself make sure that your headers are from the same version of Protocol Buffers as your link time library Version verification failed in build mir pkdHET mir 0 21 0 16 04 20160330 obj x86 64 linux gnu src protobuf mir protobuf pb cc Aborted core dumped The MXNet version on Single machine USE DIST KVSTORE 0 works well I have found that the ps lite package has used the v2 5 0 of protobuf but the program needs v2 6 0 I do not know how to solve this Thanks,,yajiedesign,2017-04-25 10:39:55,2017-09-30 11:07:21
IS,color channel order of rec files,If I use bin im2rec or im2rec py to generate rec files from images according to or is the channel order of the rec file always RGB or it is dependent on the color parameter for im2rec as user input,,"piiswrong,yajiedesign",2017-05-17 18:18:09,2017-09-30 11:07:26
IS,DataParallelExecutorGroup hangs,the bind function costs a long time taking around 80 seconds to complete binding operation Below shows part of my code And after debugged into the bind function i found the code hangs in function ' DataParallelExecutorGroup ' which is the bottleneck Anyone ever met this issue Thanks a lot Here is my system info ubuntu 16 04 cuda 8 0 44,,"piiswrong,yajiedesign",2017-05-18 11:52:34,2017-09-30 11:07:29
IS,New OP slice a row from a matrix with input variable not a fixed integer,formally it is like the following it will be very useful,,"zhreshold,yajiedesign",2017-05-15 06:00:40,2017-09-30 11:07:32
IS,error when calling bind on gpu 0 or gpu 1,I am using mxnet 0 9 4 python When I call bind on a mx mod Module object I got an error This only happened when I set context to mx gpu 1 Tried mx cpu or mx gpu 0 mx gpu 1 no error was raised Tried building mxnet from source with the latest version on Github did not fix this Tried rebooting the computer did not work Tried upgrade cuda did not work Tried mx gpu 0 failed too I also tried uninstall mxnet 0 9 4 and install 0 9 3a via pip mxnet cu80 it turned out that the program stuck every time,,yajiedesign,2017-03-01 11:17:57,2017-09-30 11:07:39
IS,Error running bash script get coco sh,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler GCC 5 4 0 20160609 Package used Python R Scala Julia Python MXNet version 0 9 5 post1 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace extracting val2014 COCO val2014 000000441814 jpg mv missing file operand Try 'mv help' for more information Minimum reproducible example if you are using your own code please provide a short script that reproduces the error from my HOME directory I run The directory is example rcnn Then run bash script get coco sh The three ZIP files were downloaded the error occurs in the unzip from the test2014 as well as the train2014 image folder The instances train val2014 zip extracts fine The folder data coco images is empty Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 mxnet example rcnn 2 I executed all commands until 3 bash script get coco sh What have you tried to solve it 1 I tried to redownload the data 2 I restarted the EC2 machine 3,,yajiedesign,2017-05-19 14:18:18,2017-09-30 11:07:42
IS,How to kill python daemon thread,Error Message Recently I use ssd for real time detection when I call im detect image by image then python generate too many threads and continue increase which cause slow speed how can I solve this problem,,yajiedesign,2017-05-19 16:28:16,2017-09-30 11:07:51
IS,MNIST example has low accuracy 0 1,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 GTX 960 Compiler I downloaded the 20160531 win10 x64 gpu 7z I didnt compile mxnet Package used Python R Scala Julia Python MXNet version 20160531 win10 x64 gpu 7z Or if installed from source MXNet commit hash git rev parse HEAD e3e26b5 If you are using python package please provide Python version and distribution Python 3 5 3 Continuum Analytics Inc If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace There is no stack trace Steps to reproduce I am running the example The training accuracy is always 0 05 to 0 1 for convolution network I did not try MLP My Code import numpy as np import os import urllib import gzip import struct import logging logger logging getLogger logger setLevel logging DEBUG def download data url force download True fname url split 1 if force download or not os path exists fname urllib urlretrieve url fname return fname def read data label url image url with gzip open label url as flbl magic num struct unpack II flbl read 8 label np fromstring flbl read dtype np int8 with gzip open image url 'rb' as fimg magic num rows cols struct unpack IIII fimg read 16 image np fromstring fimg read dtype np uint8 reshape len label rows cols return label image path 'handwritten digits ' I downloaded the data set and saved it to handwritten digits ' train lbl train img read data path 'train labels idx1 ubyte gz' path 'train images idx3 ubyte gz' val lbl val img read data path 't10k labels idx1 ubyte gz' path 't10k images idx3 ubyte gz' matplotlib inline import matplotlib pyplot as plt for i in range 10 plt subplot 1 10 i 1 plt imshow train img i cmap 'Greys r' plt axis 'off' plt show print 'label s' train lbl 0 10 import mxnet as mx def to4d img return img reshape img shape 0 1 28 28 astype np float32 255 batch size 100 train iter mx io NDArrayIter to4d train img train lbl batch size shuffle True val iter mx io NDArrayIter to4d val img val lbl batch size data mx symbol Variable wouldata' first conv layer conv1 mx sym Convolution data data kernel 5 5 num filter 20 tanh1 mx sym Activation data conv1 act type tanh pool1 mx sym Pooling data tanh1 pool type max kernel 2 2 stride 2 2 second conv layer conv2 mx sym Convolution data pool1 kernel 5 5 num filter 50 tanh2 mx sym Activation data conv2 act type tanh pool2 mx sym Pooling data tanh2 pool type max kernel 2 2 stride 2 2 first fullc layer flatten mx sym Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 500 tanh3 mx sym Activation data fc1 act type tanh second fullc fc2 mx sym FullyConnected data tanh3 num hidden 10 softmax loss lenet mx sym SoftmaxOutput data fc2 name isoftmax' AUTOTEST OUTPUT IGNORED CELL model mx model FeedForward ctx mx gpu 0 use GPU 0 for training others are same as before symbol lenet num epoch 10 learning rate 0 1 model fit X train iter eval data val iter batch end callback mx callback Speedometer batch size 200 output logs INFO root Start training with gpu 0 INFO root Epoch 0 Batch 200 Speed 10550 22 samples sec Train accuracy 0 096900 INFO root Epoch 0 Batch 400 Speed 10455 60 samples sec Train accuracy 0 099150 INFO root Epoch 0 Batch 600 Speed 10474 79 samples sec Train accuracy 0 100100 INFO root Epoch 0 Resetting Data Iterator INFO root Epoch 0 Time cost 5 730 INFO root Epoch 0 Validation accuracy 0 098000 INFO root Epoch 1 Batch 200 Speed 10510 17 samples sec Train accuracy 0 096900 INFO root Epoch 1 Batch 400 Speed 10436 99 samples sec Train accuracy 0 099150 INFO root Epoch 1 Batch 600 Speed 10366 46 samples sec Train accuracy 0 100100 INFO root Epoch 1 Resetting Data Iterator INFO root Epoch 1 Time cost 5 756 INFO root Epoch 1 Validation accuracy 0 098000 INFO root Epoch 2 Batch 200 Speed 10485 69 samples sec Train accuracy 0 096900 INFO root Epoch 2 Batch 400 Speed 10388 89 samples sec Train accuracy 0 099150 INFO root Epoch 2 Batch 600 Speed 10516 51 samples sec Train accuracy 0 100100 INFO root Epoch 2 Resetting Data Iterator INFO root Epoch 2 Time cost 5 742 INFO root Epoch 2 Validation accuracy 0 098000 INFO root Epoch 3 Batch 200 Speed 10514 61 samples sec Train accuracy 0 096900 INFO root Epoch 3 Batch 400 Speed 10480 61 samples sec Train accuracy 0 099150 INFO root Epoch 3 Batch 600 Speed 10477 18 samples sec Train accuracy 0 100100 INFO root Epoch 3 Resetting Data Iterator INFO root Epoch 3 Time cost 5 727 INFO root Epoch 3 Validation accuracy 0 098000 INFO root Epoch 4 Batch 200 Speed 10380 61 samples sec Train accuracy 0 096900 INFO root Epoch 4 Batch 400 Speed 10426 29 samples sec Train accuracy 0 099150 INFO root Epoch 4 Batch 600 Speed 10497 75 samples sec Train accuracy 0 100100 INFO root Epoch 4 Resetting Data Iterator INFO root Epoch 4 Time cost 5 758 INFO root Epoch 4 Validation accuracy 0 098000 INFO root Epoch 5 Batch 200 Speed 10544 32 samples sec Train accuracy 0 096900 INFO root Epoch 5 Batch 400 Speed 10491 81 samples sec Train accuracy 0 099150 INFO root Epoch 5 Batch 600 Speed 10489 34 samples sec Train accuracy 0 100100 INFO root Epoch 5 Resetting Data Iterator INFO root Epoch 5 Time cost 5 718 INFO root Epoch 5 Validation accuracy 0 098000 INFO root Epoch 6 Batch 200 Speed 10512 22 samples sec Train accuracy 0 096900 INFO root Epoch 6 Batch 400 Speed 10439 01 samples sec Train accuracy 0 099150 INFO root Epoch 6 Batch 600 Speed 10403 41 samples sec Train accuracy 0 100100 INFO root Epoch 6 Resetting Data Iterator INFO root Epoch 6 Time cost 5 749 INFO root Epoch 6 Validation accuracy 0 098000 INFO root Epoch 7 Batch 200 Speed 10485 94 samples sec Train accuracy 0 096900 INFO root Epoch 7 Batch 400 Speed 10455 69 samples sec Train accuracy 0 099150 INFO root Epoch 7 Batch 600 Speed 10532 22 samples sec Train accuracy 0 100100 INFO root Epoch 7 Resetting Data Iterator INFO root Epoch 7 Time cost 5 728 INFO root Epoch 7 Validation accuracy 0 098000 INFO root Epoch 8 Batch 200 Speed 10525 94 samples sec Train accuracy 0 096900 INFO root Epoch 8 Batch 400 Speed 10436 42 samples sec Train accuracy 0 099150 INFO root Epoch 8 Batch 600 Speed 10426 20 samples sec Train accuracy 0 100100 INFO root Epoch 8 Resetting Data Iterator INFO root Epoch 8 Time cost 5 743 INFO root Epoch 8 Validation accuracy 0 098000 INFO root Epoch 9 Batch 200 Speed 10522 15 samples sec Train accuracy 0 096900 INFO root Epoch 9 Batch 400 Speed 10494 32 samples sec Train accuracy 0 099150 INFO root Epoch 9 Batch 600 Speed 10525 46 samples sec Train accuracy 0 100100 INFO root Epoch 9 Resetting Data Iterator INFO root Epoch 9 Time cost 5 715 INFO root Epoch 9 Validation accuracy 0 098000 What have you tried to solve it I was working on another project But the accuracy was very low So I tried the basic example from website and it still remained the same,,"ysh329,ysh329,ysh329,ysh329,ysh329,yajiedesign",2017-05-01 18:13:02,2017-09-30 17:15:04
IS,How to use im2rec when both input and label are images,How to use im2rec when both input and label are images I have seen how to make multilabel rec using im2rec but there does not seem to be any instruction on how to make rec where the labels are images as well Does mxnet also support this function,,yajiedesign,2016-10-05 03:56:00,2017-09-30 17:15:08
IS,Unable to improve accuracy with the existing cnn for text classification code for image classification,1 I tried to use the existing cnn for text classification code for image classification I made appropriate changes in the code 2 The code is running without any errors 1 I have avoided using the functions which were used for converting word2 vectors and all other word vector conversion functions 2 changed the forward propagation in cnn by using standard cnn model output of each layer is fed as input to successive layers unlike the forward prop in cnn for text model 3 from future import print function Model namedtuple Model 'mod exec' isymbol' wouldata' 'label' 'param blocks' def product tuple1 prod 1 for x in tuple1 prod prod x return prod def our model batch size 100 filter sizes 3 3 3 filter count 8 16 32 input x mx sym Variable wouldata' shape batch size 784 input y mx sym Variable isoftmax label' shape batch size 10 x tensor mx sym Reshape data input x target shape batch size 1 28 28 current input x tensor encoder shapes activations for layer i n output in enumerate filter count W mx sym Variable name 'weight' str layer i shape filter count layer i int current input infer shape 1 0 1 filter sizes layer i filter sizes layer i b mx sym Variable 'bias' str layer i shape filter count layer i out mx sym Convolution data current input weight W bias b num filter filter count layer i kernel filter sizes layer i filter sizes layer i stride 2 2 out mx sym relu data out current input out encoder append W activations append out out mx sym Dropout data out p 0 5 t product out infer shape 1 0 out mx sym Reshape data out target shape batch size t batch size cls weight mx sym Variable 'cls weight' shape 10 t batch size cls bias mx sym Variable 'cls bias' shape 10 fc mx sym FullyConnected data out weight cls weight bias cls bias num hidden 10 sm mx sym SoftmaxOutput data fc label input y name isoftmax' return sm def setup model ctx batch size initializer mx initializer Xavier mod our model batch size batch size arg names mod list arguments input shapes input shapes wouldata' batch size 784 arg shape out shape aux shape mod infer shape input shapes arg arrays mx nd zeros s ctx for s in arg shape args grad for shape name in zip arg shape arg names if name in isoftmax label' wouldata' continue args grad name mx nd zeros shape ctx param blocks arg dict dict zip arg names arg arrays for i name in enumerate arg names if name in isoftmax label' wouldata' continue arg dict name np random uniform 0 1 0 1 arg dict name shape arg dict name mx init Xavier shape arg dict name shape param blocks append i arg dict name args grad name name mod exec mod bind ctx ctx args arg dict args grad args grad grad req 'add' out dict dict zip mod list outputs mod exec outputs data mod exec arg dict wouldata' label mod exec arg dict isoftmax label' return Model mod exec mod exec symbol mod data data label label param blocks param blocks def train cnn model X train batch y train batch X dev batch y dev batch batch size optimizer 'rmsprop' max grad norm 5 0 learning rate 0 05 epoch 10 m model opt mx optimizer create optimizer opt lr learning rate updater mx optimizer get updater opt for iteration in range epoch tic time time num correct 0 num total 0 for begin in range X train batch shape 0 batch size batchX X train batch begin batch size begin 1 batch size batchY y train batch begin batch size begin 1 batch size if batchX shape 0 batch size continue m data batchX m label batchY m mod exec forward is train True m mod exec FeedForward is train True m mod exec backward num correct np sum mx nd equal mx nd argmax mx nd array batchY axis 1 mx nd argmax m mod exec outputs 0 axis 1 asnumpy num total len batchY norm 0 for idx weight grad name in m param blocks grad batch size l2 norm mx nd norm grad asscalar norm l2 norm l2 norm norm math sqrt norm for idx weight grad name in m param blocks if norm max grad norm grad max grad norm norm updater idx grad weight grad 0 0 '''if iteration 3 0 and iteration 0 opt lr 0 5 print areset learning rate to g' opt lr file logs ''' toc time time train time toc tic train acc num correct 100 float num total num correct 0 num total 0 for begin in range 0 X dev batch shape 0 batch size batchX X dev batch begin batch size begin 1 batch size batchY y dev batch begin begin batch size if batchX shape 0 batch size continue m data batchX m mod exec forward is train False num correct np sum mx nd equal mx nd argmax mx nd array batchY axis 1 mx nd argmax m mod exec outputs 0 axis 1 asnumpy num total len batchY dev acc num correct 100 float num total print 'Iter d Train Time 3fs Training Accuracy 3f Dev Accuracy thus far 3f' iteration train time train acc dev acc file logs x train np reshape x train 1 784 x test np reshape x test 1 784 batch size 100 model setup model mx cpu 0 batch size 100 initializer mx initializer Xavier train cnn model x train y train x test y test batch size 100 optimizer 'rmsprop' max grad norm 5 0 learning rate 0 05 epoch 10,,yajiedesign,2017-05-20 17:09:40,2017-09-30 17:15:12
IS,sym arg params aux params mx model load checkpoint 'vgg16' 0 failed in mxnet 0 9 5,keep getting the following error when trying to load a model in python src ndarray ndarray cc 700 Check failed header kMXAPINDArrayListMagic Invalid NDArray file format,,"SCP-173-cool,yajiedesign",2017-05-16 03:56:34,2017-09-30 17:15:15
IS,Multiple calls to train cause cudaMalloc to fail,If calling train several times I ran out of GPU memory pooled storage manager h 80 cudaMalloc failed out of memory looks like GPU buffers are never cleared and memory runs out It needs to be solved to be able to use MXNet in the larger application where multiple train attempts are made in sequence without restarting the app,,yajiedesign,2017-05-04 01:54:45,2017-09-30 17:15:19
IS,Segmentation fault after using mx io ImageRecordIter in python,When I declare an iterator in my training code it causes segmentation fault after all the operations are done So all the training testing is done w o any problem but after every operation is finished it ends with segmentation fault error The error goes away when I delete the iterator e g setting train iter None but I wonder what is causing this issue,,"tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,ysh329,ysh329,yajiedesign",2016-10-09 14:13:29,2017-09-30 17:15:22
IS,How did the GraphExecutor RunOps make sure the computation is not deadlocked,When digging the source code of mxnet I am curious about mxnet is methods to solve the compute deadlock problem or maybe this is no deallock at all If there is no risk of deadlock please kindly give a brief explanation Thanks,,yajiedesign,2017-05-21 14:28:15,2017-09-30 17:15:25
IS,same train code 0 8 0 is 4 times faster than 0 9 5,my training code use mxnetv0 8 0 Speed 377 82 samples sec but when use mxnetv0 9 5 Speed 90 0 samples sec environment,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,piiswrong,piiswrong,piiswrong,eric-haibin-lin,piiswrong,piiswrong,piiswrong,piiswrong,yajiedesign",2017-05-17 08:44:21,2017-09-30 17:15:28
IS,Enhancement Segmentation of namespace mx sym,With ongoing development having a flat list of operators in mx sym becomes cumbersome There will be more and more closely related groups of operators for support of certain special types of computations and I think we should reflect that in terms of different namespaces for operators for easier navigation and usage keeping development of operators more consistent by forcing them to belong to a certain namespace and therfore to comply in interfaces functionality with the other operators in that namespace Some examples of such namespaces mx sym random for all random sampling methods mx sym la for all advanced single double precision linear algebra as matrix decompositions etc I think that can improve the outside perception of MXNet by the user community,,"asmushetzel,yajiedesign",2017-05-22 08:59:51,2017-09-30 17:15:31
IS,Activation followed by Reshape wastes memory,Remark I'm planning to work on this within next 4 weeks or so unless someone else already will provide a fix Open the issue here to track that Below the description copied from an email thread I came across this analyzing some memory issue in a NMT project The networks there are very short but wide and the application is very memory intensive What happened is that a tiny fragment namely an activation followed by reshape caused a big memory issue detailed analysis at the end The short summary is that the reshape operator is basically an identity when it comes to forward backward operations but as this is not known by the memory planning kernel it just sees an in place operation but not that the tensor content is not changed it may use more memory than needed for storing forward pass results This is absolutely non transparent for a user And could be avoided for example by either 1 flag reshape operator as not altering tensor content and use this flag in memory planning or 2 fuse such operator automatically with the preceding one The impact in this case was very significant 3GB memory savings when the order of the two operators was switched ANALYSIS The reason seems that your application is a very wide and short network resp consists mainly out of wide short subnetworks In this type of networks it makes a huge difference if the system misses a single opportunity to reuse memory along the computation graph Best look at two examples where I and O are inputs outputs of the network R is a reshape operator and A an activation operator First option I R A O R and A operator specify that they can do in place forward So the forward pass needs a single chunk of memory that is passed through the network and at the end holds the output On the backward path both R and A specify that input output gradients can be in place so there is another single chunk of memory passed backward through the network holding the gradients The interesting issue is how the backward gradients are computed in the A and R operator The R operator is an identity so it registers with nnvm that it does neither need the original data inputs nor the data outputs from the forward pass in order to compute the backward gradient The A operator is written in a way that it uses the data outputs from the forward pass together with the backward propagated gradients to derive its input gradients That is due to the fact that for complex activation functions like tanh and sigmoid it is more efficient to express the derivative in terms of the forward output instead in terms of the forward input In this setup A uses then the final output O as well as topmost gradients in order to do back propagation So here we can get along with 2 memory chunks Second option I A R O Again the forward pass would need a single chunk of memory But because of the backward pass it needs 2 Operator R needs the output of the forward pass as input for the backward pass as explained above Therfore it puts a lock on the output of R in the forward path such that it will be preserved for backward computation This lock causes one more memory chunk to be generated during forward pass So you end up with 3 chunks instead of 2 for overall execution Of course from the high level viewpoint this memory overhead is not necessary Operator R could have used the data associated with O instead of the data associated with R s forward output for back propagation But this would require that the system would realize that R is the identity and therefore the output of A and the output of R are identical Which it can t at the moment It only can see that R performs some in place computation in the forward pass it can t see that this computation is the identity,,"asmushetzel,yajiedesign",2017-05-22 09:07:29,2017-09-30 17:15:35
IS,build with mkl cannot find lmkl intel lp64 lmkl intel thread lmkl core,I am trying to build MXNet from source and using MKL 1 Enable USE MKL2017 1 in make config mk 2 wget MKL VERSION tgz 3 tar zxvf mklml lnx MKL VERSION tgz 4 cp rf mklml lnx MKL VERSION MKLML ROOT 5 Set LD LIBRARY PATH LD LIBRARY PATH MKLML ROOT lib 6 make j USE BLAS mkl Then I am getting bin ld cannot find lmkl intel lp64 bin ld cannot find lmkl intel thread bin ld cannot find lmkl core Where can I get those three libraries They are not included in the downloaded MKL files based on,,"piiswrong,yajiedesign",2017-05-22 15:04:05,2017-09-30 17:15:38
IS,FCN XS example can not run,About FCN XS example Environment info Operating System Ubuntu 16 04 Compiler g 5 4 Package used Python R Scala Julia Python MXNet version mxnet mkl cu80 0 9 5 Segmentation fault Error message from gdb Thread 90 python received signal SIGSEGV Segmentation fault Switching to Thread 0x7ffe9ebf4a00 LWP 5866 0x00007fff7a953c29 in mkl vsl sub kernel l9 sBRngMCG31M1 from usr local lib python2 7 dist packages mxnet libmklml intel so To reproduce simply run fcn xs example with mxnet package downloaded,,"SCP-173-cool,SCP-173-cool,yajiedesign",2017-05-22 15:49:59,2017-09-30 17:15:41
IS,Support video IO decoding and transformation,Video classification 1 2 3 object detectction 4 and object segmentation 5 have been very active research areas in recent years Caffe2 added a module for efficient video IO decoding and transformation this month Please support the same set of functionalities to facilitate more video related researches with MXNet 1 2 3 4 5,,yajiedesign,2017-05-23 10:02:22,2017-09-30 17:15:44
IS,Ca not load json file more than twice in linux using cpp package,When I use Symbol LoadJSON from cpp package in multithreads it will crash down How can I fix that Environment info Operating System CentOS 6 6 ubuntu14 04 Compiler GCC4 8 4 MXNet version 0 9 4 error information 03 26 14 home XXX mxnet dmlc core include dmlc logging h 300 03 26 14 home XXX mxnet dmlc core include dmlc json h 628 Check failed is fail Error at Line 41 around Expect number Stack trace returned 10 entries bt 0 lib libmnn x64 linux so ZN4dmlc15LogMessageFatalD2Ev 0x29 0x7f2857a9b039 bt 1 lib libmnn x64 linux so ZN4dmlc10JSONReader10ReadNumberIjEEvPT 0xc5 0x7f2859211485 bt 2 lib libmnn x64 linux so 0x1ccfd32 0x7f2859220d32 bt 3 lib libmnn x64 linux so 0x1ccffe0 0x7f2859220fe0 bt 4 lib libmnn x64 linux so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x100 0x7f2859226240 bt 5 lib libmnn x64 linux so 0x1cd0742 0x7f2859221742 bt 6 lib libmnn x64 linux so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x100 0x7f2859226240 bt 7 lib libmnn x64 linux so 0x1ccf60f 0x7f285922060f bt 8 lib libmnn x64 linux so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7f2858438a8f bt 9 lib libmnn x64 linux so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7f28591f8bd1 03 26 14 include mxnet cpp symbol hpp 81 Check failed MXSymbolCreateFromJSON json str c str handle 0 Details 03 26 14 home XXX mxnet dmlc core include dmlc json h 628 Check failed is fail Error at Line 41 around Expect number Stack trace returned 10 entries bt 0 lib libmnn x64 linux so ZN4dmlc15LogMessageFatalD2Ev 0x29 0x7f2857a9b039 bt 1 lib libmnn x64 linux so ZN4dmlc10JSONReader10ReadNumberIjEEvPT 0xc5 0x7f2859211485 bt 2 lib libmnn x64 linux so 0x1ccfd32 0x7f2859220d32 bt 3 lib libmnn x64 linux so 0x1ccffe0 0x7f2859220fe0 bt 4 lib libmnn x64 linux so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x100 0x7f2859226240 bt 5 lib libmnn x64 linux so 0x1cd0742 0x7f2859221742 bt 6 lib libmnn x64 linux so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x100 0x7f2859226240 bt 7 lib libmnn x64 linux so 0x1ccf60f 0x7f285922060f bt 8 lib libmnn x64 linux so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7f2858438a8f bt 9 lib libmnn x64 linux so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7f28591f8bd1,,"thirdwing,yajiedesign",2017-04-19 07:42:38,2017-09-30 17:15:50
IS,im2rec does not support the tif format,Hi I would like to create a rec file for tif format images by using im2rec py First i have run im2rec for creating list file but the file obtained was empty I think that im2rec does not support the tif format Please can you help me to create rec file from tif images Thanks in advance Environment info Operating System Windows 8 1 Compiler Package used Python R Scala Julia Python MXNet version 0 9 5 Python version and distribution 2 7,,"piiswrong,SCP-173-cool,yajiedesign",2017-05-21 14:16:36,2017-09-30 17:15:53
IS,is there a way to get the number of images in a rec file,Suppose that we have a rec file is there any function I can use to get the number of images in it,,"mli,yajiedesign",2017-05-20 03:32:06,2017-09-30 17:15:56
IS,Automatic Batching for Dynamic Graphs,piiswrong This seems very relevant to MXNet is efforts to support dynamic graphs 5705 any idea whether PyTorch is thinking about adding support for something like this,,"sbodenstein,piiswrong,yajiedesign",2017-05-23 16:02:08,2017-09-30 17:15:59
IS,TypeError fit got an unexpected keyword argument 'monitor',Environment info Operating System Win7 Compiler Python Package used Python MXNet version example is in mxnet 0 9 3 and my mxnet version is windows binary build 20160531 Python version and distribution Python 2 7 12 Error Message example image classification python train mnist py INFO root start with arguments Namespace batch size 64 disp batches 100 gpus N one kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File D Zhenxingjian MxNet mxnet 0 9 3 mxnet 0 9 3 example image classificati on common fit py line 183 in fit monitor monitor TypeError fit got an unexpected keyword argument 'monitor' Steps to reproduce 1 I have changed nothing in that file 2 I just coded 'python train mnist py' 3 What have you tried to solve it 1 Before this error I found out that some dll are missing So I download them and copy them into path 2 I can run the normal test import mxnet as mx a mx nd ones 2 3 print a 2 asnumpy 2 2 2 2 2 2 3,,yajiedesign,2017-02-23 01:54:33,2017-09-30 17:16:02
IS,C API for mxnet current status,This issue is some kind of duplicate of For the production purposes now I thinking about migrating from keras to mxnet I also excited about mxnet stuff like usage of multi GPU its performance and so on but the main reason is production purposes As I currently think or hope previously trained on Python mxnet GPU model could be executed directly on C CPU code simply by model load in C code Am I right If it so could somebody please give some example of that,,yajiedesign,2017-03-15 10:33:20,2017-09-30 17:16:06
IS,Can I run the provided matlab interface in mxnet for training the network,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you I have run the matlab demo m in code mxnet matlab folder so could I train a network based on matlab interface for example do mnist classification Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-05-24 11:51:35,2017-09-30 17:16:09
IS,Does mxnet provide operator to arearange' data,Hi I'm new to mxnet and I'm looking for operator layer doing folloving manipulation with data I have two tensors with shapes eg 1 1 8 8 and 1 1 4 4 I want to concat these two along second axis channels so the first one has to be rearanged to get same width and height as second one I can take each four neigbouring pixels with stride 2 and put them to four channels so I will get new tensor with 1 4 4 4 Does mxnet provide similar function or I need to implement it by myself thanks Jan,,yajiedesign,2017-05-24 12:28:13,2017-09-30 17:16:12
IS,concat argument dim axis,The concat operator takes the argument dim to specify along which axis to concat How do you all feel about changing the argument name to axis Almost every operator in mxnet uses the key word name axis to specify this information and it is a little hard to remember that it is dim for concat,,yajiedesign,2017-05-24 14:32:31,2017-09-30 17:16:15
IS,0 9 5 nnpack,Does the 0 9 5 release has NNPACK installed by default I tried to build MXNET without with NNPCAK but get similar performance as NNPACK installed the faster one reported,,"piiswrong,yajiedesign",2017-05-24 14:39:00,2017-09-30 17:16:18
IS,saving and loading params yields worse results,Environment info Ubuntu 16 04 2 LTS n l Linux as 4 10 0 041000 generic 201702191831 SMP Sun Feb 19 23 33 19 UTC 2017 x86 64 x86 64 x86 64 GNU Linux gcc version 5 4 0 20160609 Ubuntu 5 4 0 6ubuntu1 16 04 4 Python 2 7 12 MXNet commit hash fdadaca2785068abcc2f31101769e7e065d86776 GPUs 4 x GTX 1080 Ti Symptom Image classification example is running and saving after every epoch Eventually after stopping and loading an already saved epoch will result in a degrading model performance cd root mxnet example image classification python train imagenet py data train mnt fast 02 train rec data val mnt fast 02 val rec gpu 0 1 2 3 network resnet batch size 158 lr 0 045 disp batches 5 model an model prefix resnet01 num classes 5 data nthreads 12 image shape 3 256 256 rgb mean 159 199 191 kv store device top k 3 num epochs 1000 load epoch 28 Output INFO root start with arguments Namespace batch size 158 benchmark 0 data nthreads 12 data train ' mnt fast 02 train rec' data val ' mnt fast 02 val rec' disp batches 5 dtype 'float32' gpus '0 1 2 3' image shape '3 256 256' kv store wouldevice' load epoch 28 lr 0 045 lr factor 0 1 lr step epochs '30 60' max random aspect ratio 0 25 max random h 36 max random l 50 max random rotate angle 10 max random s 50 max random scale 1 max random shear ratio 0 1 min random scale 1 model prefix ' resnet01' mom 0 9 monitor 0 network aresnet' num classes 5 num epochs 1000 num examples 1281167 num layers 50 optimizer isgd' pad size 0 random crop 1 random mirror 1 rgb mean '159 199 191' test io 0 top k 3 wd 0 0001 19 27 43 src io iter image recordio 2 cc 135 ImageRecordIOParser2 mnt fast 02 train rec use 5 threads for decoding 19 27 44 src io iter image recordio 2 cc 135 ImageRecordIOParser2 mnt fast 02 val rec use 5 threads for decoding INFO root Loaded model resnet01 0028 params 19 28 48 src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable INFO root Epoch 28 Batch 5 Speed 610 95 samples sec Train accuracy 0 978903 INFO root Epoch 28 Batch 5 Speed 610 95 samples sec Train top k accuracy 3 0 998945 INFO root Epoch 28 Batch 10 Speed 534 55 samples sec Train accuracy 0 955696 INFO root Epoch 28 Batch 10 Speed 534 55 samples sec Train top k accuracy 3 0 997468 INFO root Epoch 28 Batch 15 Speed 536 39 samples sec Train accuracy 0 925316 INFO root Epoch 28 Batch 15 Speed 536 39 samples sec Train top k accuracy 3 0 991139 INFO root Epoch 28 Batch 20 Speed 527 45 samples sec Train accuracy 0 859494 INFO root Epoch 28 Batch 20 Speed 527 45 samples sec Train top k accuracy 3 0 979747 INFO root Epoch 28 Batch 25 Speed 545 23 samples sec Train accuracy 0 806329 INFO root Epoch 28 Batch 25 Speed 545 23 samples sec Train top k accuracy 3 0 958228 INFO root Epoch 28 Batch 30 Speed 534 52 samples sec Train accuracy 0 729114 INFO root Epoch 28 Batch 30 Speed 534 52 samples sec Train top k accuracy 3 0 949367 INFO root Epoch 28 Batch 35 Speed 529 25 samples sec Train accuracy 0 654430 INFO root Epoch 28 Batch 35 Speed 529 25 samples sec Train top k accuracy 3 0 927848 mxnet example image classification resnet01 0003 params INFO root Epoch 28 Batch 90 Speed 525 33 samples sec Train top k accuracy 3 0 937975 INFO root Epoch 28 Batch 95 Speed 530 39 samples sec Train accuracy 0 758228 INFO root Epoch 28 Batch 95 Speed 530 39 samples sec Train top k accuracy 3 0 946835 INFO root Epoch 28 Batch 100 Speed 533 11 samples sec Train accuracy 0 731646 INFO root Epoch 28 Batch 100 Speed 533 11 samples sec Train top k accuracy 3 0 951899 INFO root Epoch 28 Batch 105 Speed 520 15 samples sec Train accuracy 0 755696 INFO root Epoch 28 Batch 105 Speed 520 15 samples sec Train top k accuracy 3 0 951899 INFO root Epoch 28 Batch 110 Speed 541 82 samples sec Train accuracy 0 800000 INFO root Epoch 28 Batch 110 Speed 541 82 samples sec Train top k accuracy 3 0 968354 INFO root Epoch 28 Batch 115 Speed 530 67 samples sec Train accuracy 0 788608 INFO root Epoch 28 Batch 115 Speed 530 67 samples sec Train top k accuracy 3 0 965823 INFO root Epoch 28 Batch 120 Speed 520 34 samples sec Train accuracy 0 759494 It produces similar behavior when using some other networks as well I suppose it is because of batch normalization but that also does not really make sense If you have any suggestions what I might doing wrong please let me know,,"piiswrong,piiswrong,piiswrong,yajiedesign",2017-04-27 17:37:42,2017-09-30 17:16:22
IS,Split layers of network to multiple modules using SequentialModule,Hi I'm trying to split layers of network e g LeNet to different modules and train the network using SequentialModule like the example did However the example tries to do model and data parallelization at the same time Basically I just want to do the regular networking training like data layer0 layer1 ouput layer and then backprop but layers are split into multiple modules I tried to change the input of the fully connected layer in second module by modifying this line in the foregoing example to make activation function 1 as input to next layer fc2 in the example I changed the data data to data act1 L30 But I got this error Can someone tell me how can I split layers of network into multiple module connect them together as a chain and train the whole chain with data input at layer0 and label at last layer Environment info Operating System ubuntu 14 04 Compiler gcc 4 8 4 Package used Python build from source Python version and distribution python 2 7 10,,"EvanzzzZ,yajiedesign",2017-05-19 16:39:57,2017-09-30 17:16:25
IS,how to use RNN parameters in rnn example,piiswrong I found that FusedRNNCell used in cudnn lstm bucketing py finally call mxnet symbol RNN These is a parameters mode 'gru' 'lstm' 'rnn relu' 'rnn tanh' in RNN i want to know how mode control the rnn net struction but i can not debug into mxnet symbol RNN Could you show me some infos about mxnet symbol RNN i think the infos in mxnet symbol RNN is not enough,,"szha,yajiedesign",2017-05-15 08:03:50,2017-09-30 17:16:29
IS,fit got an unexpected keyword argument 'monitor',Hello please take a look and help me with this problem Environment info Operating System windows 10 Compiler I use built package for VC12 Package used Python MXNet version tried with 0 9 3 0 9 5 0 10 0 Or if installed from source Python version and distribution python version 2 7 Error Message python train mnist py network mlp INFO root start with arguments Namespace batch size 64 disp batches 100 gpus None kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 Traceback most recent call last File train mnist py line 76 in module fit fit args sym get mnist iter File D mxnet example image classification common fit py line 187 in fit monitor monitor TypeError fit got an unexpected keyword argument 'monitor' Steps to reproduce I run the example image classification and I meet the error when run python train mnist py network mlp 1 I have download 2 newest package from vc12 base package and 20170525 mxnet x64 vc14 cpu 7z 2 Extract setup pre built package 3 run the example from newest github source code project python train mnist py network mlp What have you tried to solve it 1 I have tried with 3 version 0 9 3 0 9 5 0 10 0 Not yet for previous Regards Thanks,,yajiedesign,2017-05-25 07:55:51,2017-09-30 17:16:32
IS,Can I use autograd for custom op written with python,I have just found that mxnet supports autograd functionality with ndarrays but I cannot find any example related to it Specifically I want to use autograd for backward function of my custom op layer Is there any how to is for that,,"piiswrong,yajiedesign",2017-05-25 03:56:41,2017-09-30 17:16:35
IS,Check if symbol requires top gradient or not,Is there a python symbol API to check whether a symbol requires a top gradient or not If not can we get one,,yajiedesign,2017-05-25 16:49:16,2017-09-30 17:16:39
IS,mx sym SVMOutput breaks LR toy example,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 2 LTS Installed with pip according to Error Message Please paste the full error message including stack trace 64700 300 64700 1 1024 300 1 300 1 1024 1024 1 infer shape error Arguments target 1024 1 data 1024 300 Traceback most recent call last File svmexample py line 33 in module label shapes mx io DataDesc name 'target' shape batch size 1 layout 'NC' File home martin local lib python3 5 site packages mxnet module module py line 388 in bind state names self state names File home martin local lib python3 5 site packages mxnet module executor group py line 205 in init self bind exec data shapes label shapes shared group File home martin local lib python3 5 site packages mxnet module executor group py line 301 in bind exec shared group File home martin local lib python3 5 site packages mxnet module executor group py line 548 in bind ith exec arg shapes aux shapes self symbol infer shape input shapes File home martin local lib python3 5 site packages mxnet symbol py line 747 in infer shape res self infer shape impl False args kwargs File home martin local lib python3 5 site packages mxnet symbol py line 871 in infer shape impl ctypes byref complete File home martin local lib python3 5 site packages mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator svmoutput0 Shape inconsistent Provided 1024 1 inferred shape 1024 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx import mxnet ndarray as nd import numpy as np import logging import sys import os logging basicConfig stream sys stdout level logging DEBUG Config the logging np random seed 777 xy np loadtxt os path join 'w8a csv' delimiter ' ' dtype np float32 x data xy 0 1 y data xy 1 print x data shape y data shape sample num x data shape 0 dimension x data shape 1 batch size 1024 data mx sym Variable data target mx sym Variable target fc mx sym FullyConnected data data num hidden 1 name 'fc' pred mx sym SVMOutput data fc label target print pred infer shape data batch size dimension net mx mod Module symbol pred data names wouldata' label names 'target' context mx cpu 0 net bind data shapes mx io DataDesc name wouldata' shape batch size dimension layout 'NC' label shapes mx io DataDesc name 'target' shape batch size 1 layout 'NC' net init params initializer mx init Normal sigma 0 01 net init optimizer optimizer isgd' optimizer params 'learning rate' 1E 2 'momentum' 0 9 train iter mx io NDArrayIter x data y data batch size shuffle False label name 'target' feval lambda labels pred pred 0 5 labels mean metric mx metric CustomMetric feval feval name acc net fit train data train iter eval metric metric num epoch 1000 Hello I was able to successfully run the code above using LogisticRegressionOutput however when I change the output to SVMOutput using the same configuration I get the Inconsistent shape error shown above Printing the inferred shape is the same in both cases of SVM and LR however SVM complains and throws an error I believe the error is thrown by Symbol py after calling svm output inl h I compared the implementations between SVM and LR and it looks like the SVM implementation has an extra check that throws the error Here is the code for LR starts at line 48 in regression output inl g CHECK EQ in data size 2U RegressionOutputOp Input data label CHECK EQ out data size 1U RegressionOutputOp Output output Stream xpu s ctx get stream xpu Tensor xpu 2 data in data reg enum kData FlatTo2D xpu real t s Tensor xpu 2 out out data reg enum kOut FlatTo2D xpu real t s Assign out req reg enum kOut F ForwardOp data While the SVM implementation looks like this CHECK EQ in data size 2U Expecting data label CHECK EQ out data size 1U Expecting output CHECK EQ req size 1U Expecting output size req size Stream xpu s ctx get stream xpu Tensor xpu 2 DType data in data svm enum kData FlatTo2D xpu DType s Tensor xpu 2 DType out out data svm enum kOut FlatTo2D xpu DType s Assign out req svm enum kOut F mshadow op identity data My question is why does the SVMOutput and LogisticRegressionOutput differ in the inferred shape and is there a solution to run this example using SVMOutput,,yajiedesign,2017-05-25 21:58:05,2017-09-30 17:16:43
IS,mxnet no get mnist in test utils,import mxnet as mx mnist mx test utils get mnist AttributeError 'module' object has no attribute 'get mnist',,"pracheer,yajiedesign",2017-05-26 14:46:49,2017-09-30 17:16:47
IS,Relu implemented with mshadow,Trying to understand how to add new operators in mxnet but got confused about relu defined in file elementwise unary op h here L140 Since mshadow op h already provide implementation of relu see here L73 Why the elementwise operation need to recreate a relu structure and also include the index as params Rather than just using the one provided by mshadow op Is this something must do with reregistering operator with nnvm or just duplicated code implemented,,"EvanzzzZ,ZihengJiang,EvanzzzZ,ZihengJiang,EvanzzzZ,EvanzzzZ,ZihengJiang,yajiedesign",2017-05-22 21:47:36,2017-09-30 17:16:50
IS,weighted cross entropy for imbalanced data,Hi I'm new on MxNet and I'm dealing some imbalanced multi class data As far as I know Keras allows users to set class weights or per sample weight in its fit function In tensorflow there is similar functions tf nn weighted cross entropy with logits or tf losses softmax cross entropy I did some search Similar question has been issued Somebody said it is possible in MxNet but I just can not find any example Is there any similar function in MxNet or can anybody kindly tell me how to implement it Thanks a lot 1535 1783,,"gurumurthys,yajiedesign",2017-05-08 08:14:57,2017-09-30 17:16:54
IS,Unsynchronized access to comm buf,L99 L206 Looks like in these two places the access to comm buf is unsynchronized This can happen if push or pull is called twice with different keys if they are not present Is this a problem or it never happens,,"tqchen,yajiedesign",2017-05-26 22:13:31,2017-09-30 17:16:57
IS,a bug in 'tools im2rec py' while argument 'pass through' is turned on,I have found a bug in 'tools im2rec py' while argument 'pass through' is turned on See Line 105 of 'tools im2rec py' with open fullpath as fin 'fullpath' represents the path of an image which is a binary file Reading it with the default mode might result in a truncated string The workaround is to open the image file with binary mode with open fullpath 'rb' as fin And then reinstall mxnet cd python python setup py install Thank you for reading and wish you not to be stuck by this issue,,"piiswrong,yajiedesign",2017-05-26 13:06:36,2017-09-30 17:17:00
IS,Compile install MXNet using MKL,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you MXNet commit hash git rev parse HEAD d35fc765f9526bd836ba4a431729d130a862db3f If you are using python package please provide Python version and distribution python3 5 Error message src operator mkl mkl pooling inl h In member function virtual void mxnet op MKLPoolingOp xpu DType Forward const mxnet OpContext const std vector mxnet TBlob const std vector mxnet OpReqType const std vector mxnet TBlob const std vector mxnet TBlob src operator mkl mkl pooling inl h 185 11 error dnnAlgorithmPoolingAvgIncludePadding was not declared in this scope dnnAlgorithmPoolingAvgIncludePadding dnnAlgorithmPoolingAvg What have you tried to solve it 1 modify root of MKL directory,,"piiswrong,glingyan,glingyan,glingyan,glingyan,yajiedesign",2017-05-22 15:47:06,2017-09-30 17:17:03
IS,Inference speed may decrease slightly when batch size increase,Recently I did some speed benchmark experiments of CNN inference task over a few popular deep learning frameworks including mxnet Though mxnet performs excellently in most modern CNN structures i e resnet inception I also found mxnet is inference speed curves often drop slightly when batch size is 16 32 or 64 as below Is this pretty common resnet50 inception v3 Some of other benchmark results can be found here Titan X and here GTX1080,,"nicklhy,piiswrong,nicklhy,xioryu,ptrendx,nicklhy,piiswrong,nicklhy,piiswrong,nicklhy,piiswrong,nicklhy,xioryu,nicklhy,xioryu,ptrendx,nicklhy,nicklhy,yajiedesign",2017-05-23 01:48:06,2017-09-30 17:17:06
IS,New padding value in max Pooling,The padding behaviour of max Pooling layer has changed at some point padding with a value of 3 4e 38 probably the most negative float instead of zero While the reason for this is understandable i e never pick the padding value as output it leads to very undesirable side effects for the cases in which the kernel size is less or equal the padding size Namely the huge negative padding value makes it to the output As a possible fix we were thinking about actually forbidding padding sizes greater or equal to the kernel sizes After all it is a meaningless operation as it is creating values not dependent on the input This could be applied to convolutions also And if someone really wants to do it anyway for some reason the padding layer is already there for this special cases,,"matteosal,piiswrong,taliesinb,matteosal,yajiedesign",2017-05-10 14:09:27,2017-09-30 17:17:11
IS,remove update metric in module faces weird performance,Hi I faced an issue involves update metric in module L875 Based on the document this function is to Evaluate and accumulate evaluation metric on outputs of the last forward computation e g train accuracy However for some reasons I'm not sure time costs of this function is quite high in each training iteration epoch based on my test train with cifar10 dataset network resnet num layers 110 batch size 128 time cost for the whole iteration 15 2210628986 time cost for forward 0 000531911849976 time cost for backward 0 000345945358276 Also this issue reported similar performances Please correct me if I'm wrong my thinking is the update metric function is not necessary in training iterations if one does not care about the metrics for training accuracy Then I tried to remove this funtion in module fit L497 After removing it I observed some weird performance As one can see from this sample output in the first three iterations it seems I avoided time costs of update metric successfully however in the next iterations time cost for the whole iteration increased largely and the value became similar to the one when I did nothing on the update metric My questions are 1 can someone tell me why the update metric function costs so much time in a certain iteration 2 how can I skip the update metric in an iteration and only do forward back ward and update by modifying the fit function in module L377,,"luoyetx,eric-haibin-lin,yajiedesign",2017-05-20 20:49:23,2017-09-30 17:17:15
IS,Why APIs are different between mxnet module BaseModule fit and mxnet model FeedForward fit,Why APIs are different between mxnet module BaseModule fit and mxnet model FeedForward fit I'm confused about these similar fit Besides I found many a different input parameters such as mxnet model FeedForward fit supports work load list but mxnet module BaseModule fit does not I'm recently finetuning a pretrained model according to official guide However guide uses mxnet module BaseModule fit to make fit but it does not support different power GPUs Because I want to use work load list parameter according to officail Run MXNet on Multiple CPU GPUs with Data Parallelism mxnet documentation advanced usage guide can anyone help me Thanks a lot,,"ysh329,yajiedesign",2017-05-29 14:15:52,2017-09-30 17:17:19
IS,Gradient Accumulation Question,I'm working on how MXNet accumulates gradient within one batch By gradient accumulation I mean for one batch update the gradients for all data samples in this batch are accumulated by sum or mean This should be done during backward As I follow the code the GraphExecutor Backward will run operations through an engine And to be more specific in function GraphExecutor RunOps I found the gradient calculation node pushed into engine here L804 Then my question is where specifically is the function to accumulate gradients during gradient calculation I think this operation may register somewhere but I did not find it I have not found too many helpful posts about this Really appreciate it if someone can help,,"piiswrong,yajiedesign",2017-05-28 05:35:30,2017-09-30 17:17:23
IS,Better enum generator in cpp package,Currently the enum generated for cpp package in cpp package include mxnet cpp op h directly copies the enum name declared in NNVM registry e g the enum generated for PadMode is to L29 and change the enums used in cpp package example cc accordingly We should change it sooner rather than later before the cpp package grows huge Any comment,,"eric-haibin-lin,lx75249,eric-haibin-lin,yajiedesign",2017-05-17 19:01:56,2017-09-30 17:17:31
IS,R I P to Antti Pekka Hynninen,With great sadness I inform you that Antti Pekka hynninen died with his wife in a car accident on last Sunday Antti Pekka joined us earlier this year He made a great contribution to MXNet especially on FP16 supports He is one of the most talented engineers I have met It is a great loss to us his friends and his family Please join us to memorize him on,,"mli,phunterlau,ZihengJiang,antinucleon,pluskid,eric-haibin-lin,terrychenism,bhavinthaker,kevinthesun,sandeep-krishnamurthy,nswamy,tornadomeet,sxjscience,zihaolucky,gengyifeng,yzhliu,Ldpe2G,dsqx71,zackchase,Godricly,Soonhwan-Kwon,kindruth,thatindiandude,yajiedesign",2017-05-25 20:16:25,2017-09-30 17:17:35
IS,some doubt of simple bind,Model Net Load arg params aux params mx model load checkpoint prefix Model Saved epoch 1 Model Net Config Get OCR Net data shape data Model Batch Size 3 50 130 input shapes dict data shape executor Load Model Net Load simple bind ctx mx gpu input shapes executor Model Net simple bind ctx mx gpu input shapes what is the diff between 'Model Net Load' and 'Model Net ' on method simple bind,,"alues,alues,yajiedesign",2017-05-29 20:34:22,2017-09-30 17:17:39
IS,amalgamation fails to build,In mxnet 0 10 0 under amalgamation when type make I see the following error mxnet predict all cc 35701 19 error cannot convert const dim t aka const long int to gnu cxx alloc traits std allocator const unsigned int value type aka const unsigned int in assignment data at i shapes i data mxnet predict all cc 51537 15 error cannot convert nnvm dim t aka long int to mx uint aka unsigned int in assignment shape data p out shapes out index data mxnet predict all cc 51631 14 error cannot convert nnvm dim t aka long int to const mx uint aka const unsigned int in assignment out shape p shapes index data,,"piiswrong,yajiedesign",2017-05-30 22:17:16,2017-09-30 17:17:51
PR,Add parameter of number per class,We could control the number of per class images in making list via num perclass,,"SCP-173-cool,yajiedesign",2017-05-31 07:28:36,2017-09-30 17:17:55
IS,L2Normalization errors when use channel Mode,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OSX 10 12 Compiler gcc Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source yes MXNet commit hash git rev parse HEAD f8840029ea10fae37f5cdf6c8cc8802d0faa02cc If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 in the api document l2Normalization 2 the following calculation is not correct please check it out I also tried it in a 4D matrix it is also not correct x 1 2 3 4 2 2 5 6 L2Normalization x mode 'channel' 0 31622776 0 44721359 0 94868326 0 89442718 0 37139067 0 31622776 0 92847669 0 94868326 3 What have you tried to solve it 1 None I think these is some errors when calculate 2 3,,yajiedesign,2017-05-31 09:52:36,2017-09-30 17:17:58
IS,Help how to convert params files,I trained a model with mxnet v0 7 now I want to upgrade mxnet to v0 10 How can I use the old params and sysbol json files in the new version Is there a way to convert them Or I must train it again using the new version of mxnet,,yajiedesign,2017-05-31 12:20:08,2017-09-30 17:18:07
IS,Benchmarking module in MXNet,Thinking if it make sense to have a benchmarking module in mxnet Even though there are many examples in the repo that can be used for benchmarking it will be more convenient to have a module in mxnet that packaging all the common applications together Most of the popular applications have dominant datasets for benchmarking All the user need to do is giving his designed model or symbol and setup the configurations the interface handle the dataset retrieving data iterators and potentially generate meaningful benchmarking metrics when comparing to the baseline The convenience of quickly prototyping and benchmarking their models might attract more users to the community,,"EvanzzzZ,yajiedesign",2017-05-31 18:20:47,2017-09-30 17:18:11
IS,prediction size not matching,Environment info Operating System Deep Learning AMI for Amazon Linux Package used Python MXNet version 0 9 5 I running the rnn example mxnet example rnn lstm bucketing py After the model training I use the following command to make predictions on the validation data predicts model predict data val predicts NDArray 95040x10000 0 The size of the prediction 95040 does not seem to match the data val which is 98940 sum data val data c size for c in range 6 98940 What did I do wrong or did I miss something Thanks,,"piiswrong,yajiedesign",2017-05-31 00:24:15,2017-09-30 17:18:14
IS,how to use huffman tree softmax in mxnet,hi i want to use huffman softmax in mxnet to speed up lstm training just like huffman softmax used in word2vec ref effword But i don not know how to build network graphs because i think huffman softmax network graph should have ability to train sparse data could you give me some suggestion,,yajiedesign,2017-05-31 02:34:16,2017-09-30 17:18:18
IS,im2rec attributeErroe 'module' object has no attribute 'MXIndexedRecordIO',Hi all I have used im2rec py to convert caltech101 images into record io format 1 I have created caltech lst succesfully using os system 'python s tools im2rec py list 1 recursive 1 shuffle 1 data caltech data 101 ObjectCategories' MXNET HOME 2 Then when I run this os system python s tools im2rec py train ratio 0 8 test ratio 0 2 num thread 4 pass through 1 data caltech data 101 ObjectCategories MXNET HOME I have this error attributeErroe 'module' object has no attribute 'MXIndexedRecordIO' Please someone has an idea to fix this error Thanks in advance Environment info Operating System Windows 8 1 Package used Python R Scala Julia Python MXNet version 0 9 5 Python version and distribution 2 7,,"eric-haibin-lin,yajiedesign",2017-05-23 14:17:58,2017-09-30 17:18:21
IS,question language binding interface,theres are some questions about the mxnet if it is not right way to ask by a issue please let me know I have three question about the language binding interface in mxnet is there any go interface in current state after i googled i only find the gomxnet repo which seems has been deprecated for a long time what is the relationship between NNVM interface and framework language binding interface python language binding support in ctypes and cython is there any compartion between these two method Thanks,,yajiedesign,2017-06-01 13:58:34,2017-09-30 17:18:24
IS,nvvm is NNSymbolListInputNames,nvvm is NNSymbolListInputNames should be implement as a new method in symbol to list input names,,"alues,yajiedesign",2017-06-01 18:58:04,2017-09-30 17:18:27
IS,VGG structure is wrong,I think the provided VGG structure as in mxnet example image classification symbols VGG py is not the original VGG16 It is not corresponding the provided pre trained VGG16 model as in For example in the first conv group there are two conv layers but in VGG py there is only one So in this case if we load the pre trained VGG16 we are just loading part of the whole structures,,yajiedesign,2017-06-01 22:40:31,2017-09-30 17:18:30
IS,Example of mxnet based superresolution py fails,Hai I was executing superresolution py example But i got an error below at line 33 ImportError No module named data Kindly suggest how to proceed,,"szha,szha,szha,szha",2017-09-29 10:20:36,2017-09-30 17:28:41
IS,distributed training on multiple machines speed is slowing down instead of an expected increase,Hi all I am trying to run distributed training on multiple machines data parallelism with mxnet using the VOC dataset I have been successful in doing this on AWS machines g2 2x large whether it has been 1 2 or 5 machines given all the hyperparameters are kept the same e g batch size on each gpu Instance Type GPU Card Num of GPU Speed samples sec g2 2x large k520 1 9 10 g2 2x large k520 2 9 10 g2 2x large k520 5 9 10 The speed samples sec stays relatively the same regardless of the number of machines I have which means that the training time will subsequently be reduced as I increase the number of GPU b c the batch will decrease This is ideal because that is in accordance with mxnet documentation However when I do this on my own local network I unfortunately am getting a decrease in speed when I add on more machines given all the hyperparameters are kept the same e g batch size on each gpu kv store dist sync The decrease in speed is pretty significant almost 5x slower on 2 machines than if I just used 1 machine Instance Type GPU Card Num of GPU Speed samples sec local gtx 1080 1 40 50 local gtx 1080 2 8 10 From this speed decrease it means the training time will take longer as compared to when I was training with only 1 machine which is less than ideal Do you know why this might be the case What have you tried to solve it I thought it might be a bandwidth issue and used iperf to test the connection speed but that seems to be comparable between my local machines and aws instances Interval Transfer Bandwidth local gtx 1080 local gtx 1080 0 0 10s 1012 MBytes 848 MBits sec g2 2x large k520 g2 2x large k520 0 0 10s 867 MBytes 727 MBits sec If anyone could advise me on this I would greatly appreciate it Thank you so much Environment info Operating System Ubuntu 14 04 Ubuntu 16 04 Compiler Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD 703e8eeb117109ea345639610780e30775056412 If you are using python package please provide Python version and distribution Python 2 7 8,,"piiswrong,piiswrong,szha",2017-05-25 23:09:30,2017-09-30 17:29:14
IS,problem with bi lstm sort example,According to bi lstm sort example readme it is said that we need to generate the data first Firstly generate data by cd data python gen data py But I can not find the corresponding file for generating training data Can anybody fix this problem,,"xlvector,gurumurthys,szha",2016-11-24 13:40:01,2017-09-30 17:29:16
IS,An advice for set params when using kvstore,finally I found why my training loss can not decreased with kvstore that is because kvstore can not get the new weight value after its init kvstore only updates the weight in itself set param can not affect the weight of kvstore actually but In my training I need to exchange the weight value periodically So I suggest kvstore can accept the new weight value by using set params in the training time,,"eric-haibin-lin,szha",2017-04-27 12:24:11,2017-09-30 17:29:17
IS,UserWarning input name is not found in symbol list arguments,I ran NN on raspberrypi Please ignored OOM prolem std bad alloc I wanna fix userWarning Thanks a lot Error Messages,,"ysh329,szha",2017-04-11 08:50:15,2017-09-30 17:29:19
IS,How to check number of gradients collected by master node from python interface,Hi In distributed synchronous training when setting kv store dist sync I want to keep checking how many works' gradients are currently aggregated by the kvstore which is quite similar to the ConditionalAccumulator num accumulated in TensorFlow L1081 In particular for example if there are n workers in a distributed cluster for each training iteration I want to see the number of gradients gathered by the master or parameter server increasing from 0 to n After gradients of all n workers are collected by the master node they are used to update the model which is the synchronization point and then the next iteration begins I know that through cpp part I can somehow get number of gradients accumulated by master node is there any possibility to achieve this from python interface Since that will help a lot for my further process,,szha,2017-06-04 01:13:35,2017-09-30 17:29:20
IS,How to make step by step predictions using RNN with bucketing,By step by step predictions I mean using values sampled from current time step is output as input in the next time step For example to generate a sentence using the lstm model in example rnn lstm bucketing py I need to start from a particular word feed it to the network and get outputs then sample a word from the outputs and fix it as the input in the next time step Now in every time step I need to construct a data batch and pad the sequence to a legal length one of the buckets' length because forward can only accept DataBatch Are there any existing interfaces I found a LSTMInferenceModel in tutorial but it seems to be deprecated or methods to make it a more elegant process Thanks,,"Cloudyrie,szha",2017-06-03 11:33:49,2017-09-30 17:29:22
IS,MXIndexedRecordIO,Hi The class called MXIndexedRecordIO is not recognized in the recordio module When i run the code below record mx recordio MXIndexedRecordIO 'tmp idx' 'tmp rec' 'w' for i in range 5 record write idx i arecord d' i record close i had this error AttributeError 'module' object has no attribute 'MXIndexedRecordIO' OS Windows Python 2 7 Mxnet 0 9 5,,szha,2017-06-04 11:17:18,2017-09-30 17:29:23
IS,Q Checkpointing and how much to train,Thanks for your help and the great work you are doing here Im training my first LSTM model from R environment using the shakespeare file Very easy to see that with time the model improves and generates better sequences Still I'm not sure 1 How much time should I give it for training 2 If I stop it after N hours days weeks is there a way to check the model and continue the training from the point it stopped 3 Any kind of checkpointing Im using the mx lstm function Thanks for assisting Offer,,"offerm,szha",2017-06-04 19:09:45,2017-09-30 17:29:24
IS,How to use kvstore with sequential module,Hi I'm using SequentialModule to split layers of network e g LeNet to different modules and wiring them together But when I tried to use kvstore for the wired model I got some errors during initializing the kvstore It seems when the second module second layer are going to be initialized the index of parameters will still start from '0' which lead to conflict with parameter indexes in the first layer please check the error message for detailed information Is there any solution to avoid this to make wired modules work with kvstore Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 4 Package used Python MXNet version 0 9 5 build from source Python version and distribution 2 7 10 Error Message Minimum reproducible example Please check the code I used for this issue,,szha,2017-06-05 02:39:02,2017-09-30 17:29:26
IS,Why Overrided Function backward of CustomOp Not be Called,I subclassed mxnet operator CustomOp and mxnet operator CustomOpProp following the way described in All overloaded function worked well except the backward function had never been called in the execution of Module fit And there was no warning or error logged Why and how to fix it Have I missed something else to be set or configured Compared to other networks with subclass of CustomOp that backward function works well I used symbols slice axis and concat in this network dose it matter I worked on windows 7 the mxnet 0 9 5 is compiled with vs2013 and installed in python 2 7,,"piiswrong,szha",2017-06-01 11:13:26,2017-09-30 17:29:27
IS,GPU stream error when running with gpu context,Trying to add a new operator in C The code compiled and run as expected with cpu context but when I ran it with gpu context it throw the following error Default GPU stream was used when MSHADOW FORCE STREAM was on My guess is I used NewTensor in the forward backward operation in a way not as expected Any clue here,,"EvanzzzZ,szha",2017-06-05 04:12:24,2017-09-30 17:29:28
IS,mx sym UpSampling causes SIGFPE silently,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source Yes MXNet commit hash git rev parse HEAD 6e81d76e6830b70a4a2278ebc08e9d3e3af1c937 If you are using python package please provide Python version and distribution Python 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 on linux2 Error Message No stack trace at all Just SIGFPE Minimum reproducible example Simply set scale to a large integer and method to 'biliner' Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 choose the 'nearest' method 2 3,,szha,2017-06-05 06:00:04,2017-09-30 17:29:30
IS,251MB memory in gpu 0 even when context is not mx gpu 0,I just found that passing kvstore mx kvstore create 'local' into mx mod Module fit would produce 251MB memory in gpu 0 even when context is not mx gpu 0 Is it a bug,,"nicklhy,szha",2017-06-05 09:46:06,2017-09-30 17:29:31
IS,weight norm implemention,Does anyone implement weight normalizaiton using mxnet I use python symbol to implement it like follows But I'm not sure whether it can work Any suggestion will be appreciated,,szha,2017-06-05 11:54:34,2017-09-30 17:29:32
IS,where to find the synset of the published model,jychoi84 Could you please point where to find the synsets of the published model LocationNet on Thanks,,"luoyetx,szha",2017-05-26 18:53:32,2017-09-30 17:29:34
IS,Question Chunking keys within a server,Hi I have a question regarding accuracy of chunking a large key into a smaller key within a single server I know MXNET KVSTORE BIGBOUND ARRAY is used to chunk a key to different servers for processing Our implementation now chunks a physical key into a series of virtual keys and these virtual keys are sent individually to one central server for optimization The server only sees the virtual keys We still use the vars for the original physical keys to synchronize access to the buffers The virtual keys use buffers of the original physical keys we chunk the buffer and assign different segments to the virtual keys We then send these virtual keys directly to optimize For example update self index weight grad state In here index is a virtual key instead of the original key Our problem is in a 0 3 accuracy drop with MNIST In the first epoch chunking vs not chunking is train accuracy starts to differs in 100 150 batches but seems to be identical in the first 100 batches We do not know what may cause the problem but it would be great if you can answer the following questions 1 Is it fine to chunk keys and send each segments individually to server for optimization in general 2 Can this be related to the optimizer The optimizer is setup with states with physical keys and it does not understand virtual keys so it assigns default state for the keys it does not know 3 Can this be related to update in per key parameters For example learning rate for individual parameter it only updates the learning rate for parameter it understands i e physical keys but not virtual keys Many thanks,,szha,2017-06-04 18:33:32,2017-09-30 17:29:35
IS,Visualization Error due to usage of split method,Environment info Operating System MacOs Compiler Package used Python R Scala Julia Python Jupyter Notebook MXNet version 0 9 5 vsLSTM visBug ipynb zip Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 13 If you are using R package please provide R sessionInfo Error Message KeyError u isplit9 output 1' Hi there I have been lately playing with bi directional LSTMs and came across with an issue when visualizing the model in mxnet The visualization of the forward hidden states works whereas that when both the backward and forward hidden states are returned as a group mx viz plot network returns an error It looks like the error is given by the usage of the split method It seems that there is no problem with the model implementation since the outputs are returned correctly and so it might be due to the visualization method itself Could you please help me on that I am attaching a demo of the code showing where the problem is vsLSTM vizBug ipynb zip Thank you very much,,szha,2017-06-06 18:49:50,2017-09-30 17:29:37
IS,convert model py fails to convert model based on resent 50,protoc version libprotoc 3 3 0 Traceback most recent call last File scratch nihao repo mxnet 0 9 5 tools caffe converter convert model py line 177 in module main File scratch nihao repo mxnet 0 9 5 tools caffe converter convert model py line 173 in main convert model args prototxt args caffemodel args save model name File scratch nihao repo mxnet 0 9 5 tools caffe converter convert model py line 131 in convert model rescale factor 1 rescale factor TypeError unsupported operand type s for 'int' and 'google protobuf pyext message RepeatedScalarContainer',,"arikpoz,szha",2017-05-25 14:48:45,2017-09-30 17:29:38
IS,Issue converting caffe model with multiple outputs to mxnet,I am trying to use the caffe to mxnet converter tool at convert model py on a caffe model with more than one final output and running into issues It looks like the converter tool is setup to handle a single final output layer in convert symbol py so some of my layers that are only in the DAGs of the other output layers are culled out at least I think that is what is happening Because of this I get an assertion error pasted below in convert model py Does the converter tool currently not support caffe models with multiple outputs Sorry if my nomenclature is wrong I am new to deep learning caffe and mxnet Environment info Operating System Ubuntu 14 04 5 LTS Compiler 4 8 4 Ubuntu 4 8 4 2ubuntu1 14 04 3 Package used Python R Scala Julia Python MXNet version 0 9 5 Python version and distribution 2 7 6 from Ubuntu repos Error Message Please paste the full error message including stack trace convert model py is reading in the prototxt file and iterating over each layer assuming that the layer exists in the arg shape dic but since it is getting culled out it does not and causes the crash Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Run convert model py against a prototxt file with multiple outputs,,"arikpoz,szha",2017-05-18 01:31:09,2017-09-30 17:29:39
IS,Incorrect loss values with SVMOutput Hinge loss,I am testing hinge loss SVMOutput and it appears to be producing negative loss values which should not be possible given it has the form max 0 Is MXNet using an alternative calculation for this loss function that allows negative values Perhaps this max occurs after eval metric updates Here is some code references gist This gives output which includes negative loss values which should not be possible 2017 05 26 13 41 34 380 Node 0 Epoch 0 Batch 50 Train lr 0 0001 Train iter per sec 1 08 Train samples per sec 69 21 Train loss 0 01018502615 Train accuracy 0 4840686275 2017 05 26 13 42 20 844 Node 0 Epoch 0 Batch 100 Train lr 0 0001 Train iter per sec 1 08 Train samples per sec 68 87 Train loss 0 0001527548686 Train accuracy 0 545625 2017 05 26 13 43 07 365 Node 0 Epoch 0 Batch 150 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 79 Train loss 0 0001172707253 Train accuracy 0 5746875 2017 05 26 13 43 53 881 Node 0 Epoch 0 Batch 200 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 8 Train loss 0 0001611461397 Train accuracy 0 5728125 2017 05 26 13 44 26 449 Node 0 Epoch 0 Train loss 0 000160 2017 05 26 13 44 26 451 Node 0 Epoch 0 Train accuracy 0 599107 2017 05 26 13 44 26 451 Node 0 Epoch 0 Time cost 219 891 2017 05 26 13 44 28 393 Node 0 Epoch 0 Validation loss 0 02287905058 Validation accuracy 0 51171875 2017 05 26 13 45 15 985 Node 0 Epoch 1 Batch 50 Train lr 0 0001 Train iter per sec 1 85 Train samples per sec 68 76 Train loss 0 0005699406987 Train accuracy 0 6253063725 2017 05 26 13 46 02 525 Node 0 Epoch 1 Batch 100 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 76 Train loss 0 0001366008772 Train accuracy 0 6034375 2017 05 26 13 46 49 059 Node 0 Epoch 1 Batch 150 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 77 Train loss 0 0001785517042 Train accuracy 0 6221875 2017 05 26 13 47 35 596 Node 0 Epoch 1 Batch 200 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 76 Train loss 0 0003049087618 Train accuracy 0 5928125 2017 05 26 13 48 08 183 Node 0 Epoch 1 Train loss 0 000430 2017 05 26 13 48 08 184 Node 0 Epoch 1 Train accuracy 0 633929 2017 05 26 13 48 08 185 Node 0 Epoch 1 Time cost 219 782 2017 05 26 13 48 10 143 Node 0 Epoch 1 Validation loss 0 0001243009465 Validation accuracy 0 7421875 2017 05 26 13 48 57 773 Node 0 Epoch 2 Batch 50 Train lr 0 0001 Train iter per sec 1 85 Train samples per sec 68 77 Train loss 0 0001136356458 Train accuracy 0 6409313725 2017 05 26 13 49 44 326 Node 0 Epoch 2 Batch 100 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 74 Train loss 0 0002889547544 Train accuracy 0 619375 2017 05 26 13 50 30 855 Node 0 Epoch 2 Batch 150 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 78 Train loss 0 0003283168329 Train accuracy 0 6315625 2017 05 26 13 51 17 405 Node 0 Epoch 2 Batch 200 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 75 Train loss 0 0001538071374 Train accuracy 0 6328125 2017 05 26 13 51 49 995 Node 0 Epoch 2 Train loss 0 000471 2017 05 26 13 51 49 996 Node 0 Epoch 2 Train accuracy 0 613393 2017 05 26 13 51 49 997 Node 0 Epoch 2 Time cost 219 844 2017 05 26 13 51 51 938 Node 0 Epoch 2 Validation loss 0 01205513696 Validation accuracy 0 78515625 2017 05 26 13 52 39 544 Node 0 Epoch 3 Batch 50 Train lr 0 0001 Train iter per sec 1 85 Train samples per sec 68 78 Train loss 2 118847652e 05 Train accuracy 0 6357230392 2017 05 26 13 53 26 107 Node 0 Epoch 3 Batch 100 Train lr 0 0001 Train iter per sec 1 07 Train samples per sec 68 73 Train loss 0 0001829522732 Train accuracy 0 6059375,,"jmerkow,jmerkow,yuruofeifei,jmerkow,yuruofeifei,jmerkow,szha",2017-05-26 20:57:59,2017-09-30 17:29:41
IS,Speed Diff mx mod Module vs mx model FeedForward,Edit To give a quick summary is the mx mod Module method slower than the usual ctx bind etc which is called with mx model FeedForward Using mx model FeedForward I get 240 a second with mx mod Module I get 160 a second I can not quite figure out why but method A is a bit faster 30 faster maybe Method A standard Where I have train x train y loaded into RAM 140GB So I load a raw data set generate my features this takes maybe 30 minutes and then keep in RAM The reason I like method B is because it gives me complete control over many aspects it is very easy for me to limit this to 8GB total RAM compared to 140GB for example Now I understand that holding all features in RAM would be fast however I asynchronously prefetch batches to keep a queue of 10 so after the first batch I should have no cost generating features on the fly because that is done multi threaded whilst my GPU is processing the weights gradients I store the data as dtype 'bool' because that costs me 4 bits compared to say 32 bits and all my data is just a binary 1 0,,"piiswrong,piiswrong,szha",2016-08-26 11:29:18,2017-09-30 17:29:42
PR,doc update,This is to add update reorganize doc for Gluon and for x 7875 mobilenet api doc x 7264 conv rnn doc and new contrib package x 7067 var dropout doc x 7992 new fluent methods x missed doc in 7910 x missed doc in 7403 x dataset doc x model zoo doc x model zoo doc fix mx gluon model zoo vision instead of mx gluon models 7023 x move the legacy rnn under symbol since it only supports symbol x loss doc fix for 7918 x executor doc page x fix broken links array creation routines in NDArray API x make references to entities clickable links for Gluon docs Updated doc can be found at,,"szha,piiswrong,szha,szha,piiswrong,szha,szha",2017-09-28 23:37:32,2017-09-30 18:18:20
IS,Runtime Error Handling,In general when there are data dependent conditions that can cause a layer to fail we want to avoid making this failure something that aborts the process With a usual error handling DMLC THROW EXCEPTION during asynchronous engine execution the process is terminated A language like Mathematica wants to share the process with MXNet and it cannot have this shared process terminate We would like to have a principled solution built into MXNet Is there a way for MXNet to handle this or is there some approach we can implement,,"sbodenstein,sbodenstein,taliesinb,piiswrong,piiswrong,taliesinb,yajiedesign",2017-05-03 12:53:10,2017-09-30 18:26:13
IS,How to use symbol and constant to construct a network,I want to construct a network by symbol But if my network has a constant how to construct my network Like this A mx symbol Variable 'A' B 5 C A B C A B Those above codes are wrong How to construct a network which has symbol and constant like C A B or C A B,,yajiedesign,2016-08-25 10:01:18,2017-09-30 18:26:16
IS,Training cifar10 py breaks when using USR PROFILER,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler g nvcc Package used Python R Scala Julia MXNet version latest Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace root d6dfc4c90761 mxnet example image classification python train cifar10 py network resnet num layers 110 batch size 128 gpus 0 1 2 3 INFO root start with arguments Namespace batch size 128 benchmark 0 data nthreads 4 data train wouldata cifar10 train rec' data val wouldata cifar10 val rec' disp batches 20 dtype 'float32' gpus '0 1 2 3' image shape '3 28 28' kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '200 250' max random aspect ratio 0 max random h 36 max random l 50 max random rotate angle 0 max random s 50 max random scale 1 max random shear ratio 0 min random scale 1 model prefix None mom 0 9 monitor 0 network aresnet' num classes 10 num epochs 300 num examples 50000 num layers 110 optimizer isgd' pad size 4 random crop 1 random mirror 1 rgb mean '123 68 116 779 103 939' test io 0 top k 0 wd 0 0001 23 18 15 src io iter image recordio cc 220 ImageRecordIOParser data cifar10 train rec use 4 threads for decoding 23 18 15 src io iter image recordio cc 220 ImageRecordIOParser data cifar10 val rec use 4 threads for decoding 23 18 15 mxnet dmlc core include dmlc logging h 300 23 18 15 src c api c api ndarray cc 381 Operator zeros cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered Stack trace returned 10 entries bt 0 mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7feadfb2b5cc bt 1 mxnet python mxnet lib libmxnet so MXImperativeInvoke 0x673 0x7feae069e9a3 bt 2 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7feb828bce40 bt 3 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7feb828bc8ab bt 4 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7feb82acc3df bt 5 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7feb82ad0d82 bt 6 python PyObject Call 0x43 0x4b0cb3 bt 7 python PyEval EvalFrameEx 0x5faf 0x4c9faf bt 8 python PyEval EvalCodeEx 0x255 0x4c2765 bt 9 python PyEval EvalFrameEx 0x68d1 0x4ca8d1 Traceback most recent call last File train cifar10 py line 53 in module fit fit args sym data get rec iter File mxnet example image classification common fit py line 187 in fit monitor monitor File mxnet python mxnet module base module py line 444 in fit for training True force rebind force rebind File mxnet python mxnet module module py line 388 in bind state names self state names File mxnet python mxnet module executor group py line 205 in init self bind exec data shapes label shapes shared group File mxnet python mxnet module executor group py line 301 in bind exec shared group File mxnet python mxnet module executor group py line 605 in bind ith exec context self logger File mxnet python mxnet module executor group py line 583 in get or reshape arg arr nd zeros arg shape context dtype arg type File mxnet python mxnet ndarray py line 900 in zeros return internal zeros shape shape ctx ctx dtype dtype File mxnet python mxnet ctypes ndarray py line 133 in generic ndarray function c array ctypes c char p c str str i for i in kwargs values File mxnet python mxnet base py line 78 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 23 18 15 src c api c api ndarray cc 381 Operator zeros cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered Stack trace returned 10 entries bt 0 mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7feadfb2b5cc bt 1 mxnet python mxnet lib libmxnet so MXImperativeInvoke 0x673 0x7feae069e9a3 bt 2 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7feb828bce40 bt 3 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7feb828bc8ab bt 4 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7feb82acc3df bt 5 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7feb82ad0d82 bt 6 python PyObject Call 0x43 0x4b0cb3 bt 7 python PyEval EvalFrameEx 0x5faf 0x4c9faf bt 8 python PyEval EvalCodeEx 0x255 0x4c2765 bt 9 python PyEval EvalFrameEx 0x68d1 0x4ca8d1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 I use mxnet docker for GPU without re build mxnet the system works well with the command python train cifar10 py network resnet num layers 110 batch size 128 gpus 0 1 2 3 2 I modify config mk in mxnet make directory and set USR PROFILER 1 3 make 4 after that I can use profiler and I can run the sample py code in mxnet example profiler 5 However I cannot run python train cifar10 py network resnet num layers 110 batch size 128 gpus 0 1 2 3 It shows the error message above What have you tried to solve it 1 No idea 2 3,,"piiswrong,yajiedesign",2017-06-08 23:29:26,2017-09-30 18:26:20
IS,can anyone give an example of using set lr mult,I can not find how to use set lr mult or set wd mult on the website Can anyone give an example,,"zihaolucky,fullfanta,yajiedesign",2016-10-17 01:37:46,2017-09-30 18:26:22
IS,train test and validation sets with im2rec,Hi I want to split my data into train validation and test sets I'm using im2rec py to generate the rec files with the following commands python im2rec py list 1 recursive 1 shuffle 1 test ratio 0 3 train ratio 0 7 images images images python im2rec py num thread 4 pass through 1 images images images Which gives me a train rec and test rec But how can I get also a validation set Thanks,,"ysh329,yajiedesign",2017-06-08 09:20:30,2017-09-30 18:26:25
IS,CNN SVM,So I am trying to create a CNN SVM model for CIFAR 10 dataset There is no error while running the code but the train accuracy and validation accuracy do not increase although it is already run on 50 epoch from 1st epoch till 50 still around 0 1 Is there anything that I miss for using the SVMOutput because when I use SoftmaxOutput the accuracy is increasing for every epoch,,"ysh329,yajiedesign",2017-06-05 06:31:52,2017-09-30 18:26:29
IS,Reuse Variable but with different attributes,Sometimes we need to share weights in network And Its' advised in mxnet to reuse variable by applyinng one weight and referencing multiple times That is OK when variable attributes like lr mult wd mult is fixed But what if we want to change the learning rate in different network structure but still share the same weight My solution is to manually call symbol set attr with different learning rate before different module initialization is that OK do you have a simpler way Is that OK to define multiple variables with same name in this case Thanks,,"piiswrong,yajiedesign",2017-06-08 01:48:21,2017-09-30 18:26:32
IS,How do you load a params file for mxnet,Environment info Operating System MacOS Sierra Compiler n a Package used Python R Scala Julia python MXNet version latest stable version How do you load a params file in MXnet,,"ysh329,ysh329,yajiedesign",2017-06-09 01:08:49,2017-09-30 18:26:35
IS,New Function Add Reset into kvstore enable set params to reset weights in kvstore,I think this is a good enhancement to set params when using kvstore for details please see 6005,,yajiedesign,2017-05-24 11:36:27,2017-09-30 18:26:38
IS,New Function Let kvstore and kvstore dist support partial weight update,I have finished the code which is based on mxnet 0 8 0 My new distribution training system can do partial weight update now see 6421 see the repository,,yajiedesign,2017-06-10 13:27:56,2017-09-30 18:26:41
IS,how to use multiple gpus when using mxnet based keras,As mxnet seems to support keras now is there any interface to use multiple gpus thanks For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-06-11 00:13:42,2017-09-30 18:26:45
IS,does mxnet have arctan2 implementation as a symbol,,,"dsqx71,yajiedesign",2017-06-10 14:35:29,2017-09-30 18:26:48
IS,Is there a method for automatic adjustment of learning rate,with callback function we can make learning rate drop by some rate after fixed epochs or batches in training Is there a method for automatic adjustment of learning rate for example according some evaluation metric in training For example if the accuracy of validation set does not increase for 3 epoch then learning rate drop by 1 10,,yajiedesign,2017-06-11 00:50:26,2017-09-30 18:26:51
IS,How dose mxnet symbol Custom add arguments,How dose mxnet symbol Custom add arguments that can be manipulated and saved loaded to param file in the same way as weight bias for mxnet symbol Convolution and gamma for mxnet symbol LeakyReLu,,"piiswrong,yajiedesign",2017-06-11 03:38:47,2017-09-30 18:26:55
IS,cudnn rnn inl h 233 cudnnRNNBackwardData,when i make cause error cudnn rnn inl h 233 cudnnRNNBackwardData cudnn rnn inl h 260 cudnnRNNBackwardWeights collect2 error ld returned 1 exit status Makefile 205 recipe for target 'bin im2rec' failed gcc Ubuntu 4 8 4 1ubuntu15 4 8 4 libcudnn so 5 1 10 cuda8 0 i have add in config mk ADD LDFLAGS L home user cudnn8 0 lib64 ADD CFLAGS I home user cudnn8 0 include,,yajiedesign,2017-06-12 06:11:51,2017-09-30 18:26:58
IS,'Per example' net output,Hi MXnet newbie I'm running a 'Lenet' like arch with isoftmax cross entropy' loss in a istep by step' style i e not mod fit instead using 'bind' 'init' 'forward' etc I'm trying to get a 'per example' loss value for the examples in my batch How can this be done with mxnet Thanks in advance Tal code data mx symbol Variable wouldata' lbl mx sym Variable isoftmax label' conv1 mx sym Convolution data data kernel 5 5 num filter 20 relu1 mx sym Activation data conv1 act type relu pool1 mx sym Pooling data relu1 pool type max kernel 2 2 stride 2 2 conv2 mx sym Convolution data pool1 kernel 5 5 num filter 10 relu2 mx sym Activation data conv2 act type relu pool2 mx sym Pooling data relu1 pool type max kernel 2 2 stride 2 2 first fullc layer flatten mx sym Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 500 relu3 mx sym Activation data fc1 act type relu second fullc layer fc2 mx sym FullyConnected data relu3 num hidden 1 softmax loss lenet mx sym SoftmaxOutput data fc2 name isoftmax' lenet mx sym softmax cross entropy fc2 lbl initial state mod mx mod Module symbol lenet bind tell the module the data and label shapes so that memory could be allocated on the devices for computation mod bind data shapes train iter provide data label shapes train iter provide label init parameters mod init params initializer mx init Xavier magnitude 2 init optimizer mod init optimizer optimizer isgd' optimizer params 'learning rate' 0 1 Training use accuracy as the metric Data data train img part data size 60000 part data data part data size part label train lbl part data size num samp part data shape 0 num batch np ceil num samp batch size Metric metric mx metric create 'acc' Tree params newPlayer pPlayer num samp 1 tree lrn rate 10 4 tree samp prob 0 5 bDebug False batch size 2 Eps 10 5 with mx Context mx gpu 1 for epoch in np arange 10 for batch num in np arange num batch ind vec prob vec for sampleInd in np arange batch size ind prob newPlayer sample tree samp prob ind vec append ind prob vec append prob this batch img train img ind vec this batch lbl train lbl ind vec train iter mx io NDArrayIter to4d this batch img mx nd array this batch lbl batch size batch size train iter mx io NDArrayIter to4d train img ind vec mx nd array train lbl ind vec batch size batch size for batch in train iter mod forward batch is train True compute predictions mod update metric metric batch label accumulate prediction accuracy for sampleInd in np arange batch size softmax vec mod get outputs 0 asnumpy sampleInd curr loss np log softmax vec np int batch label 0 asnumpy sampleInd newPlayer update ind np exp tree lrn rate curr loss prob Eps mod backward compute gradients mod update update parameters using SGD,,yajiedesign,2017-05-17 08:45:45,2017-09-30 18:27:02
IS,Distributed training worker server or scheduler lose control cmd,I use dist sync training on 2 machines launch n 2 H host my cmd I found control message lost when training and the worker blocked on cond wait I tried ps lite tests and run local n n test connection or test simple app or test kv app and they all run well but when build libmxnet so use USE DIST KVSTORE 1 and build my program use libmxnet so my program may lost control signal my program is like this if dist kv new kvstr wouldist sync' if worker kv RunServer 1 in training stage if batch num 50 0 print output of net here i use NDArray WaitAll 2 the signal lost may occured in 1 or 2 ablove if at 1 just like scheduler could not receive all AddNode cmd from workers and servers when gdb all process stop at sleeping and recv byte is 0 or 100 or 150 never full just because it not receive workers and servers AddNode cmds when worker stopped at 2 it always stop at cond wait function in gdb It seems that it have not receive Barrier cmd from scheduler I tried PS RESEND 1 and PS RESEND TIMEOUT 100000 and the scheduler process coredump because it send Barrier cmd with request false to worker more than 10 times I tried modify Makefile not use O3 just use O2 or O1 but the question still exist Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,yajiedesign,2017-03-03 06:27:30,2017-09-30 18:27:05
IS,Compile for Android arm64 failed amalgamation,Environment info Operating System ubuntu 16 04 Compiler aarch64 linux android g Package used Python R Scala Julia Python MXNet version 0 10 0 MXNet commit hash git rev parse HEAD ba2d9f61aa7b744c4ec3243097fb7170ad06012c Python version Python3 5 Error Message home android toolchain bin aarch64 linux android g std c 11 Wno unknown pragmas Wall DMSHADOW USE CBLAS 0 DMSHADOW USE SSE 0 DDISABLE OPENMP 1 DMSHADOW USE CUDA 0 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DDMLC LOG STACK TRACE 0 DMSHADOW FORCE STREAM DMXNET USE OPENCV 0 DMXNET PREDICT ONLY 1 isystem home android toolchain include c 4 9 x I opt OpenBLAS I opt OpenBLAS include D NDK MATH NO SOFTFP 1 O3 fPIC o jni libmxnet predict o c jni predictor cc I pwd I pwd mshadow I pwd include I pwd nnvm include I pwd nnvm I pwd dmlc core include I pwd dmlc core src I pwd dlpack include I home android toolchain include c 4 9 x In file included from jni predictor cc 4 0 jni mxnet predict all cc 8542 0 warning MSHADOW FORCE STREAM redefined define MSHADOW FORCE STREAM command line 0 0 note this is the location of the previous definition In file included from jni predictor cc 4 0 jni mxnet predict all cc 14207 0 warning API END redefined define API END catch dmlc Error except return NNAPIHandleException except return 0 NOLINT In file included from home cx mxnet arm amalgamation src c api c api cc 26 0 from jni mxnet predict all cc 61 from jni predictor cc 4 home cx mxnet arm amalgamation src c api c api common h 22 0 note this is the location of the previous definition define API END catch dmlc Error except return MXAPIHandleException except return 0 NOLINT In file included from jni predictor cc 4 0 jni mxnet predict all cc 14213 0 warning API END HANDLE ERROR redefined define API END HANDLE ERROR Finalize catch dmlc Error except Finalize return NNAPIHandleException except return 0 NOLINT In file included from home cx mxnet arm amalgamation src c api c api cc 26 0 from jni mxnet predict all cc 61 from jni predictor cc 4 home cx mxnet arm amalgamation src c api c api common h 28 0 note this is the location of the previous definition define API END HANDLE ERROR Finalize catch dmlc Error except Finalize return MXAPIHandleException except return 0 NOLINT In file included from jni mxnet predict all cc 64 0 from jni predictor cc 4 home cx mxnet arm amalgamation src c api c api ndarray cc In function 'int MXAutogradBackward mx uint void void int ' home cx mxnet arm amalgamation src c api c api ndarray cc 493 26 warning unused variable aret' Wunused variable MXAPIThreadLocalEntry ret MXAPIThreadLocalStore Get In file included from jni mxnet predict all cc 74 0 from jni predictor cc 4 home cx mxnet arm amalgamation src initialize cc In function 'void mxnet segfault logger int ' home cx mxnet arm amalgamation src initialize cc 16 9 warning unused variable istack' Wunused variable void stack MAX STACK SIZE In file included from home cx mxnet arm amalgamation nnvm include nnvm base h 9 0 from home cx mxnet arm amalgamation nnvm include nnvm op h 9 from home cx mxnet arm amalgamation include mxnet base h 9 from jni mxnet predict all cc 27 from jni predictor cc 4 jni mxnet predict all cc At global scope home cx mxnet arm amalgamation dmlc core include dmlc registry h 232 51 error specialization of istatic dmlc Registry EntryType dmlc Registry EntryType Get with EntryType nnvm Op ' after instantiation Registry EntryType Registry EntryType Get jni mxnet predict all cc 11255 1 note in expansion of macro 'DMLC REGISTRY ENABLE' DMLC REGISTRY ENABLE nnvm Op In file included from jni predictor cc 4 0 jni mxnet predict all cc In member function 'nnvm Symbol nnvm Symbol GetChildren const' jni mxnet predict all cc 11688 16 warning unused variable 'fnum vis output' Wunused variable static auto fnum vis output Op GetAttr FNumVisibleOutputs FNumVisibleOutputs In file included from home cx mxnet arm amalgamation nnvm include nnvm base h 9 0 from home cx mxnet arm amalgamation nnvm include nnvm op h 9 from home cx mxnet arm amalgamation include mxnet base h 9 from jni mxnet predict all cc 27 from jni predictor cc 4 jni mxnet predict all cc At global scope home cx mxnet arm amalgamation dmlc core include dmlc registry h 232 51 error specialization of istatic dmlc Registry EntryType dmlc Registry EntryType Get with EntryType nnvm PassFunctionReg ' after instantiation Registry EntryType Registry EntryType Get jni mxnet predict all cc 11950 1 note in expansion of macro 'DMLC REGISTRY ENABLE' DMLC REGISTRY ENABLE nnvm PassFunctionReg I have no idea about the error that says specialization of xxx after instantiation Help,,yajiedesign,2017-06-13 02:38:01,2017-09-30 18:27:11
IS,How to concat mxnet cpp NDArray,What is the best way to concat mxnet cpp NDArray I have a std vector mxnet cpp NDArray and context is gpu How can I merge them and get new NDArray,,"lx75249,yajiedesign",2017-06-10 07:46:15,2017-09-30 18:27:14
IS,MXNet has some strange problems about speed on both GPU and CPU,First I give the configurations about my MXNet version 0 8 0 9 3 CUDA 1 cuDNN 1 USE OPENCV 1 USE OPENMP 1 blas openblas GPU GTX 980M CPU I7 6700HQ Problem 1 I run a deep and plain CNN model with different input size on GPU and I found that larger input size is much faster than smaller input size 500x500x3 130ms 1000x1000x3 50ms But when the size comes to 256x256x3 the speed is normal 5ms Problem 2 Follow the Problem 1 I switch the mode to CPU The input size is set to 256x256x3 on GPU the time consumption is 5ms however on CPU it increases to 455ms which is much slower than that on GPU 50ms is expected So far I have tried everything that I can but the problems remain I post my symbol and evaluation code Symbol Everyone can try this code Any ideas,,"piiswrong,chinakook,yajiedesign",2017-05-14 13:15:56,2017-09-30 18:27:18
IS,Segmentation Fault mx sym Convolution ops in CPU but working in GPU,I met a wired issue about mx sym Convolution operation with the error message of segmentation fault in ubuntu machine and Segmentation fault 11 in MAC machine under mx cpu context However the same code is working under mx gpu 0 Environment info Operating System Ubuntu16 06 and MAC OS Package used Python R Scala Julia Python MXNet version '0 10 0' with default pip install mxnet If you are using python package please provide Python version and distribution Python 2 7 10 Error Message gdb ex r args python conv debug py What have you tried to solve it One possible problem is the code in CPU tried to access invalid memory when H is small than filter is H However this issue should be covered by setting padding as 1 pad input is H to 3 same as filter is H This issue is taken care by GPU implementation but not the CPU implementation 1 It is working by declaring the context to mx gpu 0 2 OR It is working by setting x with higher height kernel size 3 in this case,,yajiedesign,2017-06-13 16:49:21,2017-09-30 18:27:22
IS,Get error TBlob get with shape new and old shape do not match total elements Can anyone help me with this,I'm implementing a simple segmentation using mxnet I tried this code I get error TBlob get with shape new and old shape do not match total elements I have checked the infer shape which is the same with input dimension I wonder what the problem is Thanks for any advice,,"tornadomeet,tornadomeet,tornadomeet,tornadomeet,tornadomeet,yajiedesign",2017-06-11 08:03:10,2017-09-30 18:27:26
IS,I get this error Check failed p 0 RecordIOWriter kMagic terminate called after throwing an instance of wouldmlc Error',I am tring transform caffe modul to mxnet and transform the network construct when I run the program get this error 11 14 29 include dmlc logging h 235 11 14 29 src recordio cc 117 Check failed p 0 RecordIOWriter kMagic terminate called after throwing an instance of wouldmlc Error' what 11 14 29 src recordio cc 117 Check failed p 0 RecordIOWriter kMagic Aborted core dumped,,yajiedesign,2016-12-12 03:19:42,2017-09-30 18:27:34
IS,Is there InfogainLoss output symbol in mxnet,I'm trying a regression work using mxnet since the label is discrete maybe InfogainLoss is better than LinearRegressionOutput So is there a solution Or I can only make loss,,"chunyang-wen,yajiedesign",2017-06-14 14:54:08,2017-09-30 18:27:38
IS,loading ndarray file in cpp fails,Trying to load nd file into mxnet through cpp API can not access the components e g const mx float M NDArray LoadToMap XXXX mean nd mean image GetData float a M 0 or size t i1 NDArray LoadToMap XXXX mean nd mean img GetShape 0 size t i2 NDArray LoadToMap XXXX mean nd mean img GetShape 1 Both will return segmentation fault error Loading the same nd file using python API works fine,,"chunyang-wen,chunyang-wen,chunyang-wen,chunyang-wen,chunyang-wen,chunyang-wen,yajiedesign",2017-06-12 21:20:32,2017-09-30 18:27:41
IS,Restrict number of core cpu,The command ctx mx cpu is taking all available CPU How to restrict to use a certain number only say 6 out of 8 core,,yajiedesign,2017-06-15 05:26:07,2017-09-30 18:27:45
IS,Would it be a good idea to output initialisation details,I found that the initialisation has no rich information so it is hard to make sure if the initialisation is correct Would it be better to output the everything done in initialisation step like what weights are initialised with what values Like caffe it has different level of log so we can see the info we need,,"zhreshold,piiswrong,yajiedesign",2017-04-11 05:13:18,2017-09-30 18:27:48
IS,Selecting argmax indices of a matrix column Is it possible,Hello I want to achieve the following in the symbolic graph Assume that I have a Nxk sized matrix in the symbolic network which is produced as the result of a FullyConnected layer What I want to achieve is for each k columns indices of the rows which has the maximum activations in the current column For example let is say we have x 2 1 3 4 5 6 Assume that we a hypothetic function argmaxColumn which produces 0 as the result of argmaxColumn x 0 and 1 2 as the result of argmaxColumn x 1 Then I aim to take the corresponding samples separately from x by using mx symbol take function Is there a way to achieve this using Symbol API,,"piiswrong,yajiedesign",2017-06-15 14:06:26,2017-09-30 18:27:51
IS,Op In layer gradient norm problem running on multiple devices,Just realized that the normalization mode 'batch' and 'valid' are problematic in SoftmaxOutput and MakeLoss layers if we dispatch data to multiple devices Suppose we have batch size 64 using 'batch' norm the gradients we get will be g1 32 g2 32 on 2 gpus and g1 g2 64 on single gpu where g1 and g2 are first half second half of the gradients respectively Things become more complicated in 'valid' norm since we might get uneven distribution for valid samples thus the gradients will become very different Summed up we got different results using gpus 0 gpus 0 1 with exactly same hyperparameter set as long as we use 'batch' or 'valid' norm in these layers which is not a desired behavior in my mind Any idea,,"zhreshold,yajiedesign",2017-06-15 19:22:50,2017-09-30 18:27:54
IS,How to use MKL or NNPACK in cpp package example,I'v tried to compile mxnet with NNPACK or MKL and succeed But I can not see any performance Improvement Maybe I miss some details Who can help me Thx a lot,,yajiedesign,2017-06-16 02:46:09,2017-09-30 18:27:58
IS,distributed ssh launcher fails if space in script path,Environment info MXNet 0 10 1 installed from commit 6eb8bd16c9ef25bf039b60a32417e6fb2adb48e1 Python 3 6 1 Error Message Steps to reproduce run distributed training from directory containing spaces in its name I had to modify launch scripts to use Python3 but do not think it makes difference What have you tried to solve it check if it is working with no spaces in path it does,,yajiedesign,2017-06-16 04:11:32,2017-09-30 18:28:01
IS,Anyone has implemented distributed deep reinforcement learning,I'm working on distributed deep reinforcement learning Any advice is helpful,,"piiswrong,chunyang-wen,yajiedesign",2016-12-21 02:30:06,2017-09-30 18:28:04
IS,Hacking codes fail when moving from CPU to GPU,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 Package used Python R Scala Julia Python 2 7 6 MXNet version 0 9 5 Error Message I have some intermediate hacking output from some files under src operators and build local mxnet on it You can think it is simply just some cout lines When I run my testing code for CPU the intermediate information will show up But when move to GPU it never reaches the output lines I'm doing exactly same thing only provide gpus 0 when running What have you tried to solve it I followed the instruction here modify the config mk still not working Any thoughts would be helpful,,yajiedesign,2017-06-16 13:45:06,2017-09-30 18:28:08
IS,Converted mxnet model from caffe can not perform as good as in cafffe,I have converted an image segmentation caffemodel to mxnet by convert model py Although I could use the converted model to predict a 1024x2048 image the performance is not as good as caffe that can be easily seen in the following Original image frankfurt 000000 016286 leftimg8bit Ground truth frankfurt 000000 016286 gtfine color Caffe frankfurt 000000 016286 leftimg8bit col Mxnet seg aspp As you can see the details in mxnet is not as good as in caffe In mxnet the sky can not not be classified correctly The network architecture is a image segmentation model modified deeplab v2 structure that uses ResNet 101 with bilinear upsampling layer in the end before the softmax loss Any hint to fix the problems Environment info Operating System Ubuntu Python 2 7 12 MXNet 0 10 Cuda 8 0,,"chinakook,chinakook,chinakook,yajiedesign",2017-06-13 18:26:33,2017-09-30 18:28:12
IS,data communication between nodes,Hello I am interesting in analyzing the traffic done during multi node training of imagenet and I am looking for the block of code that handles the shuffling of data between nodes Can anyone help me to identify that,,"piiswrong,yajiedesign",2017-06-16 00:08:54,2017-09-30 18:28:17
IS,Mxnet executor does not release memory properly in simple case,I'm trying to use mxnet in forward mode only without any gradient computation It is consuming way too much memory I tried the imperative style It runs out of memory How can I tell the graph executor to release the memory as soon as it is not needed anymore Thank you Environment info Operating System Ubuntu 16 04 Version module 'mxnet' has no attribute ' version ' But I installed it 15 days ago,,"piiswrong,yajiedesign",2017-06-18 13:08:56,2017-09-30 18:28:21
IS,Unable to release the GPU memory of the executor,Hi I need to build and train multiple model with different symbol in my program When a new model is created I need to delete the old one However the GPU memory of the old one seems never release Is there some way to manual delete the executor or release the GPU memory Environment info Operating System Ubuntu 16 04 Compiler Package used Python R Scala Julia MXNet version 0 10 0 Python version and distribution Anaconda python 3 5 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 build first executor with mod mx mod Module symbol sym1 context devs mod bind data shapes train provide data label shapes train provide label mod set params arg params aux params allow missing True 2 the GPU memory wo not release when the function where build the mod ends or manual delete it by wouldel mod' What have you tried to solve it 1 command wouldel mod' wo not help 2 'mx model FeedForward' will also suffer this problem 3 put the building command in a function but the GPU memory taken still exists when the function terminates,,yajiedesign,2017-06-19 07:15:37,2017-09-30 18:28:24
IS,DataParallelExecutorGroup layout handling for symbols,When merge multi context True in a call to get outputs of a DataParallelExecutorGroup merge multi context will try to concatenate the outputs from the different devices along the major axis The major axis is computed based on DataDesc get batch axis self symbol name attr ' layout ' for name in self output names in the initializer of DataParallelExecutorGroup What is the recommended way to set the attr ' layout ' of a symbol Simply pass attr ' layout ' layout when constructing the symbol Can the attribute be set automatically during module binding Setting attr ' layout ' is necessary as it is otherwise None leading to merge multi context trying to concatenate along dim 0 which will fail if the batch size is not divisible by the number of devices and the symbol outputs a shape 1 batch size per device X I e in case of 3 devices and batch size 128 concatenating 1 43 1 43 1 42 along dim 0 will fail,,"leezu,formath,leezu,formath,leezu,formath,yajiedesign",2017-06-19 04:12:08,2017-09-30 18:28:27
IS,installation on build from source and making symbolic link to anaconda2,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 LTS Compiler nvcc cuda 8 0 cudnn 5 1 Package used Python R Scala Julia python MXNet version latest version current in 19 june 2017 Or if installed from source MXNet commit hash git rev parse HEAD sry i do not know what it means If you are using python package please provide Python version and distribution anaconda2 python 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace import mxnet Traceback most recent call last File stdin line 1 in module File home han anaconda2 lib python2 7 site packages mxnet init py line 7 in module from base import MXNetError File home han anaconda2 lib python2 7 site packages mxnet base py line 52 in module LIB load lib File home han anaconda2 lib python2 7 site packages mxnet base py line 44 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File home han anaconda2 lib python2 7 ctypes init py line 362 in init self handle dlopen self name mode OSError home han anaconda2 bin lib libgomp so 1 version 'GOMP 4 0' not found required by home han anaconda2 lib python2 7 site packages mxnet lib libmxnet so I have to compile with my own operator files for Deep feature flow so I tried to install mxnet with Build from Source option I built mxnet sources sucessfully While build some Errors were occured but fixed with googling finally i made an symbolic link this folder home myname mxnet python mxnet to home myname anaconda2 lib python2 7 site packages mxnet and when i try to import mxnet on python2 7 with anaconda2 it prints out Traceback most recent call last File stdin line 1 in module File home han anaconda2 lib python2 7 site packages mxnet init py line 7 in module from base import MXNetError File home han anaconda2 lib python2 7 site packages mxnet base py line 52 in module LIB load lib File home han anaconda2 lib python2 7 site packages mxnet base py line 44 in load lib lib ctypes CDLL lib path 0 ctypes RTLD GLOBAL File home han anaconda2 lib python2 7 ctypes init py line 362 in init self handle dlopen self name mode OSError home han anaconda2 bin lib libgomp so 1 version 'GOMP 4 0' not found required by home han anaconda2 lib python2 7 site packages mxnet lib libmxnet so now what should i do plz help me guyz,,yajiedesign,2017-06-19 13:01:21,2017-09-30 18:28:30
IS,Train error,I want to fine tune a image classification model and the pre trained model is a object detection model I just want to use the convNets part of the object detection model to initialize the parameters of my classification model For example the object detection model is faster R CNN and the classification model is VGG16 I use the fine tune py to fine tuning my classification model but there is an error the command is python mxnet example image classification fine tune py pretrained model mx detection faster rcnn load epoch 0004 gpus 0 data train mx detection data train lst rec data val mx detection data test lst rec batch size 16 num classes 2 num examples 500000 layer before fullc conv4 Error Message Traceback most recent call last File mxnet example image classification fine tune py line 49 in module sym arg params aux params mx model load checkpoint prefix epoch File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet model py line 373 in load checkpoint symbol sym load ' s symbol json' prefix File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 1771 in load check call LIB MXSymbolCreateFromFile c str fname ctypes byref handle File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet base py line 85 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 16 23 35 src operator custom custom inl h 128 Check failed registry find param op type registry end Cannot find custom operator type proposal,,yajiedesign,2017-06-20 08:42:06,2017-09-30 18:28:34
IS,why do the following models yield different results training accuracy,I would like to fit a model save load it and continue training For a given architecture and data set my first model without save load yields a training accuracy of 1 Train accuracy 0 636530612244898 2 Train accuracy 0 8794,,yajiedesign,2017-06-20 15:46:50,2017-09-30 18:28:37
IS,memcost Makefile,I noticed that the file Makefile has been modified the prefix mxnet replaced by nnvm Then I type the command to see the allocation cost However the result of command make with inplace make with sharing and make with both is same is this a normal phenomenon,,"piiswrong,tqchen,piiswrong,yajiedesign",2017-01-11 03:57:29,2017-09-30 18:28:43
IS,is batch pad ignored in evaluation,i am working with a small validation set which is not far from my batch size trying to understand the validation metric i'm getting i'm working with mx io NDArrayIter with default last batch handle which is 'pad' i can not find in the code a place which calculates the update for the eval data metrics which refer to this pad is there a chance that it is ignored and the last batch on the validation data also contains the rolled over samples from the beginning of the validation set this may bias results for those working with small datasets a fix could change the size of the last batch going into the update so it does not include the pad edit looks like iter predict does refer to this in base module py for nbatch eval batch in enumerate eval data if num batch is not None and nbatch num batch break self forward eval batch is train False pad eval batch pad outputs out 0 out shape 0 pad for out in self get outputs but in score for nbatch eval batch in enumerate eval data if num batch is not None and nbatch num batch break self forward eval batch is train False self update metric eval metric eval batch label PS this could be solved if i could change the validation batch size to be the entrire validation set but this does not seem to work is there still a problem with different training and validation batch sizes as mentioned here url i did not get an error but the eval calucaltion seemed to work with the training batch size thanks version mxnet 0 10 1,,yajiedesign,2017-06-21 11:39:35,2017-09-30 18:28:47
IS,MKL Breaking Existing Functionality,Building against MKL DNN breaks certain existing functionality For example the Activation symbol with relu crashes during a forward pass whenever the rank of the input is not 4 Are there any other areas where linking against MKL DNN breaks existing functionality And is this expected behaviour or is this a bug,,"sbodenstein,glingyan,sergeykolychev,sbodenstein,sbodenstein,yajiedesign",2017-06-08 14:45:47,2017-09-30 18:28:50
IS,DISCUSSION Sparse Tensor Support Design,Frontend Data Structures NDArrayBase NDArray SparseNDArray Backend Data Structures Add to original NDArray to avoid v table and API change A sparse tensor is represented by two dense tensors index and data There are two possible sparse formats 1 Row sparse tensors where each row of data data i correspond to X index i shape N index shape M data shape M 2 COO sparse tensor where X tuple index i data i shape d 0 d K index shape N K data shape N API 1 Sparse op register FComputeNDArray std function void Context ctx vector NDArray inputs 2 MXImperativeInvoke If at least one of input is sparse try to use FComputeSparse If not registered call to dense on All inputs 3 Executor Memory is never shared for sparse buffers Operators can allocate reallocate memory for its output buffer,,"piiswrong,sxjscience,howard0su,piiswrong,formath,jli05,mli,formath,mli,mli,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,jli05,formath,eric-haibin-lin,eric-haibin-lin,yajiedesign",2017-01-20 03:07:50,2017-09-30 18:28:53
IS,MXNET can not exit normally,CUDA version 375 39 cudnn 5 1 10 MXNET version newest by now installed through source code gpu nvidia 1080 code 22 net mx symbol Variable name wouldata' 23 net mx symbol FullyConnected data net name 'fc1' num hidden 10 24 net mx symbol SoftmaxOutput data net name isoftmax' 25 26 model mx mod Module net context mx gpu 0 27 28 29 print model label names 30 training X np random rand num batches batch size 1000 31 training Y np random randint 0 10 size num batches batch size 32 33 34 print wouldata generated' 35 train iter mx io NDArrayIter training X 36 label training Y batch size batch size 37 38 print 'iterator allocated ' 39 40 exit 41 42 print 'before' 43 testX testY next train iter 44 print 'after' 45 print testX testY 46 47 last time time time 48 49 def batch callback batch obj 50 if batch obj nbatch 100 0 and batch obj nbatch 0 51 global last time 52 print istep d f images s' batch obj nbatch 100 batch size time time last time 53 last time time time 54 55 model fit 56 train data train iter 57 optimizer isgd' 58 optimizer params 'learning rate' learning rate 59 eval metric 60 num epoch 10 num epochs 61 batch end callback batch callback Sometimes the code exits normally sometimes it got crashed after training 02 07 24 home xxx mxnet dmlc core include dmlc logging h 300 02 07 24 home xxx mxnet mshadow mshadow stream gpu inl h 125 Check failed err CUDNN STATUS SUCCESS 4 vs 0 CUDNN STATUS INTERNAL ERROR Stack trace returned 6 entries bt 0 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fb8cc13e369 bt 1 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN7mshadow9NewStreamINS 3gpuEEEPNS 6StreamIT EEbb 0x135 0x7fb8ccc41fe5 bt 2 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x87 0x7fb8ccc50ad7 bt 3 home xxx anaconda2 bin lib libstdc so 6 0xb4870 0x7fb8be7ad870 bt 4 lib64 libpthread so 0 0x7dc5 0x7fb8e48eedc5 bt 5 lib64 libc so 6 clone 0x6d 0x7fb8e3f1473d 02 07 24 home xxx mxnet dmlc core include dmlc logging h 300 02 07 24 home xxx mxnet mshadow mshadow stream gpu inl h 125 Check failed err CUDNN STATUS SUCCESS 4 vs 0 CUDNN STATUS INTERNAL ERROR Stack trace returned 6 entries bt 0 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fb8cc13e369 bt 1 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN7mshadow9NewStreamINS 3gpuEEEPNS 6StreamIT EEbb 0x135 0x7fb8ccc41fe5 bt 2 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x87 0x7fb8ccc50ad7 bt 3 home xxx anaconda2 bin lib libstdc so 6 0xb4870 0x7fb8be7ad870 bt 4 lib64 libpthread so 0 0x7dc5 0x7fb8e48eedc5 bt 5 lib64 libc so 6 clone 0x6d 0x7fb8e3f1473d terminate called after throwing an instance of wouldmlc Error' what 02 07 24 home xxx mxnet mshadow mshadow stream gpu inl h 125 Check failed err CUDNN STATUS SUCCESS 4 vs 0 CUDNN STATUS INTERNAL ERROR Stack trace returned 6 entries bt 0 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x29 0x7fb8cc13e369 bt 1 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZN7mshadow9NewStreamINS 3gpuEEEPNS 6StreamIT EEbb 0x135 0x7fb8ccc41fe5 bt 2 home xxx anaconda2 lib python2 7 site packages mxnet 0 9 4 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFvvEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS2 8OprBlockEbENKUlvE1 clEvEUlvE E9 M invokeERKSt9 Any data 0x87 0x7fb8ccc50ad7 bt 3 home xxx anaconda2 bin lib libstdc so 6 0xb4870 0x7fb8be7ad870 bt 4 lib64 libpthread so 0 0x7dc5 0x7fb8e48eedc5 bt 5 lib64 libc so 6 clone 0x6d 0x7fb8e3f1473d terminate called recursively sometimes it just can not exit after training the memory can not be released,,"cjolivier01,cjolivier01,yajiedesign",2017-03-21 10:34:47,2017-09-30 18:28:57
IS,module forward segfaults on Anaconda docker image,I'm trying to run the Predict with pre trained models tutorial using mxnet mkl in docker I'm using the continuumio anaconda docker container On calling mod forward batch the process segfaults Environment info Running in docker container with continuumio anaconda image Dockerfile What have you tried to solve it I thought there might be a dependency conflict so I tried using Opencv 2 4 x instead of the 3 x which is installed by default This did not resolve the issue,,"piiswrong,piiswrong,glingyan,piiswrong,piiswrong,yajiedesign",2017-06-20 22:03:06,2017-09-30 18:29:00
IS,Mxnet test and validation accuracy during training,hello I am trying to have the training the test and validation accuracy at the end of each epoch To do that I indicate in the fit function the validation accuracy with the eval data parameters But I have a problem with the test accuracy I wanted to add this option in the eval end callbacks thank to the function mx callback Speedometer I do not have an error but I do not see and do not know how to get the result eval end callbacks mx contrib tensorboard LogMetricsCallback evaluation log mx callback Speedometer mod score test dataiter metric How can I solve this problem,,yajiedesign,2017-06-12 10:02:39,2017-09-30 18:29:04
IS,How to test a c operator extension,Hi I'm not quite familiar with mxnet c API and after implement a c operator extension and successfully compiled how to test the operator For example how to write a cpp file and use gdb to test the function,,"piiswrong,ysh329,ysh329,yajiedesign",2017-05-17 05:09:16,2017-09-30 18:29:08
IS,Multiple softmax training error,Hello I am actually trying to train an inception model with multiple output but I have some problems for the training steps It seems that I have a dimension batch error but I did not find the way to solve it a part of the code I use How can I solve this error,,"formath,formath,formath,formath,yajiedesign",2017-06-20 09:40:40,2017-09-30 18:29:12
IS,how to debug network layer by layer,first of all thank very much for afford such powerful tool I'm try to combine NCE loss and bucketing LSTM in Python for few days For now the network is build successful and run without complain the loss decreasing as batch feeding very slow and the PPL test on result model is very huge so I wanna to debug the model step by step print weights and each layer is forward backward tensor to find where is the problem what is I have done yet 1 install a monitor find and fix a tiny bug while using monitor with bucketing module 2 feed sample one per batch again and again 3 save weights forward backward tensor into file after that I do have all weights and each layers is forward backward tensor saved in files but while analysis those file I found that some tensor are work as defined in net symbol but some are not what I have found are 1 some layer are merge such as a element wise multiply layer follow by a softmax layer those two layers' forward output tensor are exactly same 1 in a mx sym Reshape layer the input tensor and output tensor look like have no relation with each other which is confuse me 1 the sample problem appear in backward flow so is the underlay graph executor have optimize the symbol graph and change the behavior of some layer 1 if so is possible to disable this behavior 2 if impossible are there any document about this 3 or which source code should I read 4 or how do you debug model thanks for and advices blows are my environment 1 OS centos 7 0 X86 64 1 Python Python 2 7 5 1 Mxnet da08c9203ecd1d8e3cd6a29ecf1a9238521f2351 Author ziheng ziheng apache org Date Sun Jun 4 17 43 47 2017 0700 USE PROFILE 1 not open profile while running USE CUDA 1 USE CUDNN 1 USE BLAS mkl USE MKLML2017 1 USE MKL2017 EXPERIMENTAL 0 USE OPENMP 1 USE NNPACK 0 1 gcc gcc GCC 4 8 5 20150623 Red Hat 4 8 5 4,,"winstywang,Godricly,Godricly,Godricly,Godricly,yajiedesign",2017-06-14 10:58:09,2017-09-30 18:29:16
IS,About faster rcnn C version,I want to use c code detection the object I think is faster python code my question is Is there a c version of faster rcnn if not how can i do thanks for u help,,"dtmoodie,szha",2017-03-15 04:15:35,2017-09-30 19:02:48
IS,SSD Example Multi GPU training does not scale well with more GPUs,Training a SSD model with multiple GPUs does not scale well if more than two GPUs are used Here are the estimated samples per second with increasing number of GPUs and batch size No of GPUs Batch Size Speed 1 32 11 4 samples sec 2 64 18 6 samples sec 4 128 20 9 samples sec 8 256 22 3 samples sec This issue can be reproduced by training a SSD model as described here 1 1,,"jspisak,zhreshold,piiswrong,zhreshold,zhreshold,piiswrong,zhreshold,piiswrong,piiswrong,piiswrong,zhreshold,zhreshold,piiswrong,zhreshold,piiswrong,mli,zhreshold,howard0su,zhreshold,howard0su,zhreshold,howard0su,howard0su,dtmoodie,szha",2016-12-06 16:07:10,2017-09-30 19:02:49
IS,Fix cmake build,There are several runtime issues that exist in a cmake build but not a make build All the above are resolved using the makefile build on ubuntu 16 04,,"dtmoodie,piiswrong,chinakook,szha",2017-06-23 15:52:30,2017-09-30 19:02:50
IS,Is it possible to accumulate gradients over multiple batches,Due to the limit of GPU memory I can not use a large mini batch size when training a deep network like ResNet or Inception v3 But I noticed that there is a solution in caffe that use the parameter 'iter size' to accumulate gradients over multiple batches i e accumulate gradients 10 times and then update the weights So is there a similar parameter setting in mxnet If not will it be easy to implement it myself,,"nicklhy,tornadomeet,nicklhy,tornadomeet,nicklhy,tornadomeet,nicklhy,tornadomeet,tqchen,tqchen,szha",2016-09-12 03:11:26,2017-09-30 19:02:51
IS,GPU performance of Cmake built mxnet is worse than Make built one,I have tested the three cases many times as below 1 When running train mnist py GPU on the Cmake built mxnet on Ubuntu the average epoch time cost is 0 7s But the Make built mxnet on Ubuntu takes only 0 42s 2 When running train mnist py GPU on the Cmake built mxnet on Windows the average epoch time cost is 0 822s even longer than running the script on MKL The CPU MKL mode is about 0 6s 3 The Windows MKL built mxnet version is better than the Ubuntu MKL built one Each epoch of the former takes about 0 62s and each epoch of the later takes above 0 8s It is very weird I hope the performance differences can be eliminated I suggest that mxnet should bring a standard benchmark tool and reference performance index to measure running time such as the 'make runtest' of Caffe,,"chinakook,dtmoodie,chinakook,szha",2017-06-13 16:40:04,2017-09-30 19:02:52
IS,The process has forked on macbook when running with python dataiter code,I am using my own data iter to train resnet and encountered error The process has forked and you cannot use this CoreFoundation functionality safely You MUST exec Break on THE PROCESS HAS FORKED AND YOU CANNOT USE THIS COREFOUNDATION FUNCTIONALITY YOU MUST EXEC to debug My data iter code is like this class DataIter mx io DataIter def init self images image pairs image similar neg batch size height width process num assert process num 40 super DataIter self init self batch size batch size self conut len images self height height self width width self images images self image pairs image pairs self image similar neg image similar neg self cursor self batch size self provide data positive self batch size 3 height width negative self batch size 3 height width one self batch size self provide label anchor self batch size 3 height width self queue multiprocessing Queue maxsize 4 self started True self processes multiprocessing Process target self write for i in range process num for process in self processes process daemon True process start,,szha,2017-06-25 07:12:37,2017-09-30 19:02:53
IS,why epoch Train perplexity is nan in the mxnet lstm training but batch Train perplexity is normal and keep decline,in mxnet 0 9 3 476774188 chatroom 1486442908967 50 use perplexity function But I do not see the Resetting Data Iterator process Why,,szha,2017-02-07 04:55:21,2017-09-30 19:02:54
IS,Discussion and troubleshooting on PyPI pip installation with Windows the newest 0 9 5 release,The PyPI mxnet installation is now available with windows mxnet cu80 win on Windows supports CUDA 8 0 and cuDNN 6 0 the cpu only is to be later Install 1 install NVIDIA Display Driver Newest 2 install CUDA 3 install Anaconda3 Highly Recommended or standard Python 4 pip install mxnet cu80 win Is currently in test welcome feedback,,"yajiedesign,yajiedesign,yajiedesign,szha",2017-05-09 09:12:31,2017-09-30 19:02:55
IS,mxnet is much slower than caffe on android,I predict a face image using Lightened CNN model on my android phone Mxnet on i5 machine is about 80ms Caffe on i5 machine is about 75ms Mxnet on android is about 700ms OpenBLAS arm caffe on android is adbout 200ms OpenBLAS arm So what is the root cause,,"tornadomeet,tornadomeet,ysh329,szha",2016-12-08 01:29:28,2017-09-30 19:02:56
IS,Batchsize error on multi cpus,I configured batch size 64 and dev mx cpu i for i in range 4 to use four cpus And got error like bellow I modified the dev mx cpu 0 to use only one cpu the error disappears I know each cpu takes 1 4 slice of the batch What should i do if i want to use 4 cpus on my macbook Here is the error when dev mx cpu i for i in range 4 DeprecationWarning mxnet model FeedForward has been deprecated Please use mxnet mod Module instead optimizer optimizer 14 58 06 Users travis build dmlc mxnet distro mxnet build dmlc core include dmlc logging h 304 14 58 06 src operator tensor matrix op inl h 1041 Check failed end axis size end 0 Invalid begin end get begin 0 end 64 Stack trace returned 10 entries bt 0 0 libmxnet so 0x000000010c42e2b5 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x000000010c42c099 ZN4dmlc15LogMessageFatalD1Ev 9 bt 2 2 libmxnet so 0x000000010c977ed7 ZN5mxnet2op18GetSliceAxisParamsERKNS0 14SliceAxisParamERKN4nnvm6TShapeEPiS8 S8 535 bt 3 3 libmxnet so 0x000000010c963a59 ZN5mxnet2op14SliceAxisShapeERKN4nnvm9NodeAttrsEPNSt3 16vectorINS1 6TShapeENS5 9allocatorIS7 EEEESB 233 bt 4 4 libmxnet so 0x000000010db62343 ZZN4nnvm4pass12 GLOBAL N 19InferAttrINS 6TShapeEZNKS1 3 0clENS 5GraphEEUlRKS3 E DnEES5 OS5 T PKcSC SC SC SC T0 T1 ENKUljbE clEjb 2707 bt 5 5 libmxnet so 0x000000010db60c1a ZNSt3 110 function6 funcIN4nnvm4pass12 GLOBAL N 13 0ENS 9allocatorIS5 EEFNS2 5GraphES8 EEclEOS8 3738 bt 6 6 libmxnet so 0x000000010db4aefb ZN4nnvm11ApplyPassesENS 5GraphERKNSt3 16vectorINS1 12basic stringIcNS1 11char traitsIcEENS1 9allocatorIcEEEENS6 IS8 EEEE 1419 bt 7 7 libmxnet so 0x000000010ca57080 ZN4nnvm9ApplyPassENS 5GraphERKNSt3 112basic stringIcNS1 11char traitsIcEENS1 9allocatorIcEEEE 208 bt 8 8 libmxnet so 0x000000010ca5a781 ZN4nnvm4pass10InferShapeENS 5GraphENSt3 16vectorINS 6TShapeENS2 9allocatorIS4 EEEENS2 12basic stringIcNS2 11char traitsIcEENS5 IcEEEE 929 bt 9 9 libmxnet so 0x000000010ca585a6 MXSymbolInferShape 1974 infer shape error Arguments positive 16 3 224 224 one 16 anchor 16 3 224 224 negative 16 3 224 224 Traceback most recent call last File Users nali PycharmProjects mxnet ir test py line 113 in module test train File Users nali PycharmProjects mxnet ir test py line 109 in test train epoch end callback mx callback do checkpoint models ir blur File Library Python 2 7 site packages mxnet model py line 826 in fit sym gen self sym gen File Library Python 2 7 site packages mxnet model py line 207 in train multi device logger logger File Library Python 2 7 site packages mxnet executor manager py line 326 in init self slices train data File Library Python 2 7 site packages mxnet executor manager py line 238 in init input types data types File Library Python 2 7 site packages mxnet executor manager py line 105 in bind exec arg shape aux shape sym infer shape input shapes File Library Python 2 7 site packages mxnet symbol py line 872 in infer shape res self infer shape impl False args kwargs File Library Python 2 7 site packages mxnet symbol py line 998 in infer shape impl ctypes byref complete File Library Python 2 7 site packages mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator slice axis0 14 58 06 src operator tensor matrix op inl h 1041 Check failed end axis size end 0 Invalid begin end get begin 0 end 64 Stack trace returned 10 entries bt 0 0 libmxnet so 0x000000010c42e2b5 ZN4dmlc15LogMessageFatalD2Ev 37 bt 1 1 libmxnet so 0x000000010c42c099 ZN4dmlc15LogMessageFatalD1Ev 9 bt 2 2 libmxnet so 0x000000010c977ed7 ZN5mxnet2op18GetSliceAxisParamsERKNS0 14SliceAxisParamERKN4nnvm6TShapeEPiS8 S8 535 bt 3 3 libmxnet so 0x000000010c963a59 ZN5mxnet2op14SliceAxisShapeERKN4nnvm9NodeAttrsEPNSt3 16vectorINS1 6TShapeENS5 9allocatorIS7 EEEESB 233 bt 4 4 libmxnet so 0x000000010db62343 ZZN4nnvm4pass12 GLOBAL N 19InferAttrINS 6TShapeEZNKS1 3 0clENS 5GraphEEUlRKS3 E DnEES5 OS5 T PKcSC SC SC SC T0 T1 ENKUljbE clEjb 2707 bt 5 5 libmxnet so 0x000000010db60c1a ZNSt3 110 function6 funcIN4nnvm4pass12 GLOBAL N 13 0ENS 9allocatorIS5 EEFNS2 5GraphES8 EEclEOS8 3738 bt 6 6 libmxnet so 0x000000010db4aefb ZN4nnvm11ApplyPassesENS 5GraphERKNSt3 16vectorINS1 12basic stringIcNS1 11char traitsIcEENS1 9allocatorIcEEEENS6 IS8 EEEE 1419 bt 7 7 libmxnet so 0x000000010ca57080 ZN4nnvm9ApplyPassENS 5GraphERKNSt3 112basic stringIcNS1 11char traitsIcEENS1 9allocatorIcEEEE 208 bt 8 8 libmxnet so 0x000000010ca5a781 ZN4nnvm4pass10InferShapeENS 5GraphENSt3 16vectorINS 6TShapeENS2 9allocatorIS4 EEEENS2 12basic stringIcNS2 11char traitsIcEENS5 IcEEEE 929 bt 9 9 libmxnet so 0x000000010ca585a6 MXSymbolInferShape 1974,,"kevinthesun,szha",2017-06-25 06:58:54,2017-09-30 19:02:57
IS,Can multiple gpu will reduce the accuracy compared to single gpu,Can multiple gpu will reduce the accuracy compared to single gpu,,"kevinthesun,szha",2017-06-24 15:16:03,2017-09-30 19:02:58
IS,Infer shape in custom operator,I try to implement a customOp using python if anyone knows how to implement the infer shape self in shape function if the out shape is independent of in shape but depended on in data For example if you want to implement a customOp like mx symbol arange and the out shape is only related with input data Thanks,,"Ldpe2G,Ldpe2G,szha,Ldpe2G",2017-06-26 10:15:14,2017-09-30 19:02:59
IS,Is it possible that use python to write a custom convolutional layer,I want to write a custom convolutional layer I have checked the example of softmax But softmax does not contain any parameters Anyone know how to do it,,"Ldpe2G,Ldpe2G,szha",2017-06-11 07:40:02,2017-09-30 19:03:00
IS,mxnet warpctc after python python lstm ocr py something wrong,17 59 14 home chang mxnet dmlc core include dmlc logging h 304 17 59 14 src operator slice channel inl h 198 Check failed ishape real axis static cast size t param num outputs 2400 vs 80 If squeeze axis is True the size of the sliced axis must be the same as num outputs Input shape 32 2400 axis 1 num outputs 80 Stack trace returned 10 entries bt 0 home chang mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f2d3b1b2c1c bt 1 home chang mxnet python mxnet lib libmxnet so ZNK5mxnet2op16SliceChannelProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x4c1 0x7f2d3beb2901 bt 2 home chang mxnet python mxnet lib libmxnet so 0x1097808 0x7f2d3bc7f808 bt 3 home chang mxnet python mxnet lib libmxnet so 0x1f9a91d 0x7f2d3cb8291d bt 4 home chang mxnet python mxnet lib libmxnet so 0x1f9c212 0x7f2d3cb84212 bt 5 home chang mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x518 0x7f2d3cb6e978 bt 6 home chang mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7f2d3bb3720e bt 7 home chang mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x240 0x7f2d3bb3a040 bt 8 home chang mxnet python mxnet lib libmxnet so MXSymbolInferShape 0x329 0x7f2d3bb31ed9 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f2d4448fadc infer shape error Arguments label 32 4 l0 init c 32 100 l1 init h 32 100 l0 init h 32 100 data 32 2400 l1 init c 32 100 Traceback most recent call last File lstm ocr py line 210 in module epoch end callback mx callback do checkpoint prefix 1 File python mxnet model py line 782 in fit self init params data provide data data provide label File python mxnet model py line 502 in init params arg shapes aux shapes self symbol infer shape input shapes File python mxnet symbol py line 747 in infer shape res self infer shape impl False args kwargs File python mxnet symbol py line 871 in infer shape impl ctypes byref complete File python mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError Error in operator slicechannel0 17 59 14 src operator slice channel inl h 198 Check failed ishape real axis static cast size t param num outputs 2400 vs 80 If squeeze axis is True the size of the sliced axis must be the same as num outputs Input shape 32 2400 axis 1 num outputs 80 Stack trace returned 10 entries bt 0 home chang mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f2d3b1b2c1c bt 1 home chang mxnet python mxnet lib libmxnet so ZNK5mxnet2op16SliceChannelProp10InferShapeEPSt6vectorIN4nnvm6TShapeESaIS4 EES7 S7 0x4c1 0x7f2d3beb2901 bt 2 home chang mxnet python mxnet lib libmxnet so 0x1097808 0x7f2d3bc7f808 bt 3 home chang mxnet python mxnet lib libmxnet so 0x1f9a91d 0x7f2d3cb8291d bt 4 home chang mxnet python mxnet lib libmxnet so 0x1f9c212 0x7f2d3cb84212 bt 5 home chang mxnet python mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x518 0x7f2d3cb6e978 bt 6 home chang mxnet python mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7f2d3bb3720e bt 7 home chang mxnet python mxnet lib libmxnet so ZN4nnvm4pass10InferShapeENS 5GraphESt6vectorINS 6TShapeESaIS3 EESs 0x240 0x7f2d3bb3a040 bt 8 home chang mxnet python mxnet lib libmxnet so MXSymbolInferShape 0x329 0x7f2d3bb31ed9 bt 9 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f2d4448fadc,,"alues,szha",2017-05-10 10:03:09,2017-09-30 19:03:01
IS,Anaconda mxnet package,I have some problems to install mxnet in ubuntu 16 04 with anaconda Can you provide an easily build packages for conda and upload them to anaconda org It will be very usefull for python conda users I think and more easy to install Thanks for your amazing mxnet library and for you effort,,"zihaolucky,szha,szha",2016-11-05 10:41:39,2017-09-30 19:03:03
IS,Prepare data MXNET,Hi I am new in python and mxnet I want to make the example in the link In the Prepare data part when i run this script mkdir p caltech 256 train 60 for i in 256 ObjectCategories do c basename i mkdir p caltech 256 train 60 c for j in ls i jpg shuf head n 60 do mv j caltech 256 train 60 c done done I have the error invalid syntax I did os system 'mkdir p caltech 256 train 60' and it worked and the directory was created For the rest it does not work I think that this script is made for Linux and I need to ru it on windows 8 and python 2 7 someone can help me to translate this code so i can run it,,szha,2017-06-20 09:34:40,2017-09-30 19:03:04
IS,Order matters for data names in Module,Hey I was creating a Module and it looks like the order of the list of the data names matters For example I do not think it should matter what order you specify the data names Is this the expected behavior If so there is no documentation on what order the data names need to be,,"yash1,formath,szha",2017-06-24 08:12:45,2017-09-30 19:03:05
IS,how to design a new customop,I am fresh here and encountered some problems with customop mlp mx symbol Custom data fc3 name isoftmax' op type isoftmax' I changed the name to be 'Softmax' or others and got the following errors It seems that the names of operator CustomOp and operator CustomOpProp could be any name But the name of symbol Custom must be softmax So what should I do if I want to build a new op for my research,,"Ldpe2G,Ldpe2G,szha",2017-06-17 03:20:53,2017-09-30 19:03:06
IS,Saving ndarray to disk using rec format,Hello I'm attempting to save a ndarray to disk using RecordIO Because it is larger than memory NDArrayIter can not be used Because it is not an image specifically because it is float32 not uint8 ImageRecordIter can not be used MXRecordIO can not be used because ndarray is not an integer or string What I'm specifically trying to achieve is saving the result of a convolution layer of resnet so that I can just load a precomputed value when fine tuning Any help would be appreciated Thanks,,szha,2017-06-27 11:25:14,2017-09-30 19:03:08
IS,How to get fixed size output feature map from a variable size input feature map kind of like spatial pyramid pooling,Background I am doing a project on learning discriminative features for images of various size In my case I do not want to resize the images into fixed width and height in order for them to be feed into the predefined network Problem For convolutional layers it is ok that the input images have different size But if we go from conv layer to fully connected layer we must make sure the output of last conv layer is fixed size For example output feature map for 2 conv layers may be 512 16 18 and 512 24 32 I want to get a fixed size output say 3 3 or 3 4 by dividing the input feature maps into 3 3 or 3 3 regions of the same size and then doing pooling operations max pooling or sum pooling Ideally after this transformation the output feature maps of the two images will become 512 3 3 regardless of the original image size Question Can anyone shed some light on how to implement this in MXNet preferably using a composition of existing symbols,,szha,2017-05-04 07:19:40,2017-09-30 19:03:10
IS,how to share auxiliary states of batchnorm in rnn with variable length input,hi I want to add batchnorm layer after i2h in gru Here is the code and it returns the aux params index out of range error I find several issues related but seems have not soloved like 3076 and 2663 Thanks for your time and effort,,szha,2017-05-05 01:46:45,2017-09-30 19:03:11
IS,same train code works different between v0 8 0 and the blooding edge v0 9 5 v0 9 5 has bug absolutely,totally same training code and used some mathematical operators with cnn layers it can converge on v0 8 0 but never in v0 9 5,,"piiswrong,zihaolucky,piiswrong,reminisce,reminisce,reminisce,reminisce,reminisce,kevinthesun,kevinthesun,piiswrong,kevinthesun,reminisce,reminisce,kevinthesun,szha",2017-04-07 23:23:29,2017-09-30 19:03:12
IS,Mxnet does not contain any example codes of attention model,As you can see above in the title I do not find any attention example codes I'm a newer for attention model I wish Mxnet developers or any contributors can provide a notebook for attention model,,"Ldpe2G,fhieber,szha",2017-06-27 11:13:05,2017-09-30 19:03:13
IS,some problems of mx nd Convolution,NO 1 According I try to test 3D convolution but why the result are all zeros import mxnet as mx def test Convolution3D conv3D data mx nd arange 24 reshape 2 1 2 2 3 conv3D ex conv3D simple bind mx cpu arg dicts wouldata' data conv3D ex forward return conv3D ex outputs 0 asnumpy x mx sym Variable wouldata' shape 2 1 2 2 3 conv3D mx sym Convolution data x kernel 2 2 3 pad 1 1 1 num filter 2 print test Convolution3D conv3D NO 2 and another question import mxnet as mx a mx nd arange 24 reshape 2 1 2 2 3 b mx nd Convolution data a kernel 2 2 3 no bias True pad 1 1 1 num filter 2 print b asnumpy Error message MXNetError 22 39 32 src c api c api ndarray cc 55 Check failed num inputs infered num inputs 1 vs 2 Expecting 2 inputs got 1 in operator Convolution,,"reminisce,szha",2017-06-20 14:57:40,2017-09-30 19:03:14
IS,discuss predict class with c,currently using pre trained model with c deploy is too complexity such as an example is in image classification predict cc in which you can found that it is not friendly for users to use predict function in src c api c predict api cc one inconvenient lies in then initialization after initialized the predict handle we can use it do forward but if we only changed the input shape such as batch size we need init again like this or other places and please give some advices to design interface i'm glad to add this thanks,,"tornadomeet,piiswrong,piiswrong,lx75249,lx75249,szha,szha",2017-03-31 10:13:46,2017-09-30 19:03:15
IS,Error in operator global pool 19 16 46 g deeplearn mxnet src operator pooling inl h 196 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 7 exceeds input 2 padded to 2,Any ideas what might cause that error Intuition about what sort of things I should focus on I'm getting this error when calling model Predict Environment info Operating System Windows Compiler Visual Studio Package used Python R Scala Julia C Error Message Error in operator global pool 19 16 46 g deeplearn mxnet src operator pooling inl h 196 Check failed param kernel 0 dshape 2 2 param pad 0 kernel size 7 exceeds input 2 padded to 2 at mxnet csharp Util CallCheck Int32 ret in F Projects mxnet csharp mxnet csharp Util cs line 81 at mxnet csharp Symbol InferShape Dictionary 2 argShapes List 1 inShape List 1 outShape List 1 auxShape in F Projects mxnet csharp mxnet csharp Symbol cs line 415 at mxnet csharp Symbol SimpleBind Context context Dictionary 2 inputShapes OpReqType gradReq Dictionary 2 typeDict Dictionary 2 group2Ctx in F Projects mxnet csharp mxnet csharp Symbol cs line 575 at mxnet csharp FeedForward InitPredictor Dictionary 2 inputShapes in F Projects mxnet csharp mxnet csharp FeedForward Predict cs line 107 at mxnet csharp FeedForward Predict IDataIter inputX Nullable 1 numBatch Boolean returnData Boolean reset in F Projects mxnet csharp mxnet csharp FeedForward Predict cs line 23 at test console Program TrainTest String path in F Projects mxnet csharp test console Program cs line 163 at test console Program Main String args in F Projects mxnet csharp test console Program cs line 82 What have you tried to solve it 1 Tried changing batch size Minimum reproducible example int batch size 1 uint size IntPtr array new IntPtr IntPtr strARray NativeMethods MXListAllOpNames out size out strARray var pnet Symbol Load Inception BN symbol json Context ctx new Context DeviceType KCpu 0 Speedometer speed new Speedometer batch size 50 DoCheckpoint doCheckpoint new DoCheckpoint path CustomMetric customMetric new CustomMetric l p Accuracy l p batch size Accuracy Optimizer optimizer new CcSgd momentum 0 9f learningRate 0 001f wd 0 00001f rescaleGrad 1 0f batch size FeedForward model null try var modelload FeedForward Load path ctx ctx numEpoch 1 optimizer optimizer initializer new Xavier factorType FactorType In magnitude 2 34f model new FeedForward pnet new List Context ctx numEpoch 1 optimizer optimizer initializer new Xavier factorType FactorType In magnitude 2 34f argParams modelload ArgParams auxParams modelload AuxParams catch Exception ignored if model null model new FeedForward pnet new List Context ctx numEpoch 1 optimizer optimizer initializer new Xavier factorType FactorType In magnitude 2 34f ReadData rdpredict new ReadData images batch size true true var testOut model Predict rdpredict batch size,,szha,2017-06-30 02:23:10,2017-09-30 19:03:17
IS,kvstore dist server h remove memcopy when Pull,the diff is below diff git a src kvstore kvstore dist server h b src kvstore kvstore dist server h index 02d4a38 441522d 100644 a src kvstore kvstore dist server h b src kvstore kvstore dist server h 212 8 212 7 class KVStoreDistServer int len stored shape 0 response keys req data keys response lens len TODO mli try to remove this CopyFrom response vals CopyFrom static cast const float stored data dptr len response vals ps SArray real t static cast const float stored data dptr len false server Response req meta response,,szha,2017-06-30 06:09:16,2017-09-30 19:03:18
IS,distributed mxnet is too slow when parameter size are increased,As input are sparse feature the total parameters are 1 3G the speed of distributed trainning are too slow So mxnet can not support large parameters,,"formath,szha",2017-06-30 07:40:08,2017-09-30 19:03:20
IS,Is there a way to limit the allocation of GPU memory that mxnet uses,Hello I'm curious if there is a way to limit the allocation of GPU memory that mxnet uses For example in theano setting cnmem in theanorc limits the maximum amount of memory used by the system Thank you,,"dtmoodie,szha",2017-06-27 06:52:35,2017-09-30 19:03:21
IS,Test 'good' trained model but any detection,Environment info Operating System Linux and Windows Compiler gcc 5 4 0 Package used Python R Scala Julia Python 2 7 MXNet commit hash git rev parse HEAD Python version and distribution Python 2 7 Linuxmint 18 1 I make a train with ssd example ssd and model vgg16 reduced The train seems to be good Epoch 498 Validation VL 0 846089 Epoch 498 Validation PL 0 770777 Epoch 498 Validation BUS 0 631579 Epoch 498 Validation UTILITAIRE 0 933206 Epoch 498 Validation MOTO 0 678569 Epoch 498 Validation PT 0 788829 Epoch 498 Validation mAP 0 774841 but when I run the trained model to an example nothing is detected even with images I used to make the train I used the code to test model I try to change threshold but nothing is better Anybody has a solution,,"zhreshold,zhreshold",2017-09-28 14:17:07,2017-09-30 20:31:22
IS,successful install and import mxnet in cmd but Pycharm cannot find FullyConnect and other nn functions And the code works both with Pycharm and cmd,Environment info Operating System windows Compiler PyCharm Package used Python R Scala Julia python MXNet version 0 9 2 Python version and distribution 2 7 12 I try to run demo in both terminal and pycharm following happens 1 these code work fine in cmd 2 these code work fine in Pycharm 3 but Pycharm cannot find declaration for FullyConnected or ActivationFunction I try to reinstall pycharm and invalidate cash with in pycharm but it did not work It seem that pycharm cannot find classes and function not write in py files,,szha,2017-01-19 09:30:58,2017-10-01 00:49:54
IS,im2rec py Segmentation fault,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler g 4 8 4 cuda 8 0 Package used Python R Scala Julia Python MXNet version v0 8 Or if installed from source from source MXNet commit hash git rev parse HEAD ceb9f0187a31d528e5566f810d933cf4834d3282 If you are using python package please provide Python version and distribution Anaconda3 4 2 0 or Anaconda2 4 2 0 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace 16 14 29 src c api operator custom inl h 66 New registration is overriding existing custom operator custom op Segmentation fault core dumped Minimum reproducible example if you are using your own code please provide a short script that reproduces the error I follow this tutorial when i execute this line error raise os system 'python s tools im2rec py list 1 recursive 1 shuffle 1 test ratio 0 2 data caltech data 101 ObjectCategories' MXNET HOME Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"ysh329,ysh329,ysh329,szha",2017-03-15 08:38:04,2017-10-01 00:49:55
PR,simplifying R package for efficiency and robustness,avoid for loops whenever possible prefer seq len and seq along over 1 n or 1 length x other small changes,,"bfgray3,thirdwing,bfgray3,thirdwing,bfgray3,bfgray3,thirdwing,bfgray3,bfgray3,thirdwing,thirdwing,bfgray3,bfgray3,piiswrong,bfgray3",2017-08-30 02:28:43,2017-10-01 00:54:22
PR,Removing WaitToWrite in dist kvstore,As reported in 8097 WaitToWrite blocks push until all gradients are computed which was introduced in 7082 After discussing it again with we decided to remove it and updated the document accordingly,,"eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong,rahul003,eric-haibin-lin",2017-09-30 18:49:43,2017-10-01 05:49:59
PR,Add ability to query MXNet version to C API,It is useful for language bindings to have access to the version number directly,,"sbodenstein,piiswrong,sbodenstein,sbodenstein",2017-09-24 14:22:48,2017-10-01 05:59:09
PR,OSX CMake fixes for tests binary,These changes allow the test projects to be properly built under OSX Some redundant sections in CMakeLists txt have also been removed,,"KellenSunderland,KellenSunderland,KellenSunderland,KellenSunderland,cjolivier01,cjolivier01,KellenSunderland,KellenSunderland,cjolivier01,KellenSunderland,piiswrong,cjolivier01,cjolivier01",2017-09-25 09:51:42,2017-10-01 06:22:27
PR,Enhancement for distributed sparse linear regression example,added the log loss metric used in avazu kaggle competition removed irrelevant code in the LR example added a readme file to the sparse folder,,"eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin",2017-09-12 20:15:13,2017-10-01 06:25:07
PR,LibsvmIter Doc Updates,Added a few clarifications to the libsvm iter documentation As libsvm iter is a wrapper of libsvm parser in dmlc core there is no guarantee that the file is split evenly when part idx and num parts are provided Also updated test io py to verify the number of batches read Also enabled test Cifar10Rec for better test coverage Preview 801 api python io io html mxnet io LibSVMIter,,eric-haibin-lin,2017-09-30 06:05:44,2017-10-01 06:26:09
IS,Strange output values from sigmoid activation C API,Environment info Operating System Ubuntu 16 04 Compiler g Package used Python R Scala Julia C MXNet version 0 11 Or if installed from source MXNet commit hash git rev parse HEAD branch 0 11 0 Error Message There is no error but there is some discrepancy in the output of the sigmoid layer specifically some of the outputs are greater than 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error 1 I defined a multi layer perceptron using the code provided below Why are some outputs greater than 1 which should not be the case as the final layer is using the sigmoid activation function What have you tried to solve it 1 This was a bug in the code resolved by adding NDArray WaitAll after the call to GetData,,szha,2017-09-30 22:56:02,2017-10-01 08:33:54
IS,gluon of the python package cannot currectly install,L76 Please fix,,,2017-07-18 07:28:24,2017-10-01 16:20:57
PR,correct boolean values for ndarray,,,"szha,reminisce,szha",2017-09-29 19:29:26,2017-10-01 18:30:31
IS,ImageIter is very slow when preprocessing cifar10 in windows,Environment info Operating System Windows 10 Package used Python R Scala Julia Python MXNet version 0 11 20170930 from this Python version and distribution Python 3 6 2 Anaconda 4 4 0 CPU Core i5 6500 GPU 1080Ti Error Message ImageIter is very slow when preprocessing cifar10 multi thread image preprocessing seems not work I want to training a simple resnet model from this page here is the mxnet data loading and preprocessing code When training without ImageIter and preprocessing using NDArrayIter mxnet code is 2x faster than my tensorflow code But when add above image preprocessing mxnet is alomst 2x slower than tensorflow tensorflow using ndarray iter and same preprocessing tensor not read from file Setting MXNET CPU WORKER NTHREADS from 1 to 4 leads to same training performance and the GPU load is only 20 tensorflow is 50 CPU load is 58 mechanical hard disk load is less than 10 and sometime is zero,,"yajiedesign,ptrendx,yajiedesign",2017-09-30 12:41:28,2017-10-02 00:45:59
IS,MXNet 0 11 0 Release Feedback NEWS md and CONTRIBUTORS md,As pointed out in this email thread 3Cgeneral incubator apache org 3E 1 CONTRIBUTORS md calls the project DMLC MXNet 2 The NEWS md refers to 0 11 0 rc3 as the latest version This should refer to the version being released rather than the rc3,,"mbaijal,yajiedesign",2017-09-06 00:34:43,2017-10-02 01:56:40
IS,MXNet 0 11 0 Release Feedback NOTICE file,As discussed in this email thread 3Cgeneral incubator apache org 3E All added Notices should be removed from the top level NOTICE file as indicated in this comment Unless the dependencies listed have explicit notice requirements they do not seem to nothing needs to be added to the NOTICE file,,"mbaijal,mbaijal",2017-09-06 00:26:08,2017-10-02 01:59:39
PR,Issue 7750 MXNet 0 11 0 Release Feedback README File,Refer to Issue 7750 1 Installation instructions are already included under contents in the readme 2 Updated 14 links in this PR 3 During release we must make sure the whole R package is removed 4 Already correct on master 5 Now part of this PR with the second commit,,"mbaijal,mbaijal,piiswrong,mbaijal,madjam,yajiedesign,yajiedesign,mbaijal",2017-09-27 01:27:58,2017-10-02 02:06:25
IS,want to use local cudnn in MXNet how to modify the Makefile,I modified Makefile from it failed so how to modify it thx,,"piiswrong,piiswrong,szha",2016-10-29 06:10:06,2017-10-02 02:45:56
IS,Fix CSS issue in mxnet docs site header,Issue When browser window size is reduced header gets distorted After reducing size to very small size headers will not even be visible Reproduce the issue open Keep shrinking reducing size of web page in your browser Observe that first headers get distorted later even disappears Possible solutions Solution 1 Fix minimum width so all headers will be visible and not get distorted Solution 2 Change CSS for headers such that it gets elongated and all headers are listed one below other when users shrinks web page size,,"sandeep-krishnamurthy,sandeep-krishnamurthy,pluskid,pluskid,pluskid,sandeep-krishnamurthy,kevinthesun,pluskid,kevinthesun,kevinthesun,pluskid,kevinthesun,pluskid,kevinthesun,pluskid,kevinthesun,pluskid,pluskid,kevinthesun,kevinthesun,pluskid,kevinthesun,kevinthesun,szha",2016-10-15 08:18:29,2017-10-02 02:50:54
IS,VOTE for MXNet is new logo,We need a new logo to replace the current one since I received several negative comments about the latter William Wang from Amazon kindly provides us several candidates Please thumb up the post below if you like it Any comment and suggestion is also welcome Please vote with github is thumb up feature image,,"mli,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,mli,mli,mli,piiswrong",2016-09-14 22:35:03,2017-10-02 02:56:16
IS,New MXNet logo proposals,Hi everyone I wanted follow up on the previous open issue and share a few new options for the MXnet logo update Please vote for the proposal s if you would like to use it as an update to the MXnet logo If there is enough interest for a specific design I can share assets needed for next steps github like Cheers William,,"smolix,jspisak,jonsafari,asmushetzel,pineking,jonsafari",2016-12-27 20:29:48,2017-10-02 02:56:36
IS,Python3 5 MXNet does not execute for loop,I try to follow The Custom Iterator section is code in the tutorial page at But I found these two line are never executed data mx nd array g d 1 for d g in zip self provide data self data gen label mx nd array g d 1 for d g in zip self provide label self label gen My python is version is python3 5 and mxnet is 0 95 Does any could help to fix this problem,,szha,2017-08-31 08:33:48,2017-10-02 03:51:13
PR,Removed redundant hash of out shape in algoreg key,The additional incorporation of out shape was not harmful functionally but is not needed,,DickJC123,2017-10-02 02:30:07,2017-10-02 05:12:20
IS,Error in make scalapkg,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Mac OS X 10 12 2 Compiler Package used Python R Scala Julia Scala MXNet version 0 9 1 Or if installed from source MXNet commit hash git rev parse HEAD d0728ca552d919fd40616149016ba4dd664c4e20 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace make scalapkg cd myhome Tools mxnet scala package mvn clean package Posx x86 64 gpu Dcxx g Dcflags DMSHADOW FORCE STREAM Wall O3 I myhome Tools mxnet mshadow I myhome Tools mxnet dmlc core include fPIC I myhome Tools mxnet nnvm include Iinclude funroll loops Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local Cellar opencv 2 4 13 2 include opencv I usr local Cellar opencv 2 4 13 2 include DMSHADOW USE CUDNN 1 I usr local opt openblas include DMXNET USE DIST KVSTORE I myhome Tools mxnet ps lite include I myhome Tools mxnet deps include DMXNET USE NVRTC 0 Dldflags pthread lm lcudart lcublas lcurand L usr local cuda lib64 L usr local cuda lib lopenblas L usr local Cellar opencv 2 4 13 2 lib lopencv calib3d lopencv contrib lopencv core lopencv features2d lopencv flann lopencv gpu lopencv highgui lopencv imgproc lopencv legacy lopencv ml lopencv nonfree lopencv objdetect lopencv ocl lopencv photo lopencv stitching lopencv superres lopencv ts lopencv video lopencv videostab lcudnn L usr local opt openblas lib L usr local lib graphviz myhome Tools mxnet deps lib libprotobuf lite a myhome Tools mxnet deps lib libzmq a lcuda Dlddeps myhome Tools mxnet ps lite build libps a myhome Tools mxnet dmlc core libdmlc a myhome Tools mxnet nnvm lib libnnvm a INFO Scanning for projects ERROR ERROR Some problems were encountered while processing the POMs ERROR wouldependencies dependency artifactId' for ml dmlc mxnet libmxnet init scala platform libtype with value 'libmxnet init scala platform ' does not match a valid id pattern line 77 column 19 ERROR The build could not read 1 project Help 1 ERROR ERROR The project ml dmlc mxnet mxnet macros 2 11 0 1 2 SNAPSHOT myhome Tools mxnet scala package macros pom xml has 1 error ERROR wouldependencies dependency artifactId' for ml dmlc mxnet libmxnet init scala platform libtype with value 'libmxnet init scala platform ' does not match a valid id pattern line 77 column 19 ERROR ERROR To see the full stack trace of the errors re run Maven with the e switch ERROR Re run Maven using the X switch to enable full debug logging ERROR ERROR For more information about the errors and possible solutions please read the following articles ERROR Help 1 http cwiki apache org confluence display MAVEN ProjectBuildingException make scalapkg Error 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error make scalapkg Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error After building mxnet 1 make scalapkg 2 3 What have you tried to solve it 1 Modify pom xml 2 3,,"Ldpe2G,Ldpe2G,yzhliu,yzhliu,phunterlau,szha,yzhliu",2016-12-31 08:47:29,2017-10-02 10:43:40
PR,Autograd with multiple devices,,,piiswrong,2017-10-02 06:26:05,2017-10-02 18:48:52
PR,Perl Adding Gluon interface to Perl miscellaneous changes in order to sync with Python,tlby could you please see if you can spot anything out of place Thanks,,"sergeykolychev,tlby,tlby,tlby,tlby,sergeykolychev,sergeykolychev,sergeykolychev,tlby,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,piiswrong,sergeykolychev,tlby,sergeykolychev,sergeykolychev,sergeykolychev,piiswrong,gautamkmr,sergeykolychev",2017-09-24 19:52:48,2017-10-02 19:06:48
PR,add variational dropout to gluon ptb example,changes 1 remove the download shell script 2 add cell api usage to the model 3 add variational dropout when using cell requires 7067,,"szha,piiswrong,szha,szha,piiswrong,szha,piiswrong,szha",2017-07-18 19:36:15,2017-10-03 03:48:06
PR,Issue 7264 MKL supports 2D 3D 4D input tensor only for concatenation,,,"adstraw,piiswrong,szha,adstraw",2017-09-13 00:38:18,2017-10-03 18:00:40
PR,Update loss md,,,szha,2017-10-03 03:31:09,2017-10-03 18:23:42
PR,Adding code owners,nswamy In order to make the master branch protected all the committers should be part of code owner,,"gautamkmr,eric-haibin-lin,gautamkmr,gautamkmr,szha,gautamkmr,piiswrong,gautamkmr,szha,gautamkmr,piiswrong,gautamkmr,gautamkmr,szha,CodingCat,CodingCat,CodingCat,gautamkmr,CodingCat,gautamkmr",2017-10-02 20:18:08,2017-10-03 20:38:36
PR,Stable sum,,,piiswrong,2017-10-03 20:50:47,2017-10-03 21:07:46
PR,Gluon Object detection preview,,,zhreshold,2017-10-03 22:04:55,2017-10-03 22:05:19
PR,Update nn md,,,szha,2017-10-03 20:04:53,2017-10-03 23:07:42
IS,CSVIter and LibSVMIter not returning correct number of batches per epoch,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System DeepLearninig AMI Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD ae975e5f8a70f9e2c36f78278f2553cdd4d87e79 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Run the above code Same kind of error for libsvm iterator 2 3,,"eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-09-27 20:52:33,2017-10-04 05:14:37
IS,autograd backward segfaults how to get gradients with respect to a subset of variables in mxnet,MXNet version 0 11 0 Python version and distribution Anaconda 3 Error Message I get erratic segmentation faults from various destructors related to autograd Any help on what I should try to debug this Is there a simpler way to get gradients with respect to only a subset of variables,,"piiswrong,ZiyueHuang",2017-10-03 02:18:01,2017-10-04 11:33:05
IS,how can I get the ndarray'grad for multi gnu simply,My code like this def test mult gpu a b with mx autograd record return a b as in context a context a mx nd array 1 2 ctx mx context gpu 0 b mx nd array 3 4 ctx mx context gpu 1 a attach grad b attach grad mult test test mult gpu a b print 'test mult ' mult test asnumpy mult test backward print 'test backward' a grad asnumpy b grad asnumpy I want to get b is grad but in this code b grad 0 0 b as in context a context can not backward b'grad how can I get it simply,,"junranhe,ZiyueHuang,junranhe",2017-10-04 12:07:06,2017-10-04 12:52:15
IS,Performance does not improve scalability issue with GPUs with running train imagenet py,While training AlexNet CNN with ImageNet data i do not see performance improvement in fact i see slight performance degradation with increasing number of GPUs python train imagenet py data train local ImageNet MXNet data MXNet data rec data val local ImageNet MXNet data MXNet data test rec gpus 0 1 2 3 network alexnet batch size 256 num epochs 1 kv store device Per epoch and batch size GPU 64 With 1 GPU Time cost 910 sec With 2 GPU Time cost 924 sec With 4 GPU Time cost 964 sec I have 4 Titan Xps However with synthetic data as shown in the demo i see good scalability,,"ptrendx,ptrendx",2017-09-08 18:32:40,2017-10-04 18:40:36
PR,remove unnecessary flatten in model zoo,since dense now has explicit flatten option some flatten layers in model zoo are not necessary any more,,szha,2017-10-04 18:25:55,2017-10-04 20:49:37
PR,add storage type logging to graph executor,This is mentioned in the tutorial PR 7921 and should be merged before that The log message will be printed if env var MXNET INFER STORAGE TYPE VERBOSE LOGGING 1,,eric-haibin-lin,2017-10-04 00:45:53,2017-10-04 20:50:16
PR,Add three sparse tutorials,Preview at,,"eric-haibin-lin,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,bhavinthaker,bhavinthaker,bhavinthaker,eric-haibin-lin,bhavinthaker,eric-haibin-lin,eric-haibin-lin,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,bhavinthaker,aaronmarkham,eric-haibin-lin,aaronmarkham,aaronmarkham,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,aaronmarkham,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin",2017-09-16 23:19:31,2017-10-04 22:52:32
PR,changed url references from dmlc to apache incubator mxnet,Patches 8145,,aaronmarkham,2017-10-04 21:47:08,2017-10-04 22:53:13
IS,references to dmlc in contribute md need updating,Like this Should change the remote dmlc url to the apache incubator mxnet url,,"aaronmarkham,eric-haibin-lin",2017-10-04 20:44:31,2017-10-04 23:09:44
PR,update sparse LR example,fix wrong metric name log loss nll loss fix wrong README instruction for distributed training added weighted loss for the output layer add a list of sparse optimizers as argument choices,,"eric-haibin-lin,szha,ZiyueHuang,eric-haibin-lin,ZiyueHuang,eric-haibin-lin,ZiyueHuang",2017-10-04 05:50:13,2017-10-05 05:52:58
IS,Missing include file in cpp package,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Windows 10 Compiler VS2017 Professional Package used Python R Scala Julia cpp package MXNet version MXNet 0 11 release built for Windows 20170926 mxnet x64 vc14 gpu prebuildbase win10 x64 vc14 v2 Yajedesign Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Severity Code Description Project File Line Suppression State Error active E1696 cannot open source file mxnet cpp op h MxNetWrapper c Projects MxNet 0 11 cpp package include mxnet cpp optimizer hpp 37 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 Tried to set all include paths 'cpp package include' 'nnvm include' 'include' 2 Found only op h in 'nnvm include nnvm' 3 Found nowhere 'mxnet cpp op h' although it is referenced in 'mxnet cpp MxNetCpp h',,"eric-haibin-lin,yajiedesign",2017-09-27 08:59:37,2017-10-05 07:06:17
PR,use fma in fully connected,when use bias broadcast the bias to output first and then use kAddTo which in turn keeps beta in gemm to be 1,,"szha,szha",2017-10-04 04:00:43,2017-10-05 21:57:14
PR,update arg params in image classification py,Removed the benchmark arg because it does not work with other args,,eric-haibin-lin,2017-10-05 23:58:06,2017-10-06 00:25:05
PR,Add gluon losses,Added back some of the losses reverted in 8010 Losses that are not added back include label is not necessarily a constant It needs to be included in the loss,,"piiswrong,piiswrong",2017-10-04 23:57:20,2017-10-06 02:07:16
PR,stable sum,,,"piiswrong,szha,piiswrong,szha,szha,szha,eric-haibin-lin",2017-10-03 21:07:39,2017-10-06 03:05:10
PR,Fix LeakyRELU Bug,mx symbol LeakyReLU crashes when using vector inputs For example this crashes for the forward pass This PR is a simple fix along with a test for mx sym LeakyReLU x act type elu slope 1 0 for vector inputs,,"sbodenstein,sbodenstein,piiswrong,sbodenstein,piiswrong,sbodenstein",2017-10-02 05:13:15,2017-10-06 08:44:06
IS,scala example core dump when model bind,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System mac Compiler scala sbt MXNet version 0 11 0 Or if installed from source yes MXNet commit hash git rev parse HEAD a5edbf94094581ee27157eae4f2113115a3994e7 Minimum reproducible example convert from,,"Ldpe2G,yzhliu",2017-10-05 21:09:29,2017-10-06 18:41:22
PR,Fix arange start and stop arguments default and required settings,The arange description text differs from the configuration of defaults and required parameters The start parameter should be optional and be defaulting to 0 The stop parameter should be required and not be marked as optional,,"kottmann,piiswrong,cjolivier01,piiswrong,kottmann,kottmann,cjolivier01,kottmann,kottmann,piiswrong",2017-09-08 10:37:58,2017-10-06 19:47:33
PR,Add infer storage type function for sparse slice operator,This change will allow to fallback to dense operator for slice for features such as slicing along more than one axis which is not supported for sparse,,"anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin,anirudh2290,eric-haibin-lin",2017-10-05 06:50:29,2017-10-06 21:51:27
IS,very low training accuracies with alexnet or vgg but good with resnet,Hi all With resnet I get the training accuracies as expected but when I just use some other network alexnet or vgg the training accuracies dont change Not sure why this is NFO root Epoch 0 Batch 880 Speed 231 02 samples sec Train accuracy 0 000391 INFO root Epoch 0 Batch 900 Speed 234 54 samples sec Train accuracy 0 001563 INFO root Epoch 0 Batch 920 Speed 232 63 samples sec Train accuracy 0 000000 INFO root Epoch 0 Batch 940 Speed 230 30 samples sec Train accuracy 0 001172 INFO root Epoch 0 Batch 960 Speed 229 14 samples sec Train accuracy 0 000781 INFO root Epoch 0 Batch 980 Speed 225 48 samples sec Train accuracy 0 001563 INFO root Epoch 0 Batch 1000 Speed 233 67 samples sec Train accuracy 0 001953 INFO root Epoch 0 Batch 1020 Speed 232 90 samples sec Train accuracy 0 001563 INFO root Epoch 0 Batch 1040 Speed 233 26 samples sec Train accuracy 0 000781 INFO root Epoch 0 Batch 1060 Speed 230 20 samples sec Train accuracy 0 001563 INFO root Epoch 0 Batch 1080 Speed 229 51 samples sec Train accuracy 0 000781 INFO root Epoch 0 Batch 1100 Speed 233 14 samples sec Train accuracy 0 000781 INFO root Epoch 0 Batch 1120 Speed 227 37 samples sec Train accuracy 0 000391,,"piiswrong,mli,piiswrong,rahul003",2017-03-17 18:18:53,2017-10-06 23:30:19
IS,when import error libmxnet so undefined symbol gfortran concat string,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler g 5 4 Package used Python R Scala Julia Python MXNet version Or if installed from source just synced and build only difference is in make config mk changed to USE BLAS openblas MXNet commit hash git rev parse HEAD If you are using python package please provide python2 7 sudo apt get install reinstall libopenblas dev sudo apt get install reinstall python numpy sudo apt get install reinstall libopenblas base Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Python 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 on linux2 Type help copyright credits or license for more information import mxnet Traceback most recent call last File stdin line 1 in module File home local ANT fengy mxnet ubuntu mxnet python mxnet init py line 25 in module from base import MXNetError File home local ANT fengy mxnet ubuntu mxnet python mxnet base py line 110 in module LIB load lib File home local ANT fengy mxnet ubuntu mxnet python mxnet base py line 102 in load lib lib ctypes CDLL lib path 0 ctypes RTLD LOCAL File usr lib python2 7 ctypes init py line 362 in init self handle dlopen self name mode OSError home local ANT fengy mxnet ubuntu mxnet python mxnet lib libmxnet so undefined symbol gfortran concat string exit Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 If I set USE LAPACK 0 in config mk I can import 2 Checked liblapack locate liblapack so output etc alternatives liblapack so etc alternatives liblapack so 3 etc alternatives liblapack so 3gf usr lib liblapack so usr lib liblapack so 3 usr lib liblapack so 3gf usr lib lapack liblapack so usr lib lapack liblapack so 3 usr lib lapack liblapack so 3 6 0 usr lib openblas base liblapack so usr lib openblas base liblapack so 3 var lib dpkg alternatives liblapack so var lib dpkg alternatives liblapack so 3 tried to change this without success LD LIBRARY PATH usr lib openblas base mxnet ubuntu mxnet lib tried to add usr lib lapack 3,,,2017-10-07 04:44:30,2017-10-07 05:12:14
PR,Minor enhancement for the auto encoder example,add batch size num units num iterations as parser arguments add visualization option for inputs outputs which is commonly used when people try out auto encoder for the first time,,"eric-haibin-lin,szha",2017-10-06 22:47:26,2017-10-07 21:41:58
PR,documentation fix,mx nd random uniform mx nd random uniform,,startakovsky,2017-10-07 22:33:38,2017-10-08 01:07:27
IS,rec generated by im2rec py has same value for different key in idx,I generate a large rec file over 1M files using im2rec with MXIndexedRecordIO and observe that in data idx file after specific lines the first number in each line keeps increasing while the second one keeps unchanged like this 32989 18446744073709551615 32990 18446744073709551615 32991 18446744073709551615 32992 18446744073709551615 32993 18446744073709551615 32994 18446744073709551615 32995 18446744073709551615 32997 18446744073709551615 32998 18446744073709551615 From this line L226 it seems that the second number determines the position for a specific key So different key should have a different position I have tried read through the full rec file and found that the last few items are not the same and their position should be different So what does the second column in idx file mean and is it normal to have same values in second columns Is there any documentation Thanks,,"zhreshold,ptrendx,ptrendx,ptrendx,ptrendx",2017-09-29 18:38:32,2017-10-08 03:15:11
IS,I can not run the mnist tutorial by GPU,when I run the tutorials the url is url Environment info Operating System Ubuntu16 04 Compiler Package used Python R Scala Julia Python3 6 MXNet version MXNet 0 11 0 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python3 6 If you are using R package please provide R sessionInfo Error Message INFO root start with arguments Namespace add stn False batch size 64 disp batches 100 dtype 'float32' gpus '0' kv store wouldevice' load epoch None lr 0 05 lr factor 0 1 lr step epochs '10' model prefix None mom 0 9 monitor 0 network 'mlp' num classes 10 num epochs 20 num examples 60000 num layers None optimizer isgd' test io 0 top k 0 wd 0 0001 10 38 33 home travis build dmlc mxnet distro mxnet build dmlc core include dmlc logging h 308 10 38 33 src storage storage cc 109 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Stack trace returned 10 entries bt 0 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x230aac 0x7f9b12c43aac bt 1 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a15ab7 0x7f9b14428ab7 bt 2 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a18239 0x7f9b1442b239 bt 3 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a187bc 0x7f9b1442b7bc bt 4 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x153b691 0x7f9b13f4e691 bt 5 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1558f04 0x7f9b13f6bf04 bt 6 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154061a 0x7f9b13f5361a bt 7 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1544737 0x7f9b13f57737 bt 8 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154c52a 0x7f9b13f5f52a bt 9 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154cc24 0x7f9b13f5fc24 Traceback most recent call last File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet symbol symbol py line 1484 in simple bind ctypes byref exe handle File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet base py line 145 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 10 38 33 src storage storage cc 109 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Stack trace returned 10 entries bt 0 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x230aac 0x7f9b12c43aac bt 1 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a15ab7 0x7f9b14428ab7 bt 2 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a18239 0x7f9b1442b239 bt 3 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a187bc 0x7f9b1442b7bc bt 4 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x153b691 0x7f9b13f4e691 bt 5 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1558f04 0x7f9b13f6bf04 bt 6 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154061a 0x7f9b13f5361a bt 7 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1544737 0x7f9b13f57737 bt 8 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154c52a 0x7f9b13f5f52a bt 9 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154cc24 0x7f9b13f5fc24 During handling of the above exception another exception occurred Traceback most recent call last File home shipeng mxnet example image classification train mnist py line 96 in module fit fit args sym get mnist iter File home shipeng mxnet example image classification common fit py line 214 in fit monitor monitor File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet module base module py line 460 in fit for training True force rebind force rebind File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet module module py line 417 in bind state names self state names File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet module executor group py line 231 in init self bind exec data shapes label shapes shared group File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet module executor group py line 327 in bind exec shared group File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet module executor group py line 603 in bind ith exec shared buffer shared data arrays input shapes File home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet symbol symbol py line 1490 in simple bind raise RuntimeError error msg RuntimeError simple bind error Arguments data 64 1 28 28 softmax label 64 10 38 33 src storage storage cc 109 Check failed e cudaSuccess e cudaErrorCudartUnloading CUDA unknown error Stack trace returned 10 entries bt 0 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x230aac 0x7f9b12c43aac bt 1 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a15ab7 0x7f9b14428ab7 bt 2 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a18239 0x7f9b1442b239 bt 3 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1a187bc 0x7f9b1442b7bc bt 4 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x153b691 0x7f9b13f4e691 bt 5 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1558f04 0x7f9b13f6bf04 bt 6 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154061a 0x7f9b13f5361a bt 7 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x1544737 0x7f9b13f57737 bt 8 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154c52a 0x7f9b13f5f52a bt 9 home shipeng anaconda3 envs gluon lib python3 6 site packages mxnet libmxnet so 0x154cc24 0x7f9b13f5fc24 Process finished with exit code 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx mnist mx test utils get mnist batch size 100 train iter mx io NDArrayIter mnist 'train data' mnist 'train label' batch size shuffle True val iter mx io NDArrayIter mnist 'test data' mnist 'test label' batch size data mx sym var wouldata' conv1 mx sym Convolution data data kernel 5 5 num filter 20 tanh1 mx sym Activation data conv1 act type tanh pool1 mx sym Pooling data tanh1 pool type max kernel 2 2 stride 2 2 conv2 mx sym Convolution data pool1 kernel 5 5 num filter 50 tanh2 mx sym Activation data conv2 act type tanh pool2 mx sym Pooling data tanh2 pool type max kernel 2 2 stride 2 2 flatten mx sym flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 500 tanh3 mx sym Activation data fc1 act type tanh fc2 mx sym FullyConnected data tanh3 num hidden 10 lenet mx sym SoftmaxOutput data fc2 name isoftmax' lenet model mx mod Module symbol lenet context mx gpu lenet model fit train iter eval data val iter optimizer isgd' optimizer params 'learning rate' 0 1 eval metric 'acc' batch end callback mx callback Speedometer batch size 100 num epoch 10,,,2017-10-07 14:07:32,2017-10-08 03:20:00
PR,Fix input output format for gluon layers,,,piiswrong,2017-10-07 23:57:22,2017-10-08 03:52:08
PR,Making mixed precision work with all optimizers,,,"ptrendx,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,ptrendx,ptrendx,piiswrong,ptrendx,ptrendx,piiswrong",2017-08-29 15:41:00,2017-10-08 05:22:44
PR,bilinear sampler corner cases amendment,Deal with the corner cases such as the operations around the edge of the feature map correctly without extra condition jump statement,,"sxjscience,sxjscience,sxjscience",2017-10-01 16:22:23,2017-10-08 14:11:09
IS,Bulk Closing Issues,Hello As you may have noticed we contributors of mxnet are bulk closing old inactive issues To avoid confusion here are several things to know Why We are closing issues that have not been updated for at least 90 days and do not have labels so that contributors can better focus our efforts on active issues Many issues have not been active because the issues have already been patched and the involved parties forgot to follow up Others might be due to incomplete information making the issues not actionable Some may be issues on APIs that are deprecated and better solution API has been developed By closing these old inactive issues we can get a better signal to noise ratio from users to guide our efforts What do I need to do If the issue is a problem that has not been solved and is still impacting you you have the choice of commenting on the closed issue and asking for reopen Alternatively you could comment on this issue quoting the issue number that you believe should be reopened and we will happily reopen it and help If the issue is already resolved remember to give the contributor a thumbs up,,"szha,zhreshold",2017-09-28 20:18:17,2017-10-08 21:16:49
IS,Mxnet has very bad performance on iris dataset and sonar dataset comparing Tensorflow,Not only in train Accuracy but also in prediction I test some codes both in mxnet and in Tensorflow Then I got the following result 1 sonar dataset a Tensorflow codes train Accuracy 0 857143 prediction I test three samples The original label is 0 1 0 the prediction is 1 1 0 About 66 accuracy in prediction b mxnet codes train Accuracy 0 6 prediction I test three samples The original label is 0 1 0 the prediction is 1 1 1 About 33 accuracy in prediction 2 iris dataset a Tensorflow codes train Accuracy 1 0 prediction I test ten samples The original label is 1 0 2 1 0 1 1 1 2 1 the prediction is 1 0 2 1 0 0 1 2 2 1 About 80 accuracy in prediction b mxnet codes train Accuracy 0 97 prediction I test ten samples The original label is 1 0 2 1 0 1 1 1 2 1 the prediction is 2 1 2 2 1 2 2 2 2 2 About 20 accuracy in prediction As you can see the above mentioned when I use model to do prediction mxnet codes always get the worst result than Tensorflow codes I do not know why I hope the mxnet developers should pay attention to these Could your guys write some codes on iris dataset or sonar dataset to show how mxnet work My codes can be download at You can run them to compare mxnet and tensorflow,,"yajiedesign,szha",2017-04-04 07:34:00,2017-10-08 21:39:35
IS,what is the output of mx contrib sym ctc loss,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos Compiler Package used Python R Scala Julia python MXNet version 0 10 Or if installed from source pip MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution python 2 7 Error Message I want to know what does the mx contrib sym ctc loss data label return Signature mx contrib sym ctc loss args kwargs Docstring Connectionist Temporal Classification Loss The shapes of the inputs and outputs data sequence length batch size alphabet size 1 label batch size label sequence length out batch size label is a tensor of integers between 1 and alphabet size If a sequence of labels is shorter than label sequence length use the special padding character 0 at the end of the sequence to conform it to the correct length For example if label sequence length 4 and one has two sequences of labels 2 1 and 3 2 2 the resulting tensor should be padded to be 2 1 0 0 3 2 2 0 The data tensor consists of sequences of activation vectors The layer applies a softmax to each vector which then becomes a vector of probabilities over the alphabet Note that the 0th element of this vector is reserved for the special blank character See Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks A Graves et al for more information Defined in src operator contrib ctc loss cc L79 Parameters data Symbol Input data to the ctc loss op label Symbol Ground truth labels for the loss name string optional Name of the resulting symbol Returns Symbol The result symbol my network defined below def sym ocrCTC batch size 100 num layers 2 num hidden 128 seq len 30 stack mx rnn SequentialRNNCell for i in range num layers if i num layers 1 laster layer stack add mx rnn LSTMCell num hidden 63 prefix 'lstm l d ' i else stack add mx rnn LSTMCell num hidden num hidden prefix 'lstm l d ' i data mx sym Variable wouldata' has the shape of Batch size 30 100 label mx sym Variable 'label' stack reset outputs states stack unroll seq len inputs data merge outputs True pred mx sym swapaxes data outputs dim1 0 dim2 1 loss mx contrib sym ctc loss data pred label label ctc loss mx sym make loss loss return ctc loss Is there any problem,,"piiswrong,sbodenstein,szha",2017-06-29 13:20:55,2017-10-08 21:39:36
IS,Question is that possible to use GridGenerator to resize image,,,szha,2017-07-02 04:13:24,2017-10-08 21:39:37
IS,Check failed dilated ksize y AddPad dshape 2 param pad 0 5 vs 3 kernel size exceed input,R 3 4 0 on windows 8 1 x64 mxnet 0 10 1 When I run codes as follows I met problems train data matrix w2ctrainset test data matrix w2ctestset train x train 1 train y train 1 train x t train x 10 train x t train x train array train x dim train array c 10 10 1 ncol train x data mx symbol Variable data first cnn conv1 mx symbol Convolution data data kernel c 5 5 num filter 20 tanh1 mx symbol Activation data conv1 act type tanh pool1 mx symbol Pooling data tanh1 pool type max kernel c 2 2 stride c 2 2 second cnn conv2 mx symbol Convolution data pool1 kernel c 5 5 num filter 50 tanh2 mx symbol Activation data conv2 act type tanh pool2 mx symbol Pooling data tanh2 pool type max kernel c 2 2 stride c 2 2 first fullc flatten mx symbol Flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 500 tanh3 mx symbol Activation data fc1 act type tanh second fullc fc2 mx symbol FullyConnected data tanh3 num hidden 10 loss lenet mx symbol SoftmaxOutput data fc2 train mx set seed 0 model mx model FeedForward create lenet X train array y train y ctx mx cpu num round 20 array batch size 100 learning rate 0 05 momentum 0 9 eval metric mx metric accuracy The error messages are as follows model mx model FeedForward create lenet X train array y train y ctx mx cpu num round 20 array batch size 100 learning rate 0 05 momentum 0 9 eval metric mx metric accuracy 13 58 17 D Program Files x86 Jenkins workspace mxnet mxnet dmlc core include dmlc logging h 304 13 58 17 d program files x86 jenkins workspace mxnet mxnet src operator convolution inl h 442 Check failed dilated ksize y AddPad dshape 2 param pad 0 5 vs 3 kernel size exceed input Error in symbol infer shape list Error in operator convolution1 13 58 17 d program files x86 jenkins workspace mxnet mxnet src operator convolution inl h 442 Check failed dilated ksize y AddPad dshape 2 param pad 0 5 vs 3 kernel size exceed input I do not where I was wrong The original w2ctrainset has 187038 obs of 101variables All in it are numbers Please help me,,szha,2017-07-03 06:08:00,2017-10-08 21:39:38
IS,a strange ndarray asscalar time cost problem,hi I'm using RNN to train language model on PTB code here with K20 the time cost of a batch is about 26s after profiling I found that metric update take about 25s about 96 of total with some print and compare I found the key problem is loss asscalar which copy data from GPU to CPU memory and the under engine function is NDArray SyncCopyToCPU and the most time consuming function is Wait To Read so is this a normal performance if this is normal then another strange problem is time cost of save parameters to file which is only 0 2s obviously save parameter need copy data to CPU also then what is the difference of those two scenes tks for any help to reproduce this problem just add a break here L632,,szha,2017-07-03 08:11:48,2017-10-08 21:39:39
IS,Is SyncCopyToCPU really thread safe,In mxnet NDArray SyncCopyToCPU it calls WaitToRead at first and then do the copy However I was just wondering whether the src data would be changed during the copy Because there is no mechanism to prevent src from being changed by other operation which may be pushed into engine and executed after the call of WaitToRead,,"tqchen,szha",2017-07-03 17:45:50,2017-10-08 22:48:13
IS,OpWrapperGenerator py fails,OpWrapperGenerator py in c backend fails mxnet cpp package scripts python OpWrapperGenerator py Traceback most recent call last File OpWrapperGenerator py line 405 in module raise e IndexError list index out of range,,"CNevd,szha",2017-06-30 14:48:14,2017-10-08 23:52:30
IS,Plan for mxnet version of MobileNets,Google has published an article 'MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications' on 17 Apr They propose a MobileNets for mobile and embedded vision applications which is fast and small with high accuracy And Google plan on releasing models in Tensor Flow Is anyone interested in this Does the Mxnet committee plan to do this project in the future If so when the mxnet version will be released,,"zhreshold,mli,zhreshold,thatindiandude,ysh329,szha",2017-04-19 08:33:16,2017-10-08 23:52:54
IS,gpu memory allocate will be error when using multiprocessing Process,reproduce code,,"tornadomeet,tornadomeet,xlvector,piiswrong,leezu,szha",2017-01-13 06:41:28,2017-10-08 23:52:55
IS,There are 2 install pages of the Mac OS and this is confusing,Search request install MxNet on Mac OS shows at least for me following 2 pages as the top results mxnet io get started osx setup html Installing MXNet on OS X Mac mxnet io get started install html Installing MXNet Even though both of them states Installing MXNet one of the page is actually about compiling MxNet from sources and only second one is about actual installation IMHO first page need to be renamed to something like installing MXNet from source or compiling MxNet from source,,"b0noI,piiswrong,sandeep-krishnamurthy,b0noI,szha",2017-06-29 22:16:36,2017-10-08 23:52:56
IS,build mxnet with MKL installed on system,If I want to build mxnet with an existing MKL I need to set USE INTEL PATH and run make j USE BLAS mkl Do I still need to set USE MKL2017 1 Setting USE MKL2017 seems always downloading MKL and use the downloaded MKLML to build mxnet correct Will it turn on different code in mxnet by setting USE MKL2017 1,,szha,2017-06-28 15:37:50,2017-10-08 23:52:57
IS,Does cudnn Batchnorm support 5D input,I am trying to use 3D convolution as well as 3D batch norm which means their input are 5D tensors There is nothing wrong with conv layer but I got an error when using cudnn batch norm I heard 5D is supported by cudnn bn but why I got this error Something wrong with the mxnet implementation,,"mli,pluskid,leezu,leezu,szha",2017-03-02 11:19:50,2017-10-08 23:52:58
IS,faster rcnn train error,when i run bash script vgg voc07 sh 0 i found there is something wrong in pascal voc py size cv2 imread the info is Segmentation fault core without any other information i add some print info above this and under this then make sure that the problem is here but i have tried cv2 imread in python it work well can anybody help me with it thank you,,szha,2017-05-03 10:36:17,2017-10-08 23:52:59
IS,question about LinearRegressionOutput operator,I use the LinearRegressionOutput operator as the final layer of the net but it seems not work A simple code like this It is strange and I'm confused,,"CNevd,Godricly,szha",2017-07-07 06:49:22,2017-10-08 23:53:00
IS,Save Load model Python,After training my feed forward model I want to save it to disk so I can load it back up at a later point The way I do it seems rather cumbersome but I could not figure out a better way Any Advice would be appreciated This all works but there is the parameter epoch which has to be specified for loading the files Is there a way independent on epoch to save load models I can think of two obvious but hacky solutions a Save with a fixed number for epoch that actually does not reflect the actual epoch of training b Add an additional model save params after the save checkpoint Environment info Package used Python MXNet version 0 10 0,,"CNevd,szha",2017-07-07 13:59:17,2017-10-08 23:53:01
IS,support converting caffe mtcnn and mobilenet model,current convert tool has issues to support converting caffe mtcnn and mobilenet model I made patch for group symbols and BatchNorm detail document see write mtcnn c version code for mxnet and test MobileNet model under mxnet Can you help check if it is possible to add support the two models by converting caffe models,,"joey2014,piiswrong,szha",2017-07-07 13:51:33,2017-10-08 23:53:02
IS,Data provided by label shapes do not match names specified by label names vs isoftmax label',I get the warning Data provided by label shapes do not match names specified by label names vs isoftmax label' when I bind a model for inference The data iterator I use for inference has no labels The model seems to have isoftmax label' as one of the label names Since isoftmax label' is not present in the iterator I get the above mentioned warning I think if bind sets for training False lack of label in the iterator should not generate a warning Error Message Data provided by label shapes do not match names specified by label names vs isoftmax label' Minimum reproducible example Steps to reproduce Just run the above code Environment info Ubuntu 16 04 Compiler GCC Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 202de02cd713eb17300404b842261f126b6e3c97 If you are using python package please provide Python version and distribution Python 3 6 1 Anaconda 4 4 0 64 bit If you are using R package please provide R sessionInfo,,"indhub,szha",2017-07-07 23:49:19,2017-10-08 23:53:03
IS,Build Error confusing namespaces,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System NixOS 17 03 Linux Compiler gcc 5 4 0 Package used Python R Scala Julia when c libmxnet building MXNet version from github latest revisions Or if installed from source MXNet commit hash git rev parse HEAD mxnet 202de02cd713eb17300404b842261f126b6e3c97 cub a50619b006fc753b8eb4cfccdea17698cba40d5e dlpack 36e573893fc39d324ccf2f2962300da6da5898a2 ps lite acdb698fa3bb80929ef83bb37c705f025e119b82 dmlc core 40390f83a8384f4ad87eba7d12b8bb3f2def79e2 nnvm c96dd0e126a788089fe700cf6effe4e87bc40e05 If you are using python package please provide No python packages Python version and distribution Nothing If you are using R package please provide No R packages R sessionInfo I'm not using R now Error Message Please paste the full error message including stack trace What have you tried to solve it It seems the TBlob is in the mxnet namespace and the TShape is in the nnvm namespace in the code However those are witten with mshadow I assume there are wrong in my operation However there are confusing code now,,szha,2017-07-07 16:07:04,2017-10-08 23:53:04
PR,Modify the io In the previous version the iterators in C can only,have one label which is not sufficient for multi task training Now this version assumes the DataInst has data label1 label2 6812,,szha,2017-07-03 10:58:37,2017-10-08 23:53:05
IS,SoftmaxOutput same prediction for all samples,Good morning I am using mxnet 0 10 1 on Ubuntu 16 04 python 3 package I am currently experiencing a very weird situation I am basically trying to learn the parameters for a cnn over a subset of imagenet Specifically I am targeting 11 classes bounding boxes The network is structured as follows top down I double checked the inputs and they are fine I am hoping to find some help here Thanks,,szha,2017-07-04 10:07:58,2017-10-08 23:53:06
IS,How can I train with multiple CPUs with model parallelism,I read the doc 'How can I train with multiple GPUs with model parallelism ' However my macbook only has cpus available I set the ctx as mx cpu i for i in range 4 and set the dataiter network batch size as 64 and encounters error i do not know how to fix it I set model parallel True in the data iter the same error still exists I got error like this src operator tensor matrix op inl h 1041 Check failed end axis size end 0 Invalid begin end get begin 0 end 64 infer shape error Arguments positive 16 3 224 224 one 16 anchor 16 3 224 224 negative 16 3 224 224 mxnet base MXNetError Error in operator slice axis0 16 14 17 src operator tensor matrix op inl h 1041 Check failed end axis size end 0 Invalid begin end get begin 0 end 64,,"kevinthesun,eric-haibin-lin,szha",2017-06-26 08:18:42,2017-10-08 23:53:07
IS,resnet validation bug when using identical validation training train acc reaches 1 and val 0 75,Very weird behavior in resnet validation when using NDArrayIter i was testing my validation errors on some image recognition task when i tried plugging the training set as the validation set i expected to get similar training and eval accuracies but the training accuracy reached 1 and the validation accuracy did not pass 0 75 this behavior DID NOT repeat when using alexnet as the network both accuracies reached 1 as expected does someone have any idea what could be causing this Environment info Operating System Windows 10 Compiler vc14 Package used Python R Scala Julia Python MXNet version 0 10 1 Or if installed from source yajiedesign mxnet mxnet 20170620 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 3 5 3 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace this is the training and validation error log when using alexnet validation and training set are identical C Program Files Python35 python exe C Users Admin Documents MY tryinpython230417 mxnet master example image classification mvc ff module orig resenettest py started 06 29 17 01 15 09 mean train before 0 00200461403317 mean val before 0 00200431063389 2048 1 128 128 2048 2048 1 128 128 2048 01 15 17 d program files x86 jenkins workspace mxnet mxnet src operator cudnn algoreg inl h 65 Running performance tests to find the best convolution algorithm this can take a while setting env variable MXNET CUDNN AUTOTUNE DEFAULT to 0 to disable INFO root Epoch 0 Train accuracy 0 509277 INFO root Epoch 0 Time cost 2 138 INFO root Epoch 0 Validation accuracy 0 517578 INFO root Epoch 1 Train accuracy 0 517578 INFO root Epoch 1 Time cost 0 796 INFO root Epoch 1 Validation accuracy 0 517578 INFO root Epoch 2 Train accuracy 0 517578 INFO root Epoch 2 Time cost 0 772 INFO root Epoch 2 Validation accuracy 0 517578 INFO root Epoch 3 Train accuracy 0 517578 INFO root Epoch 3 Time cost 0 799 INFO root Epoch 3 Validation accuracy 0 517578 INFO root Epoch 4 Train accuracy 0 517578 INFO root Epoch 4 Time cost 0 797 INFO root Epoch 4 Validation accuracy 0 517578 INFO root Epoch 5 Train accuracy 0 517578 INFO root Epoch 5 Time cost 0 785 INFO root Epoch 5 Validation accuracy 0 517578 INFO root Epoch 6 Train accuracy 0 517578 INFO root Epoch 6 Time cost 0 784 INFO root Epoch 6 Validation accuracy 0 517578 INFO root Epoch 7 Train accuracy 0 517578 INFO root Epoch 7 Time cost 0 785 INFO root Epoch 7 Validation accuracy 0 517578 INFO root Epoch 8 Train accuracy 0 517578 INFO root Epoch 8 Time cost 0 782 INFO root Epoch 8 Validation accuracy 0 517578 INFO root Epoch 9 Train accuracy 0 517578 INFO root Epoch 9 Time cost 0 768 INFO root Epoch 9 Validation accuracy 0 517578 INFO root Epoch 10 Train accuracy 0 517578 INFO root Epoch 10 Time cost 0 770 INFO root Epoch 10 Validation accuracy 0 517578 INFO root Epoch 11 Train accuracy 0 517578 INFO root Epoch 11 Time cost 0 782 INFO root Epoch 11 Validation accuracy 0 517578 INFO root Epoch 12 Train accuracy 0 517578 INFO root Epoch 12 Time cost 0 786 INFO root Epoch 12 Validation accuracy 0 517578 INFO root Epoch 13 Train accuracy 0 524414 INFO root Epoch 13 Time cost 0 800 INFO root Epoch 13 Validation accuracy 0 517578 INFO root Epoch 14 Train accuracy 0 550293 INFO root Epoch 14 Time cost 0 770 INFO root Epoch 14 Validation accuracy 0 517578 INFO root Epoch 15 Train accuracy 0 553711 INFO root Epoch 15 Time cost 0 796 INFO root Epoch 15 Validation accuracy 0 594727 INFO root Epoch 16 Train accuracy 0 570801 INFO root Epoch 16 Time cost 0 797 INFO root Epoch 16 Validation accuracy 0 622559 INFO root Epoch 17 Train accuracy 0 568848 INFO root Epoch 17 Time cost 0 799 INFO root Epoch 17 Validation accuracy 0 624512 INFO root Epoch 18 Train accuracy 0 584473 INFO root Epoch 18 Time cost 0 770 INFO root Epoch 18 Validation accuracy 0 518555 INFO root Epoch 19 Train accuracy 0 579102 INFO root Epoch 19 Time cost 0 813 INFO root Epoch 19 Validation accuracy 0 542969 INFO root Epoch 20 Train accuracy 0 596680 INFO root Epoch 20 Time cost 0 813 INFO root Epoch 20 Validation accuracy 0 611328 INFO root Epoch 21 Train accuracy 0 637207 INFO root Epoch 21 Time cost 0 797 INFO root Epoch 21 Validation accuracy 0 694824 INFO root Epoch 22 Train accuracy 0 676758 INFO root Epoch 22 Time cost 0 828 INFO root Epoch 22 Validation accuracy 0 607910 INFO root Epoch 23 Train accuracy 0 667480 INFO root Epoch 23 Time cost 0 797 INFO root Epoch 23 Validation accuracy 0 643066 INFO root Epoch 24 Train accuracy 0 684570 INFO root Epoch 24 Time cost 0 813 INFO root Epoch 24 Validation accuracy 0 690918 INFO root Epoch 25 Train accuracy 0 700195 INFO root Epoch 25 Time cost 0 797 INFO root Epoch 25 Validation accuracy 0 653809 INFO root Epoch 26 Train accuracy 0 691406 INFO root Epoch 26 Time cost 0 813 INFO root Epoch 26 Validation accuracy 0 683105 INFO root Epoch 27 Train accuracy 0 702148 INFO root Epoch 27 Time cost 0 813 INFO root Epoch 27 Validation accuracy 0 692871 INFO root Epoch 28 Train accuracy 0 711914 INFO root Epoch 28 Time cost 0 813 INFO root Epoch 28 Validation accuracy 0 689941 INFO root Epoch 29 Train accuracy 0 716797 INFO root Epoch 29 Time cost 0 781 INFO root Epoch 29 Validation accuracy 0 695312 INFO root Epoch 30 Train accuracy 0 717773 INFO root Epoch 30 Time cost 0 797 INFO root Epoch 30 Validation accuracy 0 705566 INFO root Epoch 31 Train accuracy 0 729492 INFO root Epoch 31 Time cost 0 800 INFO root Epoch 31 Validation accuracy 0 705566 INFO root Epoch 32 Train accuracy 0 727051 INFO root Epoch 32 Time cost 0 775 INFO root Epoch 32 Validation accuracy 0 709473 INFO root Epoch 33 Train accuracy 0 727539 INFO root Epoch 33 Time cost 0 814 INFO root Epoch 33 Validation accuracy 0 717773 INFO root Epoch 34 Train accuracy 0 733398 INFO root Epoch 34 Time cost 0 841 INFO root Epoch 34 Validation accuracy 0 722168 INFO root Epoch 35 Train accuracy 0 733887 INFO root Epoch 35 Time cost 0 791 INFO root Epoch 35 Validation accuracy 0 726074 INFO root Epoch 36 Train accuracy 0 733887 INFO root Epoch 36 Time cost 0 777 INFO root Epoch 36 Validation accuracy 0 731445 INFO root Epoch 37 Train accuracy 0 735352 INFO root Epoch 37 Time cost 0 811 INFO root Epoch 37 Validation accuracy 0 731445 INFO root Epoch 38 Train accuracy 0 735840 INFO root Epoch 38 Time cost 0 789 INFO root Epoch 38 Validation accuracy 0 733887 INFO root Epoch 39 Train accuracy 0 734375 INFO root Epoch 39 Time cost 0 792 INFO root Epoch 39 Validation accuracy 0 739258 INFO root Epoch 40 Train accuracy 0 744629 INFO root Epoch 40 Time cost 0 791 INFO root Epoch 40 Validation accuracy 0 744629 INFO root Epoch 41 Train accuracy 0 747070 INFO root Epoch 41 Time cost 0 815 INFO root Epoch 41 Validation accuracy 0 745117 INFO root Epoch 42 Train accuracy 0 746094 INFO root Epoch 42 Time cost 0 838 INFO root Epoch 42 Validation accuracy 0 746582 INFO root Epoch 43 Train accuracy 0 746094 INFO root Epoch 43 Time cost 0 776 INFO root Epoch 43 Validation accuracy 0 748535 INFO root Epoch 44 Train accuracy 0 749512 INFO root Epoch 44 Time cost 0 818 INFO root Epoch 44 Validation accuracy 0 750488 INFO root Epoch 45 Train accuracy 0 750488 INFO root Epoch 45 Time cost 0 817 INFO root Epoch 45 Validation accuracy 0 750977 INFO root Epoch 46 Train accuracy 0 747559 INFO root Epoch 46 Time cost 0 823 INFO root Epoch 46 Validation accuracy 0 753906 INFO root Epoch 47 Train accuracy 0 754883 INFO root Epoch 47 Time cost 0 806 INFO root Epoch 47 Validation accuracy 0 762695 INFO root Epoch 48 Train accuracy 0 757324 INFO root Epoch 48 Time cost 0 834 INFO root Epoch 48 Validation accuracy 0 753418 INFO root Epoch 49 Train accuracy 0 758301 INFO root Epoch 49 Time cost 0 806 INFO root Epoch 49 Validation accuracy 0 763184 INFO root Epoch 50 Train accuracy 0 762695 INFO root Epoch 50 Time cost 0 772 INFO root Epoch 50 Validation accuracy 0 771973 INFO root Epoch 51 Train accuracy 0 772461 INFO root Epoch 51 Time cost 0 795 INFO root Epoch 51 Validation accuracy 0 760742 INFO root Epoch 52 Train accuracy 0 766113 INFO root Epoch 52 Time cost 0 800 INFO root Epoch 52 Validation accuracy 0 772461 INFO root Epoch 53 Train accuracy 0 773438 INFO root Epoch 53 Time cost 0 798 INFO root Epoch 53 Validation accuracy 0 783691 INFO root Epoch 54 Train accuracy 0 777832 INFO root Epoch 54 Time cost 0 798 INFO root Epoch 54 Validation accuracy 0 780762 INFO root Epoch 55 Train accuracy 0 786133 INFO root Epoch 55 Time cost 0 834 INFO root Epoch 55 Validation accuracy 0 781738 INFO root Epoch 56 Train accuracy 0 782715 INFO root Epoch 56 Time cost 0 809 INFO root Epoch 56 Validation accuracy 0 785645 INFO root Epoch 57 Train accuracy 0 783691 INFO root Epoch 57 Time cost 0 824 INFO root Epoch 57 Validation accuracy 0 790039 INFO root Epoch 58 Train accuracy 0 791016 INFO root Epoch 58 Time cost 0 809 INFO root Epoch 58 Validation accuracy 0 794922 INFO root Epoch 59 Train accuracy 0 786133 INFO root Epoch 59 Time cost 0 826 INFO root Epoch 59 Validation accuracy 0 797852 INFO root Epoch 60 Train accuracy 0 795898 INFO root Epoch 60 Time cost 0 804 INFO root Epoch 60 Validation accuracy 0 801270 INFO root Epoch 61 Train accuracy 0 803711 INFO root Epoch 61 Time cost 0 785 INFO root Epoch 61 Validation accuracy 0 806641 INFO root Epoch 62 Train accuracy 0 809570 INFO root Epoch 62 Time cost 0 834 INFO root Epoch 62 Validation accuracy 0 808594 INFO root Epoch 63 Train accuracy 0 811035 INFO root Epoch 63 Time cost 0 803 INFO root Epoch 63 Validation accuracy 0 812012 INFO root Epoch 64 Train accuracy 0 814453 INFO root Epoch 64 Time cost 0 807 INFO root Epoch 64 Validation accuracy 0 819824 INFO root Epoch 65 Train accuracy 0 821777 INFO root Epoch 65 Time cost 0 784 INFO root Epoch 65 Validation accuracy 0 823242 INFO root Epoch 66 Train accuracy 0 829102 INFO root Epoch 66 Time cost 0 800 INFO root Epoch 66 Validation accuracy 0 830078 INFO root Epoch 67 Train accuracy 0 833984 INFO root Epoch 67 Time cost 0 811 INFO root Epoch 67 Validation accuracy 0 834961 INFO root Epoch 68 Train accuracy 0 839355 INFO root Epoch 68 Time cost 0 777 INFO root Epoch 68 Validation accuracy 0 844238 INFO root Epoch 69 Train accuracy 0 854492 INFO root Epoch 69 Time cost 0 818 INFO root Epoch 69 Validation accuracy 0 860352 INFO root Epoch 70 Train accuracy 0 863770 INFO root Epoch 70 Time cost 0 797 INFO root Epoch 70 Validation accuracy 0 875488 INFO root Epoch 71 Train accuracy 0 881836 INFO root Epoch 71 Time cost 0 807 INFO root Epoch 71 Validation accuracy 0 880371 INFO root Epoch 72 Train accuracy 0 889160 INFO root Epoch 72 Time cost 0 768 INFO root Epoch 72 Validation accuracy 0 890137 INFO root Epoch 73 Train accuracy 0 901855 INFO root Epoch 73 Time cost 0 793 INFO root Epoch 73 Validation accuracy 0 895996 INFO root Epoch 74 Train accuracy 0 906738 INFO root Epoch 74 Time cost 0 813 INFO root Epoch 74 Validation accuracy 0 905273 INFO root Epoch 75 Train accuracy 0 917480 INFO root Epoch 75 Time cost 0 800 INFO root Epoch 75 Validation accuracy 0 918945 INFO root Epoch 76 Train accuracy 0 921875 INFO root Epoch 76 Time cost 0 800 INFO root Epoch 76 Validation accuracy 0 928711 INFO root Epoch 77 Train accuracy 0 932129 INFO root Epoch 77 Time cost 0 799 INFO root Epoch 77 Validation accuracy 0 935547 INFO root Epoch 78 Train accuracy 0 939453 INFO root Epoch 78 Time cost 0 800 INFO root Epoch 78 Validation accuracy 0 944336 INFO root Epoch 79 Train accuracy 0 946289 INFO root Epoch 79 Time cost 0 787 INFO root Epoch 79 Validation accuracy 0 943848 INFO root Epoch 80 Train accuracy 0 950684 INFO root Epoch 80 Time cost 0 808 INFO root Epoch 80 Validation accuracy 0 946777 INFO root Epoch 81 Train accuracy 0 954590 INFO root Epoch 81 Time cost 0 792 INFO root Epoch 81 Validation accuracy 0 966797 INFO root Epoch 82 Train accuracy 0 962891 INFO root Epoch 82 Time cost 0 788 INFO root Epoch 82 Validation accuracy 0 970703 INFO root Epoch 83 Train accuracy 0 965332 INFO root Epoch 83 Time cost 0 769 INFO root Epoch 83 Validation accuracy 0 957031 INFO root Epoch 84 Train accuracy 0 966309 INFO root Epoch 84 Time cost 0 816 INFO root Epoch 84 Validation accuracy 0 949707 INFO root Epoch 85 Train accuracy 0 971680 INFO root Epoch 85 Time cost 0 799 INFO root Epoch 85 Validation accuracy 0 949219 INFO root Epoch 86 Train accuracy 0 983398 INFO root Epoch 86 Time cost 0 805 INFO root Epoch 86 Validation accuracy 0 979980 INFO root Epoch 87 Train accuracy 0 989746 INFO root Epoch 87 Time cost 0 800 INFO root Epoch 87 Validation accuracy 0 984863 INFO root Epoch 88 Train accuracy 0 990234 INFO root Epoch 88 Time cost 0 815 INFO root Epoch 88 Validation accuracy 0 987305 INFO root Epoch 89 Train accuracy 0 991211 INFO root Epoch 89 Time cost 0 816 INFO root Epoch 89 Validation accuracy 0 985840 INFO root Epoch 90 Train accuracy 0 994141 INFO root Epoch 90 Time cost 0 813 INFO root Epoch 90 Validation accuracy 0 990723 INFO root Epoch 91 Train accuracy 0 993652 INFO root Epoch 91 Time cost 0 813 INFO root Epoch 91 Validation accuracy 0 992676 INFO root Epoch 92 Train accuracy 0 996094 INFO root Epoch 92 Time cost 0 828 INFO root Epoch 92 Validation accuracy 0 992676 INFO root Epoch 93 Train accuracy 0 995605 INFO root Epoch 93 Time cost 0 813 INFO root Epoch 93 Validation accuracy 0 992188 INFO root Epoch 94 Train accuracy 0 996094 INFO root Epoch 94 Time cost 0 797 INFO root Epoch 94 Validation accuracy 0 992188 INFO root Epoch 95 Train accuracy 0 996582 INFO root Epoch 95 Time cost 0 813 INFO root Epoch 95 Validation accuracy 0 993164 INFO root Epoch 96 Train accuracy 0 996094 INFO root Epoch 96 Time cost 0 813 INFO root Epoch 96 Validation accuracy 0 992676 INFO root Epoch 97 Train accuracy 0 998047 INFO root Epoch 97 Time cost 0 813 INFO root Epoch 97 Validation accuracy 0 991699 INFO root Epoch 98 Train accuracy 0 994141 INFO root Epoch 98 Time cost 0 816 INFO root Epoch 98 Validation accuracy 0 991211 INFO root Epoch 99 Train accuracy 0 985840 INFO root Epoch 99 Time cost 0 813 this is the training and validation error log when using resnet validation and training set are identical INFO root Epoch 0 Train accuracy 0 570312 INFO root Epoch 0 Time cost 13 724 INFO root Epoch 0 Validation accuracy 0 509766 INFO root Epoch 1 Train accuracy 0 996582 INFO root Epoch 1 Time cost 10 845 INFO root Epoch 1 Validation accuracy 0 509766 INFO root Epoch 2 Train accuracy 1 000000 INFO root Epoch 2 Time cost 10 954 INFO root Epoch 2 Validation accuracy 0 509766 INFO root Epoch 3 Train accuracy 1 000000 INFO root Epoch 3 Time cost 10 994 INFO root Epoch 3 Validation accuracy 0 529297 INFO root Epoch 4 Train accuracy 1 000000 INFO root Epoch 4 Time cost 11 024 INFO root Epoch 4 Validation accuracy 0 753906 INFO root Epoch 5 Train accuracy 1 000000 INFO root Epoch 5 Time cost 11 079 INFO root Epoch 5 Validation accuracy 0 765137 INFO root Epoch 6 Train accuracy 1 000000 INFO root Epoch 6 Time cost 11 111 INFO root Epoch 6 Validation accuracy 0 763184 INFO root Epoch 7 Train accuracy 1 000000 INFO root Epoch 7 Time cost 11 180 INFO root Epoch 7 Validation accuracy 0 765137 INFO root Epoch 8 Train accuracy 1 000000 INFO root Epoch 8 Time cost 11 126 INFO root Epoch 8 Validation accuracy 0 762695 INFO root Epoch 9 Train accuracy 1 000000 INFO root Epoch 9 Time cost 11 174 INFO root Epoch 9 Validation accuracy 0 761719 INFO root Epoch 10 Train accuracy 1 000000 INFO root Epoch 10 Time cost 11 189 INFO root Epoch 10 Validation accuracy 0 759766 INFO root Epoch 11 Train accuracy 1 000000 INFO root Epoch 11 Time cost 11 230 INFO root Epoch 11 Validation accuracy 0 756836 INFO root Epoch 12 Train accuracy 1 000000 INFO root Epoch 12 Time cost 11 190 INFO root Epoch 12 Validation accuracy 0 755371 INFO root Epoch 13 Train accuracy 1 000000 INFO root Epoch 13 Time cost 11 176 INFO root Epoch 13 Validation accuracy 0 763184 INFO root Epoch 14 Train accuracy 1 000000 INFO root Epoch 14 Time cost 11 241 INFO root Epoch 14 Validation accuracy 0 758301 INFO root Epoch 15 Train accuracy 1 000000 INFO root Epoch 15 Time cost 11 275 INFO root Epoch 15 Validation accuracy 0 761230 INFO root Epoch 16 Train accuracy 1 000000 INFO root Epoch 16 Time cost 11 236 INFO root Epoch 16 Validation accuracy 0 758301 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx import numpy as np import matplotlib pyplot as plt from symbols import resnet fp16 as nettt import sys import time print istarted ' time strftime ' x X' startedd time time target dim 128 256 batcher 64 num images 2 512 20 was 20 epox 500 00 was 10 create images rand 0 rand with circles 1 xcoords np ones target dim 1 np arange target dim ycoords np transpose xcoords def add random circle da img xcoords ycoords tolerance 1 0 center x int np random rand target dim center y int np random rand target dim radius np random rand target dim 2 0 distances np power np power xcoords center x 2 np power ycoords center y 2 0 5 da img np where np abs distances radius tolerance 1 return da img nn train int num images 2 nn val int num images 2 image list np random rand nn train nn val target dim target dim llabel np round np random rand nn train nn val train indeces np arange nn train val indeces nn train np arange nn val train lbl llabel train indeces val lbl llabel val indeces train img np zeros nn train 1 target dim target dim for indexer in range nn train img now np random rand target dim target dim image list train indeces indexer img now add random circle img now xcoords ycoords if train lbl indexer 0 img now add random circle img now xcoords ycoords img now add random circle img now xcoords ycoords train img indexer 0 img now astype np float32 255 nn val len val indeces val img np zeros nn val 1 target dim target dim for indexer in range nn val img now np random rand target dim target dim image list val indeces indexer img now add random circle img now xcoords ycoords if val lbl indexer 0 img now add random circle img now xcoords ycoords val img indexer 0 img now astype np float32 255 normalizing by removing mean of training set meaner np mean train img print 'mean train before ' str np mean train img print 'mean val before ' str np mean val img train img meaner val img meaner test remove this val img train img val lbl train lbl train mx io NDArrayIter train img train lbl batcher shuffle True val mx io NDArrayIter val img val lbl batcher shuffle True print train img shape print train lbl shape print val img shape print val lbl shape da net nettt get symbol num classes 2 num layers 152 image shape '1 ' str target dim ' ' str target dim devices mx gpu import logging logging getLogger setLevel logging DEBUG batch end callbacks mx callback log train metric 5 2 40 model mx mod Module context devices symbol da net model fit train data train eval data val eval metric 'acc' num epoch epox optimizer 'adam' optimizer params 'learning rate' 1e 5 'wd' 1e 5 initializer mx init Xavier rnd type 'gaussian' factor type in magnitude 2 finishedd time time print 'finished ' time strftime ' x X' print isex ' str finishedd startedd,,"piiswrong,kevinthesun,kevinthesun,szha",2017-06-28 22:19:12,2017-10-08 23:53:09
IS,How to do Xavier initialization on specific convolutional layer,I define a convolution in caffe prototxt file layer name rpn conv 3x3 type Convolution bottom conv5 3 top rpn conv 3x3 param lr mult 1 0 param lr mult 2 0 convolution param num output 384 kernel size 3 pad 1 stride 1 weight filler type gaussian std 0 01 bias filler type constant value 0 So I initilize this layer in Mxnet arg params 'rpn conv 3x3 weight' mx random normal 0 0 01 shape arg shape dict 'rpn conv 3x3 weight' arg params 'rpn conv 3x3 bias' mx nd zeros shape arg shape dict 'rpn conv 3x3 bias' But when the weight filler is like type xavier I read the Mxnet initilize document but I can not figure it out how to initilize this layer by xavier as the above way,,"CNevd,szha",2017-07-08 09:35:58,2017-10-08 23:53:10
IS,does epoch size para NOT worked in model FeedForward interface,when train my data with model FeedForward I set epoch size 100 and set batch size 16 so I think the ONE EPOCHS should just contained 1600 samples though in real train dataset the sample number is very bigger than 1600 the main reason that my want control the save frequency When I set epoch end callback to save model I hope when 1600 samples after the model could be saved for one time but when I tested I found no save is called just like followed lr could be updated with my set but nothing saved so Does the epoch size is not worked at all in this interface that has been depreciated any help is very appreciated 2017 07 09 22 21 25 866 Node 0 Epoch 0 Batch 50 Speed 79 13 samples sec 2017 07 09 22 21 36 195 Node 0 Epoch 0 Batch 100 Speed 77 46 samples sec 2017 07 09 22 21 36 195 Node 0 Update 101 Change learning rate to 1 00000e 03 2017 07 09 22 21 46 506 Node 0 Epoch 0 Batch 150 Speed 77 58 samples sec 2017 07 09 22 21 56 875 Node 0 Epoch 0 Batch 200 Speed 77 16 samples sec a 2017 07 09 22 21 56 876 Node 0 Update 201 Change learning rate to 1 00000e 04,,szha,2017-07-09 14:38:31,2017-10-08 23:53:11
IS,All Keras 1 2 2 examples Illegal instruction core dump,Any time I call Keras model train method there is an illegal instruction error I can reproduce this behavior with any all the Keras examples Environment info Operating System Ubuntu 16 04 3 LTS GNU Linux 4 4 0 96 generic x86 64 Python 2 7 12 default Nov 19 2016 06 48 10 GCC 5 4 0 20160609 MXNet version 0 11 0 Keras 1 2 2 Error Message Minimum reproducible example steps to reproduce From your clone of the Keras repo examples directory run any of the example from the command line python addition rnn py for example,,"szha,szha,szha,szha,szha,szha,cjolivier01,szha,cjolivier01",2017-10-09 01:46:42,2017-10-09 02:30:43
PR,Add diagnose script in tools,Can be used to check python os cpu mxnet No dependency Tested ok on python2 3,,"zhreshold,szha,zhreshold",2017-10-08 23:47:53,2017-10-09 02:42:11
IS,compile error with multiple machines support,Environment info Operating System centos 7 1 MXNet version v0 9 Python versio v2 7 Error Message I want to train with multiple machines support so I compile with USE DIST KVSTORE 1 Because my machines can not access the network I do follow the issue 1018 to download the zeromq 4 1 4 and put it in the path mxnet ps lite But it also error as follows Could anyone help me,,"mli,mli",2017-02-08 03:10:18,2017-10-09 03:10:47
PR,shared module bug fix,,,formath,2017-10-09 05:15:47,2017-10-09 12:31:34
PR,Add barriers in kvstore init,Problem When init a new key value in kvstore dist only rank 0 worker push the new key value to the kvstore dist server If the pull in rank 1 faster than the push in rank 0 the parameter istored' in file 'kvstore dist server' will be none And this will make a mistake Solve Add barriers in init function All the other workers will wait the push in rank 0,,"solin319,eric-haibin-lin,solin319,eric-haibin-lin,solin319,eric-haibin-lin,solin319,eric-haibin-lin,eric-haibin-lin,solin319,eric-haibin-lin,eric-haibin-lin",2017-09-14 01:24:51,2017-10-09 15:56:49
PR,hotfix delete error,A glimpse of compiler complaining about this,,zhreshold,2017-10-06 21:49:17,2017-10-09 18:06:53
IS,Changes in C source code are not reflected in MXNet python run,Operating System Ubuntu 16 04 Package used Python R Scala Julia Python MXNet version 0 11 0 Python version and distribution 3 5 2 I built the mxnet package from source and I wanted to play with the C source code to see what happens I inserted the following statement inside src operator cudnn rnn inl h After rebuilding the Python package using the following commands make j 2 USE OPENCV 1 USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 cd python pip no cache dir install upgrade force reinstall e I run an RNN application that is using mxnet yet noticed that the above trace is not printed I also checked the mx file to make sure that it is indeed the correct source directory Could someone please tell me what steps I am missing here Thank you,,ZiyueHuang,2017-10-09 04:34:51,2017-10-09 18:57:26
PR,pr template,This is the template proposed in 8105 for future pull requests,,szha,2017-09-30 18:28:07,2017-10-09 20:06:01
IS,Proposal PR Template,What I d like to propose that we use PR template in MXNet The template should consist of a checklist for the required items for merging a PR e g lint and testing as well as the items of the change e g features Why It should serve two purposes It reminds both the contributor and reviewer committers of checking the prerequisites before merging It serves as a progress tracking tool on which PRs are ready for review and merging so that reviewer committers How Contributor should maintain the status of PR by checking unchecking items in the checklist Committer should uncheck items that hasn t been passed yet e g contributor checked lint but lint failed on CI and make comments on what more should be done By default committers review PRs that are shown to be complete in the PR list FAQ What if I as a contributor want to request for review of things working in progress Everyone can still do what everyone is doing now requesting reviewer to take a look by them What if some items in the checklist don t apply to my PR They can be deleted Template Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage Examples are either not affected by this change or have been fixed to be compatible with this change For user facing API changes API doc string has been updated Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"szha,bhavinthaker,szha",2017-09-29 20:37:24,2017-10-09 20:06:09
PR,Add export model,export serving API for model serving,,"kevinthesun,lupesko,lupesko,lupesko,lupesko,lupesko,lupesko,lupesko,lupesko,kevinthesun,kevinthesun,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,lupesko,lupesko,piiswrong,kevinthesun,kevinthesun",2017-09-27 21:42:42,2017-10-09 20:06:53
PR,Delete workspace before build,Description Delete workspace before build Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"gautamkmr,mbaijal,gautamkmr,indhub",2017-10-10 00:50:40,2017-10-10 01:20:13
IS,Could you put GPU version python package for windows to PyPi,I want to use mxnet python package on windows OS I have found a GPU version on PyPi but not CPU version Could you put one on the website,,szha,2017-10-09 05:52:11,2017-10-10 01:47:06
IS,Create cusolver handle failed,Hi guys recently I was bothered by converting caffemodel to mxnet so I updated my mxnet version from 0 10 01 to 0 11 01 But I found there was something wrong when using mx gpu to run calculation I tried some ways to work around but it looked useless Here is the report Environment info Operating System Ubuntu 14 04 Compiler g 4 8 Package used Python R Scala Julia Python MXNet version 0 11 01 If installed from source Yes MXNet commit hash git rev parse HEAD b5648a43955f7d05c0e53c1ab61a58bd402b4416 If you are using python package please provide numpy 1 13 1 Python version and distribution 2 7 6 Error Message What have you tried to solve it 1 I have searched on the internet but found nothing 2 I have re compiled the source it reported an error when linking to the GOMP after modifying ADD LDFLAGS in config mk compiling went well However this script was still been blocked 3 When I tried to back to version 0 10 01 all things went well 4 mx cpu mode is ok,,szha,2017-10-06 01:24:22,2017-10-10 03:50:24
PR,catch all with better format,Description Improve the tools diagnose py script Checklist Essentials x Changes are complete i e I finished coding on this PR x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Improve catch exception and feedback,,"zhreshold,szha,szha,szha,zhreshold",2017-10-10 04:51:49,2017-10-10 05:25:11
IS,MakeLoss with CustomOp is useless,The MakeLoss is useless when dealing with CustomOp python import numpy as np import mxnet as mx class Weightsoftmax mx operator CustomOp def init self super Weightsoftmax self init def forward self is train req in data out data aux x in data 0 asnumpy y np exp x x max axis 1 y y sum axis 1 self assign out data 0 req 0 mx nd array y def backward self req out grad in data out data in grad aux prob out data 0 asnumpy weight np array 1 1 f label in data 1 asnumpy f label f label astype np float32 label np concatenate 1 f label f label axis 1 out prob label out 0 weight 0 out 1 weight 1 out out shape 2 out shape 3 self assign in grad 0 req 0 mx nd array out operator register weightsoftmax class WeightsoftmaxProp mx operator CustomOpProp def init self super WeightsoftmaxProp self init need top grad False def list arguments self return 'indata' 'label' def list outputs self return 'output' def infer shape self in shapes data shape in shapes 0 label shape in shapes 1 output shape data shape return data shape label shape output shape def create operator self ctx shapes dtypes return Weightsoftmax x mx nd array 1 2 3 4 5 6 7 8 9 2 3 4 5 2 3 3 4 2 l mx nd array 1 0 1 0 0 1 1 1 0 vm mx sym Variable 'm' vn mx sym Variable 'n' dx mx nd empty x shape out mx symbol Custom indata vm label vn op type 'weightsoftmax' name loss t out mx symbol MakeLoss name myloss data out grad scale 0 normalization 'null' exec out bind ctx mx cpu args 'm' x 'n' l args grad 'm' dx exec forward print exec outputs 0 asnumpy exec backward out grads mx nd ones like dx print exec grad arrays 2 68941432e 01 2 68941432e 01 2 68941432e 01 2 68941432e 01 9 52574134e 01 9 52574134e 01 9 82013762e 01 9 82013762e 01 9 99089003e 01 7 31058598e 01 7 31058598e 01 7 31058598e 01 7 31058598e 01 4 74258736e 02 4 74258736e 02 1 79862101e 02 1 79862101e 02 9 11051233e 04 2 98823807e 02 8 12287331e 02 2 98823807e 02 8 12287331e 02 5 26954047e 03 1 05841570e 01 1 09112643e 01 1 09112643e 01 1 01221929e 04 2 98823789e 02 8 12287331e 02 2 98823789e 02 8 12287331e 02 5 26954141e 03 1 05841570e 01 1 09112643e 01 1 09112643e 01 1 01227917e 04 NDArray 1x2x3x3 0 None,,"chinakook,zhreshold,eric-haibin-lin,chinakook,chinakook",2017-09-30 04:17:24,2017-10-10 05:30:10
PR,Dummy PR to test Continuous Integration builds,Description This is a dummy PR to test CI builds Please do not review merge or do any other action on this PR I will close the PR once the tests are done,,indhub,2017-10-09 23:08:28,2017-10-10 05:49:37
IS,How to define custom callback function,I want to define my own callback function that will be used at every training epoch not just saving the model is parameters Does anyone has any idea about this Any help will be appreciated,,"ZiyueHuang,eric-haibin-lin",2017-10-10 02:56:01,2017-10-10 17:09:19
PR,Menu structure changes,Removed Tutorials link from the sub menu Fixed Community sub menu as per requirement,,"thinksanky,piiswrong,piiswrong,thinksanky,thinksanky,szha,piiswrong",2017-09-30 05:30:10,2017-10-10 18:17:05
PR,Zeroes CSR still needs a valid row pointer array,Fix for Looks like problem was introduced here In function void FillZerosCsrImpl mshadow Stream xpu s NDArray dst Why CSR matrix even if all zeros must have m 1 items in its row pointer csr IndPtr array Current implementation leaves all aux arrays empty While I was here added a couple more assertions and also reduced memory allocation for situations where lhs and rhs are the same array Added unit test,,"cjolivier01,eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,eric-haibin-lin,piiswrong,cjolivier01,eric-haibin-lin,eric-haibin-lin,cjolivier01,cjolivier01,eric-haibin-lin,cjolivier01,cjolivier01,eric-haibin-lin,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,reminisce,cjolivier01,cjolivier01,cjolivier01,reminisce,cjolivier01,eric-haibin-lin,cjolivier01",2017-09-18 16:50:21,2017-10-10 18:51:57
PR,Gpu samplers,Provide a generic implementation for all random samplers that can run on cpu and gpu Remarks This will fail until PR297 is merged into mshadow The design pattern adopted for GPU is to generate as many random seeds as samples should be drawn and then draw each sample using its own randstate This is a pattern that is considered to be ok according to Nvidia documentation There is a theoretical chance that the generated random sequences by the individual randstates are somehow correlated but Nvidia claims that they never observed such an effect and just want to mention that they can not prove that it can never happen theoretically So in practice this should generated high quality uncorrelated samples Above design pattern is also ok w r t setup time of the randstates as we use consistently sequence 0 of the randstate so the setup does not have to skip through multiple sequences when initializing Again according to Nvidia I decided to use a consistent design pattern even for uniform normal distributions as this makes the code more consistent The design pattern allows easy implementation of all types of rejection sampling methods so we should be able to add other distributions easily whenever we need them Above pattern will automatically enable multi threaded sampling on CPU if openMP is on The sampling methods for exponential gamma Poisson negative Binomial are all standard and mostly the same as STL uses The rejection method for big lambdas for the Poisson distribution is slightly different but also theoretically sound reference in the code There is a bit of a problem for the case of fp16 Basically there do not exist any samplers that natively work on this limited precision I do not know why we recently added fp16 as a valid output for random sampling operators We should have never done this but instead insist that an explicit casting operator must be used that then can also handle all issues of overflow underflow in a centralized way Anyway somehow this got added which causes the problem that we have to convert fp32 samples to fp16 and as these are samples from a distribution there is always the chance of an overflow Ugly This was already the case in the prior implementation so it is not newly introduced here And IMO we should not solve it within the samplers but instead by a separate casting operator Adding additional outputs functionalities to the random sampler such as re parametrization CDF etc is not part of this PR We should add it as a separate step,,"asmushetzel,piiswrong,asmushetzel,asmushetzel,piiswrong,asmushetzel,piiswrong",2017-10-08 16:08:07,2017-10-10 19:32:49
PR,Fix unused variable warnings,Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-10-10 16:39:16,2017-10-10 22:45:59
PR,Fix segfaults and tests for distributed kvstore,Description Fixes segfaults in kvstore dist for row sparse and fixes tests in dist sync kvstore haibin lin and I worked on this Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage Changes 1 Move copying of indices to happen before ZPull because after ZPull we release the locks and the later function do not see actual variable 2 nullptr was not handled for offsets variable 3 fixes tests which were broken when WaitToWrite was removed in Also added comments describing how they are no guarantees for immediate consecutive pushes 4 Removed the usage of reference in BroadcastRowSparse to keep it consistent with style of Broadcast Comments When pushes happen one after the other without a pull in between there is no ordering guarantee we provide now that the WaitToWrite was removed in 8116 for performance reasons The second push may start and finish before first push finishes Please comment if we missed something or if you have any suggestions or if any clarifications are needed,,"rahul003,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,rahul003,rahul003,szha,rahul003",2017-10-10 16:19:04,2017-10-11 02:42:37
PR,style transfer example,adding example for style transfer,,"zhanghang1989,szha,szha,szha,szha,szha,szha,zhreshold,piiswrong,zhanghang1989,szha,zhanghang1989,zhanghang1989,piiswrong,zhanghang1989,szha,zhanghang1989,piiswrong,zhanghang1989,piiswrong",2017-08-23 18:32:24,2017-10-11 02:44:59
PR,Enable MKL support for full pooling,,,"adstraw,piiswrong,adstraw,adstraw,piiswrong,adstraw",2017-09-08 16:50:59,2017-10-11 02:45:56
PR,Get bz2 data fix,Added documentation for get bz2 data function Also fixed the chdir issue,,"anirudh2290,zhreshold,anirudh2290",2017-09-25 07:35:21,2017-10-11 02:48:55
PR,update issue template,,,"szha,szha,yzhliu,szha,thirdwing,szha",2017-09-28 20:24:31,2017-10-11 04:48:05
IS,can mxnet support incremental training,go on trainning a pre trained modle,,"formath,zihaolucky,zihaolucky,zihaolucky",2017-06-12 03:10:10,2017-10-11 05:54:38
IS,Ask MKL2017 and MKL DNN,It seem Mxnet with MKL2017 config use MKL DNN api for ML and not MKL2017 api prepare mkl sh L87 L87 on the intel doc both are similar but different Intel MKL DNN includes functionality similar to Intel R Math Kernel Library Intel R MKL 2017 but is not API compatible We are investigating how to unify the APIs in future Intel MKL releases It is a bit confusing,,eric-haibin-lin,2017-10-10 09:51:10,2017-10-11 06:19:14
IS,AttributeError using gluon HybridBlock,,,"tornadomeet,ZiyueHuang,tornadomeet",2017-10-11 07:51:42,2017-10-11 08:01:36
IS,wrong shape after using concat when using HybridBlock,,,"tornadomeet,ZiyueHuang,tornadomeet",2017-10-11 08:49:14,2017-10-11 09:15:11
IS,the order of multiple outputs with group symbol is wrong,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description As title says Environment info Required Python Info 'Version ' '2 7 12' 'Compiler ' 'GCC 5 4 0 20160609' 'Build ' wouldefault' 'Nov 19 2016 06 48 10' 'Arch ' '64bit' 'ELF' Pip Info 'Version ' '9 0 1' 'Directory ' ' usr local lib python2 7 dist packages pip' MXNet Info 'Version ' '0 11 0' 'Directory ' ' root local lib python2 7 site packages mxnet 0 11 0 py2 7 egg mxnet' Traceback most recent call last File diagnose py line 108 in check mxnet with open commit hash 'r' as f IOError Errno 2 No such file or directory ' root local lib python2 7 site packages mxnet 0 11 0 py2 7 egg mxnet COMMIT HASH' System Info 'Platform ' 'Linux 4 2 0 27 generic x86 64 with Ubuntu 16 04 xenial' isystem ' 'Linux' 'node ' 'cc569e4d80be' arelease ' '4 2 0 27 generic' haversion ' ' 32 14 04 1 Ubuntu SMP Fri Jan 22 15 32 26 UTC 2016' Hardware Info 'machine ' 'x86 64' 'processor ' 'x86 64' Architecture x86 64 CPU op mode s 32 bit 64 bit Byte Order Little Endian CPU s 40 On line CPU s list 0 39 Thread s per core 2 Core s per socket 10 Socket s 2 NUMA node s 2 Vendor ID GenuineIntel CPU family 6 Model 63 Model name Intel R Xeon R CPU E5 2650 v3 2 30GHz Stepping 2 CPU MHz 2099 558 CPU max MHz 3000 0000 CPU min MHz 1200 0000 BogoMIPS 4600 95 Virtualization VT x L1d cache 32K L1i cache 32K L2 cache 256K L3 cache 25600K NUMA node0 CPU s 0 9 20 29 NUMA node1 CPU s 10 19 30 39 Flags fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant tsc arch perfmon pebs bts rep good nopl xtopology nonstop tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4 1 sse4 2 x2apic movbe popcnt tsc deadline timer aes xsave avx f16c r drand lahf lm abm ida arat epb pln pts dtherm tpr shadow vnmi flexpriority ept vpid fsgsbase tsc adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm llc cqm occup llc Network Test Setting timeout 10 Timing for MXNet DNS 0 2300 sec LOAD 1 8641 sec Timing for PYPI DNS 0 2203 sec LOAD 1 2060 sec Timing for FashionMNIST DNS 0 2876 sec LOAD 1 2119 sec Timing for Conda DNS 0 2487 sec LOAD 1 1629 sec Timing for Gluon Tutorial en DNS 0 2410 sec LOAD 0 7284 sec Timing for Gluon Tutorial cn DNS 0 2394 sec LOAD 1 3999 sec Package used Python R Scala Julia Python Build info Required if built from source Compiler gcc clang mingw visual studio gcc MXNet commit hash 0ae1503687f803b4aae7ab1dd894abc54f6c9ed8 Build config Paste the content of config mk or the build command Error Message Paste the complete error message including stack trace Minimum reproducible example Steps to reproduce Paste the commands you ran that produced the error 1 Just run the script and bug should be reproduced What have you tried to solve it,,,2017-10-11 05:54:09,2017-10-11 11:08:23
PR,fix typo of gluon hybird tutorial,,,tornadomeet,2017-10-11 08:17:27,2017-10-11 17:37:39
IS,Module bind Error When Calling Scala API from Java,Environment info Operating System RHEL 64bit Compiler JDK 1 8 Package used Python R Scala Julia Scala MXNet version 0 10 Error Message What have you tried to solve it 1 Giving the label a name label or softmax label has no effect same stack trace 2 Provide an empty array as a label identical shape as the data code crashes out earlier due to heap space running out also doing this is non optimal due to the resource usage unless absolutely necessary 3 Tried switching the order of the NDArray is shape arguments no effect I have a suspicion that it is tied to how I am setting up the iterator but I am not certain if anyone has run into this issue or can identify a mistake in what I have written then I would greatly appreciate it,,"Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G,Ldpe2G",2017-07-20 21:43:39,2017-10-11 20:29:42
PR,Solve problem in kvstore init,Resubmit as 7893 Add a init test in file dist sync kvstore py haibin lin,,"solin319,eric-haibin-lin,eric-haibin-lin,rahul003,solin319,eric-haibin-lin",2017-10-09 06:15:07,2017-10-11 21:44:53
PR,Perl Fix sporadically failing test increase number of training epochs,Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,sergeykolychev,2017-10-11 05:18:01,2017-10-11 21:50:54
PR,Unit test framework for C timing of generic operators and activation improvement,Description NOTE This also contains changes from PR 'optimize basic omp' Unit test framework for C timing of generic operators Activation operator converted to Kernel from MShadow Performance improves see below especially in backward pass OLD Timing 50 iterations of 10 calls shape 1 1 28 28 Fully connected Timing Forward 56 215 ms avg 0 11243 ms X 500 passes Fully connected Timing Backward 69 322 ms avg 0 138644 ms X 500 passes Timing 50 iterations of 10 calls shape 1 3 28 28 Fully connected Timing Forward 24 187 ms avg 0 048374 ms X 500 passes Fully connected Timing Backward 33 798 ms avg 0 067596 ms X 500 passes Timing 50 iterations of 10 calls shape 50 1 18 32 Fully connected Timing Forward 98 219 ms avg 0 196438 ms X 500 passes Fully connected Timing Backward 35 933 ms avg 0 071866 ms X 500 passes Timing 50 iterations of 10 calls shape 50 3 18 32 Fully connected Timing Forward 346 737 ms avg 0 693474 ms X 500 passes Fully connected Timing Backward 60 141 ms avg 0 120282 ms X 500 passes Timing 50 iterations of 10 calls shape 20 3 128 128 Fully connected Timing Forward 3607 84 ms avg 7 21567 ms X 500 passes Fully connected Timing Backward 387 725 ms avg 0 77545 ms X 500 passes NEW Timing 50 iterations of 10 calls shape 1 1 28 28 Fully connected Timing Forward 44 111 ms avg 0 088222 ms X 500 passes Fully connected Timing Backward 0 84 ms avg 0 00168 ms X 500 passes Timing 50 iterations of 10 calls shape 1 3 28 28 Fully connected Timing Forward 16 093 ms avg 0 032186 ms X 500 passes Fully connected Timing Backward 1 419 ms avg 0 002838 ms X 500 passes Timing 50 iterations of 10 calls shape 50 1 18 32 Fully connected Timing Forward 137 882 ms avg 0 275764 ms X 500 passes Fully connected Timing Backward 38 945 ms avg 0 07789 ms X 500 passes Timing 50 iterations of 10 calls shape 50 3 18 32 Fully connected Timing Forward 340 161 ms avg 0 680322 ms X 500 passes Fully connected Timing Backward 68 256 ms avg 0 136512 ms X 500 passes Timing 50 iterations of 10 calls shape 20 3 128 128 Fully connected Timing Forward 3465 03 ms avg 6 93005 ms X 500 passes Fully connected Timing Backward 322 912 ms avg 0 645824 ms X 500 passes Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,cjolivier01",2017-10-11 22:10:58,2017-10-11 22:18:59
IS,How to use softlabel in MXNET,Hi Does any one know how to use softlabel such as 0 7 0 0 0 3 0 0 0 rather than one hot label 1 0 0 0 0 0 in image classification task I need set soft label for distilling the dark knowledge during training But I have no ideal how to do it Many thanks,,"Ldpe2G,chinakook",2017-09-25 09:04:32,2017-10-12 01:56:29
IS,SSD Training failing with different number of classes,OS Ubuntu 16 04 MXNet version 0 11 0 Python 3 5 2 I am trying to train SSD with different number of classes not 20 but it is failing to train However if I change the number of classes to 20 for the same set of data input it works I have tried setting number of classes from 10 to 21 Let me know if any more details are required,,"Prasad9,zhreshold,Prasad9",2017-09-28 06:44:00,2017-10-12 03:24:57
IS,module fit does not work for image segmentation,Hello I'm having troubles using mxnet to do image segmentation Here is a small sample source where method module fit does nothing and prints nothing it seems as learning never happened without any error message When I run this code it immediately prints end Function module fit does nothing In the example I have data as 32 randomly generated 2x2 images with values 0 9 and the result of segmentation is a 2x2 image with values 0 1 For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Win7 64 bit Compiler VS 2015 Package used Python R Scala Julia Python MXNet version 0 11 Or if installed from source yes from 0 11 branch and master branch same result MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Anaconda 5 0 0 python 3 5 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace no error message no warning Minimum reproducible example if you are using your own code please provide a short script that reproduces the error import mxnet as mx import numpy as np def main input x mx sym Variable data c input x c mx sym Convolution data c num filter 4 kernel 3 3 pad 1 1 c mx sym Activation data c act type relu c mx sym Convolution data c num filter 1 kernel 1 1 c mx sym LogisticRegressionOutput data c name softmax c mx sym SoftmaxOutput data c name softmax symbol c train x np random randint 10 size 32 1 2 2 train y train x 0 val x np random randint 10 size 8 1 2 2 val y val x 0 batch size 2 num epoch 10 it train mx io NDArrayIter train x train y batch size batch size shuffle True it val mx io NDArrayIter val x val y batch size batch size module mx mod Module symbol symbol context mx cpu module fit it train it val num epoch num epoch print end if name ' main ' main Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"Soonhwan-Kwon,Soonhwan-Kwon",2017-10-10 12:56:57,2017-10-12 06:30:24
IS,CMake mxnet with cpp package compile Failed on MacOS,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System MacOS Compiler gcc clang Package used Python R Scala Julia MXNet version 0 11r3 Error Message What have you tried to solve it build mxnet without cpp package succeed,,,2017-09-06 12:35:04,2017-10-12 13:48:09
PR,Fix pip test install libgfortran3 on docker,Description The nightly test on Internal Jenkins called pip test fails because the docker does not have the libfortran so 3 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes In this PR I have modified this dockerfile to install libfortran3 which is required for pip testing I manually reproduced this issue on a host machine and verified that this change fixes the issue Comments I tried some other suggestions available online like 'export' but they do not fix the issue the so does not exist on the docker at all While I have done some manual testing I will monitor it to make sure it works once merged,,"mbaijal,mbaijal,szha",2017-10-12 00:27:08,2017-10-12 17:38:22
PR,SSD Example Fixed training bug,Fixed import error bugs Fixed character encoding issue Converted Python 2 to Python 3,,"Prasad9,zhreshold,szha,zhreshold,Prasad9,szha,zhreshold,Prasad9,piiswrong,zhreshold,zhreshold",2017-09-20 09:52:38,2017-10-12 17:42:48
PR,DONT MERGE Test empty commit,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,gautamkmr,2017-10-12 04:57:40,2017-10-12 20:02:56
IS,Writing a customer trainer in MxNet that tracks uncertainty in the weights,I want to implement Bayes by Backpropagation BbB in MxNet but two notable features of this algorithm cause me some confusion For one when I look at example optimizers L558 I note that they are given the gradient already so I can specify the weights I want used for that given gradient computation In BbB I need to add a noise term to the weights when I compute the gradient So I would need to control the weights used to compute the gradient Second I need to track both the mean weights and the standard deviation so I would need to attach more than just the gradient What are the best approaches for me to solve these problems,,,2017-10-10 21:28:28,2017-10-12 20:20:02
IS,test loss t fails sporadically and causes builds to fail,test loss fails sporadically and causes builds to fail test loss t was added in PR 8015 Links to build failures Environment info Operating System Ubuntu MXNet version Commit hashes Build 481 759f5091eb5feae68ffc5914d6cf8798dd2af6d7 Build 483 92e5198d659ab8f69e22bbbb9c54a36e5a25a2fa Build 490 ee9771580765393b194172bab169dd67ac575209 Error Message Steps to reproduce Run build and unit test,,"indhub,sergeykolychev,sergeykolychev,indhub",2017-10-11 01:09:36,2017-10-12 22:24:37
PR,Fixing the Caught error,Description Fixing the Caught error Checklist Essentials x Changes are complete i e I finished coding on this PR Changes x Print actual error when error happens in Jenkinsfile Comments This change is on CI infra only,,"gautamkmr,szha,piiswrong,gautamkmr,szha",2017-10-10 03:10:06,2017-10-13 01:10:30
IS,how to import a trained embedding weight into a nlp model,,,szha,2017-10-12 09:01:25,2017-10-13 02:20:07
IS,how to train a model using a imported embedding weight and ensure the embedding weight be not updated during the training,,,,2017-10-11 06:09:28,2017-10-13 02:20:25
IS,check numeric grad with FullyConnected fails,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System DL AMI Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 861e929cec8fa8fbab06884d9605debb74cd7217 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 run the above code 2 3 What have you tried to solve it 1 2 3,,"eric-haibin-lin,ptrendx",2017-08-02 21:09:50,2017-10-13 03:22:13
IS,csr binary operator bug,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System DeepLearning Ubuntu AMI Compiler Package used Python R Scala Julia Python MXNet version Or if installed from source MXNet commit hash git rev parse HEAD 9d56db66e2e94a8a3d9bf020b9682e91e7baf203 If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 python 2 import mxnet as mx a mx nd sparse zeros 'csr' 1 1 b mx nd elemwise add a a What have you tried to solve it 1 2 3,,"eric-haibin-lin,cjolivier01,cjolivier01",2017-09-16 23:03:00,2017-10-13 03:22:38
IS,why the Validation accuracy drops so much when I use mx mod BucketingModule to redefine a symbol model,this is a very important problem,,,2017-10-13 01:34:29,2017-10-13 03:26:40
IS,Need help numpy array to mxnet ndarray is too slow,numpy array to mxnet ndarray is too slow t3 0 293313980103 sometimes i get 1 5s Need help thanks It slows down my training,,madjam,2017-09-14 12:15:37,2017-10-13 06:16:49
PR,Temporarily disable some unit tests to fix the build,Description Temporarily disable the following unit tests that have been causing build failures test rms This can be re enabled once is fixed test autograd save memory This can be re enabled once is fixed Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"indhub,gautamkmr,indhub",2017-10-13 07:07:51,2017-10-13 10:40:20
PR,WIP Julia CI build,Ref issuecomment 328028136 I can help for adding Julia CI build Any tips for how to start,,"szha,szha,szha,gautamkmr,indhub,gautamkmr,indhub,szha,mbaijal,marcoabreu,marcoabreu",2017-10-09 17:46:43,2017-10-13 11:17:01
PR,Fixed Issue 7750 Update README md File,Fix Issue 7750 Update README md File,,"mbaijal,mbaijal,mbaijal",2017-10-02 19:58:54,2017-10-13 16:48:03
PR,add export to gluon,Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"piiswrong,szha",2017-10-11 00:17:45,2017-10-13 17:34:17
PR,ReleaseFeedback License Files,Description I have made as many changes as possible from this issue Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Refer to the Issue 7749 x Fixed 2b Added MIT License x Fixed 2a Added BSD License x Fixed 2d Added Boost Software License x Fixed 2f Fixed the mentioned file I checked the rest of the files that have the keyword All rights reserved 28 in total and ensured that no file under ASF has this phrase x Checked 2g Verified that this is not incorrect in files in the MXNet repo I have not corrected the submodules where the error exists in 53 files Comments There are some more comments in the mentioned issue which are currently under discussion and blocked These will be part of a new PR if necessary so this one can be merged,,"mbaijal,bhavinthaker,mbaijal",2017-10-13 00:12:16,2017-10-13 18:33:57
PR,Sequential aug,Description Add SequentialAug to mx image to allow composed augmenters in sequence Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Comments A simple addition to allow users to apply augmenters one by one,,"zhreshold,zhreshold",2017-10-12 18:53:12,2017-10-13 19:48:38
PR,Basic CPU Kernel OMP selection based upon whether GPU has been used,First iteration for performance enhancements If GPU is not used then use OMP for running CPU kernels GPU usage is triggerred by ThreadedEngine or NaiveEngine Currently the intended net effect of this PR is to allow for normal OMP behavior for GPU builds when the GPU is not used More robust OMP thread management is forthcoming,,"cjolivier01,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,szha,szha,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-09-11 22:16:38,2017-10-13 19:55:57
PR,Multiplatform docker based builds,Set of docker files tool to build MXNet for different architectures and collect the artifacts,,"larroy,piiswrong,sergeykolychev,larroy,larroy,larroy,piiswrong,larroy,piiswrong,mbaijal,larroy,larroy,larroy,larroy,piiswrong,larroy,larroy,sergeykolychev",2017-09-07 17:08:11,2017-10-13 20:35:20
PR,Operators for sum csr axis 0 and sum csr axis 1,Adds operator sum csr axis 0 dense and sum csr axis 1 Tried 128 100M shape csr matrix and was able to perform sum along axis 0 and 1 Density is 0 1 Allocation fails for 128 100M for dense NDarray for 1M and 10M the speedup for sparse operator is 300X for 1M and 1200X for 10M for density of 0 1 uniform distribution Completes one TODO in 8168 haibin lin,,"anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,reminisce,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,anirudh2290,reminisce,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290",2017-10-08 01:48:48,2017-10-13 21:08:14
PR,Fix arange,Description Fix the bug reported in Checklist Essentials Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x When the start stop and step are floats the current code will calculate the output size by casting them to double in order to have high precision x Update MShadow to the latest version,,"sxjscience,piiswrong,piiswrong,piiswrong,piiswrong",2017-10-13 16:10:04,2017-10-14 00:43:32
PR,Proper float64 support for unary elemwise operators mshadow op h,Modified mshadow op h s t elemwise unary operators support dtype float64 properly Previous code does everything in float32 Had to switch from math h to cmath and use std prefix in order to avoid naming clashes,,"mseeger,piiswrong,piiswrong,mseeger,piiswrong,mseeger,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,mseeger,mseeger,mseeger,larroy,mseeger,mseeger,mseeger,mseeger,mseeger,mseeger,mseeger,szha,mseeger,asmushetzel,mseeger,piiswrong,asmushetzel,mseeger,piiswrong,mseeger,mseeger,piiswrong,mseeger",2017-09-24 12:12:35,2017-10-14 00:44:35
PR,Prepare for 0 12 0 Release,Description Bump up version to 0 12 0 Update NEWS for v0 12 0 and link to release notes Checklist Essentials Passed code style checking make lint x Changes are complete i e I finished coding on this PR x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Updated all the necessary files with version 0 12 0 Comments Coming up in In another PR before tagging the RC Updated README md Updated NEWS md if needed,,"mbaijal,piiswrong,piiswrong,piiswrong,mbaijal,mbaijal,mbaijal,mbaijal,eric-haibin-lin,mbaijal,eric-haibin-lin,mbaijal,eric-haibin-lin,eric-haibin-lin,mbaijal,mbaijal,eric-haibin-lin,mbaijal,eric-haibin-lin,eric-haibin-lin",2017-10-13 19:16:35,2017-10-14 00:44:55
PR,Feature branch gluon,Description This PR is for website documentation Adding a new Gluon Menu which has About and The Straight Dope menu Update Community Menu tab which now includes Contribute Community and Powered By sub menu items Changed the main page text from MXNet to Apache MXNet Updated Copyright statement in Footer Added a new Gluon index page for About content The Straight Dope now links to gluon mxnet io Renamed Learn tab to Tutorials Removed Tutorials sub menu item under Docs menu Checklist Essentials Passed code style checking make lint x Changes are complete i e I finished coding on this PR All changes have test coverage x For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"thinksanky,sandeep-krishnamurthy,thinksanky,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,thinksanky,szha,thinksanky,piiswrong,thinksanky,thinksanky",2017-10-10 18:04:11,2017-10-14 00:45:20
PR,OMP num threads 0 1,Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,mbaijal,eric-haibin-lin,mbaijal",2017-10-13 23:09:10,2017-10-14 00:45:38
PR,scipy style sparse ndarray constructor and misc sparse fixes,Description fix wrong usage of rowsparsepull in sparse LR example update csr matrix row sparse array interface like scipy misc doc updates Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"eric-haibin-lin,piiswrong,piiswrong,eric-haibin-lin,szha,szha,eric-haibin-lin,eric-haibin-lin",2017-10-13 18:57:20,2017-10-14 02:10:02
PR,GPerftools update also include include mxnet h as sources for CLion,Description GPerftools update also include include mxnet h as sources for CLion Checklist Essentials x Passed code style checking make lint Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,piiswrong",2017-10-12 00:19:16,2017-10-14 02:24:39
IS,update examples of mxnet,Hello some examples use deprecated api Can you update them Thanks for example multi task,,,2017-10-13 09:40:53,2017-10-14 02:45:36
IS,should add row sparse push for kvstore,hi all haibin lin I noticed that you have add a function for kvstore named 'row sparse pull' why not to add a 'row sparse push' it is very like my 'partial weight' which is in L241 my partial weights can support distribution training,,"eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-08-23 02:40:31,2017-10-14 05:19:28
PR,Update tests for log2 log10,This PR fixes a failing test introduced by 8012,,eric-haibin-lin,2017-10-14 07:29:47,2017-10-14 07:31:47
IS,Trying to use random in hybrid blocks TypeError type class 'mxnet gluon parameter Parameter' not supported,I am trying to include noise in the weights by creating a new layer based on a Dense NN block I believe the random numbers are causing an error Is there a way to use random numbers in a Hybrid block,,,2017-10-13 18:26:18,2017-10-14 15:28:07
PR,Fix arange,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,piiswrong,2017-10-14 00:37:48,2017-10-14 20:07:46
PR,WIP New faster version of the RecordIO iterator,Does not yet support shuffle ignores that setting,,"ptrendx,zhreshold,ptrendx,piiswrong,ptrendx,piiswrong,szha,zhreshold,ptrendx,szha,ptrendx,ptrendx,ptrendx,eric-haibin-lin,ptrendx",2017-07-21 21:52:28,2017-10-14 20:09:21
PR,Dev,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"piiswrong,eric-haibin-lin,mseeger,mseeger,mseeger,piiswrong,piiswrong",2017-10-14 02:30:00,2017-10-14 20:15:34
IS,mxnet example rcnn rcnn symbol symbol vgg py Error mx symbol contrib,I just overcome an issue in this file Trying to run the script with bash command return error line 293 in get vgg test rois mx symbol contrib Proposal AttributeError 'module' object has no attribute 'contrib' I think it should be fixed into mx contrib symbol Fixed this and works fine Best,,zhreshold,2017-10-13 22:46:20,2017-10-14 21:11:29
IS,How to implement logistical regression with unknown feature count,Hi After reading through the tutorials howto architechture I still can not find solution of my problem My problem can be simplified as follow 1 I want to implement a logistical regresion for online learning it seems give the answer 2 I can not get the feature count because the feature unseen before will be produced Thx Jing,,"formath,eric-haibin-lin",2017-07-18 09:47:35,2017-10-14 21:59:41
PR,Dev,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,piiswrong,2017-10-14 20:15:26,2017-10-14 23:19:54
PR,Temporarily disable 'test row sparse pull' on GPU,Description Temporarily disable 'test row sparse pull' on GPU This has been causing intermittent build failures Can be enabled back after is resolved Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"indhub,eric-haibin-lin,indhub",2017-10-13 22:13:55,2017-10-15 02:44:33
IS,When does kWriteInplace happen,I am trying to make an OP that do inplace in imperative mode For instance,,eric-haibin-lin,2017-08-09 18:21:12,2017-10-15 02:45:30
IS,a problem in distribute training,Environment info Operating System ubuntu14 04 Package used Python R Scala Julia Python MXNet version 0 10 1 Or if installed from source install from source If you are using python package please provide Python version and distribution python 2 7 Problem Message 1 When I train a model in distribute computers in MXNet 0 10 1 I find it slower than train it in MXNet 0 9 4 2 I use profile to analyze the program In MXnet 0 10 1 I find the grad arrays were pushed to kv store after all the backward layers compute finished So the compute can not cover the time of data communicate 3 In MXNet 0 9 4 the grad array in each convolution layer will push to kv store directly after this layers' backward compute Why do I meet this problem and how to resolve it,,"solin319,solin319,eric-haibin-lin,solin319,eric-haibin-lin",2017-07-10 07:34:44,2017-10-15 02:52:14
IS,can row sparse array support other sparse store,I just refer to RowSparseNDArray which is just only fit for all 0 in one row array 1 2 3 0 0 0 4 0 5 0 0 0 array 1 2 3 4 0 5 dtype float32 0 0 0 dtype float32 And can mxnet support the matrix like that 1 2 1 2 3 1 3 4 5 every row has different colums If not i still need to increase storage in order to keep fixed length,,"reminisce,reminisce,eric-haibin-lin",2017-10-13 04:09:29,2017-10-15 02:59:41
PR,Proper float64 support for unary binary elemwise operators,Wrapper for math h functions Rewrote mshadow op h Fixed bugs log10 log2 gradients New unit tests for unary binary operators Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"mseeger,piiswrong,mseeger",2017-10-15 04:13:46,2017-10-15 08:44:14
PR,Disable Caffe Converter integration tests,Description Disable Caffe Converter integration tests The test is failing because it is not able to download files from caffe server This test can be enabled again after is fixed Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"indhub,mbaijal",2017-10-15 16:22:52,2017-10-15 16:33:07
PR,Refactor AdaGrad optimizer to support sparse tensors unary and binary refactoring for new infer storage type logic,Refactor AdaGrad optimizer to support sparse tensors Add sparse support for plus scalar minus scalar clip Some additional unary and binary refactoring for new infer storage type logic,,"cjolivier01,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,szha,eric-haibin-lin,eric-haibin-lin,cjolivier01,cjolivier01,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,cjolivier01,eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,eric-haibin-lin,eric-haibin-lin,cjolivier01,cjolivier01,eric-haibin-lin,cjolivier01,cjolivier01,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,cjolivier01,eric-haibin-lin,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01",2017-09-14 18:58:13,2017-10-15 20:34:21
PR,Disable Caffe Converter integration tests,Description Disable Caffe Converter integration tests The test is failing because it is not able to download files from caffe server 8284 This test can be enabled again after is fixed Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,indhub,2017-10-15 17:04:16,2017-10-15 21:07:42
PR,Proper float64 support for unary binary elemwise operators,Wrapper for math h functions Rewrote mshadow op h Fixed bugs log10 log2 gradients New unit tests for unary binary operators Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"mseeger,mseeger,mseeger",2017-10-15 08:47:27,2017-10-15 21:38:53
IS,Mxnet Cpp Segmentation Fault when Defining the Graph,I have a simple C program that goes as follows I have also tried different types of computation FullyConnected etc but whatever I tried I always get a segmentation fault from NNGetOpInfo Why is this and how can I solve it,,lebeg,2017-10-15 22:00:50,2017-10-15 23:18:19
PR,Discussion Support caching multiple graphs in HybridBlock,It would be nice to have a HybridBlock with support for parameter sharing similar to the old BucketingModule This PR implements that by adding an optional key and key setter The caches for op and graph are specific to each key while the parameter cache is the same for all keys The user defined key setter method is called at the beginning of forward and can be used to set the key based on the passed data e g based on batch size or sequence length In the user defined hybrid forward method the user can access the key via self key This is helpful as once the Block is hybridized it is not anymore possible to access the shape in hybrid forward This works and could be merged But the design might not be optimal So lets discuss here if you have any ideas for a better design For example depending on the layout the following key setters could be used,,"leezu,piiswrong,leezu,piiswrong,leezu",2017-10-15 07:08:02,2017-10-16 01:45:41
PR,fix autograd context bug,leezu reference,,piiswrong,2017-10-15 21:16:22,2017-10-16 02:23:08
PR,generate op frontend code for IDE auto complete,Description This PR aims at providing auto complete for text editors by actually writing the generated op functions to module source codes The solution should work for pip setuptools install command python setup py develop should remain runtime generated Checklist Essentials x Changes are complete i e I finished coding on this PR x Passed code style checking make lint x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Decouple code generation and function registration Existing tests already covered this change x Write generated code to source files as part of the post install script of install command Comments Backward compatible This change should not affect python setup py develop Since pip uses a separate setup py another change will happen in the distro package to take effect in pip,,"szha,piiswrong,piiswrong,szha,piiswrong,piiswrong,szha,piiswrong,szha,reminisce,eric-haibin-lin",2017-10-12 21:15:39,2017-10-16 03:18:07
PR,CMAKE Fix windows cmake build,The new NNVM source tree contains nnvm compiler which should not be included when compiling nnvm update the source tree to only contain the nnvm core library,,"tqchen,cjolivier01,piiswrong,tqchen,tqchen,tqchen,piiswrong,tqchen,tqchen,cjolivier01,cjolivier01",2017-10-11 21:14:54,2017-10-16 16:51:18
PR,WIP NCCL support,This PR aims to add support for NVIDIA NCCL library to MXNet,,"ptrendx,piiswrong,ptrendx,mli,ptrendx,piiswrong,ptrendx,piiswrong,piiswrong,ptrendx",2017-03-21 19:44:05,2017-10-16 17:33:13
PR,Added my code signing key,Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change mments,,cjolivier01,2017-10-16 16:58:12,2017-10-16 22:58:08
PR,remove usage of install command from code gen,Description This change removes the usage of install command to avoid the side effect of inconsistent behavior than before Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Revert setup py and add the code gen logic Comments This change makes the additional assumption that libmxnet so can be loaded onto the current system Before the change the assumption is only on its existence,,"szha,mbaijal",2017-10-16 22:55:23,2017-10-17 04:42:45
PR,Negative begin and end support for csr slice,Description Negative begin and end support for csr slice Support getitem int for CSRNDArray such as a 1 Test by cc haibin lin for review Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,ZiyueHuang,ZiyueHuang,eric-haibin-lin,piiswrong,ZiyueHuang,ZiyueHuang,eric-haibin-lin,reminisce,reminisce,ZiyueHuang,ZiyueHuang",2017-10-12 16:35:23,2017-10-17 05:17:22
PR,fix wrong documentation for make loss,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,eric-haibin-lin,2017-10-17 05:19:50,2017-10-17 05:40:29
IS,There is a bug in metric py,In the base class EvalMetric object these is a bug in update dict function of this base class The bug is if self output names is not None pred pred name for name in self output names can not work because self output names is an 'int' object for example 2 but 'int' object is not iterable,,,2017-10-17 07:58:22,2017-10-17 08:03:28
PR,Preparing for 0 12 0 rc0 Final changes before RC,Description Preparing for 0 12 0 rc0 Final changes before RC Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Updated the NEWS md to match release notes with updates from new PRs x Added the 0 12 0 release tag to the NEWS md Comments The 0 12 0 tag does not exist at the moment but I believe this is the right way to do it since code must not be edited after RC There was a comment about not using the 0 11 0 rc tag during last release here 3Cgeneral incubator apache org 3E This is also documented here,,"mbaijal,szha,szha,mbaijal,szha,mbaijal,szha",2017-10-16 20:31:39,2017-10-17 14:58:51
PR,Enable smoothing in softmax operator,As proposed by in 6996 this PR helps reduce memory usage when using smoothing during softmax computation by doing the smoothing directly in the operator This modification was tested by and I and we found it reduced memory usage during training on a typical MT model by 3GB This PR should be backwards compatible This PR will require to be in place before it will be able to be merged,,"KellenSunderland,KellenSunderland,fhieber,KellenSunderland,KellenSunderland,piiswrong,cjolivier01,fhieber,KellenSunderland",2017-10-02 12:41:31,2017-10-17 15:00:53
PR,v0 12 regression Fix registration of children for Block,If the attribute is already declared e g as None the registration would fail Consider the following example The regression was introduced by,,"leezu,piiswrong,piiswrong,szha,szha,piiswrong,szha,leezu,szha,leezu",2017-10-15 06:34:44,2017-10-17 15:02:45
PR,Revert CMAKE Fix windows cmake build,Reverts apache incubator mxnet 8227,,"cjolivier01,cjolivier01,piiswrong,cjolivier01",2017-10-17 03:04:37,2017-10-17 17:58:05
PR,Fix typo in Gluon L1loss,Description Fix typo in Gluon L1loss Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Fix typo Comments NA,,astonzhang,2017-10-17 22:01:43,2017-10-17 22:02:17
PR,fixed broken links https was pointing to http for mxnet io,Description Fixed 4 links Other links are already in the previous commit and validated fine manually in the current master Everything should look good now Script is complaining even though the links are valid and working,,"thinksanky,szha,thinksanky",2017-10-16 20:24:24,2017-10-17 22:27:26
PR,Update rnn md,Description Fix typo in Gluon rnn doc Checklist Changes x Change model mx nd ones 2 3 5 to model mx nd ones 2 3,,szha,2017-10-17 18:24:08,2017-10-18 03:52:03
PR,fluent methods for missed ops,Description Add missed ops to api doc and some of the fluent methods Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x add missed api doc x add fluent methods Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"szha,ZiyueHuang,eric-haibin-lin,eric-haibin-lin",2017-10-18 01:15:19,2017-10-18 05:57:21
PR,update ps lite,,,piiswrong,2017-10-18 00:49:50,2017-10-18 08:49:04
IS,Custom operators with distributed kvstore,Description It is currently not possible to run distributed training with kvstore wouldist async' when using custom operators details below Environment info Required Python 3 MXNet compiled with USE DIST KVSTORE 1 Build info Required if built from source MXNet compiled with USE DIST KVSTORE 1 Steps to reproduce Use kvstore wouldist async' with a network that contains a custom operator registered using mx operator register my custom op Detailed problem description When using kvstore wouldist async' the parameter updates are run on the parameter server In order to do so the optimizer is pickled and sent to the sever Among other things the optimizer holds a reference to the symbol see here L104 Now when the server loads the pickled optimizer object it tries to instantiate the symbol This works fine only if the symbol does not use any custom object The reason for this is the following In order to tell mxnet about custom operators one needs to use the mx operator register annotation So for the server one would need to first register the class by importing the module containing it and only then start the server The problem is that the server gets started as soon as mxnet is important so that one does not have any chance of ever registering the custom operator on the server see here L85 Ideally one would write the server code something like this,,"tdomhan,piiswrong,piiswrong,tdomhan,tdomhan",2017-10-17 18:56:58,2017-10-18 11:53:42
PR,Fix unused type warning,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-10-17 15:17:36,2017-10-18 15:15:10
PR,sparse Remove usage of arange in FillDnsZerosRspImpl,Description using mshadow range with big range will return a tensor with wrong shape Replaced by kernel launch Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here related issue,,eric-haibin-lin,2017-10-17 01:42:31,2017-10-18 17:21:39
IS,A3C code does not learn,Hi I tried to run your code on my GPU but the variable final score mean is always around 20 Did you succeed to make it work If yes did you use the default hyper parameters How much time does the learning take for you Mohamed,,piiswrong,2017-10-13 00:46:34,2017-10-18 23:30:40
IS,asnumpy is slowly how can I speed up it,I use pretrained mod for prediction I found asnumpy was slowly and took up most of the time 0 009676933288574219 0 010257959365844727 1 285344123840332 in fact the rlt dimension is only 16x10 NDArray 16x10 0,,"szha,zhreshold",2017-10-17 02:13:41,2017-10-19 02:22:15
IS,RELEASE Announcing v0 9 Release Candidate 1,We are announcing v0 9 rc1 This includes backend refactor to use NNVM for operator registration and graph optimization Schedule Due to the big scope of changes we first release a release candidate to collect feedbacks Initial feedback period is two weeks If no issue is raised during this period we will merge v0 9 to master on Oct 28th If you need more time please respond to this issue preferably with specific concerns to ask for an extension Immediately before v0 9 is merged in to master we will tag v0 8 which is the last release before NNVM refactor We will continue to develop v0 9 during this period If you are interested in cutting edge development please use List of Changes in v0 9 excluding those in v0 8 Refactor Symbolic GraphExecutor StaticGraph and NDArray Imperative MXFuncInvoke modules to use NNVM for registration and optimization mxnet nnvm is added GraphExecutor is rewritten StaticGraph is removed Simple op registered with MXNET REGISTER SIMPLE OP is deprecated but still supported through legacy support and all simple ops in master are factored to register with NNVM They are also moved to src operator tensor OperatorProperty which is the old way of registering layers is also deprecated but still supported through legacy support and we will gradually move them to NNVM Guide for registering operators with NNVM is available here MXFuncInvoke is replaced with MXImperativeInvoke Now ALL operators including symbols like Convolution are available under both mx nd and mx sym for ndarray operation and symbolic operation Cython interface for mx ndarray and mx symbol are added for reduced python side overhead Enable with make cython make cython2 and make cython3 are added cc optimizers are deprecated and removed ccsgd is now an alias of sgd SGD and Adam are now implemented as one step ndarray operation to match the speed of ccsgd mx image and mx image ImageIter are added for high performance image IO and augmentation This is meant to replace cv2 This should make writing high performance image data iterators much easier Try mx image when you find mx io ImageRecordIter too restrictive MXProfiler is added example profiler for profiling performance of layers List of Pending Changes These are not available in v0 9 rc1 and may or may not be available in final v0 9 release Vastly 10x 100x improved CPU speed with MKLDNN optimizations from Intel Improved RNN support More details later Layout support for nn layers NCHW NHWC HCDHW List of Changes in v0 8 CaffeOp and CaffeIter for interfacing with Caffe by WrapCTC plugin for sequence learning by Improved Multi GPU performance by CuDNN RNN support by OpenCV plugin for parallel image IO by More operators as simple op Simple OP element wise op with axis and broadcast Cudnn auto tuning for faster convolution by More applications Faster RCNN by,,"piiswrong,tornadomeet,piiswrong,gengyifeng,sbodenstein,piiswrong,taliesinb,tqchen",2016-10-12 07:36:40,2017-10-19 04:42:58
IS,How to use mxnet,I want to use mxnet in deep learning I have CSV file with binary features and 2 class labels 0 1 How can I train and test my data with mxnet and deep belief network Thanks,,,2017-02-12 12:57:28,2017-10-19 04:43:10
IS,v0 9 3 Amalgamation for Android broken,Amalgamation for Android still breaking in the recent release It looks like the USE OPENCV 0 is being ignored,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2017-01-24 01:44:54,2017-10-19 04:43:27
IS,error ISO C forbids comparison between pointer and integer,Ca not build mxnet after solving many other problems I still have the following error Environment info Operating System Ubuntu 16 10 Compiler gnu compilers 4 8 4 9 5 6 Package used Python R Scala Julia MXNet commit hash git rev parse HEAD 0f6d583417b7233ede66642b0319a7c39ff1b672 Python version and distribution 2 7 3 5 Error Message g std c 11 c DMSHADOW FORCE STREAM Wall O3 I home nikita mxnet mshadow I home nikita mxnet dmlc core include fPIC I home nikita mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include I opt intel mkl include mkl include DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSDHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr local include opencv I usr local include fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home nikita mxnet src operator mkl DMKL EXPERIMENTAL 1 DMSHADOW USE CUDNN 1 DMXNET USE DIST KVSTORE I home nikita mxnet ps lite include I home nikita mxnet deps include I home nikita Git public caffe include I home nikita Git public caffe build src DUSE CUDNN 1 I home nikita Git public torch install include I home nikita Git public torch install include TH I home nikita Git public torch install include THC DMXNET USE TORCH 1 I home nikita Git public warp ctc include I home nikita Git public SFrame oss src unity lib I home nikita Git public SFrame oss src I home nikita mxnet cub DMXNET USE NVRTC 1 MMD c plugin torch torch base cc o build plugin torch torch base o plugin torch torch base cc In member function void mxnet TorchState SetStream mshadow Stream Device with xpu mshadow gpu plugin torch torch base cc 46 16 warning variable cs set but not used Wunused but set variable cudaStream t cs THCState getCurrentStream CudaState In file included from home nikita mxnet dmlc core include dmlc io h 14 0 from include mxnet base h 10 from plugin torch torch base h 9 from plugin torch torch base cc 7 home nikita mxnet dmlc core include dmlc logging h In instantiation of dmlc LogCheckError dmlc LogCheck NE const X const Y with X THCharStorage Y long int plugin torch torch base h 77 5 required from here home nikita mxnet dmlc core include dmlc logging h 95 24 error ISO C forbids comparison between pointer and integer fpermissive home nikita mxnet dmlc core include dmlc logging h 76 9 if x op y return LogCheckError home nikita mxnet dmlc core include dmlc logging h 95 24 DEFINE CHECK FUNC NE home nikita mxnet dmlc core include dmlc logging h 76 11 note in definition of macro DEFINE CHECK FUNC if x op y return LogCheckError Makefile 218 recipe for target 'build plugin torch torch base o' failed make build plugin torch torch base o Error 1,,"piiswrong,piiswrong,piiswrong,piiswrong,piiswrong",2017-02-01 18:28:38,2017-10-19 04:43:38
IS,How to create a Custom Operator with extra parameters in Python,say if I have a layer need some input parameters e g size 10 How to set that in Custom Operator I didnt see any examples in,,"Oh233,opringle",2017-03-27 06:13:13,2017-10-19 04:43:57
IS,MinPy next step prototype,Need input from you guys Especially ppl from MXNet team We have been discussing this already for a while so background might not be clear to you guys Our goal Integrate JIT and autograd with MXNet MinPy python interface Autograd integration is already on the way We are proposing a uniform interface for both JIT and autograd For JIT we cache user operations and evaluate them lazily only when user requests a print or asnumpy By doing this we can optimize computing sequence and cache them for future use It functions as a layer between Python user code and NNVM engine code As an example user might have code in a tight loop The graph structure generated in the loop are the same between iterations In the first iteration we optimize this computing sequence so that in future rounds we may use the optimized computing sequence to do calculation on different data We need a way to detect and cache graph structure That is the intention of this proposal In this case three element wise operations could be merged into one The first time we encounter this code we send the computing sequence to NNVM for optimization The second time we look up our cache and run the optimized computing sequence instead There are many more corners cases including those where JIT interact with autograd Please refer to this gist for a proof of concept written in Python Implementation proposal We intend to write the code in C api directly alongside NDArray functions and methods Header file is here We need to intercept MXImperativeInvoke Instead of calling underlying functions directly we place them in our sequence buffer By placing the function and its arguments we assure the involved arrays are properly referenced and not freed prematurely At a later stage when JIT boundary is encountered we flush the sequence buffer and push it to engine NNVM We achieve lazy evaluation in this way A similar approach goes for autograd operations When gradient sequences is calculated we push them into the JIT queue so they can also get optimized A sample not complete implementation code is here cn,,"hotpxl,tqchen,hotpxl,tqchen,jermainewang",2017-03-24 23:52:54,2017-10-19 04:44:04
IS,Discussion CreateBackwardOp interface for Operator,Based on discussion in we propose two new interfaces for OperatorProperty class By default these interfaces return stateful for all current operators Upon creating OpExecutor for a node following logic will be used If it is a forward node go straight create its OpExecutor same as original logic If it is a backward node and its forward node exists reuse its forward OpExecutor same as original logic If it is a backward node but no forward node exists create the OpExecutor using the new interface the new logic Therefore nothing will be changed for all current MXNet codes The only part that uses this currently is AutogradRuntime It will first check whether the given symbol is stateless if not raise error Then it will use the third logic above to create its backward OpExecutor The implementation overhead is to override above interfaces for stateless operators This should be easy since most of the current operator is stateless so the CreateBackwardOperatorEx usually follows the same logic as CreateOperatorEx Please leave your comments,,"jermainewang,piiswrong,jermainewang,jermainewang,piiswrong",2017-03-28 01:58:18,2017-10-19 04:44:09
IS,Discussion Support Higher order Gradient,mxnet currenlty only supports calculating the gradient of a loss constructed via chaining mxnet symbol is This is unfortunately not enough to implement some more advanced training methods such as the newly poposed Improved Training of Wasserstein GAN For that method the loss function itself will contain a reference to the gradient of the network with respect to the inputs gradient penalty Furthermore for some reinforcement learning methods it is important to compute a Hessian Vector product The key to the auto computation of the Hessian Vector product and the Hessian matrix is the R Operator Right product of Jacobian matrix and the input vector We can refer to the paper Fast Exact Multiplication by the Hessian barak papers nc hessian pdf and the Theano is document of ROp r operator for more details For that we can add another R forward and R backward functions in MXNet to compute the R output and the R gradient over some parameter w The workflow will be like this Like the traditional forward backward computation of the gradient we first compute net forward and net backward to get the output gradient and then use net R forward param v and net R backward param v to get the Hessian vector product To compute the Hessian matrix we can call the subprogram of computing the Hessian vector product multiple times We can refer to Wiki Reverse Hessian vector products and Theano is implementation of Hessian L1874 L1880,,"leezu,piiswrong,sxjscience,tqchen,nicklhy,ZihengJiang,sxjscience",2017-04-05 14:17:06,2017-10-19 04:44:29
IS,Scala build failed with undefined symbol cudaRegisterFatBinary,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu Compiler gcc version 5 4 0 20160609 Ubuntu 5 4 0 6ubuntu1 16 04 4 Package used Python R Scala Julia Scala Or if installed from source Latest commit Error Message Please paste the full error message including stack trace INFO INFO Building MXNet Scala Package Core 0 1 2 SNAPSHOT INFO INFO INFO maven clean plugin 2 5 clean default clean mxnet core 2 11 INFO INFO maven resources plugin 2 7 resources default resources mxnet core 2 11 WARNING Using platform encoding ANSI X3 4 1968 actually to copy filtered resources i e build is platform dependent INFO skip non existing resourceDirectory mxnet scala package core src main resources INFO INFO maven compiler plugin 3 3 compile default compile mxnet core 2 11 INFO No sources to compile INFO INFO maven scala plugin 2 15 2 compile default mxnet core 2 11 INFO Checking for multiple versions of scala WARNING Invalid POM for ml dmlc mxnet mxnet macros 2 11 jar 0 1 2 SNAPSHOT transitive dependencies if any will not be available enable debug logging for more details WARNING Expected all dependencies to require Scala version 2 11 8 WARNING ml dmlc mxnet mxnet init 2 11 0 1 2 SNAPSHOT requires scala version 2 11 8 WARNING ml dmlc mxnet mxnet init 2 11 0 1 2 SNAPSHOT requires scala version 2 11 8 WARNING ml dmlc mxnet mxnet core 2 11 0 1 2 SNAPSHOT requires scala version 2 11 8 WARNING org scala lang scala reflect 2 11 8 requires scala version 2 11 8 WARNING org scalatest scalatest 2 11 2 2 4 requires scala version 2 11 2 WARNING Multiple versions of scala libraries detected INFO includes java scala INFO excludes INFO mxnet scala package core src main scala 1 info compiling INFO Compiling 49 source files to mxnet scala package core target classes at 1491959797163 INFO compiler plugin BasicArtifact org scalamacros paradise 2 11 8 2 1 0 ERROR error java lang UnsatisfiedLinkError mxnet scala package init native linux x86 64 target libmxnet init scala linux x86 64 so mxnet scala package init native linux x86 64 target libmxnet init scala linux x86 64 so undefined symbol cudaRegisterFatBinary INFO at java lang ClassLoader NativeLibrary load Native Method INFO at java lang ClassLoader loadLibrary0 ClassLoader java 1941 INFO at java lang ClassLoader loadLibrary ClassLoader java 1824 INFO at java lang Runtime load0 Runtime java 809 INFO at java lang System load System java 1086 INFO at ml dmlc mxnet init Base tryLoadInitLibrary Base scala 44 INFO at ml dmlc mxnet init Base init Base scala 21 INFO at ml dmlc mxnet init Base clinit Base scala INFO at ml dmlc mxnet NDArrayMacro initNDArrayModule NDArrayMacro scala 150 INFO at ml dmlc mxnet NDArrayMacro init NDArrayMacro scala 41 INFO at ml dmlc mxnet NDArrayMacro clinit NDArrayMacro scala INFO at java lang Class forName0 Native Method INFO at java lang Class forName Class java 348 INFO at scala reflect macros runtime JavaReflectionRuntimes JavaReflectionResolvers class resolveJavaReflectionRuntime JavaReflectionRuntimes scala 16 INFO at scala reflect macros runtime MacroRuntimes MacroRuntimeResolver resolveJavaReflectionRuntime MacroRuntimes scala 52 INFO at scala reflect macros runtime MacroRuntimes MacroRuntimeResolver resolveRuntime MacroRuntimes scala 65 INFO at scala reflect macros runtime MacroRuntimes anonfun standardMacroRuntime 3 apply MacroRuntimes scala 35 INFO at scala reflect macros runtime MacroRuntimes anonfun standardMacroRuntime 3 apply MacroRuntimes scala 35 INFO at scala collection mutable MapLike class getOrElseUpdate MapLike scala 194 INFO at scala collection mutable AbstractMap getOrElseUpdate Map scala 80 INFO at scala reflect macros runtime MacroRuntimes class standardMacroRuntime MacroRuntimes scala 35 INFO at scala tools nsc Global anon 1 standardMacroRuntime Global scala 462 INFO at scala tools nsc typechecker AnalyzerPlugins anon 12 default AnalyzerPlugins scala 416 INFO at scala tools nsc typechecker AnalyzerPlugins anon 12 default AnalyzerPlugins scala 413 INFO at scala tools nsc typechecker AnalyzerPlugins class invoke AnalyzerPlugins scala 375 INFO at scala tools nsc typechecker AnalyzerPlugins class pluginsMacroRuntime AnalyzerPlugins scala 413 INFO at scala tools nsc Global anon 1 pluginsMacroRuntime Global scala 462 INFO at scala reflect macros runtime MacroRuntimes class macroRuntime MacroRuntimes scala 22 INFO at scala tools nsc Global anon 1 macroRuntime Global scala 462 INFO at scala tools nsc typechecker Macros MacroExpander anonfun expand 1 apply Macros scala 579 INFO at scala tools nsc typechecker Macros MacroExpander anonfun expand 1 apply Macros scala 573 INFO at scala tools nsc Global withInfoLevel Global scala 211 INFO at scala tools nsc typechecker Macros MacroExpander expand Macros scala 572 INFO at scala tools nsc typechecker Macros MacroExpander apply Macros scala 560 INFO at org scalamacros paradise typechecker Expanders Expander class expand 1 Expanders scala 64 INFO at org scalamacros paradise typechecker Expanders Expander anonfun expandAnnotationMacro 1 apply Expanders scala 121 INFO at org scalamacros paradise typechecker Expanders Expander anonfun expandAnnotationMacro 1 apply Expanders scala 121 INFO at org scalamacros paradise typechecker Expanders Expander class onlyIfExpansionAllowed 1 Expanders scala 58 INFO at org scalamacros paradise typechecker Expanders Expander class expandAnnotationMacro Expanders scala 121 INFO at org scalamacros paradise typechecker Namers anon 3 expandAnnotationMacro Namers scala 13 INFO at org scalamacros paradise typechecker Namers Namer anon 2 org scalamacros paradise typechecker Namers Namer class anon maybeExpand 1 Namers scala 360 INFO at org scalamacros paradise typechecker Namers Namer anon 2 anonfun 4 apply Namers scala 372 INFO at org scalamacros paradise typechecker Namers Namer anon 2 anonfun 4 apply Namers scala 372 INFO at scala collection immutable Stream flatMap Stream scala 489 INFO at org scalamacros paradise typechecker Namers Namer anon 2 maybeExpand Namers scala 372 INFO at org scalamacros paradise typechecker Namers Namer MaybeExpandeeCompleter completeImpl Namers scala 322 INFO at org scalamacros paradise typechecker Namers Namer MaybeExpandeeCompleter completeImpl Namers scala 317 INFO at scala tools nsc typechecker Namers LockingTypeCompleter class complete Namers scala 1693 INFO at org scalamacros paradise typechecker Namers Namer MaybeExpandeeCompleter complete Namers scala 299 INFO at scala reflect internal Symbols Symbol info Symbols scala 1514 INFO at scala tools nsc typechecker Namers Namer anonfun moduleClassTypeCompleter 1 apply Namers scala 809 INFO at scala tools nsc typechecker Namers Namer anonfun moduleClassTypeCompleter 1 apply Namers scala 806 INFO at scala tools nsc typechecker Namers anon 1 completeImpl Namers scala 1685 INFO at scala tools nsc typechecker Namers LockingTypeCompleter class complete Namers scala 1693 INFO at scala tools nsc typechecker Namers anon 1 complete Namers scala 1683 INFO at scala reflect internal Symbols Symbol info Symbols scala 1514 INFO at scala reflect internal Symbols ModuleClassSymbol implicitMembers Symbols scala 3404 INFO at scala tools nsc typechecker Implicits ImplicitSearch getClassParts 1 Implicits scala 1021 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch getParts 1 Implicits scala 1059 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch getParts 1 Implicits scala 1066 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch getParts 1 Implicits scala 1066 INFO at scala tools nsc typechecker Implicits ImplicitSearch companionImplicitMap Implicits scala 1093 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch implicitsOfExpectedType Implicits scala 1113 INFO at scala tools nsc typechecker Implicits ImplicitSearch bestImplicit Implicits scala 1377 INFO at scala tools nsc typechecker Implicits class inferImplicit Implicits scala 73 INFO at scala tools nsc Global anon 1 inferImplicit Global scala 462 INFO at scala tools nsc typechecker Implicits class inferImplicit Implicits scala 38 INFO at scala tools nsc Global anon 1 inferImplicit Global scala 462 INFO at scala tools nsc typechecker Typers Typer applyImplicitArgs Typers scala 149 INFO at scala tools nsc typechecker Typers Typer adaptToImplicitMethod 1 Typers scala 823 INFO at scala tools nsc typechecker Typers Typer adapt Typers scala 1162 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5410 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5472 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5478 INFO at scala tools nsc typechecker Typers Typer typedSelectOrSuperCall 1 Typers scala 4812 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5344 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer silent Typers scala 680 INFO at scala tools nsc typechecker Typers Typer normalTypedApply 1 Typers scala 4524 INFO at scala tools nsc typechecker Typers Typer typedApply 1 Typers scala 4580 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5343 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5472 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5478 INFO at scala tools nsc typechecker Typers Typer typedSelectOrSuperCall 1 Typers scala 4812 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5344 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer silent Typers scala 680 INFO at scala tools nsc typechecker Typers Typer normalTypedApply 1 Typers scala 4524 INFO at scala tools nsc typechecker Typers Typer typedApply 1 Typers scala 4580 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5343 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedBlock Typers scala 2382 INFO at scala tools nsc typechecker Typers Typer anonfun typedOutsidePatternMode 1 1 apply Typers scala 5318 INFO at scala tools nsc typechecker Typers Typer anonfun typedOutsidePatternMode 1 1 apply Typers scala 5318 INFO at scala tools nsc typechecker Typers Typer typedOutsidePatternMode 1 Typers scala 5317 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5353 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer transformedOrTyped Typers scala 5605 INFO at scala tools nsc typechecker Typers Typer typedDefDef Typers scala 2208 INFO at scala tools nsc typechecker Typers Typer typedMemberDef 1 Typers scala 5308 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5359 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedByValueExpr Typers scala 5452 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedStat 1 Typers scala 3047 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala collection immutable List loop 1 List scala 173 INFO at scala collection immutable List mapConserve List scala 189 INFO at scala tools nsc typechecker Typers Typer typedStats Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer typedTemplate Typers scala 1921 INFO at scala tools nsc typechecker Typers Typer typedClassDef Typers scala 1762 INFO at scala tools nsc typechecker Typers Typer typedMemberDef 1 Typers scala 5309 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5359 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedByValueExpr Typers scala 5452 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedStat 1 Typers scala 3047 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala collection immutable List loop 1 List scala 173 INFO at scala collection immutable List mapConserve List scala 189 INFO at scala tools nsc typechecker Typers Typer typedStats Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer typedPackageDef 1 Typers scala 5015 INFO at scala tools nsc typechecker Typers Typer typedMemberDef 1 Typers scala 5312 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5359 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5448 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 apply Analyzer scala 102 INFO at scala tools nsc Global GlobalPhase anonfun applyPhase 1 apply mcV sp Global scala 440 INFO at scala tools nsc Global GlobalPhase withCurrentUnit Global scala 431 INFO at scala tools nsc Global GlobalPhase applyPhase Global scala 440 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 anonfun run 1 apply Analyzer scala 94 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 anonfun run 1 apply Analyzer scala 93 INFO at scala collection Iterator class foreach Iterator scala 893 INFO at scala collection AbstractIterator foreach Iterator scala 1336 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 run Analyzer scala 93 INFO at scala tools nsc Global Run compileUnitsInternal Global scala 1501 INFO at scala tools nsc Global Run compileUnits Global scala 1486 INFO at scala tools nsc Global Run compileSources Global scala 1481 INFO at scala tools nsc Global Run compile Global scala 1582 INFO at scala tools nsc Driver doCompile Driver scala 32 INFO at scala tools nsc MainClass doCompile Main scala 23 INFO at scala tools nsc Driver process Driver scala 51 INFO at scala tools nsc Driver main Driver scala 64 INFO at scala tools nsc Main main Main scala INFO at sun reflect NativeMethodAccessorImpl invoke0 Native Method INFO at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 62 INFO at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43 INFO at java lang reflect Method invoke Method java 498 INFO at org scala tools maven executions MainHelper runMain MainHelper java 161 INFO at org scala tools maven executions MainWithArgsInFile main MainWithArgsInFile java 26 INFO java lang reflect InvocationTargetException INFO at sun reflect NativeMethodAccessorImpl invoke0 Native Method INFO at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 62 INFO at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43 INFO at java lang reflect Method invoke Method java 498 INFO at org scala tools maven executions MainHelper runMain MainHelper java 161 INFO at org scala tools maven executions MainWithArgsInFile main MainWithArgsInFile java 26 ERROR Caused by java lang UnsatisfiedLinkError mxnet scala package init native linux x86 64 target libmxnet init scala linux x86 64 so mxnet scala package init native linux x86 64 target libmxnet init scala linux x86 64 so undefined symbol cudaRegisterFatBinary INFO at java lang ClassLoader NativeLibrary load Native Method INFO at java lang ClassLoader loadLibrary0 ClassLoader java 1941 INFO at java lang ClassLoader loadLibrary ClassLoader java 1824 INFO at java lang Runtime load0 Runtime java 809 INFO at java lang System load System java 1086 INFO at ml dmlc mxnet init Base tryLoadInitLibrary Base scala 44 INFO at ml dmlc mxnet init Base init Base scala 21 INFO at ml dmlc mxnet init Base clinit Base scala INFO at ml dmlc mxnet NDArrayMacro initNDArrayModule NDArrayMacro scala 150 INFO at ml dmlc mxnet NDArrayMacro init NDArrayMacro scala 41 INFO at ml dmlc mxnet NDArrayMacro clinit NDArrayMacro scala INFO at java lang Class forName0 Native Method INFO at java lang Class forName Class java 348 INFO at scala reflect macros runtime JavaReflectionRuntimes JavaReflectionResolvers class resolveJavaReflectionRuntime JavaReflectionRuntimes scala 16 INFO at scala reflect macros runtime MacroRuntimes MacroRuntimeResolver resolveJavaReflectionRuntime MacroRuntimes scala 52 INFO at scala reflect macros runtime MacroRuntimes MacroRuntimeResolver resolveRuntime MacroRuntimes scala 65 INFO at scala reflect macros runtime MacroRuntimes anonfun standardMacroRuntime 3 apply MacroRuntimes scala 35 INFO at scala reflect macros runtime MacroRuntimes anonfun standardMacroRuntime 3 apply MacroRuntimes scala 35 INFO at scala collection mutable MapLike class getOrElseUpdate MapLike scala 194 INFO at scala collection mutable AbstractMap getOrElseUpdate Map scala 80 INFO at scala reflect macros runtime MacroRuntimes class standardMacroRuntime MacroRuntimes scala 35 INFO at scala tools nsc Global anon 1 standardMacroRuntime Global scala 462 INFO at scala tools nsc typechecker AnalyzerPlugins anon 12 default AnalyzerPlugins scala 416 INFO at scala tools nsc typechecker AnalyzerPlugins anon 12 default AnalyzerPlugins scala 413 INFO at scala tools nsc typechecker AnalyzerPlugins class invoke AnalyzerPlugins scala 375 INFO at scala tools nsc typechecker AnalyzerPlugins class pluginsMacroRuntime AnalyzerPlugins scala 413 INFO at scala tools nsc Global anon 1 pluginsMacroRuntime Global scala 462 INFO at scala reflect macros runtime MacroRuntimes class macroRuntime MacroRuntimes scala 22 INFO at scala tools nsc Global anon 1 macroRuntime Global scala 462 INFO at scala tools nsc typechecker Macros MacroExpander anonfun expand 1 apply Macros scala 579 INFO at scala tools nsc typechecker Macros MacroExpander anonfun expand 1 apply Macros scala 573 INFO at scala tools nsc Global withInfoLevel Global scala 211 INFO at scala tools nsc typechecker Macros MacroExpander expand Macros scala 572 INFO at scala tools nsc typechecker Macros MacroExpander apply Macros scala 560 INFO at org scalamacros paradise typechecker Expanders Expander class expand 1 Expanders scala 64 INFO at org scalamacros paradise typechecker Expanders Expander anonfun expandAnnotationMacro 1 apply Expanders scala 121 INFO at org scalamacros paradise typechecker Expanders Expander anonfun expandAnnotationMacro 1 apply Expanders scala 121 INFO at org scalamacros paradise typechecker Expanders Expander class onlyIfExpansionAllowed 1 Expanders scala 58 INFO at org scalamacros paradise typechecker Expanders Expander class expandAnnotationMacro Expanders scala 121 INFO at org scalamacros paradise typechecker Namers anon 3 expandAnnotationMacro Namers scala 13 INFO at org scalamacros paradise typechecker Namers Namer anon 2 org scalamacros paradise typechecker Namers Namer class anon maybeExpand 1 Namers scala 360 INFO at org scalamacros paradise typechecker Namers Namer anon 2 anonfun 4 apply Namers scala 372 INFO at org scalamacros paradise typechecker Namers Namer anon 2 anonfun 4 apply Namers scala 372 INFO at scala collection immutable Stream flatMap Stream scala 489 INFO at org scalamacros paradise typechecker Namers Namer anon 2 maybeExpand Namers scala 372 INFO at org scalamacros paradise typechecker Namers Namer MaybeExpandeeCompleter completeImpl Namers scala 322 INFO at org scalamacros paradise typechecker Namers Namer MaybeExpandeeCompleter completeImpl Namers scala 317 INFO at scala tools nsc typechecker Namers LockingTypeCompleter class complete Namers scala 1693 INFO at org scalamacros paradise typechecker Namers Namer MaybeExpandeeCompleter complete Namers scala 299 INFO at scala reflect internal Symbols Symbol info Symbols scala 1514 INFO at scala tools nsc typechecker Namers Namer anonfun moduleClassTypeCompleter 1 apply Namers scala 809 INFO at scala tools nsc typechecker Namers Namer anonfun moduleClassTypeCompleter 1 apply Namers scala 806 INFO at scala tools nsc typechecker Namers anon 1 completeImpl Namers scala 1685 INFO at scala tools nsc typechecker Namers LockingTypeCompleter class complete Namers scala 1693 INFO at scala tools nsc typechecker Namers anon 1 complete Namers scala 1683 INFO at scala reflect internal Symbols Symbol info Symbols scala 1514 INFO at scala reflect internal Symbols ModuleClassSymbol implicitMembers Symbols scala 3404 INFO at scala tools nsc typechecker Implicits ImplicitSearch getClassParts 1 Implicits scala 1021 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch getParts 1 Implicits scala 1059 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch getParts 1 Implicits scala 1066 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch getParts 1 Implicits scala 1066 INFO at scala tools nsc typechecker Implicits ImplicitSearch companionImplicitMap Implicits scala 1093 INFO at scala tools nsc typechecker Implicits ImplicitSearch scala tools nsc typechecker Implicits ImplicitSearch implicitsOfExpectedType Implicits scala 1113 INFO at scala tools nsc typechecker Implicits ImplicitSearch bestImplicit Implicits scala 1377 INFO at scala tools nsc typechecker Implicits class inferImplicit Implicits scala 73 INFO at scala tools nsc Global anon 1 inferImplicit Global scala 462 INFO at scala tools nsc typechecker Implicits class inferImplicit Implicits scala 38 INFO at scala tools nsc Global anon 1 inferImplicit Global scala 462 INFO at scala tools nsc typechecker Typers Typer applyImplicitArgs Typers scala 149 INFO at scala tools nsc typechecker Typers Typer adaptToImplicitMethod 1 Typers scala 823 INFO at scala tools nsc typechecker Typers Typer adapt Typers scala 1162 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5410 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5472 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5478 INFO at scala tools nsc typechecker Typers Typer typedSelectOrSuperCall 1 Typers scala 4812 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5344 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer silent Typers scala 680 INFO at scala tools nsc typechecker Typers Typer normalTypedApply 1 Typers scala 4524 INFO at scala tools nsc typechecker Typers Typer typedApply 1 Typers scala 4580 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5343 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5472 INFO at scala tools nsc typechecker Typers Typer typedQualifier Typers scala 5478 INFO at scala tools nsc typechecker Typers Typer typedSelectOrSuperCall 1 Typers scala 4812 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5344 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer anonfun 99 apply Typers scala 4525 INFO at scala tools nsc typechecker Typers Typer silent Typers scala 680 INFO at scala tools nsc typechecker Typers Typer normalTypedApply 1 Typers scala 4524 INFO at scala tools nsc typechecker Typers Typer typedApply 1 Typers scala 4580 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5343 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedBlock Typers scala 2382 INFO at scala tools nsc typechecker Typers Typer anonfun typedOutsidePatternMode 1 1 apply Typers scala 5318 INFO at scala tools nsc typechecker Typers Typer anonfun typedOutsidePatternMode 1 1 apply Typers scala 5318 INFO at scala tools nsc typechecker Typers Typer typedOutsidePatternMode 1 Typers scala 5317 INFO at scala tools nsc typechecker Typers Typer typedInAnyMode 1 Typers scala 5353 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5360 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer transformedOrTyped Typers scala 5605 INFO at scala tools nsc typechecker Typers Typer typedDefDef Typers scala 2208 INFO at scala tools nsc typechecker Typers Typer typedMemberDef 1 Typers scala 5308 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5359 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedByValueExpr Typers scala 5452 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedStat 1 Typers scala 3047 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala collection immutable List loop 1 List scala 173 INFO at scala collection immutable List mapConserve List scala 189 INFO at scala tools nsc typechecker Typers Typer typedStats Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer typedTemplate Typers scala 1921 INFO at scala tools nsc typechecker Typers Typer typedClassDef Typers scala 1762 INFO at scala tools nsc typechecker Typers Typer typedMemberDef 1 Typers scala 5309 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5359 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typedByValueExpr Typers scala 5452 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedStat 1 Typers scala 3047 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer anonfun 65 apply Typers scala 3151 INFO at scala collection immutable List loop 1 List scala 173 INFO at scala collection immutable List mapConserve List scala 189 INFO at scala tools nsc typechecker Typers Typer typedStats Typers scala 3151 INFO at scala tools nsc typechecker Typers Typer typedPackageDef 1 Typers scala 5015 INFO at scala tools nsc typechecker Typers Typer typedMemberDef 1 Typers scala 5312 INFO at scala tools nsc typechecker Typers Typer typed1 Typers scala 5359 INFO at scala tools nsc typechecker Typers Typer runTyper 1 Typers scala 5396 INFO at scala tools nsc typechecker Typers Typer scala tools nsc typechecker Typers Typer typedInternal Typers scala 5423 INFO at scala tools nsc typechecker Typers Typer body 2 Typers scala 5370 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5374 INFO at scala tools nsc typechecker Typers Typer typed Typers scala 5448 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 apply Analyzer scala 102 INFO at scala tools nsc Global GlobalPhase anonfun applyPhase 1 apply mcV sp Global scala 440 INFO at scala tools nsc Global GlobalPhase withCurrentUnit Global scala 431 INFO at scala tools nsc Global GlobalPhase applyPhase Global scala 440 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 anonfun run 1 apply Analyzer scala 94 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 anonfun run 1 apply Analyzer scala 93 INFO at scala collection Iterator class foreach Iterator scala 893 INFO at scala collection AbstractIterator foreach Iterator scala 1336 INFO at scala tools nsc typechecker Analyzer typerFactory anon 3 run Analyzer scala 93 INFO at scala tools nsc Global Run compileUnitsInternal Global scala 1501 INFO at scala tools nsc Global Run compileUnits Global scala 1486 INFO at scala tools nsc Global Run compileSources Global scala 1481 INFO at scala tools nsc Global Run compile Global scala 1582 INFO at scala tools nsc Driver doCompile Driver scala 32 INFO at scala tools nsc MainClass doCompile Main scala 23 INFO at scala tools nsc Driver process Driver scala 51 INFO at scala tools nsc Driver main Driver scala 64 INFO at scala tools nsc Main main Main scala INFO 6 more INFO INFO Reactor Summary INFO INFO MXNet Scala Package Parent SUCCESS 7 242 s INFO MXNet Scala Package Initializer SUCCESS 5 326 s INFO MXNet Scala Package Initializer Native Parent SUCCESS 0 024 s INFO MXNet Scala Package Initializer Native Linux x86 64 SUCCESS 10 618 s INFO MXNet Scala Package Macros SUCCESS 6 951 s INFO MXNet Scala Package Core FAILURE 3 944 s INFO MXNet Scala Package Native Parent SKIPPED INFO MXNet Scala Package Native Linux x86 64 CPU only SKIPPED INFO MXNet Scala Package Examples SKIPPED INFO MXNet Scala Package Spark ML SKIPPED INFO MXNet Scala Package Full Parent SKIPPED INFO MXNet Scala Package Full Linux x86 64 CPU only SKIPPED INFO INFO BUILD FAILURE INFO INFO Total time 36 403 s INFO Finished at 2017 04 12T01 16 41 00 00 INFO Final Memory 30M 1091M INFO ERROR Failed to execute goal org scala tools maven scala plugin 2 15 2 compile default on project mxnet core 2 11 wrap org apache commons exec ExecuteException Process exited with an error 240 Exit value 240 Help 1 ERROR ERROR To see the full stack trace of the errors re run Maven with the e switch ERROR Re run Maven using the X switch to enable full debug logging ERROR ERROR For more information about the errors and possible solutions please read the following articles ERROR Help 1 ERROR ERROR After correcting the problems you can resume the build with the command ERROR mvn goals rf mxnet core 2 11 Makefile 329 recipe for target iscalapkg' failed make scalapkg Error 1 The command ' bin sh c cd mxnet make scalapkg' returned a non zero code 2 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 cd docker 2 tool sh build gpu scala,,"lxn2,lxn2,Ldpe2G,yzhliu",2017-04-12 21:10:42,2017-10-19 04:44:41
IS,Any feature to access loss value in mxnet like keras package for tensorflow,In keras optimizers py def get updates self params constraints loss while in mxnet we only have weight and grad in update function Is it convenient to change mxnet to have such feature,,"kevinthesun,kevinthesun,kevinthesun",2017-04-26 22:49:59,2017-10-19 04:44:55
IS,Model Parallelism,Does MXNET support model parallelism What I mean is I have 2 GPUs 12GB each A single Caffe model Resnet152 with some changes and additions exceeds 12GB memory and does not fit in a single GPU for training How can I solve that problem with 2 GPUs Can I split this huge model to both the GPUs and make sure they communicate gradients with each other If so what all should I change in the solver train files in MXNET,,eric-haibin-lin,2017-04-12 22:41:27,2017-10-19 04:45:15
IS,Label smoothing for SoftmaxOutput cross entropy loss,Label smoothing is a nice technique to regularize models trained with cross entropy error for example described here and applied for vision here vgg rg papers reinception pdf and for NLP MT recently here This can be done in current MXNet when passed an explicit smoothed one hot label distribution see here L101 but this is probably wasteful w r t to memory if you have a large label space e g large output vocabulary for LMs translation models It also requires taking explicit care of padding symbols for variable length models It would be nice to have such a flag in the SoftmaxOutput operator to do the smoothing internally without adding another node to the computation graph,,"fhieber,zihaolucky,tqchen",2017-07-11 12:31:06,2017-10-19 04:45:51
IS,asnumpy of NDArray halted,I am running example rcnn demo py It succedded on gpu Then I try to run whole program on cpu But the program halted at line scores executor output dict 'cls prob reshape output' asnumpy 0 in file rcnn detector py then I modified the line to But it succeed so what happens does asnumpy has some bug,,"zihaolucky,piiswrong,piiswrong,precedenceguo,zihaolucky,zihaolucky,zihaolucky,precedenceguo,zihaolucky,zihaolucky,yzhliu",2016-11-04 08:12:51,2017-10-19 04:51:15
IS,Operator documents issues tracking,This issue tries to track problems in operator documents Please feel free to add or comments below if you find other operator documents that confuse you lack details or does not make sense You are also very welcome to contribute to the documentation See also 3504 and 3513 for some guidelines of documentations Activation Ref 3538 x Softmax activation The doc says Softmax Activation is only available with CUDNN on GPUand will be computed at each location across channel if input is 4D but the act type does not contain Softmax x softrelu what exactly is softrelu Maybe good to add formulas for each of the activation type x See also add link to LeakyReLU for people looking for leaky activations BatchNorm Details provide formula and reference See e g the torch doc nn BatchNormalization Shared statistics do we support BN with shared statistics for RNNs now If yes document it BlockGrad Details give an example use case in RNN unrolling Cast Concat data and num args since we specialized the calling convention for this kind of operators It might be good to also specialize their auto gen docstring so that they are less confusing Convolution Correlation Details provide formula or more explanation Crop Formats punctuations and spaces are missing E g will be used This function support we will use the h wfor crop height Examples maybe need examples I'm a bit confused by 2nd and 3rd dim is it counting from 0 or 1 Custom Details maybe add link to documentations on how to implement custom operators Deconvolution Details maybe add formula and references I often find it is very confusing when different toolkits use different names for similar things like deconvolution unconvolution convolution transpose dialated convolution etc Dropout Ref 3566 x Details I guess everybody knows dropout but adding a formula is not bad ElementWiseSum Doc not sure why this operator is not taking a data argument Maybe examples are needed Clarification what is the difference between this and Does this do something special Embedding Ref 3596 x Details and examples I find the doc quite confusing until I check the code in RNN examples The inputs are not one hot encoded but just integer indices The input dim is the input vocabulary size while input dim of one hot encoding sounds like the axis of the multi dim tensor that is used to do one hot encoding Flatten x Details FullyConnected IdentityAttachKLSparseReg Details formula and references if any L2Normalization Details Set the l2 norm of each instance to a constant but to what constant LRN Details the doc sounds like that this is just a convolution operator LeakyReLU Details formula for each activation type LinearRegressionOutput Details formula LogisticRegressionOutput Details formula MAERegressionOutput Details formula MakeLoss Example Pooling Details document expected input and output shape add examples RNN Details formula for each cell type ROIPooling Details maybe link to some references and example directory Reshape Format shape argument missing type SVMOutput Details SequenceLast SequenceMask SequenceReverse SliceChannel Softmax Maybe should remove this at some point Deprecated for a long time SoftmaxActivation SoftmaxOutput Fix If set to true for a n k x 1 x n dimensional input tensor softmax will generate n x 1 x n output does not seem to be correct the output is still the same shape as the input just normalized along one axis SpatialTransformer Details reference and example SwapAxis Details UpSampling Related issue,,"pluskid,tqchen",2016-10-13 20:32:15,2017-10-19 04:51:28
IS,OP Mathematical functions,See context 3198 Please feel free to edit this thread or leave comment Trigonometric functions Numpy Status Assignee Comments sin v cos v tan v 3317 arcsin v arccos v arctan v hypot v 3335 arctan v degrees v radians v unwrap x deg x rad x Hyperbolic functions Numpy Status Assignee Comments sinh v 3335 cosh v tanh v arcsinh v arccosh v arctanh v Rounding Numpy Status Assignee Comments around p has nd round round v rint x santosh fix x santosh floor v ceil v trunc x Sums Numpy Status Assignee Comments prod x sum p v no axis option which is supported by sum axis nanprod x nansum x cumprod x cumsum x nancumprod x nancumsum x diff x ediff1d x gradient cross x trapz x Exponents and logarithms Numpy Status Assignee Comments exp v expm1 v 3337 exp2 x santosh log v log10 x log2 x log1p v 3337 logaddexp x santosh logaddexp2 x Other special functions Numpy Status Assignee Comments i0 x sinc x Floating point routines Numpy Status Assignee Comments signbit x copysign x frexp x ldexp x Arithmetic operations Numpy Status Assignee Comments add v reciprocal x negative v multiply v divide v power v subtract v true divide v floor divide x fmod x mod x modf x remainder x Handling complex numbers Numpy Status Assignee Comments angle real imag conj Miscellaneous Numpy Status Assignee Comments convolve clip v sqrt v cbrt x square v absolute p see nd abs fabs p see nd abs sign v maximum v minimum v fmax x fmin x nan to num x real if close x interp entropy x fft2 ifft2 rfft2 irfft2,,"mli,piiswrong,sxjscience,mli,tqchen,taliesinb,piiswrong,thatindiandude",2016-09-02 00:21:42,2017-10-19 04:51:38
IS,RFC Documentation of MXNet,There has been some complaints about the documentation quality of MXNet We actually have a lot of documents but they are a bit scattered and maybe difficult to locate In this issue we attempt to make a framework so that we can better re organize the existing documents and provide guidelines for people to contribute more documents Please feel free to add your comments and opinions below to help make the documentation better Main TODOs Introduction Tutorial How To assigned to Re organize existing examples and make sure they runs correctly Add missing how tos API doctest for operators assigned to see PR 3513 Symbolic and NDArray op add brief examples see also comments below Add API Ref for other components see below FAQ UI Bug Menu not shown on iOS so we can only see the front page Organization The organization will be similar to the existing system Introduction Tutorials How To Cookbook Usecase API References FAQ Developer is Guide Guidelines for Documents Introduction The goal of Introduction is to get an overview of MXNet and a portal to get started download installation The same contents as the README With links to Download and Installation and other language bindings Tutorials Each tutorial should be step by step detailed document for something A tutorial is an entry point of a new user without prior experience of MXNet It should Be self contained and correctly runnable a user will quit if the first example he she tries does not run Be simple and easy to run e g even without CUDA Covers many useful components of MXNet With expected outputs when the users run it With step by step explanations Brief explanations of components e g DataIter with links to more detailed documentation iPython notebook might be a good format for detailed runnable code with documentations But we should add the scripts to our regression test using tools e g runipy that could run notebooks from the command line MNIST LeNet or MLP existing notebook RNN existing notebook How To HowTos are targeting users with basic understanding of MXNet and would like to do a specific thing with it Organized by tasks We have a rich examples repository that we could re use For runable examples the documentation can be a README md in that specific folder Applications Computer Vision Image Classification Segmentation Detection Neural Art Natural Language Processing Recurrent Neural Networks Convnet Text classification NCE Loss Speech Recognition Speech LSTM Baidu CTC Unsupervised Learning Generative Adversarial Networks Auto Encoders Reinforcement Learning General Machine Learning Kaggle Multi task SVM Recommendation System Training How to do fine tuning How to convert Caffe models Multi GPU data parallelism Multi GPU model parallelism Distributed Training Training with sublinear memory Run MXNet on clouds Run MXNet with Dockers Train with variable input sizes Bucketing Stochastic Depth Visualization plot Graph plot training curves etc Prediction Example of image classification pre trained Imagenet models Example of feature extraction using pre trained Imagenet models MXNet on smart phones Customization How to define your own DataIter How to define your own operator in Python How to define your own operator in C How to use Torch operators How to use Caffe operators How to define your own loss function explain conventions for a XXXOutput operator APIs Organizations Overview index to each components and what do they do Operators general operators Loss Layers XXXLoss or XXXOutput Module API and legacy Model API Optimizers Callbacks Initializers Eval Metrics Data Iters Plugin NDArray Internals Executors KVStore Context Monitor Each component ideally should have A summary of what it is A list of existing classes that users can use A description of the interface and optionally how to write a user customized callback eval metric etc can link back to HowTos It would be very helpful if each API function could have a brief example of how that function is called especially for those automatically generated functions The Concat mxnet symbol Concat operator is a good example The document says it takes data as Symbol and num args For people not familiar with the calling convention it is hard to figure out how to use this operator Having the examples below greatly improves the document It does not need to be a fully runnable example a brief line like is already useful Currently the extra doc for operators for Python can be attached via symbol doc py Some random example of good API reference with brief examples include Python re sub numpy numpy reshape Lasagne Keras etc FAQ fix gamma How to automatically resume training from saved snapshot How to set stage wise learning rate Why GPU is slower than CPU Why multi GPU data parallism is slower than single GPU How to get intermediate outputs of a network Developer is Guide Existing system design doc NNVM etc We also need a doc for the instructions of making a release run regression test doc changelog tag,,"pluskid,vchuravy,tornadomeet,pluskid,piiswrong,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,sandeep-krishnamurthy,piiswrong,sandeep-krishnamurthy,pluskid,pluskid,sandeep-krishnamurthy,pluskid,statist-bhfz",2016-10-11 20:59:49,2017-10-19 04:51:53
IS,PERF Call for NN Layer Kernel Improvement,The current NN Layers are implemented with mshadow and have performance issues They should be replaced with optimized cpu and gpu kernels These should be easy to adapt from caffe or torch is kernels Ideally we also want support for different layout NCHW and NHWC optionally CHWN Here is the inital list of layers convolution inl h deconvolution inl h batch norm inl h pooling inl h lrn inl h,,"piiswrong,piiswrong,piiswrong",2016-10-09 16:46:53,2017-10-19 04:52:02
IS,OP Array manipulation,See context 3198 Please feel free to edit this thread or leave comment Basic operations Numpy Status Assignee Comments copyto v Changing array shape Numpy Status Assignee Comments reshape v ravel x ndarray flat x ndarray flatten x Transpose Numpy Status Assignee Comments moveaxis x rollaxis x swapaxes v ndarray T v transpose p require mx nd tranpose d instead of a transpose Changing number of dimensions Numpy Status Assignee Comments atleast 1d x atleast 2d x atleast 3d x broadcast broadcast to v broadcast arrays x expand dims v squeeze x Changing kind of array Numpy Status Assignee Comments asarray p there is nd array asanyarray asmatrix asfarray x asfortranarray ascontiguousarray asarray chkfinite x asscalar v require Joining arrays Numpy Status Assignee Comments concatenate v axis support with dim stack v column stack x dstack x hstack x vstack x Splitting arrays Numpy Status Assignee Comments split p differ from numpy array split x dsplit x hsplit x vsplit x Tiling arrays Numpy Status Assignee Comments tile v repeat v Adding and removing elements Numpy Status Assignee Comments delete x insert x append x resize x trim zeros x unique x Rearranging elements Numpy Status Assignee Comments flip x fliplr x flipud x reshape v roll x rot x,,mli,2016-09-02 00:19:36,2017-10-19 04:52:13
IS,Check list for more operators,Lacking of operators is the major disadvantage of MXNet comparing to other frameworks The current multidimensional array interface follows the numpy ndarray convention See this tutorial for a quick overview Now we are going to add the rest numpy operators into MXNet and also fix the existing operators if their are different to the according ones in numpy I grabbed numpy is routines from this page The comparison to MXNet is operators will be listed in a few following issues For an operator there are 4 states v already done and it is consistent to numpy p partially done the part should be fixed is on the comments x not exists need to add into mxnet not exists but will not support in a short time not sure whether x or leave for discussion We will use separate issues to track each operator category The schedule is survey the current status and which operators should be added or fixed examples codes to implement new operators assign jobs to contributors Also some related tasks better operator documents better unittest improve neural network related operators,,"mli,tqchen",2016-09-02 00:13:16,2017-10-19 04:52:39
IS,how to use my own loss to compute grad,Now I want to use my own loss Class MyLoss but where to use it used like below model fit X X train eval metric MyLoss batch end callback mx callback Speedometer batch size 50,,"piiswrong,szha",2016-11-21 09:57:45,2017-10-19 04:53:05
IS,Building a visualization tool for MXNet,Hi hackers I have started working on building a visualization tool for MXNet like TensorBoard for TensorFlow As suggested in 3306 that ' try to strip TensorBoard out of tensorflow ' and I'm going to work on this direction Here are some of my notes after reading TensorBoard s documentation and searching its usage on the web feel free to comment below Motivation and some backgrounds I have tried to visualize the data using matplotlib and a bunch of helper tools like tsne on my daily work and I feel tired in rendering and adjusting the size color of the images Besides it is not easy to share this data with my friends While TensorBoard provides good solutions for our daily use cases such as learning curves parameters embedding visualization also it is easy to share See TensorBoard for more Daily use cases Learning Curves Visualize a scalar metric in training testing such as accuracy loss or some custom evaluation metrics Parameters insights Like CNN is filters Or parameters' histogram over time to debug gradient vanish by checking Embedding visualization Some applications will visualize high dimension embedding data in a layer like the last FullyConnect layer is output In this case tSNE is commonly used I think these could satisfy most people and it is already supported by TensorBoard with tf scalar summary tf image summary tf histogram summary and tensorboard plugins projector tensorboard embedding visualization TensorBoard usage Some snippets from a tutorial on how to use TensorBoard If we decide to borrow TensorBoard x Decide where to put the interface and what to do next based on our experiments above,,"zihaolucky,piiswrong,piiswrong,zihaolucky,tqchen,leopd,tqchen,zihaolucky,jermainewang,jermainewang,zihaolucky,zihaolucky,zihaolucky,zihaolucky,zihaolucky,terrytangyuan,zihaolucky,piiswrong,tqchen,zihaolucky,zihaolucky,zihaolucky,piiswrong,zihaolucky,zihaolucky,zihaolucky",2016-11-27 17:41:31,2017-10-19 04:53:33
IS,How to set auxiliary state in Batchnorm manually,I set beta and gamma with a scalar mx nd ones 1 moving var and moving mean with a vector same as the output size of convolution weight mx nd ones 64 but got these messages 17 57 39 mxnet dmlc core include dmlc logging h 235 17 57 39 include mxnet tensor blob h 742 Check failed this shape Size shape Size TBlob get with shape new and old shape do not match total elements 17 57 39 mxnet dmlc core include dmlc logging h 235 17 57 39 src engine threaded engine h 306 17 57 39 include mxnet tensor blob h 742 Check failed this shape Size shape Size TBlob get with shape new and old shape do not match total elements An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging terminate called after throwing an instance of wouldmlc Error' what 17 57 39 src engine threaded engine h 306 17 57 39 include mxnet tensor blob h 742 Check failed this shape Size shape Size TBlob get with shape new and old shape do not match total elements An fatal error occurred in asynchronous engine operation If you do not know what caused this error you can try set environment variable MXNET ENGINE TYPE to NaiveEngine and run with debugger i e gdb This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error Remember to set MXNET ENGINE TYPE back to empty after debugging,,,2016-11-26 10:14:50,2017-10-19 04:53:47
IS,Amalgamation using weBLAS JS WebGL,I have got mxnet amalgamation working in a browser JS webapp however the speed is really slow comparing with native code Is there a method for the amalgamation to use weBLAS which uses WebGL and is much much faster weBLAS is a simple library with the most commonly used operations It provides 32 bit versions of each of these sscal Matrix and Vector Scale with addition sgemm Matrix Multiply sdwns Matrix and Image Downsample for Max Pooling sclmp Matrix clamp for ReLU,,,2017-03-11 16:43:42,2017-10-19 04:54:15
IS,Building with libjpeg turbo error,Description I have installed libjpeg turbo by sudo apt install libjpeg turbo8 dev on Ubuntu 16 04 Environment info Required Minimum reproducible example Just set USE LIBJPEG TURBO 1 and type make j,,"chinakook,ptrendx,ptrendx,chinakook",2017-10-18 14:01:30,2017-10-19 05:08:35
IS,make build src operator activation gpu o Error 2,I want to build mxnet from the source I follow exactly like the instruction but getting error usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 M MT build src operator tensor sparse retain gpu o src operator tensor sparse retain cu build src operator tensor sparse retain gpu d usr local cuda bin nvcc c o build src operator tensor sparse retain gpu o std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 src operator tensor sparse retain cu usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 M MT build src common utils gpu o src common utils cu build src common utils gpu d usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 M MT build src ndarray ndarray function gpu o src ndarray ndarray function cu build src ndarray ndarray function gpu d usr local cuda bin nvcc c o build src common utils gpu o std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 src common utils cu usr local cuda bin nvcc c o build src ndarray ndarray function gpu o std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 src ndarray ndarray function cu usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 M MT build src operator activation gpu o src operator activation cu build src operator activation gpu d usr local cuda bin nvcc c o build src operator activation gpu o std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 src operator activation cu usr local cuda bin nvcc std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 M MT build src operator batch norm gpu o src operator batch norm cu build src operator batch norm gpu d usr local cuda bin nvcc c o build src operator batch norm gpu o std c 11 Xcompiler D FORCE INLINES O3 ccbin g gencode arch compute 30 code sm 30 gencode arch compute 35 code sm 35 gencode arch compute 50 code sm 50 gencode arch compute 52 code sm 52 gencode arch compute 60 code sm 60 gencode arch compute 61 code sm 61 compute 61 fatbin options compress all Xcompiler DMSHADOW FORCE STREAM Wall Wsign compare O3 DNDEBUG 1 I home lemma mxnet mshadow I home lemma mxnet dmlc core include fPIC I home lemma mxnet nnvm include I home lemma mxnet dlpack include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMXNET USE NNPACK 1 DMXNET USE MKL2017 1 DUSE MKL 1 I home lemma mxnet src operator mkl I usr local include DMKL EXPERIMENTAL 1 DMXNET USE LAPACK fno builtin malloc fno builtin calloc fno builtin realloc fno builtin free I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I home lemma mxnet cub DMXNET USE LIBJPEG TURBO 0 src operator batch norm cu home lemma mxnet mshadow mshadow cuda tensor gpu inl cuh 75 error expression preceding parentheses of apparent call must have pointer to function type home lemma mxnet mshadow mshadow cuda tensor gpu inl cuh 75 error expression preceding parentheses of apparent call must have pointer to function type 2 errors detected in the compilation of tmp tmpxft 00000c5c 00000000 24 activation compute 30 cpp2 i make build src operator activation gpu o Error 2 make Waiting for unfinished jobs Python Info 'Version ' '2 7 13' 'Compiler ' 'GCC 7 2 0' 'Build ' wouldefault' 'Sep 30 2017 18 12 43' 'Arch ' '64bit' '' Pip Info 'Version ' '9 0 1' 'Directory ' ' home lemma anaconda2 lib python2 7 site packages pip' MXNet Info An error occured trying to import mxnet This is very likely due to missing missing or incompatible library files Traceback most recent call last File diagnose py line 103 in check mxnet import mxnet File home lemma mxnet python mxnet init py line 25 in module from base import MXNetError File home lemma mxnet python mxnet base py line 111 in module LIB load lib File home lemma mxnet python mxnet base py line 102 in load lib lib path libinfo find lib path File home lemma mxnet python mxnet libinfo py line 59 in find lib path 'List of candidates n' str ' n' join dll path RuntimeError Cannot find the files List of candidates home lemma mxnet python mxnet libmxnet so home lemma mxnet python mxnet lib libmxnet so home lemma mxnet python mxnet build Release libmxnet so usr lib libmxnet so usr lib openblas base libmxnet so usr local cuda lib64 libmxnet so usr include boost libmxnet so usr include boost libmxnet so usr include arpack libmxnet so usr include boost python libmxnet so usr include atlas libmxnet so usr include armadillo bits libmxnet so usr include clang 3 8 include libmxnet so opt intel impi 2017 3 196 include64 libmxnet so opt intel impi 2017 3 196 bin64 libmxnet so opt intel mkl lib intel64 lin libmxnet so opt intel mkl include libmxnet so usr local cuda extras CUPTI lib64 libmxnet so home lemma liblinear libmxnet so opt intel mkl lib intel64 libmxnet so libmxnet so libmxnet so PYTHON DIAGNOSE OUTPUT System Info 'Platform ' 'Linux 4 4 0 96 generic x86 64 with debian jessie sid' isystem ' 'Linux' 'node ' 'lemma' arelease ' '4 4 0 96 generic' haversion ' ' 119 14 04 1 Ubuntu SMP Wed Sep 13 08 40 48 UTC 2017' Hardware Info 'machine ' 'x86 64' 'processor ' 'x86 64' Architecture x86 64 CPU op mode s 32 bit 64 bit Byte Order Little Endian CPU s 4 On line CPU s list 0 3 Thread s per core 1 Core s per socket 4 Socket s 1 NUMA node s 1 Vendor ID GenuineIntel CPU family 6 Model 42 Stepping 7 CPU MHz 2716 699 BogoMIPS 6585 08 Virtualization VT x L1d cache 32K L1i cache 32K L2 cache 256K L3 cache 6144K NUMA node0 CPU s 0 3 Network Test Setting timeout 10 Timing for MXNet DNS 0 0048 sec LOAD 1 3020 sec Timing for PYPI DNS 0 0278 sec LOAD 0 4209 sec Timing for FashionMNIST DNS 0 1405 sec LOAD 0 9515 sec Timing for Conda DNS 0 1107 sec LOAD 0 1716 sec Timing for Gluon Tutorial en DNS 0 0021 sec LOAD 0 4672 sec Timing for Gluon Tutorial cn DNS 0 4809 sec LOAD 0 8304 sec MXNet commit hash 9034855a1f251310ee487ff780f69912d5a844c3 Build config USE CUDA 1 USE CUDA PATH usr local cuda USE MKL2017 1 USE BLAS openblas USE MKL2017 EXPERIMENTAL 1 USE BLAS openblas USE NNPACK 1 USE S3 1 ADD LDFLAGS L home lemma NNPACK lib ADD CFLAGS I home lemma NNPACK include I home lemma NNPACK deps pthreadpool include I have NVidia GTX560 single CUDA8 0 Intel i5 4 cores,,,2017-10-18 08:40:11,2017-10-19 05:44:58
IS,Bug in example,1 In example ssd evaluate net py Line84 DetRecordIter using the default RGB means not the params set in example ssd evaluate py 2 In example rcnn rcnn symbol symbol vgg py and symbol resnet py all mx symbol contrib Proposal need to update to mx contrib symbol Proposal,,"zhreshold,zhreshold",2017-10-17 02:39:18,2017-10-19 21:57:44
IS,Incorrect implied shape inside loss function,Ive seen this brought up in a couple other issues but it hasnt been resolved as far as I know The data I am feeding into my loss function is of the following shape batch size 32 When the output and label are fed into the loss function tho I get the following error MXNetError Shape inconsistent Provided 32 51 inferred shape 32 1 Why is the loss function implying an incorrect shape In the line above said loss function Gluon knows the correct shape of each input matrix but the loss function auto implies the shape incorrectly,,"zhreshold,zhreshold,zhreshold,zhreshold",2017-10-19 18:45:47,2017-10-19 22:41:41
PR,add warmup lr scheduler,Add warmup lr scheduler it can overcome optimization challenges early in large batch size This method was used in Accurate Large Minibatch SGD Training ImageNet in 1 Hour,,"solin319,zhreshold,zhreshold,zhreshold,zhreshold,zhreshold,solin319,solin319,piiswrong,solin319,solin319,solin319,zhreshold,solin319,zhreshold,solin319",2017-09-23 08:30:54,2017-10-20 07:03:48
IS,How to add NNVM operator with auxiliary states,When adding new operators with auxiliary states how to set the attributes with NNVM REGISTER OP,,zheng-da,2017-10-20 03:13:05,2017-10-20 08:57:12
IS,'module' object has no attribute 'nd',Description 'module' object has no attribute 'nd' when run command python mxnet py while a short MXNet python program for validating MXNet Installation runs successfully in terminal Environment info Required system ubuntu 14 04 mxnet version 0 11 0 Everything is okay However when I write these code to python file mxnet py and run python mxnet py it returned the mentioned error It seems that it did not find mxnet library,,,2017-10-20 13:25:57,2017-10-20 13:32:45
PR,Misc fixes for sparse distributed training,Description As 8116 removes wait to write diff 0cd6fcb2cd941d4c4a829bb3d7ea3d63L274 when updating comm buff it is not safe to pass NDArray to the callback for row sparse pull Now changed to NDArray Removed the usage of mshadow range in FillDnsZerosRspImpl since mshadow range uses float to calculate the tensor shape and is inaccurate for large shapes Added unit test for pulling empty sparse weights Removed wrong misleading comments Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"eric-haibin-lin,rahul003",2017-10-19 04:57:28,2017-10-21 07:23:21
PR,Fix typo,Description Fix typo in example readme Checklist Essentials NA Passed code style checking make lint X Changes are complete i e I finished coding on this PR NA All changes have test coverage NA For user facing API changes API doc string has been updated X To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes N A Comments N A,,"piiswrong,piiswrong",2017-10-20 20:45:47,2017-10-21 14:22:05
PR,Fix the Readme,Description Removed the link to the non existent 0 12 0 tag Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Updated README md,,mbaijal,2017-10-21 02:38:24,2017-10-21 19:02:16
PR,Allow test to converge,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-10-19 21:05:14,2017-10-21 19:06:22
PR,solve problem in print cudnn autotune,Convolution algorithm was cached the results of Get as well as Find So we must use param cudnn tune to control weather to print cudnn autotune message 7631,,"solin319,piiswrong,solin319",2017-09-22 02:12:09,2017-10-21 19:13:03
PR,Perl emulate Python zip for Perl,An alternate version of zip that can be used a bit more like python is version and runs a bit faster gives Rate orig repl pure orig 21964 s 32 37 repl 32190 s 47 7 pure 34657 s 58 8,,"tlby,sergeykolychev,tlby,sergeykolychev,tlby,sergeykolychev,tlby,sergeykolychev,sergeykolychev,tlby,sergeykolychev,tlby,tlby",2017-10-09 17:42:24,2017-10-21 19:15:09
PR,add profile option for frontend profiling to image script,,,"szha,eric-haibin-lin,szha,szha",2017-10-07 20:51:44,2017-10-21 21:32:37
PR,Fix Typo classification,Description Fix a typo in the example readme Checklist Essentials NA Passed code style checking make lint X Changes are complete i e I finished coding on this PR NA All changes have test coverage NA For user facing API changes API doc string has been updated X To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,,2017-10-21 14:25:47,2017-10-21 23:38:16
IS,test profiler test profiler failure Jetson TX2 CPU,Description The test test profiler test profiler is consistently failing Environment info Required Jetson TX2 aarch64 Ubuntu 16 04 Jetpack 3 1 full install Error Message ERROR test profiler test profiler Traceback most recent call last File usr local lib python2 7 dist packages nose case py line 197 in runTest self test self arg File home nvidia incubator mxnet cpu tests python unittest test profiler py line 31 in test profiler profiler profiler set config mode isymbolic' filename profile filename File home nvidia incubator mxnet cpu python mxnet profiler py line 41 in profiler set config c str filename File home nvidia incubator mxnet cpu python mxnet base py line 146 in check call raise MXNetError py str LIB MXGetLastError MXNetError 08 21 50 src c api c api cc 104 Need to compile with USE PROFILER 1 for MXNet Profiler Stack trace returned 5 entries bt 0 home nvidia incubator mxnet cpu python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x44 0x7f96593114 bt 1 home nvidia incubator mxnet cpu python mxnet lib libmxnet so MXSetProfilerConfig 0x50 0x7f97f1a828 bt 2 usr lib aarch64 linux gnu libffi so 6 ffi call SYSV 0x64 0x7f9dcfde60 bt 3 usr lib aarch64 linux gnu libffi so 6 ffi call 0xc0 0x7f9dcfe7b8 bt 4 usr lib python2 7 lib dynload ctypes aarch64 linux gnu so ctypes callproc 0x670 0x7f9dd3cb30,,KellenSunderland,2017-10-22 08:58:13,2017-10-22 09:29:39
PR,Timing output for test factorization module when Verbose enabled,Description This is a real world type sparse model test so it is often helpful to enable timing outputs Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"cjolivier01,szha",2017-10-20 20:37:44,2017-10-22 22:08:57
PR,Use omp get max threads when OMP NUM THREADS environment set,Using wrong API call here Only relevant if OMP NUM THREADS environment variable is set Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-10-22 01:59:55,2017-10-22 22:11:07
PR,CPU optimization for ActivationOp,Significant improvement on CPU magnitude of order in some cases especially on backward pass Very slight improvement on GPU The single outlier on CPU is forward pass for 1x1x28x28 OLD MSHADOW APPROACH CPU Timing 50 iterations of 10 calls shape 1 1 28 28 Activation Operator CPU Timing Forward 18 948 ms avg 0 037896 ms X 500 passes Activation Operator CPU Timing Backward 1 658 ms avg 0 003316 ms X 500 passes Timing 50 iterations of 10 calls shape 1 3 28 28 Activation Operator CPU Timing Forward 57 973 ms avg 0 115946 ms X 500 passes Activation Operator CPU Timing Backward 4 748 ms avg 0 009496 ms X 500 passes Timing 50 iterations of 10 calls shape 50 1 18 32 Activation Operator CPU Timing Forward 703 446 ms avg 1 40689 ms X 500 passes Activation Operator CPU Timing Backward 56 255 ms avg 0 11251 ms X 500 passes Timing 50 iterations of 10 calls shape 50 3 18 32 Activation Operator CPU Timing Forward 2107 77 ms avg 4 21554 ms X 500 passes Activation Operator CPU Timing Backward 168 483 ms avg 0 336966 ms X 500 passes Timing 50 iterations of 10 calls shape 20 3 128 128 Activation Operator CPU Timing Forward 24122 2 ms avg 48 2443 ms X 500 passes Activation Operator CPU Timing Backward 1908 7 ms avg 3 8174 ms X 500 passes GPU Timing 50 iterations of 10 calls shape 1 1 28 28 Activation Operator GPU Timing Forward 1 637 ms avg 0 003274 ms X 500 passes Activation Operator GPU Timing Backward 1 665 ms avg 0 00333 ms X 500 passes Timing 50 iterations of 10 calls shape 1 3 28 28 Activation Operator GPU Timing Forward 1 562 ms avg 0 003124 ms X 500 passes Activation Operator GPU Timing Backward 1 661 ms avg 0 003322 ms X 500 passes Timing 50 iterations of 10 calls shape 50 1 18 32 Activation Operator GPU Timing Forward 1 635 ms avg 0 00327 ms X 500 passes Activation Operator GPU Timing Backward 1 702 ms avg 0 003404 ms X 500 passes Timing 50 iterations of 10 calls shape 50 3 18 32 Activation Operator GPU Timing Forward 1 83 ms avg 0 00366 ms X 500 passes Activation Operator GPU Timing Backward 2 041 ms avg 0 004082 ms X 500 passes Timing 50 iterations of 10 calls shape 20 3 128 128 Activation Operator GPU Timing Forward 2 08 ms avg 0 00416 ms X 500 passes Activation Operator GPU Timing Backward 2 688 ms avg 0 005376 ms X 500 passes NEW MXNET OP APPROACH CPU Timing 50 iterations of 10 calls shape 1 1 28 28 Activation Operator CPU Timing Forward 80 748 ms avg 0 161496 ms X 500 passes Activation Operator CPU Timing Backward 1 176 ms avg 0 002352 ms X 500 passes Timing 50 iterations of 10 calls shape 1 3 28 28 Activation Operator CPU Timing Forward 7 881 ms avg 0 015762 ms X 500 passes Activation Operator CPU Timing Backward 2 181 ms avg 0 004362 ms X 500 passes Timing 50 iterations of 10 calls shape 50 1 18 32 Activation Operator CPU Timing Forward 111 48 ms avg 0 22296 ms X 500 passes Activation Operator CPU Timing Backward 5 408 ms avg 0 010816 ms X 500 passes Timing 50 iterations of 10 calls shape 50 3 18 32 Activation Operator CPU Timing Forward 333 439 ms avg 0 666878 ms X 500 passes Activation Operator CPU Timing Backward 21 331 ms avg 0 042662 ms X 500 passes Timing 50 iterations of 10 calls shape 20 3 128 128 Activation Operator CPU Timing Forward 3429 19 ms avg 6 85837 ms X 500 passes Activation Operator CPU Timing Backward 286 324 ms avg 0 572648 ms X 500 passes GPU Timing 50 iterations of 10 calls shape 1 1 28 28 Activation Operator GPU Timing Forward 1 618 ms avg 0 003236 ms X 500 passes Activation Operator GPU Timing Backward 1 671 ms avg 0 003342 ms X 500 passes Timing 50 iterations of 10 calls shape 1 3 28 28 Activation Operator GPU Timing Forward 1 629 ms avg 0 003258 ms X 500 passes Activation Operator GPU Timing Backward 1 728 ms avg 0 003456 ms X 500 passes Timing 50 iterations of 10 calls shape 50 1 18 32 Activation Operator GPU Timing Forward 1 753 ms avg 0 003506 ms X 500 passes Activation Operator GPU Timing Backward 1 756 ms avg 0 003512 ms X 500 passes Timing 50 iterations of 10 calls shape 50 3 18 32 Activation Operator GPU Timing Forward 1 704 ms avg 0 003408 ms X 500 passes Activation Operator GPU Timing Backward 1 791 ms avg 0 003582 ms X 500 passes Timing 50 iterations of 10 calls shape 20 3 128 128 Activation Operator GPU Timing Forward 2 032 ms avg 0 004064 ms X 500 passes Activation Operator GPU Timing Backward 2 143 ms avg 0 004286 ms X 500 passes Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-10-16 18:54:33,2017-10-23 03:41:15
IS,Training set NDArrayIter empty after first epoch,The code contained within the loop for i batch in enumerate train data is only run once which I assume is because for some reason train data is empty undefined after the first full pass thru the data first epoch I have to redefine train data below at the end of the for e in range epochs loop in order for every epoch to run train data mx io NDArrayIter train data mx train label mx batch size shuffle True Am I missing a flag somewhere Why would this be happening,,"reminisce,reminisce,reminisce,reminisce,reminisce,reminisce",2017-10-21 03:27:20,2017-10-23 05:40:35
IS,SqueezeNet pretrained model download link not working,The link for squeezenet model is not working Please upload the squeezenet model,,zhreshold,2017-10-18 05:37:03,2017-10-23 05:47:39
IS,Does gluon have a crop layer like caffe crop layer,Package used Python R Scala Julia I'm using Python What have you tried to solve it 1 look at the gluon document 2 look at the gluon source code,,"zhreshold,chinakook",2017-10-22 13:46:47,2017-10-23 13:23:51
IS,mxnet gluon data vision ImageRecordDataset key error,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description Brief description of the problem in no more than 2 sentences Steps to reproduce Paste the commands you ran that produced the error 1 generate record file 2 python incubator mxnet tools im2rec py shape images shape 1 recursive true train ratio 0 9 list true 3 python incubator mxnet tools im2rec py shape images shape 1 4 Use any network to train your own record file the error will show What have you tried to solve it 1 regenerate record file 2 use python2 7 and python3 to generate record file,,zhreshold,2017-10-19 13:14:38,2017-10-23 13:24:05
IS,dist sync kvstore can not load balance,hi I do distributed training with 80000 classes the base network is inception resent v2 the last predict layer are concat by 8 fc layer weight shape is 10000 512 train with 4 machine 8 gpu 4 worker 4 server I find the networkflow are not balance 3 machine with 4 Gb s and 1 machine with 7Gbs how can I may load balance with dist kvstore 8 inception restnet v2 4 8gpu 4worker 4 server 8 gpu 4 3 4Gb s 7Gb s mxnet dist kvstore softmax loss kvstore,,"junranhe,solin319,junranhe",2017-10-16 10:38:08,2017-10-24 01:34:27
PR,CMAKE ARM USE SSE option to govern USE SSE in mshadow and msse2 compil,er flags Updated mshadow Description This fixes Cmake build on ARM NO SSE support nor X86 specific includes like emmintrin h,,"larroy,cjolivier01,cjolivier01,larroy,larroy,cjolivier01,cjolivier01,larroy,larroy,larroy,cjolivier01,larroy",2017-10-23 09:52:47,2017-10-24 10:00:45
IS,gluon loss SoftmaxCrossEntropyLoss missed ignore label parameter,I found the mx symbol SoftmaxOutput has parameter ignore label however i can not find gluon loss SoftmaxCrossEntropyLoss has such parameter if gluon loss SoftmaxCrossEntropyLoss does not have such parameter how can i use loss function with ingore label pull have added ignore label how can i use it with gluon,,"chinakook,chinakook",2017-10-24 02:32:32,2017-10-24 12:51:23
PR,Pvanet Deep but Lightweight Neural Neural Networks for Real time Object Detection,article adress Pvanet Deep but Lightweight Neural Neural Networks for Real time Object Detection Result of classification network INFO root Epoch 82 Batch 2000 Speed 586 96 samples sec accuracy 0 668164 top k accuracy 5 0 874805 INFO root Epoch 82 Batch 2050 Speed 586 14 samples sec accuracy 0 664766 top k accuracy 5 0 876250 INFO root Epoch 82 Batch 2100 Speed 589 28 samples sec accuracy 0 668438 top k accuracy 5 0 870938 INFO root Epoch 82 Batch 2150 Speed 587 12 samples sec accuracy 0 669766 top k accuracy 5 0 877266 INFO root Epoch 82 Batch 2200 Speed 590 23 samples sec accuracy 0 664297 top k accuracy 5 0 874922 INFO root Epoch 82 Batch 2250 Speed 584 57 samples sec accuracy 0 672266 top k accuracy 5 0 876836 INFO root Epoch 82 Batch 2300 Speed 590 03 samples sec accuracy 0 674492 top k accuracy 5 0 876172 INFO root Epoch 82 Batch 2350 Speed 588 57 samples sec accuracy 0 670820 top k accuracy 5 0 874453 INFO root Epoch 82 Batch 2400 Speed 587 81 samples sec accuracy 0 673672 top k accuracy 5 0 876094 INFO root Epoch 82 Batch 2450 Speed 591 53 samples sec accuracy 0 671406 top k accuracy 5 0 873828 INFO root Epoch 82 Batch 2500 Speed 582 21 samples sec accuracy 0 671992 top k accuracy 5 0 874805 INFO root Epoch 82 Train accuracy 0 663086 INFO root Epoch 82 Train top k accuracy 5 0 849609 INFO root Epoch 82 Time cost 2180 302 INFO root Saved checkpoint to pvanet models pvanet 0083 params INFO root Epoch 82 Validation accuracy 0 640804 INFO root Epoch 82 Validation top k accuracy 5 0 854931 Result of rpn and faster rcnn training INFO root Epoch 9 Batch 9940 Speed 2 57 samples sec RPNAcc 0 991533 RPNLogLoss 0 023411 RPNL1Loss 0 322666 RCNNAcc 0 943286 RCNNLogLoss 0 157866RCNNL1Loss 0 834379 INFO root Epoch 9 Batch 9960 Speed 2 66 samples sec RPNAcc 0 991529 RPNLogLoss 0 023437 RPNL1Loss 0 322645 RCNNAcc 0 943291 RCNNLogLoss 0 157865RCNNL1Loss 0 834185 INFO root Epoch 9 Batch 9980 Speed 2 48 samples sec RPNAcc 0 991519 RPNLogLoss 0 023454 RPNL1Loss 0 322520 RCNNAcc 0 943320 RCNNLogLoss 0 157764RCNNL1Loss 0 833864 INFO root Epoch 9 Batch 10000 Speed 2 63 samples sec RPNAcc 0 991529 RPNLogLoss 0 023437 RPNL1Loss 0 322378 RCNNAcc 0 943317 RCNNLogLoss 0 157765 RCNNL1Loss 0 833716 INFO root Epoch 9 Batch 10020 Speed 2 49 samples sec RPNAcc 0 991519 RPNLogLoss 0 023461 RPNL1Loss 0 322260 RCNNAcc 0 943335 RCNNLogLoss 0 157711 RCNNL1Loss 0 833482 INFO root Epoch 9 Train RPNAcc 0 991520 INFO root Epoch 9 Train RPNLogLoss 0 023459 INFO root Epoch 9 Train RPNL1Loss 0 322241 INFO root Epoch 9 Train RCNNAcc 0 943339 INFO root Epoch 9 Train RCNNLogLoss 0 157700 INFO root Epoch 9 Train RCNNL1Loss 0 833458 INFO root Epoch 9 Time cost 3940 801 INFO root Saved checkpoint to model e2e 0010 params Result of the whole Object Detection network INFO root reading annotations for 4301 4952 INFO root reading annotations for 4401 4952 INFO root reading annotations for 4501 4952 INFO root reading annotations for 4601 4952 INFO root reading annotations for 4701 4952 INFO root reading annotations for 4801 4952 INFO root reading annotations for 4901 4952 INFO root saving annotations cache to data cache voc 2007 test annotations pkl INFO root AP for aeroplane 0 5641 INFO root AP for bicycle 0 6811 INFO root AP for bird 0 5965 INFO root AP for boat 0 4123 INFO root AP for bottle 0 3641 INFO root AP for bus 0 7021 INFO root AP for car 0 7254 INFO root AP for cat 0 7801 INFO root AP for chair 0 3233 INFO root AP for cow 0 5812 INFO root AP for diningtable 0 5688 INFO root AP for dog 0 7511 INFO root AP for horse 0 7783 INFO root AP for motorbike 0 6953 INFO root AP for person 0 6612 INFO root AP for pottedplant 0 2878 INFO root AP for sheep 0 5344 INFO root AP for sofa 0 5900 INFO root AP for train 0 6550 INFO root AP for tvmonitor 0 6214 INFO root Mean AP 0 5937 Mark 1 Currently the classification network result is 64 70 6 as article result of object detection is poor than article is I will improve it in the days ahead 2 There has no 4 contributors as show actually it is just me but I used different computers and proxy next time I will pay attention to it,,"qingzhouzhen,piiswrong,qingzhouzhen,qingzhouzhen,chinakook,piiswrong,qingzhouzhen,piiswrong,piiswrong,qingzhouzhen,qingzhouzhen,szha",2017-09-07 03:46:22,2017-10-25 01:10:00
PR,Sparse operator performance improvement,Description Fix for reported long running unit test test module test factorization module py Do not use OMP for some simple sparse specific operations since the OMP overhead dramatically reduces performance even under relatively dense conditions such as the referenced unit test above Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-10-24 19:34:17,2017-10-25 03:59:18
PR,dtype default to source array dtype for sparse ndarrays,Description csr matrix source always creates a new CSRNDArray with dtype float32 instead of using source dtype which is not consistent with scipy sparse csr matrix Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"eric-haibin-lin,piiswrong,eric-haibin-lin,piiswrong",2017-10-24 02:28:48,2017-10-25 04:09:06
PR,fix using default mean pixels,Description Hotfix for default mean pixel values for example ssd evaluate py Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change 8310,,zhreshold,2017-10-19 21:55:05,2017-10-25 04:17:27
PR,fix gluon data RecordFileDataset,Description Fix 8348 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Comments Fix KeyError when idx in xxx idx file is not countinous or not starting from 0,,"zhreshold,piiswrong",2017-10-19 22:43:35,2017-10-25 04:19:01
IS,DeferredInitializationError after calling trainer step,Description Dear all am completely new to mxnet I am trying to implement a simple unet architecture for semantic segmentation with mxnet url This is my unet implementation,,,2017-10-25 05:49:16,2017-10-25 05:57:13
IS,MXNet training slowing down after several epochs inception bn with 1 million images,Hi I encounter the training speed slowing down issue during my model training Specifically I am using P2 16xlarge of Amazon EC2 instance to train an inception bn network with 1 million of images The AMI I am using is Deep Learning AMI Amazon Linux Version 3 3 Oct2017 the version of MXNet is 0 11 0 The training speed keep going down after around 5 epochs Here are some example logs INFO root Epoch 6 Batch 420 Speed 490 41 samples sec accuracy 0 479903 INFO root Epoch 6 Batch 430 Speed 486 56 samples sec accuracy 0 479966 INFO root Epoch 6 Batch 440 Speed 497 60 samples sec accuracy 0 480021 INFO root Epoch 6 Batch 450 Speed 492 58 samples sec accuracy 0 480291 INFO root Epoch 6 Batch 460 Speed 528 04 samples sec accuracy 0 480422 INFO root Epoch 6 Batch 470 Speed 464 52 samples sec accuracy 0 480718 INFO root Epoch 6 Batch 480 Speed 484 11 samples sec accuracy 0 480919 INFO root Epoch 6 Batch 490 Speed 484 18 samples sec accuracy 0 480966 INFO root Epoch 6 Batch 500 Speed 495 10 samples sec accuracy 0 481116 INFO root Epoch 6 Batch 510 Speed 486 60 samples sec accuracy 0 481291 INFO root Epoch 6 Batch 520 Speed 484 40 samples sec accuracy 0 481406 INFO root Epoch 6 Batch 530 Speed 491 89 samples sec accuracy 0 481499 INFO root Epoch 6 Batch 540 Speed 489 70 samples sec accuracy 0 481537 INFO root Epoch 6 Batch 550 Speed 498 87 samples sec accuracy 0 481486 INFO root Epoch 6 Batch 560 Speed 491 44 samples sec accuracy 0 481388 INFO root Epoch 6 Batch 570 Speed 496 57 samples sec accuracy 0 481283 INFO root Epoch 6 Batch 580 Speed 498 24 samples sec accuracy 0 481148 INFO root Epoch 6 Batch 590 Speed 496 46 samples sec accuracy 0 481292 INFO root Epoch 6 Batch 600 Speed 502 20 samples sec accuracy 0 481486 INFO root Epoch 6 Batch 610 Speed 493 88 samples sec accuracy 0 481719 INFO root Epoch 6 Batch 620 Speed 487 72 samples sec accuracy 0 481916 INFO root Epoch 6 Batch 630 Speed 492 16 samples sec accuracy 0 482013 INFO root Epoch 6 Batch 640 Speed 490 07 samples sec accuracy 0 482199 INFO root Epoch 6 Batch 650 Speed 496 52 samples sec accuracy 0 482236 INFO root Epoch 6 Batch 660 Speed 488 37 samples sec accuracy 0 482194 INFO root Epoch 6 Batch 670 Speed 489 15 samples sec accuracy 0 482369 INFO root Epoch 6 Batch 680 Speed 486 11 samples sec accuracy 0 482336 INFO root Epoch 6 Batch 690 Speed 487 78 samples sec accuracy 0 482385 INFO root Epoch 6 Batch 700 Speed 488 26 samples sec accuracy 0 482207 INFO root Epoch 6 Batch 710 Speed 483 65 samples sec accuracy 0 482353 INFO root Epoch 6 Batch 720 Speed 488 62 samples sec accuracy 0 482530 INFO root Epoch 6 Batch 730 Speed 492 90 samples sec accuracy 0 482764 INFO root Epoch 6 Batch 740 Speed 485 88 samples sec accuracy 0 483002 INFO root Epoch 6 Batch 750 Speed 487 54 samples sec accuracy 0 483150 INFO root Epoch 6 Batch 760 Speed 489 82 samples sec accuracy 0 483384 INFO root Epoch 6 Batch 770 Speed 481 67 samples sec accuracy 0 483433 INFO root Epoch 6 Batch 780 Speed 491 70 samples sec accuracy 0 483405 INFO root Epoch 6 Batch 790 Speed 482 48 samples sec accuracy 0 483634 INFO root Epoch 6 Batch 800 Speed 488 26 samples sec accuracy 0 483785 INFO root Epoch 6 Batch 810 Speed 486 53 samples sec accuracy 0 483826 INFO root Epoch 6 Batch 820 Speed 478 62 samples sec accuracy 0 483890 INFO root Epoch 6 Batch 830 Speed 478 11 samples sec accuracy 0 484072 INFO root Epoch 6 Batch 840 Speed 484 99 samples sec accuracy 0 484117 INFO root Epoch 6 Batch 850 Speed 478 26 samples sec accuracy 0 484237 INFO root Epoch 6 Batch 860 Speed 482 54 samples sec accuracy 0 484493 INFO root Epoch 6 Batch 870 Speed 479 48 samples sec accuracy 0 484404 INFO root Epoch 47 Batch 240 Speed 334 17 samples sec accuracy 0 916478 INFO root Epoch 47 Batch 250 Speed 335 57 samples sec accuracy 0 916451 INFO root Epoch 47 Batch 260 Speed 333 79 samples sec accuracy 0 916337 INFO root Epoch 47 Batch 270 Speed 334 70 samples sec accuracy 0 916426 INFO root Epoch 47 Batch 280 Speed 332 91 samples sec accuracy 0 916433 INFO root Epoch 47 Batch 290 Speed 333 09 samples sec accuracy 0 916304 INFO root Epoch 47 Batch 300 Speed 333 47 samples sec accuracy 0 916262 INFO root Epoch 47 Batch 310 Speed 332 90 samples sec accuracy 0 916235 INFO root Epoch 47 Batch 320 Speed 330 95 samples sec accuracy 0 916241 INFO root Epoch 47 Batch 330 Speed 332 39 samples sec accuracy 0 916210 INFO root Epoch 47 Batch 340 Speed 332 93 samples sec accuracy 0 916016 INFO root Epoch 47 Batch 350 Speed 332 01 samples sec accuracy 0 915943 INFO root Epoch 47 Batch 360 Speed 332 62 samples sec accuracy 0 915826 INFO root Epoch 47 Batch 370 Speed 332 01 samples sec accuracy 0 915958 INFO root Epoch 47 Batch 380 Speed 331 88 samples sec accuracy 0 916082 INFO root Epoch 47 Batch 390 Speed 331 62 samples sec accuracy 0 916041 INFO root Epoch 47 Batch 400 Speed 331 34 samples sec accuracy 0 916152 INFO root Epoch 47 Batch 410 Speed 331 10 samples sec accuracy 0 916344 INFO root Epoch 47 Batch 420 Speed 331 38 samples sec accuracy 0 916521 INFO root Epoch 47 Batch 430 Speed 330 01 samples sec accuracy 0 916437 INFO root Epoch 47 Batch 440 Speed 329 00 samples sec accuracy 0 916503 INFO root Epoch 47 Batch 450 Speed 330 19 samples sec accuracy 0 916592 INFO root Epoch 47 Batch 460 Speed 328 81 samples sec accuracy 0 916715 INFO root Epoch 47 Batch 470 Speed 328 26 samples sec accuracy 0 916729 INFO root Epoch 47 Batch 480 Speed 328 83 samples sec accuracy 0 916722 INFO root Epoch 47 Batch 490 Speed 328 94 samples sec accuracy 0 916557 INFO root Epoch 47 Batch 500 Speed 328 48 samples sec accuracy 0 916616 INFO root Epoch 47 Batch 510 Speed 328 63 samples sec accuracy 0 916623 INFO root Epoch 47 Batch 520 Speed 328 70 samples sec accuracy 0 916585 INFO root Epoch 47 Batch 530 Speed 327 73 samples sec accuracy 0 916667 INFO root Epoch 47 Batch 540 Speed 326 74 samples sec accuracy 0 916669 INFO root Epoch 47 Batch 550 Speed 327 39 samples sec accuracy 0 916632 INFO root Epoch 47 Batch 560 Speed 327 91 samples sec accuracy 0 916656 INFO root Epoch 47 Batch 570 Speed 327 19 samples sec accuracy 0 916720 INFO root Epoch 47 Batch 580 Speed 326 79 samples sec accuracy 0 916668 INFO root Epoch 47 Batch 590 Speed 326 28 samples sec accuracy 0 916710 INFO root Epoch 47 Batch 600 Speed 325 61 samples sec accuracy 0 916744 INFO root Epoch 47 Batch 610 Speed 326 42 samples sec accuracy 0 916776 INFO root Epoch 47 Batch 620 Speed 326 10 samples sec accuracy 0 916758 INFO root Epoch 47 Batch 630 Speed 325 54 samples sec accuracy 0 916842 INFO root Epoch 47 Batch 640 Speed 326 44 samples sec accuracy 0 916777 INFO root Epoch 47 Batch 650 Speed 324 77 samples sec accuracy 0 916733 INFO root Epoch 47 Batch 660 Speed 324 26 samples sec accuracy 0 916772 INFO root Epoch 47 Batch 670 Speed 324 54 samples sec accuracy 0 916740 INFO root Epoch 47 Batch 680 Speed 326 14 samples sec accuracy 0 916713 Even though the training have not finished the speed now is even slower than I used a P2 8xlarge which just have 8 GPUs to train the net 330 samples sec Is there anybody looking into this issue If you need any more information please let me know Thanks beforehand,,reminisce,2017-10-21 00:30:43,2017-10-25 07:37:59
IS,Find models' definition of python style in Model Zoo in Gluon Package,How can I get the model is python definition in Gluon Package Not only json file For example for vgg16 as def vgg16 symbol data mx sym Variable wouldata' conv1 conv1 1 mx sym Convolution data data kernel 3 3 pad 1 1 num filter 64 name conv1 1 relu1 1 mx sym Activation data conv1 1 act type relu name relu1 1 conv1 2 mx sym Convolution data relu1 1 kernel 3 3 pad 1 1 num filter 64 name conv1 2 relu1 2 mx sym Activation data conv1 2 act type relu name relu1 2 pool1 mx sym Pooling data relu1 2 kernel 2 2 stride 2 2 pool type max name pool1 conv2 conv2 1 mx sym Convolution data pool1 kernel 3 3 pad 1 1 num filter 128 name conv2 1 relu2 1 mx sym Activation data conv2 1 act type relu name relu2 1 conv2 2 mx sym Convolution data relu2 1 kernel 3 3 pad 1 1 num filter 128 name conv2 2 relu2 2 mx sym Activation data conv2 2 act type relu name relu2 2 pool2 mx sym Pooling data relu2 2 kernel 2 2 stride 2 2 pool type max name pool2 conv3 conv3 1 mx sym Convolution data pool2 kernel 3 3 pad 1 1 num filter 256 name conv3 1 relu3 1 mx sym Activation data conv3 1 act type relu name relu3 1 conv3 2 mx sym Convolution data relu3 1 kernel 3 3 pad 1 1 num filter 256 name conv3 2 relu3 2 mx sym Activation data conv3 2 act type relu name relu3 2 conv3 3 mx sym Convolution data relu3 2 kernel 3 3 pad 1 1 num filter 256 name conv3 3 relu3 3 mx sym Activation data conv3 3 act type relu name relu3 3 pool3 mx sym Pooling data relu3 3 kernel 2 2 stride 2 2 pool type max name pool3 conv4 conv4 1 mx sym Convolution data pool3 kernel 3 3 pad 1 1 num filter 512 name conv4 1 relu4 1 mx sym Activation data conv4 1 act type relu name relu4 1 conv4 2 mx sym Convolution data relu4 1 kernel 3 3 pad 1 1 num filter 512 name conv4 2 relu4 2 mx sym Activation data conv4 2 act type relu name relu4 2 conv4 3 mx sym Convolution data relu4 2 kernel 3 3 pad 1 1 num filter 512 name conv4 3 relu4 3 mx sym Activation data conv4 3 act type relu name relu4 3 pool4 mx sym Pooling data relu4 3 kernel 2 2 stride 2 2 pool type max name pool4 conv5 conv5 1 mx sym Convolution data pool4 kernel 3 3 pad 1 1 num filter 512 name conv5 1 relu5 1 mx sym Activation data conv5 1 act type relu name relu5 1 conv5 2 mx sym Convolution data relu5 1 kernel 3 3 pad 1 1 num filter 512 name conv5 2 relu5 2 mx sym Activation data conv5 2 act type relu name relu5 2 conv5 3 mx sym Convolution data relu5 2 kernel 3 3 pad 1 1 num filter 512 name conv5 3 relu5 3 mx sym Activation data conv5 3 act type relu name relu5 3 pool5 mx sym Pooling data relu5 3 kernel 2 2 stride 2 2 pool type max name pool5 fc6 flat6 mx sym Flatten data pool5 name flat6 fc6 mx sym FullyConnected data flat6 num hidden 4096 name fc6 relu6 mx sym Activation data fc6 act type relu name relu6 drop6 mx sym Dropout data relu6 p 0 5 name drop6 fc7 fc7 mx sym FullyConnected data drop6 num hidden 4096 name fc7 relu7 mx sym Activation data fc7 act type relu name relu7 drop7 mx sym Dropout data relu7 p 0 5 name drop7 fc8 fc8 mx sym FullyConnected data drop7 num hidden 1000 name fc8 softmax mx sym Softmax data fc8 name softmax return softmax I need this kind of file because it is convenient for me to modify the network architecture not just use the network simply Moreover it is also convenient to check the name and other parameters of each layer For example if I want to modify the resnet152 v2 in model zoo in Gluon Package how should I do except inefficiently rewriting it is symbol in python by myself My ideas 1 load the resnet152 v2 using Gluon model zoo API and save as json file then convert the json file to py file with def resnet152 v2 symbol in it But I do not know how to do the second step Any solutions Thanks,,szha,2017-10-24 14:15:15,2017-10-25 07:51:43
PR,WIP 2bit gradient compression,Description Implements 2bit gradient compression by quantizing each value in gradient array to 2bits using two user specified thresholds one for positive and one for negative values Please review and look at the last section with my questions haibin lin This is a work in progress I'm currently running this with different kind of models to get performance results Important files to review Operators two bit quantize inl h two bit quantize cc KVStore local comm h KVStore dist kvstore dist h kvstore dist server h Documentation about gradient compression kvstore py two bit quantize cc Checklist Essentials x Passed code style checking make lint Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x two bit quantize and dequantize operators x Reduce operation in kvstore local comm h x Distributed kvstore changes at worker and server x Tests for operator local kvstore distributed kvstore with predefined and random data The results have been compared with expected values by implementing this logic in python x API changes for Kvstore Module and Trainer in python Comments Problem When training large scale deep learning models especially with distributed training communication becomes a bottleneck for networks whose computation is not high compared to the communication Approach We can compress the gradients by considering only those elements that exceed a threshold Only these elements are encoded and sent The elements of the gradient that are near zero can safely be delayed by aggregating them in a residual array When the updated residual with gradient of next iterations exceed the threshold these values are sent Effectively these values are updated at a lower frequency On the receiver is end we decompress the data and use the decompressed weights Specifically in this PR 2bit quantization has been implemented Two bit quantization Uses two thresholds to quantize the data one positive threshold and one negative threshold Any positive value greater than or equal to the positive threshold is set to one value say 01 any negative value lower than or equal to the negative threshold is set to second value say 10 and others are set to third value say 0 We need three values to represent data in this fashion and hence two bits We understand this leads to one bit going waste but that is an optimization to be done later The error in quantization is accumulated as residual and carried over to the next iterations This is added in the next iteration to the gradient before quantizing An example below with thresholds of 2 0 and 2 0 Quantization at work Format of compressed gradient The first two elements are the thresholds used for quantization The third element is the size of the original array These values are required to dequantize the gradient Any element from the 4th element represents compressed gradient Each value from the 4th element represents upto 16 elements in the original array For the example above we get 3 Only worker to server communication is compressed Server to worker communication during pull involves sending original sized arrays I read a couple of papers based on gradient compression They only exchange gradients and not the full weights In MXNet with dist kvstore workers pushes updates to the gradient but server maintains weights in full and returns full weights This limits how much communication we can reduce The full weights are much less sparse than gradients If the full model weights are compressed I believe it would lead to much worse accuracy What do you think,,"rahul003,rahul003,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,rahul003,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,piiswrong,piiswrong,piiswrong,solin319,rahul003",2017-10-19 02:13:44,2017-10-25 21:44:23
IS,Long Running Unit Tests check factorization machine module on CPU,Description Test factorization machine model with sparse operators takes long to run as shown in build logs Test factorization machine model with sparse operators ok 3404 9069s Environment info Required 1 Ran as part of Python 2 CPU on Jenkins ubuntu g2 8xlarge Full log here 2 Also tested on c4 8xlarge as follows by haibin lin check factorization machine module adam and check factorization machine module sgd both takes less than 5 seconds But check factorization machine module adagrad is much slower 200 second MXNet commit hash MXNet 0 12 0 rc0 tag branch,,"mbaijal,cjolivier01",2017-10-26 01:16:17,2017-10-26 01:59:06
IS,I can not figure out why ImageRecordIter use only 1 thread for decoding,I modified src io iter image recordio 2 cc to add more log info like this my cpu is 5930k it has 6 physical cores omp get num procs should log out 12 so the test result is ok but in src io iter image recordio 2 cc it log out 1,,anirudh2290,2017-10-26 00:00:40,2017-10-26 10:48:02
IS,A3C with multiple workers,Hi When I run the a3c from this repo using the file launcher py it can specify the number of workers When I specify more than 1 I get the error no module named dmlc tracker I checked this closed issue and tried to add sys path insert 0 your mxnet python before importing mxnet but the problem is still the same Do you have any suggestion about this problem Thank you Med,,"anirudh2290,anirudh2290",2017-10-23 19:26:58,2017-10-26 16:25:03
PR,upgrade MKL,Description upgrade mklml dependency Checklist Essentials x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x upgrade MKL version from 20170720 to 20170908,,"szha,eric-haibin-lin",2017-10-21 22:58:18,2017-10-26 22:27:25
PR,Lint fix,Description Minor Lint fix Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,larroy,2017-10-24 00:58:36,2017-10-27 17:44:12
PR,CMAKE Cmake changes upgrade training test so it converge,Redo 8311 The reason failure happens after its mergeis due to the problem of test case giving wrong parameters I have fixed that so that now it actually converges to meanful value,,"tqchen,tqchen,tqchen,cjolivier01,tqchen",2017-10-19 04:30:57,2017-10-27 19:19:00
PR,fix diagnose if hashtag not found,Description Fix diagnose message if not installed from pip Checklist Essentials x Changes are complete i e I finished coding on this PR x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"zhreshold,zhreshold",2017-10-19 06:39:22,2017-10-27 21:10:30
PR,rename and restructure model zoo models,Description Rename classifier to output and keep only the last dense or conv layer in output Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Rename classifier to output x Move extra layers that were in classifier to features Comments This change is to make the division of features and output consistent across different networks classifier is renamed to output because the network does not necessarily perform classification I have tested locally that the pre trained weights can still be loaded for all models after the change,,"szha,piiswrong,mli",2017-10-27 06:50:07,2017-10-27 23:39:10
PR,C Framework and validation tests for operator validation and performance,Description C Framework and validation tests for operator validation and performance Normally used for gtest tests Note Sparse is not fully supported in this version Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01",2017-10-26 03:38:45,2017-10-27 23:42:54
IS,errors in linking stage cannot find lcuda,Description When I tried to compile mxnet from sources I encountered a linking error showing usr bin ld cannot find lcuda Environment info Required I have installed cuda 8 0 and cudnn 5 1 for cuda8 0 successfully Tensorflow and torch run well on this computer And the OS is ubuntu 16 04 some environment variables What have you tried to solve it I tried some suggestions in previous issues related to linking errors the no find error kept occurring,,"sergeykolychev,sergeykolychev",2017-10-26 10:51:20,2017-10-28 00:17:28
IS,warning when subclassing AI MXNet DataIter,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description As previously suggested here in order to ingest a variety of data types and convert them to floating point I have subclassed AI MXNet DataIter This works very well I have done this several times and each time I get the following warning Prototype mismatch sub MyData MyDataIterator inner none vs at eval 76 line 8 Obviously MyData MyDataIterator will vary but otherwise the message is the same I am not the familiar with Mouse and so I am having trouble resolving this What does this warning mean and how can I fix my code to make it go away It seems that maybe use PDL in the same file is necessary to get this Environment info Required Package used Python R Scala Julia Perl Build info Required if built from source MXNet commit hash Paste the output of git rev parse HEAD here 66f9b33b6965bb0ebf73b7c5d625b021f0d2adb7 Build config Paste the content of config mk or the build command Error Message Paste the complete error message including stack trace Minimum reproducible example File Example pm package ExampleIterator use Mouse extends AI MXNet DataIter use PDL Steps to reproduce Paste the commands you ran that produced the error 1 Create ExampleIterator pm as shown 2 perl MExampleIterator e 0,,"adamcrussell,sergeykolychev,adamcrussell",2017-10-24 18:26:18,2017-10-28 00:59:39
PR,Optimizations to set to copy fill ops,Description Optimizations to set to copy fill full ops Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,reminisce,reminisce,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,reminisce,cjolivier01,piiswrong,szha,cjolivier01,szha,cjolivier01,szha,cjolivier01,szha,cjolivier01,szha,cjolivier01,cjolivier01,szha,cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-10-18 21:11:45,2017-10-28 01:50:36
IS,MXNet with libgfortran3 will limit cpu usage,Description MXNet low cpu usage for version 0 11 0 Steps to reproduce We did some tests on c4 8xlarge cpu utilization was limited to 200 for pip install mxnet pre 0 11 1b20171001 For 0 12 it was around 480,,"kevinthesun,piiswrong,cjolivier01,kevinthesun,jiajiechen,lupesko,szha,jiajiechen,cjolivier01,cjolivier01,kevinthesun",2017-10-24 22:38:29,2017-10-28 03:54:31
IS,MATLAB API of Mxnet,In Mxnet is there API of matlab Thanks,,chinakook,2017-10-22 07:31:07,2017-10-28 07:20:12
IS,How to use val rec file to produce 144crops,Hi I am reading Going deeper with convolutions in Chapter 7 it mentioned 144crops I think how to use val rec to produce it Now I do not know how to code it by python and mxnet Anyone can help me or give me some informations Thanks,,"solin319,zhreshold",2017-10-22 07:30:06,2017-10-28 07:20:39
PR,Memset memcpy omp profiling tester,Description This does not assert but tests the threshold where memset or memcpy becomes slower than the equivalent OMP implementation Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-10-22 04:32:11,2017-10-28 15:44:54
PR,fix place device,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,piiswrong,2017-10-27 23:38:36,2017-10-28 17:50:44
PR,bugfix and GPU support for syevd,Content of this PR Added GPU support for the syevd operator which ensures that we have GPU support for all linalg operators Bugfix for syevd on CPU Now it also works for float32 and in fact the bug has been present for fp64 as well but somehow never showed up Add Manu Seth to contributor list as he has done most coding of PR8179 random samplers and I forgot to update the contributors list accordingly,,"asmushetzel,mseeger,mseeger,mseeger,asmushetzel,asmushetzel,piiswrong,mseeger,asmushetzel",2017-10-25 12:25:54,2017-10-28 19:58:38
PR,gluon lambda block,Description Add a HybridBlock type Lambda that supports using any op or function as stateless block Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Add lambda block and tests Comments Currently it is possible for user to plugin a function that can only work on ndarray even though this Lambda is a HybridBlock Its intended for mixed use with regular Block,,"szha,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,szha,piiswrong,piiswrong,piiswrong,piiswrong,szha",2017-10-25 05:16:43,2017-10-28 20:01:35
PR,Fixes 8292 Bug on Slice accessing uninitialized memory in param beg,in Out of bounds array access Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"larroy,larroy",2017-10-24 00:36:21,2017-10-28 20:43:50
IS,mx nd array indexing broken in armv7 raspberrypi jessie 8 0 5 dimensional tensor,Description test ndarray slice broken in armv7 multidimensional array indexing broken for 5 dimensions Environment info Required armv7 raspberrypi jessie 8 0 Simple reproduction,,"larroy,piiswrong,larroy,larroy,reminisce,larroy,larroy,reminisce,larroy",2017-10-16 12:27:19,2017-10-28 20:44:03
IS,topk mshadow range returns wrong result,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description Brief description of the problem in no more than 2 sentences Environment info Required Steps to reproduce Paste the commands you ran that produced the error 1 2 What have you tried to solve it 1 2,,"eric-haibin-lin,eric-haibin-lin",2017-10-16 22:04:54,2017-10-28 20:46:39
PR,fix topk with big shape,Description fix haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage NA For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x topk with big shape add unittest Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,piiswrong,ZiyueHuang,piiswrong,piiswrong,ZiyueHuang,piiswrong,piiswrong,ZiyueHuang",2017-10-23 20:16:56,2017-10-28 20:46:39
PR,A hybrid model with sync and async in kvstore,Add key threshold to make the parameters can updated in a hybrid model with sync and async In python we can use 'model kvstore init key nd array 9999999 key thre ' to init key threshold Does MXNet need this feature haibin lin,,"solin319,piiswrong,piiswrong",2017-10-21 01:46:15,2017-10-28 20:47:57
PR,test,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,,2017-10-29 00:15:54,2017-10-29 00:16:47
IS,L2Normalization document is wrong,L2Normalization document mxnet symbol L2Normalization mxnet symbol L2Normalization data None eps Null mode Null name None attr None with mode channel it normalizes each channel in the array by its L2 norm for i in 0 N out i data i sqrt sum data i 2 eps with mode spatial it normalizes the cross channel norm for each position in the array by its L2 norm for dim in 2 N for i in 0 N out i take out indices i axis dim sqrt sum take out indices i axis dim 2 eps dim I think argument mode channel and mode spatial should be reversed,,szha,2017-07-10 06:38:14,2017-10-29 00:26:26
IS,Help implementing multiple instance bag learning with MXNet,I am attempting to implement bag learning with MXNet but I am running into an issue with executor group Code This error a raises from decide slices L216 called here L296 Im looking for suggestions on a solution workaround for this My first idea is to create a new module that takes 2 symbols One for the regular single instance network and the second for the multiple instance network then pass gradients data between I was hoping there was a solution that was less involved though,,"jmerkow,szha",2017-07-10 19:04:11,2017-10-29 00:26:27
IS,Reuse feature extractor for multiple inputs,Hey I would like to use AlexNet to extract features from my images Then I want to pass those features into a RNN My problem is that I first want to extract the features from 5 images and then pass all extracted features to the RNN The important thing here is that I want to reuse the feature extractor for every image so I do not need to have AlexNet five times in my network Is it possible to do that How can I define the network so that I pass 5 different images through the same feature extractor and then pass everything to the RNN Thanks in advance Best Sebastian,,szha,2017-07-10 16:32:58,2017-10-29 00:26:27
IS,sharedctypes for ndarray,In my data iterator I want to fill an array with multiple threads Right now the code fill If I would do this in parallel with numpy I would write the data to a sharedctypes RawArray and when done apply the function np ctypeslib as array to this What is the right way to do this in mxnet I could write it to a numpy array but if I understand correctly this would mean the data is copied once I apply mx nd array and I just want to use the reference,,szha,2017-07-10 22:58:24,2017-10-29 00:26:28
IS,Does the speech recognition example OK with warp ctc now,piiswrong Does the speech recognition example OK with warp ctc now Many thanks,,"piiswrong,Soonhwan-Kwon,szha",2017-06-30 09:20:55,2017-10-29 00:26:29
IS,Axis selection for Sequence operators,Currently all Sequence operators Sequence Reverse Mask Last require time major input data that is time batch depth In some networks this enforces users to do a lot of transpositions reshapes to apply these operators which are crucial for some use cases variable length seq2seq models It would be great if these operators would support an axis argument that defaults to 0,,"fhieber,szha",2017-07-11 11:40:02,2017-10-29 00:26:30
IS,Performance of deep speech is bad,Environment info Operating System ubuntu 14 04 GPU GTX 1080 Package used Python R Scala Julia python MXNet version 0 9 5 Or if installed from source git clone mxnet recursive Error Message I just follow the default configure script in the example of speech recognition with the corpus of librispeech clean 360 subset 1 After two epochs the CER seems to be bad with CER 70 image image image image 2 It works so slow it takes a whole week for two epochs of clean 360 subset Minimum reproducible example use the default cfg to train the deep speech model Steps to reproduce 1 python main py python main py configfile default cfg What have you tried to solve it 1 Is there any result performance you have tested in librispeech,,"piiswrong,Soonhwan-Kwon,Soonhwan-Kwon,Soonhwan-Kwon,Soonhwan-Kwon,Soonhwan-Kwon,szha",2017-05-24 07:55:57,2017-10-29 00:26:30
IS,Missing Layer normalization no operator for variance or mean and variance,MXNet should support Layer Normalization out of the box either by having an operator for it or provide support operators that make it easier While it is possible to do with current MXNet see here L18 it is a bit cumbersome quite slow and fairly tricky with shape inference due to the many broadcast ops In general it would be useful to have an operator to compute the variance with a signature similar to mean One could also think about a mean and variance operator similar to tf moments that computes both statistics jointly which is likely more efficient Regarding a full Layer Normalization op it could look as follows LayerNormalization on dimension 3 In contrast to Batch normalization it is not necessary to compute running means or store anything from training for inference,,"fhieber,szha",2017-07-11 11:55:20,2017-10-29 00:26:31
IS,Generalized batch dot matrix multiplication,Currently mx nd sym batch dot is constrained to 3d input where the first dimension is considered the batch size It would be great to be able to have n dimensional inputs to a matrix multiplication and do the matrix multiplication on the last two dimensions only This avoids having to combine split dimensions before and after Desired behavior The transpose flags would always only transpose the last two dimensions,,"fhieber,szha,nicklhy,fhieber",2017-07-11 12:36:43,2017-10-29 00:26:32
IS,improve speed of training,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Distributor ID Ubuntu Description Ubuntu 14 04 5 LTS Release 14 04 Codename trusty Compiler gcc Ubuntu 4 8 4 2ubuntu1 14 04 3 4 8 4 Package used Python R Scala Julia python2 7 6 MXNet version Or if installed from source '0 10 1' MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace No error message but mxnet is slow Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error I am running the code on an amazon ec2 instance EC2 g2 2xlarge the reported speed using K80 single GPU EC2 p2 2xlarge is 28 images sec which is double the speed Is there any configuration I should do to speed up my code What have you tried to solve it 1 installed opencv with cuda off 2 3,,szha,2017-07-08 06:36:53,2017-10-29 00:26:32
IS,Cannot find custom operator type when running distributed training with multiple machines,code mxnet issue 6102 MXNet version v0 9 5 downloaded using git clone b v0 9 5 URL Built with Ubuntu 16 04 CUDA 8 0 CUDNN 5 1 gcc version 5 4 0 20160609 Ubuntu 5 4 0 6ubuntu1 16 04 4 Python 2 7 12 make j USE BLAS openblas USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 USE DIST KVSTORE 1 ADD LDFLAGS L opt OpenBLAS L opt OpenBLAS lib I'm running a simple network with a customized operator class FrobeniusNorm mx operator CustomOp operator register frobenius class FrobeniusNormProp mx operator CustomOpProp It works fine on single machine however crashed when launched with command python tools launch py and kvstore set to wouldist sync' for multi machine learning 00 02 57 HOME src mxnet dmlc core include dmlc logging h 304 00 02 57 src operator custom custom inl h 128 Check failed registry find param op type registry end Cannot find custom operator type frobenius Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f54b9fa434c bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet2op12CustomOpProp4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x916 0x7f54ba7ecc36 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet2op12ParsedOpProp4InitERKN4nnvm9NodeAttrsE 0x124 0x7f54ba88a3a4 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xe8eef7 0x7f54ba880ef7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet17UpgradeJSON ParseEN4nnvm5GraphE 0x4fe 0x7f54ba89c21e bt 5 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataOS1 0x111 0x7f54ba8a1781 bt 6 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet18LoadLegacyJSONPassEN4nnvm5GraphE 0x1a44 0x7f54ba8923e4 bt 7 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataOS1 0x111 0x7f54ba8a1781 bt 8 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaIS7 EE 0x32c 0x7f54bb8f69dc bt 9 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x3c9 0x7f54babf5189 Traceback most recent call last File ctypes callbacks c line 315 in 'calling callback function' File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet kvstore server py line 38 in server controller optimizer pickle loads cmd body File usr lib python2 7 pickle py line 1388 in loads return Unpickler file load File usr lib python2 7 pickle py line 864 in load dispatch key self File usr lib python2 7 pickle py line 1223 in load build setstate state File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet symbol py line 247 in setstate check call LIB MXSymbolCreateFromJSON c str json str ctypes byref handle File usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 00 02 57 src operator custom custom inl h 128 Check failed registry find param op type registry end Cannot find custom operator type frobenius Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f54b9fa434c bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet2op12CustomOpProp4InitERKSt6vectorISt4pairINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES9 ESaISA EE 0x916 0x7f54ba7ecc36 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet2op12ParsedOpProp4InitERKN4nnvm9NodeAttrsE 0x124 0x7f54ba88a3a4 bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so 0xe8eef7 0x7f54ba880ef7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet17UpgradeJSON ParseEN4nnvm5GraphE 0x4fe 0x7f54ba89c21e bt 5 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataOS1 0x111 0x7f54ba8a1781 bt 6 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet18LoadLegacyJSONPassEN4nnvm5GraphE 0x1a44 0x7f54ba8923e4 bt 7 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataOS1 0x111 0x7f54ba8a1781 bt 8 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEESaIS7 EE 0x32c 0x7f54bb8f69dc bt 9 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKNSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEE 0x3c9 0x7f54babf5189,,szha,2017-05-04 16:24:35,2017-10-29 00:26:33
IS,Some unwise code in symbol getitem of symbol py,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Package used Python R Scala Julia python MXNet version 0 10 I found that the implementation of this code is not elegant enough Code locate on function getitem It cost much time if output list is large In my opinion use a directly member variable should be much better,,szha,2017-07-12 09:48:10,2017-10-29 00:26:34
IS,Ask Tensorboard without fit function,Hello I was wondering if it is possible to use tensor board if we use forward backward update function instead of fit function I did not find an example about it,,"zihaolucky,zihaolucky,szha",2017-07-13 08:25:42,2017-10-29 00:26:35
IS,How to find the people who are not wearing helmet in the picture,Hello everyone I know how to using image detection to find whether there is helmet or not in the picture but I do not know how to find whether there is someone without wearing helmet especially when there is another wearing helmet in the picture Would you please tell me how to solve the problem Thanks in advance Ryu Update I have tried to train a model with my picture with and without helmet about 100 images each but the recognition rate is not good when using the trained model actually sometimes the model can recognize well and sometimes it can NOT recognize correctly when my head moves Are my images for training too few Can the model be use to recognize the people without wearing helmet but another wearing helmet,,szha,2017-07-13 01:32:03,2017-10-29 00:26:35
IS,How to fix the random seed,Is there a method to fix the random seed in mxnet in order to produce same results in each run,,"eric-haibin-lin,szha",2017-07-13 09:09:29,2017-10-29 00:26:37
IS,install previously released version,Hi I want to manually install mxnet 0 9 5 on an ubuntu system But the git in may lack of several required package such as cud dmlc core mshadow nnvm ps lite Therefore I have tried download these packages from the most current version in But when I typed 'make' The error showed up Would you please guide me how to install mxnet 0 9 5 manually,,"thirdwing,szha",2017-07-09 09:43:21,2017-10-29 00:26:37
IS,What is python interface to underlying c c,How MxNet use one or both of cython and ctypes to bind to underlying c c I do not know the process of how the python interact with the underlying C C code There is little document to explain the process It prevent me from reading the code Thanks,,"reminisce,reminisce,szha",2017-06-28 12:14:16,2017-10-29 00:26:38
IS,C C API Get updated weights args params from Symbol,Hi i would like to access the updated weights during training Following the LeNet example if i just access the args map i used to initialize the Net e g args map conv1 w the weights do not get updated from iteration to iteration although training works and the output changes that is probably because the actual symbol bound weights on the gpu context are not synced to the weights that were used to initialize How can I download or update the current weights along the lines of get params for a model if i only have the currently trained symbol I'm not entirely sure if i completely understand all connections between NDArrays Symbols and Models e g how can i turn a trained stack group of symbols into a model so i would be thankful for all advice Thanks,,szha,2017-07-14 21:42:24,2017-10-29 00:26:39
IS,How can i get the shape with the net,Hi I want to design a net like this the num must be the same as the input params net is channel how can i get from the net In tensorflow I can get the shape of the tensor like net get shape 3 I'm sorry I can not get it from the mxnet example code Could any one help me with this,,"burness,phunterlau,zhreshold,wangg12,szha",2016-11-11 09:50:33,2017-10-29 00:26:40
IS,InferShapeKeyword argument name gt label 37 not found,I'm trying to reproduce a simple example of network to solve regression I followed the api and other issues but I'm not sure what I'm doing wrong Environment info Operating System Ubuntu 16 04 Package used Scala MXNet commit hash 978f53b1058deaeb8d1bd69580cb61231451c63b val X Symbol Variable x val Y Symbol Variable gt label val fc1 Symbol FullyConnected name fc1 Map data X num hidden 128 val act1 Symbol Activation name relu1 Map data fc1 act type relu val fc2 Symbol FullyConnected name fc2 Map data act1 num hidden 1 val net Symbol LinearRegressionOutput name softmax Map data fc2 label Y val mod new Module net dataNames IndexedSeq x labelNames IndexedSeq gt label val rnd scala util Random val x for j 1 to 100 i 1 to 9 yield i 1 0f rnd nextGaussian toFloat sliding 9 9 map toArray map NDArray array Shape 9 toIndexedSeq val y for i 1 to 100 yield 10 0f rnd nextGaussian toFloat sliding 1 map toArray map NDArray array Shape 1 toIndexedSeq val train dataiter new NDArrayIter x y dataName x labelName gt label mod fit train dataiter evalData None numEpoch 100 fitParams new FitParams setOptimizer new SGD learningRate 0 1f momentum 0 9f wd 0 0001f And my error is 00 07 09 src c api c api symbolic cc 398 InferShapeKeyword argument name gt label 37 not found Candidate arguments 0 x 1 fc1 weight 2 fc1 bias 3 fc2 weight 4 fc2 bias 5 gt label Stack trace returned 5 entries bt 0 usr local lib libmxnet scala linux x86 64 gpu so ZN4dmlc15LogMessageFatalD1Ev 0x39 0x7f6026b49669 bt 1 usr local lib libmxnet scala linux x86 64 gpu so ZN5mxnet14MatchArgumentsIN4nnvm6TShapeEEEvRKNS1 12IndexedGraphERKSt13unordered mapISsT St4hashISsESt8equal toISsESaISt4pairIKSsS7 EEEPSt6vectorIS7 SaIS7 EEPKc 0x4db 0x7f60275d7deb bt 2 usr local lib libmxnet scala linux x86 64 gpu so MXSymbolInferShape 0x11b3 0x7f60275d2873 bt 3 usr local lib libmxnet scala linux x86 64 gpu so Java ml dmlc mxnet LibInfo mxSymbolInferShape 0x26f 0x7f6026b42c7f bt 4 0x7f608d017a34 Exception in thread main java lang NegativeArraySizeException at ml dmlc mxnet LibInfo mxSymbolInferShape Native Method at ml dmlc mxnet Symbol inferShape Symbol scala 259 at ml dmlc mxnet Symbol inferShape Symbol scala 249 at ml dmlc mxnet module DataParallelExecutorGroup ml dmlc mxnet module DataParallelExecutorGroup bindIthExec DataParallelExecutorGroup scala 582 at ml dmlc mxnet module DataParallelExecutorGroup anonfun bindExec 1 apply DataParallelExecutorGroup scala 356 at ml dmlc mxnet module DataParallelExecutorGroup anonfun bindExec 1 apply DataParallelExecutorGroup scala 355 at scala collection TraversableLike anonfun map 1 apply TraversableLike scala 234 at scala collection TraversableLike anonfun map 1 apply TraversableLike scala 234 at scala collection immutable Range foreach Range scala 160 at scala collection TraversableLike class map TraversableLike scala 234 at scala collection AbstractTraversable map Traversable scala 104 at ml dmlc mxnet module DataParallelExecutorGroup bindExec DataParallelExecutorGroup scala 355 at ml dmlc mxnet module DataParallelExecutorGroup init DataParallelExecutorGroup scala 317 at ml dmlc mxnet module DataParallelExecutorGroup Builder build DataParallelExecutorGroup scala 223 at ml dmlc mxnet module Module bind Module scala 243 at ml dmlc mxnet module BaseModule fit BaseModule scala 355,,szha,2017-05-19 21:38:58,2017-10-29 00:26:40
IS,Any plan to have NTM,Is there a plan to develop a NTM neural turing machine I am looking forward to seeing it,,szha,2017-07-15 14:36:53,2017-10-29 00:26:41
IS,Could NOT find MKL missing MKL INCLUDE DIR MKL RT LIBRARY,hello i want to install mxnet with python interface in win10 by vs2017 as i run cmake G Visual Studio 15 2017 in gitbash console the MKL NOT FOUND error comes out cmake G Visual Studio 15 2017 Could NOT find MKL missing MKL INCLUDE DIR MKL RT LIBRARY MKL not found CMake Error at CMakeLists txt 129 include include could not find load file mshadowUtils i think i have installed Inter MKL correctly echo MKL INCLUDE DIR C Program Files x86 IntelSWTools compilers and libraries 2017 4 210 windows mkl include echo MKL RT LIBRARY C Program Files x86 IntelSWTools compilers and libraries 2017 4 210 windows mkl lib ia32 i tried export MKL FOUND 1 but the error remains Can anyone help me Thanks a lot,,szha,2017-06-11 10:45:03,2017-10-29 00:26:42
IS,The output layer name 'flatten0' automatically changed to 'flatten1' when get network twice,I use get net func to get inception network twice However the flatten layer name is 'flatten0 output' and the second time the flatten layer name is 'flatten1 output' I do not know why it is changed How can i keep the name of these output layers stable when i get the network twice first time output 'global pool output' 'flatten0 output' 'fullyconnected0 weight' 'fullyconnected0 bias' 'fullyconnected0 output' isoftmax label' isoftmax output' second time output 'global pool output' 'flatten1 output' 'fullyconnected1 weight' 'fullyconnected1 bias' 'fullyconnected1 output' isoftmax label' isoftmax output',,"piiswrong,szha",2017-07-14 05:32:39,2017-10-29 00:26:42
IS,Segmentation fault when running char lstm tutorial in mxnet notebooks,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info AWS EC2 Instance Instance type p2 xlarge Tried this on AWS Deep Learning AMI Ubuntu 1 5 AWS Deep Learning AMI Ubuntu 1 4 and also on a bare Ubuntu 14 04 AMI Package used Python R Scala Julia Python Or if installed from source MXNet commit hash git rev parse HEAD 8713d257cde97a660a459aa8a50a780944cf823c 0 10 0 release Also tried with latest master 92428fb7ba8003b3d8d9d098ef20c22123824c89 If you are using python package please provide Python version and distribution Python 2 7 6 pip 9 0 1 from usr local lib python2 7 dist packages python 2 7 Error Message Please paste the full error message including stack trace Segmentation fault core dumped When tried to debug using pdb get the following What have you tried to solve it 1 Tried to increase the number of epochs 2 Tried on g2 2xlarge instance type,,"anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,szha",2017-07-08 01:20:25,2017-10-29 00:26:43
IS,training data resides in AWS s3,How can I modify the iterator in this example to read images from AWS s3 I have png images in a folder in AWS s3 I tried passing the rootdir as s3 I have also tried to change the function that reads images from Image by PIL to imdecode by mx image I had no luck in both cases I have an image segmentation problem my input is an image and my output is an image too,,"piiswrong,szha",2017-07-13 06:15:52,2017-10-29 00:26:44
IS,Is there any tutorial on how to write data loader in C which uses RecordIO API,As title I only found various implementation in src io but feel confused with the APIs,,"madjam,szha",2017-07-16 14:52:34,2017-10-29 00:26:45
IS,rcnn example issue collection,The rcnn example has been adapted to be compatible with nnvm branch in Waiting for results to confirm everything works I invite you to try this new version right now I have collected some issues of rcnn example 4G memory is not enough for VGG e2e training 4224 2913 This issue arises because py faster rcnn caffe version only needs 3GB memory They use cudnn v3 and allocates dynamically My experiment with cudnn 5 1 uses 4GB Backward mirror will not help Python 3 Windows compatibility 4007 3995 2601 Except for print what is really incompatible Is customop compatible Relevant to 4071 The following will be fixed soon CustomOp cpu training will freeze or blocked training 4297 4244 3724 with strange solution I do not know how to solve this UPDATE Confirmed solved in 4528 for cpu demo wont test cpu training UPDATE multi gpu alternate is broken Will be fixed workspace is not enough 4431 2694 Set it to workspace 2048 like Will be fixed BN networks cannot train stably ResNet support 3852 Set use global stats True ResNet will be added soon Will be fixed cudnn auto tune problem 4656 Set env MXNET CUDNN AUTOTUNE DEFAULT 0 Will be fixed as default multigpu e2e training 3836 3639 3517 Will be fixed misc mistakes 4633 2975 Will be fixed testing memory explodes 3321 Will be fixed by module testing performance issues 3139 GPU testing will be 2x faster than caffe soon There are some interesting things 3704 the group symbol behavior is changed with nnvm 3542 I do not see anything wrong with layout 2214 the converter issue is good for reference,,"precedenceguo,piiswrong,precedenceguo,precedenceguo,Jerryzcn,szha",2017-01-18 07:16:55,2017-10-29 00:26:46
IS,import mxnet mxnet version given error OpenCV Error Assertion failed key 1 in TLSDataContainer,ry For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System CentOS7 2 Compiler g 4 8 2 Package used Python R Scala Julia Python MXNet version 62ecb60 Or if installed from source from source MXNet commit hash git rev parse HEAD 17332e5b54c37e08ad9637447025a329c457375d If you are using python package please provide Python version and distribution 2 7 5 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Error Info python c import mxnet mxnet version OpenCV Error Assertion failed key 1 in TLSDataContainer file person opencv 3 2 0 modules core src system cpp line 1186 terminate called after throwing an instance of 'cv Exception' what person opencv 3 2 0 modules core src system cpp 1186 error 215 key 1 in function TLSDataContainer Aborted core dumped Minimum reproducible example if you are using your own code please provide a short script that reproduces the error python c import mxnet mxnet version Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 compile mxnet by make 2 python setup py install in Python directory 3 cd python c import mxnet mxnet version What have you tried to solve it 1 Fixing the error OpenCV Error Assertion failed key 1 in TLSDataContainer when I using the mxnet python interfaces 2 3,,szha,2017-07-18 09:45:50,2017-10-29 00:26:47
IS,mx mod Module does not print output statistics anymore Bug,Environment info Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 138344683e65c87af20250e3f4cdcc5a72ac3cc5 I have created such a monitor mx mon Monitor 1 stat func self mean abs pattern ' ' sort False I'm passing it to a mx mod Module object as a parameter After each batch only the stats of data weight bias and softmax label are printed I have updated MxNet recently 0 9 4 was working without any problems Source code was the same,,"piiswrong,eric-haibin-lin,zhreshold,piiswrong,eric-haibin-lin,eric-haibin-lin,zhreshold,fhieber,szha",2017-04-05 01:10:22,2017-10-29 00:26:47
IS,Feed data into intermediate layer in forward pass,Is there a way to feed data into an intermediate symbol in the forward pass without constructing a new graph similar to how you can use a feed dict in tensorflow to feed data into every node of the computation graph I e conceptually I would like to do something like this My use case is a pretrained network for which I would like to compute a forward pass for either a new input X or some feature representation at an intermediate layer which might come from another network or some processed representation I presume there should be some way to do this however I have not been able to find it Sorry if this question has already been answered elsewhere,,szha,2017-07-18 15:21:45,2017-10-29 12:26:26
IS,How can I generate my own float data,I wanna train a CNN using my float data which has 1 2million samples and 57600 6 per sample How can I generate the data file for MxNet I wanna generate it using my another C program without any MxNet functions What is the simplest file format,,"piiswrong,szha",2017-07-16 23:34:30,2017-10-29 12:26:27
IS,train py in speech recognition is missing stt bucketing module,I believe that when 22f9a0dc96a0ad3741c1f8db7b66f27d9de9971a was committed the new file stt bucketing module py was not added Kwon do you have this file locally The effect I'm seeing is that train fails to import STTBucketingModule If I'm wrong about the cause of error then I will provide more information to debug,,"Soonhwan-Kwon,szha",2017-07-18 21:55:26,2017-10-29 12:26:27
IS,Proposal random seed should better to be different in each run by default,Otherwise the result is exactly the same each time Besides we could set the seed if we wish,,szha,2017-07-19 13:01:53,2017-10-29 12:26:28
IS,DISCUSS API for improved image decoding augmentation to keep up with future GPU architectures,Hi this is Przemek from NVIDIA While working on Volta support and optimizations for MXNet and other frameworks we stumbled upon a problem that I think requires discussion with the community The benchmark that we use to assess CNN performance is ResNet 50 v1 It turns out that with Volta the GPU performance is high enough that we cannot feed the GPU with decoded and augmented images even though the augmentations we use are actually pretty minimal random resize crop and mirror fast enough even though the testing system we use is very high end 8 GPUs are paired with 40 CPU cores One could argue of course that we should just move to a bigger model and circumvent this problem that way One needs to also think about the inference time though if the model becomes too big then it becomes difficult to do inference e g on mobile device That is why we would like to tackle this problem somehow and the most promising solutions require offloading at least some of the IO pipeline to the GPUs However this would be a pretty big change to MXNet is fit function since right now the placement of IO pipeline is pretty rigid the batch needs to be processed to be then split into parts for each GPU and sent to feed the actual training The proposed approach would require iterator to split and send batches itself which would require at least wrapping existing iterators What do you think about this issue and proposed solution Any advice on how this could be implemented,,"ptrendx,piiswrong,ptrendx,piiswrong,ptrendx,tqchen,ptrendx,szha",2017-07-18 17:50:26,2017-10-29 12:26:29
IS,Save a mxnet model to keras and vice versa,Is there a function or some sample code to save a mxnet model as a keras model such as h5 or load a keras model into mxnet,,szha,2017-07-18 21:43:11,2017-10-29 12:26:29
IS,gluon feature request parameter save arguments,Could the gluon save function also include an option for 1 epoch like the default mxnet one 2 global step 2 max to keep 3 keep checkpoint every n hours,,szha,2017-07-19 17:38:27,2017-10-29 12:26:30
IS,integer divide by zero,edit net ConvFactory data net kernel 1 1 num filter 1 stride 1 results in OSError exception integer divide by zero and can be fixed with net ConvFactory data net kernel 1 1 num filter 1 stride 1 1 took me a while,,szha,2017-07-17 19:29:54,2017-10-29 12:26:31
IS,Weight sharing in SequentialRNNCell throws AssertionError,Using SequentialRNNCell is params constructor option always results in an AssertionError with GRUCell This seems to stem from the fact that the RNNCell getter sets self own params False and most cells LSTMCell GRUCell call the getter even if they were not given a params argument in their respective constructors Environment info Operating System OS X MXNet version 0 10 0 Python version and distribution Python 3 6 0 Anaconda 4 3 1 x86 64 Error Message Traceback most recent call last File myParamsTest py line 8 in module seq2 add cell2 File anaconda lib python3 6 site packages mxnet rnn rnn cell py line 733 in add Either specify params for SequentialRNNCell AssertionError Either specify params for SequentialRNNCell or child cells not both Minimum reproducible example import mxnet as mx cell1 mx rnn GRUCell 10 seq1 mx rnn SequentialRNNCell seq1 add cell1 cell2 mx rnn GRUCell 10 seq2 mx rnn SequentialRNNCell params seq1 params seq2 add cell2 Steps to reproduce python myParamsTest py,,"piiswrong,szha",2017-07-19 21:17:22,2017-10-29 12:26:32
IS,There is a bug in mxnet example recommenders crossentropy py,There is a bug in mxnet example recommenders crossentropy py line 57 grad 1 p self eps 1 y The p self eps 1 y part will be 0 in some cases so I suggest use d new p self eps 1 y d new d new 0 self eps 1 grad 1 d new instead,,szha,2017-07-20 05:14:56,2017-10-29 12:26:33
IS,fatal reference is not a tree 89de7a,i just installed ubuntu 16 04 and try to install MXNet commit 62ecb60 so git clone recursive git checkout 62ecb60 git submodule update then it prints out fatal reference is not a tree 89de7ab20167909bc2c4f8acd397671c47cf3c0d Unable to checkout '89de7ab20167909bc2c4f8acd397671c47cf3c0d' in submodule path 'cub' if i ignore that fatal error then another error occurs during make operation make j8 it prints out usr bin ld cannot find lcblas collect2 error ld returned 1 exit status Makefile 242 recipe for target 'lib libmxnet so' failed make lib libmxnet so Error 1 make Waiting for unfinished jobs usr bin ld cannot find lcblas collect2 error ld returned 1 exit status Makefile 264 recipe for target 'bin im2rec' failed make bin im2rec Error 1 please help me guyz,,"Soonhwan-Kwon,szha",2017-07-12 08:53:49,2017-10-29 12:26:33
IS,ImportError cannot import name gluon when import mxnet,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu14 04 Compiler g nvcc Package used Python R Scala Julia python MXNet version 0 10 0 Or if installed from source yes MXNet commit hash git rev parse HEAD bd5df7ce0f52065ed813cc6b97e94e0f75e9b5e6 If you are using python package please provide Python version and distribution 2 7 6 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace import mxnet usr lib python2 7 dist packages numpy oldnumeric init py 11 ModuleDeprecationWarning The oldnumeric module will be dropped in Numpy 1 9 warnings warn msg ModuleDeprecationWarning Traceback most recent call last File stdin line 1 in module File usr local lib python2 7 dist packages mxnet 0 10 1 py2 7 egg mxnet init py line 63 in module from import gluon ImportError cannot import name gluon,,szha,2017-07-19 10:57:37,2017-10-29 12:26:34
IS,Error occured when runing rnn cell demo py with yajiedesign mxnet release,Environment info Operating System Win7 64 Compiler VS 2013 12 0 3 Update 4 3rd Library cudnn 4 openblas opencv2 cuda 7 5 MXNet version yajiedesign GPU release 2016 12 29 2016 11 25 2016 09 09 Python version and distribution Anaconda python 2 7 11 Error info mxnet base MXNetError 15 45 06 D Program Files x86 Jenkins workspace mxnet mxnet src operator rnn cu 24 RNN is only available for cuDNN at the moment Description 1 CNN module can be run quickly and produce good results with GPU cudnn has been put into C Program Files NVIDIA GPU Computing Toolkit CUDA v7 5 I think cudnn is OK 2 run rnn cell demo py error shows mxnet base MXNetError 15 45 06 D Program Files x86 Jenkins workspace mxnet mxnet src operator rnn cu 24 RNN is only available for cuDNN at the moment 3 I change many many prebuilt yajiedesign version error occurs still In addition I would appreciate that RNN corresponding models and symbols such like codes in bucket io py can be programmed into standard mxnet library and more easily to use It means just compose several existed symbol then I can wrap input samples labels and get complete char RNN model or speech rnn model An ideal assumption data mx sym SequenceVariable name wouldata' bucket 5 10 15 rnn mx sym RNN name 'rnn' data num filter,,"yajiedesign,yajiedesign,yajiedesign,yajiedesign,zihaolucky,yajiedesign,zihaolucky,yajiedesign,zihaolucky,zihaolucky,zihaolucky,yajiedesign,szha",2016-12-30 07:56:28,2017-10-29 12:26:35
IS,mxnet ndarray random normal uses same seed on every run,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia Python If you are using python package please provide Python version and distribution Python 2 7 12 MXNet version mxnet cu80 via pip mxnet version 0 10 1 today is nightly build Error Message No error message but the following when working through the tutorial mxnet ndarray random normal will produce the same results every time on a fresh run E g running this code import mxnet ndarray as nd a nd random normal shape 3 print a N times in a row by calling python script produces apuschkin smorrebrod Desktop mxnet tutorial python home lapuschkin Desktop mxnet tutorial 2 py 2 21220636 1 16307867 0 7740038 NDArray 3 0 lapuschkin smorrebrod Desktop mxnet tutorial python home lapuschkin Desktop mxnet tutorial 2 py 2 21220636 1 16307867 0 7740038 NDArray 3 0 lapuschkin smorrebrod Desktop mxnet tutorial python home lapuschkin Desktop mxnet tutorial 2 py 2 21220636 1 16307867 0 7740038 NDArray 3 0 lapuschkin smorrebrod Desktop mxnet tutorial python home lapuschkin Desktop mxnet tutorial 2 py 2 21220636 1 16307867 0 7740038 NDArray 3 0 lapuschkin smorrebrod Desktop mxnet tutorial python home lapuschkin Desktop mxnet tutorial 2 py 2 21220636 1 16307867 0 7740038 NDArray 3 0 and so on Is this intended behaviour to always re use initiate the random number generator with the same seed The random number can change when changing context but sticking to one context will produce the same output for that context over and over again Minimum reproducible example import mxnet ndarray as nd a nd random normal shape 3 print a Steps to reproduce 1 paste above code sample into a python script 2 run script N times 3 observe N identical random arrays What have you tried to solve it 1 manually setting a seed eg mx random seed int time time 1e6 which solves it 2 Gut feeling says 1 should not be necessary Am I wrong,,"piiswrong,szha",2017-07-20 15:53:48,2017-10-29 12:26:36
IS,Multidimensional label input,My inputs to the network are images along with a fixed number of 2D image coordinates How does one represent coordinates as labels in a lst file Also I could not really find examples of such implementations in mxnet apart from SSD Are there more,,szha,2017-07-21 00:58:33,2017-10-29 12:26:37
IS,Publish scala artifacts for CentOS 7,We have a security requirement that any third party artifacts used in production must be published by the developer to a repository i e we cannot build the artifacts ourselves However the currently published scala artifacts do not work on CentOS 7 which is what our production machines run on The published native library located at fails to load due to an incorrect GLIBC version and a missing libcblas so The latter error is due to the fact that in CentOS 7 the Atlas blas library name is satlas instead of cblas Would it be possible to publish mxnet scala artifacts for CentOS 7 We have created a PR at that should help to this end it removes the need for any manual build configuration changes by fixing the atlas blas library issue mentioned earlier We have tested that that with that fix running make and make scalapkg on a CentOS 7 3 machine after installing opencv devel and atlas devel produces a working library,,szha,2017-07-20 21:21:47,2017-10-29 12:26:37
IS,installing mxnet for python in windows,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System windows 10 Compiler The package used Python R Scala Julia python MXNet version is it not possible to install mxnet using python in windows 10 OS for CPU Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-07-21 08:02:54,2017-10-29 12:26:38
IS,amalgamation android compile error,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Compiler NDK r14 linux androideabi clang MXNet version 0 9 3 latest source MXNet commit hash git rev parse HEAD 5e9e3d0949cb8051da61ef49339541b5e6f69c03 Steps to reproduce export CC arm linux androideabi clang export CXX arm linux androideabi clang export NDK ROOT home sun Android ndk export SYS ROOT NDK ROOT sysroot export INCLUDE NDK ROOT include c 4 9 x ifndef OPENBLAS ROOT export OPENBLAS ROOT home sun Android openblas armv7 endif make clean make ANDROID 1 Error Message 1 home sun Android mxnet amalgamation dmlc core include dmlc logging h 18 10 fatal error 'execinfo h' file not found include execinfo h sloved by comment include execinfo h out 2 jni mxnet predict all cc 7649 13 error use of undeclared identifier 'fopen64' did you mean 'fopen' fp fopen64 fname rb sloved by replace fopen64 with fopen in line 2805 and 7649 of mxnet predict all cc The amalgamation seems stop updating for a year Is there any other solution for mxnet in android,,"piiswrong,arank,szha",2017-03-22 00:43:19,2017-10-29 12:26:39
IS,OSError libcudart so 7 5 cannot open shared object file No such file or directory,I upgrade my nvidia drivers and cuda version nvcc NVIDIA R Cuda compiler driver Copyright c 2005 2016 NVIDIA Corporation Built on Sun Sep 4 22 14 01 CDT 2016 Cuda compilation tools release 8 0 V8 0 44 When i finished all of it i recompile the caffe and mxnet the caffe work well but the mxnet always remind me OSError libcudart so 7 5 cannot open shared object file No such file or directory do not how to modify my configuration Thank you,,"piiswrong,piiswrong,piiswrong,szha",2016-11-15 01:31:33,2017-10-29 12:26:40
IS,Questions about the distributed training recovery using mxnet python,I found someone has mentioned before about the failure recovery behavior of distributed MXNet 2268 But that is limited in a way that it only applied to the scala version of MXNet I wonder is there any failure recovery support for the python version of MXNet or is there any for developer to define that behavior Thanks in advance,,szha,2017-07-21 22:07:43,2017-10-29 12:26:40
IS,image prediction example is not valid,The link in image classification example is not valid the link is redirected to i think the correct link is,,szha,2017-07-22 04:52:03,2017-10-29 12:26:41
IS,var duplication happens for ones like operation,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit 62ecb60 MXNet commit hash git rev parse HEAD commit 62ecb60 If you are using python package please provide Python version and distribution 2 7 12 Error Message Please paste the full error message including stack trace MXNetError 22 05 50 src executor graph executor cc 653 var duplication happens for op cumsum Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3c 0x7f601ea7384c bt 1 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet4exec13GraphExecutor13InitCachedOpsEv 0x2080 0x7f601f66cac0 bt 2 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet4exec13GraphExecutor4InitEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES4 St4lessISD ESaISt4pairIKSD S4 EEERKSt6vectorINS 7NDArrayESaISO EESS RKSN INS 9OpReqTypeESaIST EESS PNS 8ExecutorE 0xacf 0x7f601f66fe4f bt 3 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so ZN5mxnet8Executor4BindEN4nnvm6SymbolERKNS 7ContextERKSt3mapINSt7 cxx1112basic stringIcSt11char traitsIcESaIcEEES3 St4lessISC ESaISt4pairIKSC S3 EEERKSt6vectorINS 7NDArrayESaISN EESR RKSM INS 9OpReqTypeESaISS EESR PS0 0x107 0x7f601f6700c7 bt 4 usr local lib python2 7 dist packages mxnet 0 9 5 py2 7 egg mxnet libmxnet so MXExecutorBindEX 0x23c1 0x7f601f612ac1 bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f6052a4fe40 bt 6 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7f6052a4f8ab bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7f6052c5f3df bt 8 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7f6052c63d82 bt 9 usr bin python PyObject Call 0x43 0x4b0cb3 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error By the way I found out that this bug seems to be fixed in the latest released version However it is hard for me to transfer the code to the latest version so is there a light solution that I can fix this bug based on mxnet 0 9 5,,szha,2017-07-22 06:08:57,2017-10-29 12:26:42
IS,Get error 'Gradient function not returning enough gradient',I update mxnet to the latest version then run my code which works fine for old versions It gives me error as Gradient function not returning enough gradient What is this I can not find any information about this,,szha,2017-07-23 07:35:11,2017-10-29 12:26:43
IS,SSD running python demo py problems,Environment info Operating System ubuntu 16 04 Compiler Package used Python R Scala Julia Python MXNet version 10 MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 Error Message home travis build dmlc mxnet distro mxnet build dmlc core include dmlc logging h 304 09 00 53 src c api c api ndarray cc 392 Operator zeros cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet libmxnet so 0xc72fc 0x7f1b5c9ee2fc bt 1 usr local lib python2 7 dist packages mxnet libmxnet so MXImperativeInvoke 0xaca 0x7f1b5d423b5a bt 2 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f1b60ea6e40 bt 3 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7f1b60ea68ab bt 4 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7f1b610b63df bt 5 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7f1b610bad82 bt 6 python PyObject Call 0x43 0x4b0cb3 bt 7 python PyEval EvalFrameEx 0x5faf 0x4c9faf bt 8 python PyEval EvalCodeEx 0x255 0x4c2765 bt 9 python PyEval EvalFrameEx 0x68d1 0x4ca8d1 Traceback most recent call last File demo py line 99 in module ctx args nms thresh args force nms File demo py line 42 in get detector data shape mean pixels ctx ctx File home ines SSD ssd text detection master mxnet master example ssd detect detector py line 39 in init self mod bind data shapes wouldata' batch size 3 data shape data shape File usr local lib python2 7 dist packages mxnet module module py line 388 in bind state names self state names File usr local lib python2 7 dist packages mxnet module executor group py line 216 in init self bind exec data shapes label shapes shared group File usr local lib python2 7 dist packages mxnet module executor group py line 312 in bind exec shared group File usr local lib python2 7 dist packages mxnet module executor group py line 620 in bind ith exec arg arr nd zeros arg shapes j context dtype arg types j File usr local lib python2 7 dist packages mxnet ndarray py line 1003 in zeros return internal zeros shape shape ctx ctx dtype dtype File string line 15 in zeros File usr local lib python2 7 dist packages mxnet ctypes ndarray py line 72 in imperative invoke c array ctypes c char p c str str val for val in vals File usr local lib python2 7 dist packages mxnet base py line 84 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 09 00 53 src c api c api ndarray cc 392 Operator zeros cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered Stack trace returned 10 entries bt 0 usr local lib python2 7 dist packages mxnet libmxnet so 0xc72fc 0x7f1b5c9ee2fc bt 1 usr local lib python2 7 dist packages mxnet libmxnet so MXImperativeInvoke 0xaca 0x7f1b5d423b5a bt 2 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7f1b60ea6e40 bt 3 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7f1b60ea68ab bt 4 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7f1b610b63df bt 5 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7f1b610bad82 bt 6 python PyObject Call 0x43 0x4b0cb3 bt 7 python PyEval EvalFrameEx 0x5faf 0x4c9faf bt 8 python PyEval EvalCodeEx 0x255 0x4c2765 bt 9 python PyEval EvalFrameEx 0x68d1 0x4ca8d1,,szha,2017-07-22 08:24:17,2017-10-29 12:26:43
IS,I meet Segmentation fault core dumped when I load checkpoint,Environment info Operating System ubuntu14 04 Compiler gcc4 9 Package used Python R Scala Julia python MXNet version v 0 10 0 Or if installed from source yes Python version and distribution 2 7 6 Error Message Segmentation fault core dumped Minimum reproducible example I create a new operate Which could get data and label for example 128 images of fully connected layer features and their labels are the input And then compute the difference of two images the label become 0 or 1 which means whether these two images are from one class or not So the input size may be 128 and the output size will be 64 I train this model successfully But when I want to load the save model I meet the Segmentation fault Maybe I should rewrite my operate or what should I do thank you Steps to reproduce sym arg params aux params mx model load checkpoint wouldpn107 one' 10,,szha,2017-07-23 13:56:03,2017-10-29 12:26:44
IS,AttributeError 'module' object has no attribute 'CaffeOp',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Ubuntu 14 04 Compiler gcc 4 8 5 Package used Python R Scala Julia Python MXNet version latest Or if installed from source installed from GitHub MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Traceback most recent call last File caffe net py line 104 in module net get lenet File caffe net py line 32 in get lenet conv1 mx symbol CaffeOp data 0 data num weight 2 prototxt layer type Convolution convolution param num output 20 kernel size 5 stride 1 AttributeError 'module' object has no attribute 'CaffeOp' Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 cd mxnet example caffe 2 python caffe net py 3 What have you tried to solve it 1 carefully recompile caffe and mxnet by following the instruction but still shows this error 2 3,,"piiswrong,piiswrong,piiswrong,szha",2017-07-17 02:32:07,2017-10-29 12:26:45
IS,grouped convolutions seemingly not very efficiently,I implemented mobilenet but forward time looks slow in GPU I use grouped conv to implement depthwise conv set group equal to number of feature maps The computation reduced by depthwise conv dos not make inference time decrease The mobilenet impemented by TF seem to be much faster url,,"ysh329,ysh329,szha,piiswrong",2017-06-14 02:58:12,2017-10-29 12:26:45
IS,Shape inconsistent,Hi pre import mxnet as mx import numpy as np import logging def create net stack mx rnn SequentialRNNCell for i in range 3 cell mx rnn LSTMCell num hidden num hidden prefix 'lstm dl0 ' i cell mx rnn BidirectionalCell cell mx rnn LSTMCell num hidden num hidden prefix 'lstm dr0 ' i output prefix 'bi lstm d' i stack add cell X mx sym Variable wouldata' Y mx sym Variable 'lin reg label' stack reset outputs states stack unroll seq len inputs X merge outputs True fc1 mx sym FullyConnected data outputs name 'fc1' num hidden num hidden act1 mx symbol Activation data fc1 name arelu1' act type relu fc2 mx symbol FullyConnected data act1 name 'fc2' num hidden out feature size net mx sym LinearRegressionOutput data fc2 label Y name lro print net debug str return net logging getLogger setLevel logging DEBUG seq len 10 num hidden 20 in feature size 30 out feature size 10 batch size 16 Training data train data np random uniform 0 1 100 seq len in feature size train label np random uniform 0 1 100 seq len out feature size Evaluation Data eval data train data eval label train label train iter mx io NDArrayIter train data train label batch size shuffle True label name 'lin reg label' eval iter mx io NDArrayIter eval data eval label batch size shuffle False net create net a mx viz plot network net a render example context mx cpu 0 model mx mod Module context context symbol net data names wouldata' label names 'lin reg label' network structure frequent 50 wd weight decay for sgd model fit train iter eval iter optimizer isgd' optimizer params 'learning rate' 0 005 'momentum' 0 9 'wd' 0 00001 num epoch 50 eval metric 'mse' initializer mx init Xavier factor type in magnitude 2 34 batch end callback mx callback Speedometer batch size batch size frequent frequent Logs training speed and evaluation metrics periodically epoch end callback mx callback do checkpoint test model 2 test model 0032 params metric mx metric MSE score model score eval iter metric print score pre gave me pre mxnet base MXNetError Error in operator lro Shape inconsistent Provided 16 10 10 inferred shape 16 10 pre I thought the pre outputs pre from pre stack pre has size batch size sequence length 2 hidden dim then after 2 feedforward networks fc2 would be the size batch size sequence length out feature size Thanks J,,"jingpengw,szha",2017-07-24 16:33:41,2017-10-29 12:26:47
IS,kernel dies when trainning begins,why did iPython is kernel die when i used cpu but it worked normally when i used gpu mxnet 10 0 windows 10 platform,,szha,2017-07-24 09:30:13,2017-10-30 00:26:26
IS,What is the rec file format,It perhaps is kMagic 4bytes 0x0A23D7CE LRecord 4byts 3bits cflag and 29bits length unknown 8byts ImageID 8bytes Reserved 8bytes all be 0 Image data and pad Am I right what is the unknown 8 bytes and how can I make a float rec file,,szha,2017-07-24 22:19:44,2017-10-30 00:26:27
IS,batch processing with image classification demo,I noticed that under demo We use one picture to do classification one time It worked well Now I'm trying to do image classification with batch size 256 or other batch size but I'm not sure how to change the existing code I assume this line const mx uint input shape data 4 1 static cast mx uint channels static cast mx uint height static cast mx uint width the first argument 1 is batch size 1 So what I have to do is to read continue batch size images and store the data into vector std vector mx float image data std vector mx float image size batch size then set input MXPredSetInput pred hnd data image data data image size batch size and do forward MXPredForward pred hnd am I right,,szha,2017-07-26 08:05:29,2017-10-30 00:26:27
IS,Missing File according to doc tests python gpu test conv py,Error Message python can not open file 'tests python gpu test conv py' Errno 2 No such file or directory not consistent with doc L105 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 go to doc L105 2 go to 3 see test conv py missing,,"jqueguiner,szha",2017-07-26 17:15:29,2017-10-30 00:26:29
IS,Writing when slicing Weird bug needs urgent fixing,If you assign array of shape 10 to shape of 10 using the slice operator i e w 0 w 0 you get a horrible bug In 9 import mxnet as mx In 10 import ndarray as nd ImportError Traceback most recent call last ipython input 10 abe6825c56ca in module 1 import ndarray as nd ImportError No module named 'ndarray' In 11 import mxnet as mx In 12 import mxnet ndarray as nd In 13 w nd zeros shape 10 2 In 14 w 0 w 0 MXNetError Traceback most recent call last ipython input 14 1e895c2e651f in module 1 w 0 w 0 mxnet python mxnet ndarray py in setitem self key value 389 value value as in context self context 390 internal crop assign self value out self 391 begin begin end end 392 elif isinstance value numeric types 393 internal crop assign scalar self out self mxnet python mxnet ndarray py in crop assign lhs rhs begin end out name kwargs mxnet python mxnet ctypes ndarray py in imperative invoke handle ndargs keys vals out 70 ctypes c int len keys 71 c array ctypes c char p c str key for key in keys 72 c array ctypes c char p c str str val for val in vals 73 74 if original output is not None mxnet python mxnet base py in check call ret 100 101 if ret 0 102 raise MXNetError py str LIB MXGetLastError 103 104 if sys version info 0 3 MXNetError Shape inconsistent Provided 10 inferred shape 10 1 In 15 For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3 NOTE THAT THIS DOES WORK IN NUMPY In 15 import numpy as np In 16 z np zeros 10 2 In 17 z Out 17 array 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 In 18 z 0 z 0 In 19,,"zackchase,rahul003,szha",2017-07-25 19:07:21,2017-10-30 00:26:29
IS,ConfusionMatrix in C,Is there an easy way to output a confusion matrix with C API,,szha,2017-07-26 23:21:34,2017-10-30 00:26:30
IS,Model parameter update,Hi I would like to manually update the model parameter s from module either Module or BucketingModule Assume that I have created a module for my network I tried the following code The above code does not work correctly After this update the model parameter seems to be much worse and worse I am looking for your advice Thanks,,szha,2017-07-26 23:04:41,2017-10-30 00:26:31
IS,A possible bug due to lacking synchronization after Copy,In the code of src operator softmax output inl h I think there might be a bug in this line L160 This line aims to copy the contents in label to workspace However since in the case that label resides in gpu the copy is asynchronous w r t the host should not there be a synchronize sentence after this line e g cudaStreamSynchronize label stream to do synchronization Otherwise I think unexpected error may occur here L163 where the content of workspace is used Any comments Thanks a lot,,"piiswrong,szha",2017-07-07 17:50:52,2017-10-30 00:26:32
IS,Amalgamation is broken again,I built the libmxnet predict so by running the make command in When I run mxnet predict example py at mxnet tests python predict The following error comes To reproduce this pls make sure that the amalgamated so file is used Hence only libmxnet predict so should be in mxnet lib This error does not come before version 0 9,,szha,2017-07-24 17:17:57,2017-10-30 00:26:33
IS,Passing gradient between 2 networks,Hello I would like to use a LSTM in combination with a CNN as a feature extractor to classify videos I am using mxnet cu80 0 10 0 post2 in python 2 7 9 I have a pretrained resnet 50 which is loaded and then I chop off the final classification layer I should mention that 2048 is the feature dimension of the last resnet layer the sequence lenght is fixed to 10 and batch size in this case the number of sequences per batch is 2 so this explains the 20 and 2048 So lstm grad is an NDArray of size 20x2048 and so is the cnn output Thank is for your help Max,,szha,2017-07-27 19:43:15,2017-10-30 00:26:34
IS,mxnet feature extraction,I applied this web page I run Feature extraction Gave the following results 16 52 09 src nnvm legacy json util cc 190 Loading symbol saved by previous version v0 8 0 Attempting to upgrade 16 52 09 src nnvm legacy json util cc 198 Symbol successfully upgraded 16 52 09 src nnvm legacy json util cc 190 Loading symbol saved by previous version v0 8 0 Attempting to upgrade 16 52 09 src nnvm legacy json util cc 198 Symbol successfully upgraded 0 15937786 0 27564028 0 36330682 1 348001 0 07281415 2 22004104 what these results mean,,"rahul003,szha",2017-07-21 11:35:50,2017-10-30 00:26:35
IS,poor gpu performance with mx io ndarrayIter,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Window Package used Python R Scala Julia Python MXNet version 0 7 0 Python version and distribution 2 7 12 The gpu works normal with a 60 gpu load in the example image classfication train cifar10 py But when I try my own code with mx io ndarrayIter for the numpy array data the gpu load keeps zero with cpu load increase The same question happens in the example End2End Captcha Recognition OCR by xlvector github link Blog in Chinese Will it be solved once I change my data to rec And if there a guide on how to trans numpy array data to rec format,,szha,2017-07-27 12:19:36,2017-10-30 00:26:35
IS,when i make the mxnet the error occur and this is the error message can you help me i just,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message itching so lopencv sta build src operator tensor elemwise binary broadcast op logic gpu o itching usr lib x86 64 linux gnu libopencv superres so lopencv superres usr lib x86 64 linux gnu libopencv ts so usr lib x86 64 linux gnu libopencv video so lopencv video usr lib x86 64 linux gnu libopencv videostab so lopencv videostab lcudnn lopencv core lopencv imgproc lopencv imgcodecs lcuda Wl whole archive home dms install program mxnet nnvm lib libnnvm a Wl no whole archive a build src operator tensor elemwise binary op logic gpu o a build src operator contrib multibox detection gpu o a build src operator contrib multibox target gpu o a build src operator contrib proposal gpu o a build src operator contrib multibox prior gpu o a build src operator custom native op gpu o a build src ndarray ndarray function gpu o a build src operator svm output gpu o a build src operator optimizer op gpu o a build src operator fully connected gpu o a build src operator pooling v1 gpu o a build src operator lrn gpu o a build src operator grid generator gpu o a build src operator softmax activation gpu o a build src operator pooling gpu o a build src operator regression output gpu o a build src operator leaky relu gpu o a build src operator identity attach KL sparse reg gpu o a build src operator sequence mask gpu o a build src operator activation gpu o a build src operator roi pooling gpu o a build src operator cudnn batch norm gpu o a build src operator loss binary op gpu o a build src operator convolution gpu o a build src operator l2 normalization gpu o a build src operator batch norm gpu o a build src operator upsampling gpu o a build src operator concat gpu o a build src operator sequence reverse gpu o a build src operator slice channel gpu o a build src operator dropout gpu o a build src operator bilinear sampler gpu o a build src operator pad gpu o a build src operator correlation gpu o a build src operator instance norm gpu o a build src operator softmax output gpu o a build src operator rnn gpu o a build src operator convolution v1 gpu o a build src operator crop gpu o a build src operator spatial transformer gpu o a build src operator deconvolution gpu o a build src operator swapaxis gpu o a build src operator make loss gpu o a build src operator sequence last gpu o g DMSHADOW FORCE STREAM Wall Wsign compare O3 I home dms install program mxnet mshadow I home dms install program mxnet dmlc core include fPIC I home dms install program mxnet nnvm include Iinclude funroll loops Wno unused variable Wno unused parameter Wno unknown pragmas Wno unused local typedefs msse3 I usr local cuda include DMSHADOW USE CBLAS 1 DMSHADOW USE MKL 0 DMSHADOW RABIT PS 0 DMSHADOW DIST PS 0 DMSHADOW USE PASCAL 0 DMXNET USE OPENCV 1 I usr include opencv fopenmp DMSHADOW USE CUDNN 1 I usr include openblas I home dms install program mxnet cub DMXNET USE NVRTC 0 std c 11 o bin im2rec tools im2rec cc build src operator nn softmax o build src operator mkl mkl cppwrapper o build src operator mkl mkl memory o build src operator tensor elemwise binary broadcast op extended o build src operator tensor elemwise binary op extended o build src operator tensor matrix op o build src operator tensor elemwise sum o build src operator tensor init op o build src operator tensor elemwise binary broadcast op basic o build src operator tensor broadcast reduce op index o build src operator tensor broadcast reduce op value o build src operator tensor elemwise unary op o build src operator tensor elemwise binary op basic o build src operator tensor elemwise binary scalar op extended o build src operator tensor indexing op o build src operator tensor ordering op o build src operator tensor elemwise binary broadcast op logic o build src operator tensor sample op o build src operator tensor elemwise binary op logic o build src operator tensor control flow op o build src operator tensor elemwise binary scalar op basic o build src operator tensor elemwise binary scalar op logic o build src operator nnpack nnpack util o build src operator contrib multibox target o build src operator contrib multibox prior o build src operator contrib multibox detection o build src operator contrib proposal o build src operator custom native op o build src operator custom ndarray op o build src operator custom custom o build src io io o build src io image aug default o build src io iter csv o build src io image io o build src io iter image recordio o build src io iter mnist o build src common mxrtc o build src nnvm legacy op util o build src nnvm legacy json util o build src ndarray ndarray function o build src ndarray ndarray o build src operator instance norm o build src operator loss binary op o build src operator rnn o build src operator convolution v1 o build src operator crop o build src operator sequence reverse o build src operator spatial transformer o build src operator swapaxis o build src operator batch norm o build src operator operator util o build src operator operator o build src operator sequence last o build src operator correlation o build src operator make loss o build src operator svm output o build src operator deconvolution o build src operator optimizer op o build src operator lrn o build src operator pooling v1 o build src operator pad o build src operator sequence mask o build src operator grid generator o build src operator identity attach KL sparse reg o build src operator activation o build src operator bilinear sampler o build src operator fully connected o build src operator pooling o build src operator softmax output o build src operator convolution o build src operator cudnn batch norm o build src operator cross device copy o build src operator regression output o build src operator l2 normalization o build src operator upsampling o build src operator concat o build src operator leaky relu o build src operator roi pooling o build src operator cudnn convolution o build src operator slice channel o build src operator dropout o build src operator softmax activation o build src engine profiler o build src engine naive engine o build src engine threaded engine pooled o build src engine engine o build src engine threaded engine o build src engine threaded engine perdevice o build src storage storage o build src c api c api executor o build src c api c api symbolic o build src c api c api ndarray o build src c api c predict api o build src c api c api o build src c api c api error o build src executor inplace addto detect pass o build src executor graph executor o build src executor attach op execs pass o build src executor attach op resource pass o build src kvstore kvstore o build src resource o build src initialize o home dms install program mxnet dmlc core libdmlc a home dms install program mxnet nnvm lib libnnvm a build src operator nn softmax gpu o build src operator tensor elemwise binary op extended gpu o build src operator tensor elemwise binary scalar op extended gpu o build src operator tensor elemwise binary scalar op basic gpu o build src operator tensor matrix op gpu o build src operator tensor ordering op gpu o build src operator tensor elemwise binary broadcast op extended gpu o build src operator tensor sample op gpu o build src operator tensor elemwise binary op basic gpu o build src operator tensor elemwise sum gpu o build src operator tensor init op gpu o build src operator tensor broadcast reduce op index gpu o build src operator tensor elemwise binary scalar op logic gpu o build src operator tensor broadcast reduce op value gpu o build src operator tensor control flow op gpu o build src operator tensor elemwise binary broadcast op basic gpu o build src operator tensor elemwise unary op gpu o build src operator tensor indexing op gpu o build src operator tensor elemwise binary broadcast op logic gpu o build src operator tensor elemwise binary op logic gpu o build src operator contrib multibox detection gpu o build src operator contrib multibox target gpu o build src operator contrib proposal gpu o build src operator contrib multibox prior gpu o build src operator custom native op gpu o build src ndarray ndarray function gpu o build src operator svm output gpu o build src operator optimizer op gpu o build src operator fully connected gpu o build src operator pooling v1 gpu o build src operator lrn gpu o build src operator grid generator gpu o build src operator softmax activation gpu o build src operator pooling gpu o build src operator regression output gpu o build src operator leaky relu gpu o build src operator identity attach KL sparse reg gpu o build src operator sequence mask gpu o build src operator activation gpu o build src operator roi pooling gpu o build src operator cudnn batch norm gpu o build src operator loss binary op gpu o build src operator convolution gpu o build src operator l2 normalization gpu o build src operator batch norm gpu o build src operator upsampling gpu o build src operator concat gpu o build src operator sequence reverse gpu o build src operator slice channel gpu o build src operator dropout gpu o build src operator bilinear sampler gpu o build src operator pad gpu o build src operator correlation gpu o build src operator instance norm gpu o build src operator softmax output gpu o build src operator rnn gpu o build src operator convolution v1 gpu o build src operator crop gpu o build src operator spatial transformer gpu o build src operator deconvolution gpu o build src operator swapaxis gpu o build src operator make loss gpu o build src operator sequence last gpu o pthread lm lcudart lcublas lcurand L usr local cuda lib64 L usr local cuda lib lopenblas fopenmp lrt usr lib x86 64 linux gnu libopencv calib3d so lopencv calib3d usr lib x86 64 linux gnu libopencv contrib so lopencv contrib usr lib x86 64 linux gnu libopencv core so lopencv core usr lib x86 64 linux gnu libopencv features2d so lopencv features2d usr lib x86 64 linux gnu libopencv flann so lopencv flann usr lib x86 64 linux gnu libopencv gpu so lopencv gpu usr lib x86 64 linux gnu libopencv highgui so lopencv highgui usr lib x86 64 linux gnu libopencv imgproc so lopencv imgproc usr lib x86 64 linux gnu libopencv legacy so lopencv legacy usr lib x86 64 linux gnu libopencv ml so lopencv ml usr lib x86 64 linux gnu libopencv objdetect so lopencv objdetect usr lib x86 64 linux gnu libopencv ocl so lopencv ocl usr lib x86 64 linux gnu libopencv photo so lopencv photo usr lib x86 64 linux gnu libopencv stitching so lopencv stitching usr lib x86 64 linux gnu libopencv superres so lopencv superres usr lib x86 64 linux gnu libopencv ts so usr lib x86 64 linux gnu libopencv video so lopencv video usr lib x86 64 linux gnu libopencv videostab so lopencv videostab lcudnn lopencv core lopencv imgproc lopencv imgcodecs lcuda usr bin ld cannot find lopenblas usr bin ld cannot find lopencv imgcodecs usr bin ld cannot find lopenblas usr bin ld cannot find lopencv imgcodecs collect2 error ld returned 1 exit status collect2 error ld returned 1 exit status Makefile 237 recipe for target 'lib libmxnet so' failed make lib libmxnet so Error 1 make Waiting for unfinished jobs Makefile 259 recipe for target 'bin im2rec' failed make bin im2rec Error 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 i try for more than one time but the same result i get 2 3,,"Piyush3dB,szha",2017-03-30 06:48:04,2017-10-30 00:26:36
IS,java lang NullPointerException at ml dmlc mxnet spark MXNet fit MXNet scala 108,Hi I am getting the error java lang NullPointerException at scala collection mutable ArrayOps ofRef length extension ArrayOps scala 192 at scala collection mutable ArrayOps ofRef length ArrayOps scala 192 at scala collection IndexedSeqOptimized class foreach IndexedSeqOptimized scala 32 at scala collection mutable ArrayOps ofRef foreach ArrayOps scala 186 at ml dmlc mxnet spark MXNet fit MXNet scala 108 at com yash mxnet app ClassificationExample main ClassificationExample scala 71 at com yash mxnet app ClassificationExample main ClassificationExample scala at sun reflect NativeMethodAccessorImpl invoke0 Native Method at sun reflect NativeMethodAccessorImpl invoke NativeMethodAccessorImpl java 62 at sun reflect DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl java 43 at java lang reflect Method invoke Method java 498 at org apache spark deploy SparkSubmit org apache spark deploy SparkSubmit runMain SparkSubmit scala 731 at org apache spark deploy SparkSubmit doRunMain 1 SparkSubmit scala 181 at org apache spark deploy SparkSubmit submit SparkSubmit scala 206 at org apache spark deploy SparkSubmit main SparkSubmit scala 121 at org apache spark deploy SparkSubmit main SparkSubmit scala Code I am running is ClassificationExample scala available var mxnet new MXNet setBatchSize 128 setLabelName softmax label setContext devs setDimension dimension setNetwork network setNumEpoch cmdLine numEpoch setNumServer cmdLine numServer setNumWorker cmdLine numWorker setExecutorJars cmdLine jars setJava cmdLine java var model mxnet fit trainData mxnet fit trainData I am facing Null pointer execption I am able to set all objects received in var mxnet correctly Help me solve this issue,,szha,2017-06-23 12:44:11,2017-10-30 00:26:37
IS,Memory allocation failed after multiple call to Module predict,Hi I am using Module predict to calculate some feature vector of my data outputshape batchsize 4096 After 97 calls to predict mxnet reported Memory allocation failed and GPU monitoring software reported 7 6GB 8GB gpu mem usage There are plenty of system RAM available Closing the python process will release GPU mem Do I need to call some method to release GPU mem after predict,,"piiswrong,tqchen,piiswrong,piiswrong,szha",2016-09-23 19:03:11,2017-10-30 00:26:39
IS,Cannot resume training while using adam optimizer,I trained a neural network using adam optimizer When I tryed to resume training the neural network from checkpoint using mxnet module Module load with load optimizer states True an error occurred This is because when we load optimizer states the states of adam optimizer recovered by pickle loads sit on the device CPU which is different from the weight of module commonly sitting on device GPU Thus when adam update is called the error occured,,"piiswrong,piiswrong,szha",2017-03-16 10:27:21,2017-10-30 00:26:40
IS,About sync frequency in distributed training,I have two questions about distributed training in mxnet First after reading the code in train mnist py as to my understanding the only changes you need to make is 1 pass dist sync dist async kvstore to optimizer or module fit method 2 make a hosts file and make sure all IPs on that list is ssh able or mpirun able 3 call launch py in tools Is my understanding correct or am I missing something Second I just want to verify the sync frequency in distributed setting Are the weights being synced every time you call module update And if I want to sync on each machine for every n batches is there any way I can do something like module local update on every batch and module global update every n batch Thanks in advance,,"rahul003,szha",2017-07-22 16:17:46,2017-10-30 00:26:41
IS,Why monitor can not print backward statistics when using BucketingModule,when i use BucketingModule i came across a problem that it doesnot print backward weight or data code is as followed Any help will appreciate,,"tornadomeet,piiswrong,fhieber,piiswrong,fhieber,szha",2017-03-31 06:24:03,2017-10-30 00:26:41
IS,Run on multi machine no mudule named mxnet,I run model on single machine is well but use launcher py run the model on two machine this will be error import mxnet as mx ImportError No module named mxnet,,"eric-haibin-lin,szha",2017-05-10 04:45:47,2017-10-30 00:26:42
IS,bucket iterator with ctc error,I want to test flexible length input with warpctc because the rnn is an example of bucket iterator So I modified the rnn lstm py as follows I modified def lstm unroll funciton Please help me to locate the error,,szha,2016-09-02 09:43:01,2017-10-30 00:26:43
IS,Which metric can we use for unsupervised loss,Hello I need report some unsupervised learning losses during training process these losses have no label and I have no ideal which metric should use or how to write a custom metric I hope someone could help me please thanks,,eric-haibin-lin,2017-10-26 01:48:50,2017-10-30 12:17:43
IS,load multi models from different params json files,Environment info Operating System Ubuntu 14 04 16 04 Compiler gcc 4 8 4 g 4 8 4 Package used Python R Scala Julia Python MXNet version Or if installed from source master on 2017 05 25 Python version and distribution Python 2 7 Question I'm a beginner user of mxnet and i have tried to use some neural networks and trained 3 different models different params and json files the question is how can i load the all 3 models from local params json files in a single python process Or is there any api that i can refer to Thanks in advance,,szha,2017-07-31 00:37:20,2017-10-30 12:26:27
IS,How to use a classification pretrain model to train a triple loss,Recently I have tried to use mxnet to do some experiments about metric learning Now I trained a multi classification model by mxnet and then use it as a pretrain model to train a triple loss model the problem is always that raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 18 00 07 src c api c api symbolic cc 398 InferShapeKeyword argumentname label not found Candidate arguments 0 data 1 conv1 weight 2 conv1 bias 3 conv2 weight 4 conv2 bias 5 conv3 weight 6 conv3 bias 7 conv4 weight 8 conv4 bias 9 conv5 weight 10 conv5 bias 11 fc1 weight 12 fc1 bias 13 fc2 weight 14 fc2 bias 15 fc3 weight 16 fc3 bias 17 fc4 weight 18 fc4 bias 19 fc5 weight 20 fc5 bias 21 fc6 weight 22 fc6 bias Here is the multi classification model def get ocrnet data mx symbol Variable wouldata' label mx symbol Variable 'label' stage 1 conv1 mx symbol Convolution name 'conv1' data data kernel 5 5 stride 1 1 num filter 64 relu1 mx symbol Activation data conv1 act type relu lrn1 mx symbol LRN data relu1 alpha 0 0001 beta 0 75 knorm 2 nsize 5 pool1 mx symbol Pooling data lrn1 pool type max kernel 3 3 pad 1 1 stride 2 2 stage 2 conv2 mx symbol Convolution name 'conv2' data pool1 kernel 5 5 num filter 128 relu2 mx symbol Activation data conv2 act type relu lrn2 mx symbol LRN data relu2 alpha 0 0001 beta 0 75 knorm 2 nsize 5 pool2 mx symbol Pooling data lrn2 kernel 3 3 stride 2 2 pool type max stage 3 conv3 mx symbol Convolution name 'conv3' data pool2 kernel 3 3 num filter 256 relu3 mx symbol Activation data conv3 act type relu conv4 mx symbol Convolution name 'conv4' data relu3 kernel 3 3 num filter 384 relu4 mx symbol Activation data conv4 act type relu conv5 mx symbol Convolution name 'conv5' data relu4 kernel 3 3 num filter 512 relu5 mx symbol Activation data conv5 act type relu pool3 mx symbol Pooling data relu5 kernel 3 3 pad 1 1 stride 2 2 pool type max stage 4 flatten mx symbol Flatten data pool3 fc1 mx symbol FullyConnected name 'fc1' data flatten num hidden 4096 relu6 mx symbol Activation data fc1 act type relu dropout1 mx symbol Dropout data relu6 p 0 4 stage 5 fc2 mx symbol FullyConnected name 'fc2' data dropout1 num hidden 4096 relu7 mx symbol Activation data fc2 act type relu dropout2 mx symbol Dropout data relu7 p 0 4 stage 6 fc3 mx symbol FullyConnected name 'fc3' data dropout2 num hidden 3002 label mx symbol transpose data label label mx symbol Reshape data label target shape 0 return mx symbol SoftmaxOutput data fc3 label label name softmax here is the triple net def get triplet symbol arg params layer name len size all layers symbol get internals label all layers 'label' one mx sym Reshape data label shape 1 one np ones like one fc3 all layers layer name ' output' fc4 mx symbol FullyConnected name 'fc4' data fc3 num hidden 1048 fc5 mx symbol FullyConnected name 'fc5' data fc4 num hidden 512 fc6 mx symbol FullyConnected name 'fc6' data fc5 num hidden 128 l2 mx sym L2Normalization data fc6 fa mx symbol slice axis l2 axis 0 begin 0 end len size fp mx symbol slice axis l2 axis 0 begin len size end 2 len size fn mx symbol slice axis l2 axis 0 begin 2 len size end 3 len size fs fa fp fd fa fn fs fs fs fd fd fd fs mx symbol sum fs axis 1 keepdims 1 fd mx symbol sum fd axis 1 keepdims 1 loss fd fs loss one loss loss mx symbol Activation data loss act type arelu' loss triplet loss l2 one len size net mx symbol MakeLoss loss new args dict k arg params k for k in arg params if 'fc' not in k return net new args the data are put in the model in the same ways and the model train func here is def train path train txt test txt train model pre model pre epoch 20 devs mx gpu 0 batch size 120 sym arg params aux params mx model load checkpoint pre model pre epoch network new args get triplet sym arg params 'fc3' batch size 3 network get triple net batch size 3 model mx model FeedForward symbol network num epoch 30 ctx devs learning rate 0 01 wd 0 00001 initializer mx init Xavier factor type in magnitude 2 34 arg params new args aux params aux params momentum 0 9 data train OCRIter batch size 1 path train txt import logging head u' asctime 15s message s' logging basicConfig level logging DEBUG format head model fit X data train epoch end callback mx callback do checkpoint train model 5 batch end callback mx callback Speedometer batch size 50 model save train model Environment info Operating System Ubuntu 14 04 LTS Compiler GCC 4 8 Package used Python R Scala Julia Python MXNet version 0 10 release Or if installed from source installed by pypi resource Error Message Please paste the full error message including stack trace Minimum reproducible example,,szha,2017-07-28 10:18:40,2017-10-30 12:26:28
IS,when train fcn xs for VOC the train and val accuracy is always the same,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos Compiler gcc 4 8 2 Package used Python R Scala Julia python MXNet version Or if installed from source from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error image,,szha,2017-07-31 01:59:53,2017-10-30 12:26:28
PR,Fix arange bug on ARM,Description This PR provides consistent behaviour for the arange operator between ARM and x86 devices Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Fixes edge cases when running arange on ARM These cases were failing the test arange test The problem was routed in the fact that converting from floats to unsigned in C happens differently on different platforms ARM for example initializes the resulting unsigned data structure to 0 whereas x86 initializes it to the two is complement representation of the negative value Numpy uses the two is complement form both on x86 and ARM To maintain compatibility with Numpy and consistency across devices I have changed our implementation to first cast to signed then unsigned types This forces 2 is complement form on both architectures Tracked this one down together with,,KellenSunderland,2017-10-30 11:01:50,2017-10-30 16:47:13
PR,clean up math operators,mseeger,,"piiswrong,mseeger,mseeger,mseeger,piiswrong,mseeger,mseeger,mseeger",2017-10-17 21:18:12,2017-10-30 21:37:47
PR,fix condition when CSRNDArray is used in NDArrayIter with shuffle True,Description Currently if NDArrayIter wouldata' csr shuffle True there is no AssertionError haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,ZiyueHuang",2017-10-21 11:25:44,2017-10-30 22:11:58
IS,Fine tune tutorial urllib bug on Python 3,Tutorial Python version and distribution 3 6 Steps to reproduce 1 Executing the section of code to download the dataset will fail with a urllib error unless you are using Python 2 7 x,,"aaronmarkham,szha",2017-10-05 19:14:42,2017-10-30 22:14:04
PR,Fix urllib bug affecting Python 3 x in the finetune tutorial,Description Fixes 8159 by adding support for both Python 2 3 urllib implementations Checklist Essentials Passed code style checking make lint x Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Described in 8159 Comments,,aaronmarkham,2017-10-11 23:28:14,2017-10-30 22:14:04
IS,Bayesian Optimization for MXNET,Could you please provide a good reference to get started with Bayesian Optimization on models built through MXNET R package I plan to use Bayesian Optimization R package rBayesianOptimization for parameter tuning for CNN model built using MXNET Parameters Number of filter filter size stride Activation function I was able to use Bayesian Optimization for XGBoost through example provided in vignette Issue comes in where a model built in MXNET using mx model FeedForward create does not result in any evaluation log etc so that I can link it to Bayesian Optimization XGBoost install packages xgboost install packages rBayesianOptimization library rBayesianOptimization Example 2 Parameter Tuning library xgboost data agaricus train package xgboost dtrain xgb DMatrix agaricus train data label agaricus train label cv folds KFold agaricus train label nfolds 5 stratified TRUE seed 0 End Not run xgb cv bayes function max depth min child weight subsample cv xgb cv params list booster gbtree eta 0 01 max depth max depth min child weight min child weight subsample subsample colsample bytree 0 3 lambda 1 alpha 0 objective binary logistic eval metric auc data dtrain nround 100 folds cv folds prediction TRUE showsd TRUE early stopping rounds 5 maximize TRUE verbose 0 list Score cv evaluation log max test auc mean Pred cv pred OPT Res BayesianOptimization xgb cv bayes bounds list max depth c 2L 6L min child weight c 1L 10L subsample c 0 5 0 8 init grid dt NULL init points 10 n iter 20 acq ucb kappa 2 576 eps 0 0 verbose TRUE Best SA,,szha,2017-07-31 17:43:01,2017-10-31 00:26:26
PR,pvanet for classfication on Imagenet,article adress Pvanet Deep but Lightweight Neural Neural Networks for Real time Object Detection pvanet for classification on ImageNet the accuracy of the article is 70 I checked the shape of parameter to make sure the net is same with the author implement with Caffe but I still got a accuracy of 66 28 I trained with intialize mx init Normal batch size 512 lr factor 0 316 lr 0 1 num epochs 160 lr step epochs '30 60 80 100 120 140' result is INFO root Epoch 158 Batch 2350 Speed 624 06 samples sec accuracy 0 696992 cross entropy 1 213310 top k accuracy 5 0 890508 INFO root Epoch 158 Batch 2400 Speed 624 90 samples sec accuracy 0 698438 cross entropy 1 205256 top k accuracy 5 0 894531 INFO root Epoch 158 Batch 2450 Speed 625 89 samples sec accuracy 0 690547 cross entropy 1 232207 top k accuracy 5 0 890508 INFO root Epoch 158 Batch 2500 Speed 627 79 samples sec accuracy 0 695352 cross entropy 1 215836 top k accuracy 5 0 892695 INFO root Epoch 158 Train accuracy 0 683594 INFO root Epoch 158 Train cross entropy 1 284250 INFO root Epoch 158 Train top k accuracy 5 0 888672 INFO root Epoch 158 Time cost 2048 858 INFO root Saved checkpoint to pvanet models pvanet 0159 params INFO root Epoch 158 Validation accuracy 0 662847 INFO root Epoch 158 Validation cross entropy 1 374140 INFO root Epoch 158 Validation top k accuracy 5 0 869958 can this be accepted by MXNet,,"qingzhouzhen,qingzhouzhen,zhreshold,zhreshold",2017-10-25 01:22:28,2017-10-31 02:16:24
PR,add README md file,Update a README md file in example gluon,,qingzhouzhen,2017-09-25 07:34:43,2017-10-31 02:17:00
PR,website content changes modified for v0 12 0,Description Changes made to 0 12 0 version in the main page content Whats new section has 0 12 0 version added,,"thinksanky,piiswrong,thinksanky,thinksanky",2017-10-30 04:53:50,2017-10-31 04:22:06
IS,complie error in new version,I just update cuda to 8 0 and when i complie the latest version it will report cub cub cuh No such file or directory USE CUDA 1 USE CUDA PATH usr local cuda USE CUDNN 1 the makefile is as above,,reminisce,2017-10-31 04:01:46,2017-10-31 08:13:08
PR,rm redundant code,,,"formath,formath",2017-10-27 03:42:49,2017-10-31 10:47:29
IS,Autograd retain graph True bugs,Consider the following example will fix that problem but will stop the gradient from flowing back to the encoder parameters which arguably should be documented Calling backward only on hidden 0 or hidden 1 will also work Or is this not supposed to work Essentially I want to decompose the autograd graph into two parts similar to having two modules and using the input gradients of the second for the backward pass of the first,,"leezu,piiswrong,leezu,szha",2017-07-31 16:04:45,2017-10-31 12:26:27
IS,Batch processing for rcnn prediction,I want to send pictures to rcnn network for prediction by batch Does the current rcnn function like im detect and Predictor support this If they do not support this function what change should I do to achieve the goal,,szha,2017-08-01 05:39:37,2017-10-31 12:26:28
IS,BlockGrad and weight decay on backpropagation,I have a symbol for training like this The output0 will be used to calculate some accuracy metric But it seems like 'BlockGrad' will also affect network weight because of the weight decay Is there any way to ignore the output0 part when doing backpropagation but also use the output0 to evaluate the training result,,szha,2017-08-01 11:56:23,2017-10-31 12:26:29
IS,R package for GPU mxnet does not install on amazon linux AMI,I am having trouble installing mxnet GPU for R on Amazon deep learning linux AMI The environment variables are such a mess that it s a nightmare for any non expert sys admin to figure out Step 1 install the ridiculous amount of missing broken programs and R packages sudo yum install R sudo yum install libxml2 devel sudo yum install cairo devel sudo yum install giflib devel sudo yum install libXt devel sudo R install packages devtools library devtools install github igraph rigraph install packages DiagrammeR install packages roxygen2 install packages rgexf install packages influenceR install packages Cairo install packages imager Step 2 edit the config mk file cd src mxnet cp make config mk echo USE BLAS openblas config mk echo ADD CFLAGS I usr include openblas config mk echo ADD LDFLAGS lopencv core lopencv imgproc lopencv imgcodecs config mk echo USE CUDA 1 config mk echo USE CUDA PATH usr local cuda config mk echo USE CUDNN 1 config mk note even though the USE CUDA PATH is set it STILL cannot find libcudart so and needs to be linked in the make command shown later Step 3 make new config file so make command can find libcudart so etc ld so conf d cuda conf usr local cuda 8 0 lib64 sudo ldconfig note this was posted by nvidia but does absolutely nothing to help the make rpkg Step 4 set up R directories Rscript e install packages wouldevtools' repo '' cd R package Rscript e library devtools library methods options repos c CRAN '' install deps dependencies TRUE cd Step 5 make cd src mxnet sudo make j8 Result make CXX g DEPS PATH home ec2 user src mxnet deps C home ec2 user src mxnet ps lite ps cd home ec2 user src mxnet dmlc core make libdmlc a USE SSE 1 config home ec2 user src mxnet config mk cd home ec2 user src mxnet make 1 Entering directory home ec2 user src mxnet dmlc core' make 1 libdmlc a' is up to date make 1 Leaving directory home ec2 user src mxnet dmlc core' make 1 Entering directory home ec2 user src mxnet ps lite' make 1 Nothing to be done for ps' make 1 Leaving directory home ec2 user src mxnet ps lite' ar crv lib libmxnet a note even when changing the config mk file the make command always returns nothing to update Step 6 attempt to make rpkg Cd src mxnet Sudo make rpkg Error Error package or namespace load failed for mxnet onLoad failed in loadNamespace for 'mxnet' details call dyn load file DLLpath DLLpath error unable to load shared object ' usr lib64 R library mxnet libs libmxnet so' libcudart so 8 0 cannot open shared object file No such file or directory Error loading failed Execution halted ERROR loading failed So it s looking in a location that doesn t exist usr lib64 R library mxnet libs When the file actually lives home ec2 user src mxnet R package inst libs libmxnet so or home ec2 user src mxnet lib libmxnet so What I ve tried so far sudo LD LIBRARY PATH usr local cuda lib64 make rpkg This will fix the missing libcudart so 8 0 issue but it is simply replace with libmklml intel so cannot open shared object file No such file or directory as well as the original cannot find libmxnet so Also tried 1 actually creating directories usr lib64 R library mxnet libs and then copying libmxnet so there Result same error 2 adding home ec2 user src mxnet R package inst libs to the make command sudo LD LIBRARY PATH home ec2 user src mxnet R package inst libs make rpkg Result same error 3 a ridiculous amount of environment labels all of which failed export MXNET HOME usr lib64 R library mxnet libs export MXNET HOME usr lib64 R library mxnet libs libmxnet so sudo ldconfig usr local cuda lib64 sudo ln s usr lib64 R library mxnet libs usr lib sudo ln s usr lib64 R library mxnet libs libmxnet so usr lib sudo ln s usr local lib libmklml intel so usr lib sudo ln s usr local lib libiomp5 so usr lib sudo ln s usr local usr lib export LD LIBRARY PATH usr local cuda 8 0 lib64 libcudart so 8 0 export LD LIBRARY PATH usr lib64 R library mxnet libs libmxnet so usr lib export LD LIBRARY PATH usr local cuda 8 0 targets x86 64 linux lib LD LIBRARY PATH export LD LIBRARY PATH usr local cuda 8 0 lib64 libcudart so 8 0 In all ONE of these worked because I briefly got mxnet R package working before it fell apart again I ve dropped 50 hours into this installation which frankly is ridiculous I don t have 5 years of linux sys admin knowledge so if you d like please be a bit more helpful then fix environment variables I can tell that s obviously what s wrong yet have no idea what fix environment variables entails To top it off even after successful install of the R package it STILL won t work until setting Rstudio server s config file to rsession ld library path opt local lib usr local cuda lib64,,"anirudh2290,thirdwing",2017-10-25 22:42:24,2017-10-31 13:47:32
IS,Difficulty installing mxnet,Hi I am having difficulty in installing the mxnet R package on BERT console for interfacing with Excel and am wondering if you can offer some guidance I am using a 32 bit Excel and have installed the BERT Excel tool from I then proceeded to install the mxnet package with the lines below cran getOption repos cran dmlc options repos cran install packages mxnet Then I receive the message saying Package which is only available in source form and may need compilation of C C Fortran 'mxnet' These will not be installed I then tried a different set of lines install packages drat repos drat addRepo dmlc install packages mxnet Which appeared to be working but then had the following messages when I tried library mxnet Error in library mxnet there is no package called 'mxnet' In addition Warning message package 'mxnet' is not available for R version 3 4 1 I also tried installing mxnet in R Studio on Mac and I got the an error message advising that i tis not installed for arch i386 Presumably it means 32 bit systems I am wondering if anyone experienced this before Thanks,,"jeremiedb,thirdwing",2017-10-20 09:27:22,2017-10-31 13:49:11
IS,Issues with the Regression example in MXNet R package tutorial,I followed the example in Neural Network with MXNet in Five Minutes tutorial I tried the Regression example code using the Symbol system and also I tried another routine using mx mlp function Based on the documentation these two ways should be the same as mx mlp is built on the Symbol system However the results are very different as shown by the error from testing data i e 7 80 vs 23 86 The code is shown below I noticed an earlier issue A similar problem was reported The response suggested adding a extra hidden layer in the symbol system Indeed that made the two testing errors similar A few questions here 1 I am not sure why adding a extra hidden layer is needed in the symbol system way and why that extra layer made the model much worse much bigger error 2 how to implement the original regression example using mx mlp instead of symbol system As we saw above that calling mx mlp with hidden node 1is equivalent to adding a extra hidden layer in symbol system We would need to remove that hidden layer by hidden node 0 but mx mlp function does not accept this and gave us error message I hope the MXNet authors or some expert users can help me on these issues Thank you data prep data BostonHousing package mlbench train ind seq 1 506 3 train x data matrix BostonHousing train ind 14 train y BostonHousing train ind 14 test x data matrix BostonHousing train ind 14 test y BostonHousing train ind 14 using the symbol system data mx symbol Variable data fc1 mx symbol FullyConnected data num hidden 1 lro mx symbol LinearRegressionOutput fc1 mx set seed 0 model mx model FeedForward create lro X train x y train y ctx mx cpu num round 50 array batch size 20 learning rate 2e 6 momentum 0 9 eval metric mx metric rmse error measurement preds predict model test x sqrt mean preds test y 2 output sqrt mean preds test y 2 1 7 800502356 using the mx mlp function model0 mx mlp train x train y hidden node 1 out node 1 out activation rmse num round 50 array batch size 20 learning rate 2e 6 momentum 0 9 eval metric mx metric rmse error measurement preds0 predict model0 test x sqrt mean preds0 test y 2 output sqrt mean preds0 test y 2 1 23 85977502,,"jeremiedb,thirdwing,thirdwing",2017-09-20 00:02:17,2017-10-31 13:56:56
IS,does libsvm data ought to be sorted in ascending order for every row,If not ascending order does will it have a wrong result during trainning Can it possible to support random order for every row haibin lin,,"formath,formath,eric-haibin-lin",2017-10-31 08:18:08,2017-10-31 17:23:05
PR,commiting v12 changess,Description Changes done for v12 content,,thinksanky,2017-10-31 01:57:58,2017-10-31 17:35:29
PR,Simplified unary binary math operators,Description These are simplifications suggested by I added some further changes which make sure that all gradient expressions are computed in the same way as forward expressions namely either in float or double Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"mseeger,reminisce,reminisce,reminisce,mseeger,mseeger,reminisce,piiswrong,mseeger,cjolivier01,cjolivier01,mseeger,mseeger,cjolivier01,mseeger,mseeger,piiswrong,mseeger,piiswrong,cjolivier01,cjolivier01,mseeger,mseeger,mseeger,mseeger,cjolivier01,cjolivier01,mseeger,cjolivier01,mseeger,cjolivier01,piiswrong,mseeger,mseeger,cjolivier01,piiswrong,mseeger,cjolivier01,mseeger,cjolivier01,mseeger,cjolivier01,mseeger,piiswrong,mseeger,cjolivier01,mseeger,mseeger,mseeger,piiswrong",2017-10-20 15:22:30,2017-10-31 18:11:22
IS,R installation error in CentOS 7 4 GPU machine,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description I am able to compile the mxnet but not generate the R version due to a segmentation fault when it checks if the installed package can be loaded Environment info Required What have you tried to solve it I tried making with and without sudo no success I have CUDA 8 0 in CentOS version 7 4,,thirdwing,2017-10-28 00:44:27,2017-10-31 18:46:39
IS,Issue Installing mxnet for R on Ubuntu 14 04 cholmod h not found,,,"jeremiedb,thirdwing",2017-10-27 22:12:19,2017-10-31 21:02:04
IS,ndarray issue,I just test ndarray in if case import find mxnet import mxnet as mx a mx nd array 4 9 1 16 25 49 b a a 2 print a asnumpy print b asnumpy why the result b is the same as a I think the result b should be 4 9 0 16 25 49 because 1 is less than 2,,"szha,szha",2017-10-18 02:51:05,2017-11-01 08:35:57
IS,linking error,I am building MxNet from source And it shows me this error usr bin ld cannot find ljvm usr bin ld cannot find lopencv imgcodecs usr bin ld skipping incompatible usr lib libcuda so when searching for lcuda collect2 error ld returned 1 exit status make lib libmxnet so Error 1 make Waiting for unfinished jobs usr bin ld cannot find ljvm usr bin ld cannot find lopencv imgcodecs usr bin ld skipping incompatible usr lib libcuda so when searching for lcuda collect2 error ld returned 1 exit status make bin im2rec Error 1,,szha,2017-05-26 05:15:31,2017-11-01 12:26:26
IS,Low GPU usage on training,Environment info Operating System Windows Package used Python Python Version 3 4 1 MXNet version 0 10 1 CUDA 8 0 61 cudNN 5 1 MXNet version 0 10 1 GPU GTX 970 Running a few of the image classification examples on an own datasets rec file This is my GPU usage while training lenet with a batch size of 60 on 3x32x32 image data lenet 32 dataset has 1000 images one epoche takes 200 seconds training resnet with 50 layers and a batch size of 30 on the same data resnet 32 i try to stay below 3GB of Memory usage because of the bottle neck in the GTX 970 but that seems to make no difference The graphs show clearly that the GPU is used my cpu usage is around 30 40 but it seems that it spends most of the time idle,,szha,2017-07-19 16:05:09,2017-11-01 12:26:27
PR,Add Scala package dev tools for deploy,Description add scala package dev tools for deploy Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage only dev scripts change x For user facing API changes API doc string has been updated only dev scripts change x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x add dev tools for changing artifactId x do not print op definition during compiling by default avoid travis building timeout,,"yzhliu,yzhliu",2017-11-01 06:25:25,2017-11-01 16:23:24
IS,Advanced Indexing Workaround,Hi I'm aware that this issue is still being implemented but I want to ask for a viable workaround if there is any Here is what I want to do and I got an error Is there a way to work around this for the mean time Thank you,,,2017-11-01 15:48:54,2017-11-01 16:40:35
PR,Getting rid of maybe uninitialized warnings,Description Fix for issue 8305 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ptrendx,ptrendx",2017-10-17 17:14:53,2017-11-01 17:25:11
PR,Fix of log10 grad log2 grad,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"mseeger,mseeger,piiswrong,piiswrong,ptrendx,mseeger",2017-11-01 10:20:54,2017-11-01 17:26:41
PR,add default hash to ndarray,Description Define hash for NDArray for consistent py2 py3 behavior The included hash function is the default in python2 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Add hash to NDArray,,"szha,piiswrong,szha,larroy,szha,szha,larroy",2017-10-30 23:33:47,2017-11-01 17:32:25
PR,Dev temp,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,,2017-11-01 20:58:00,2017-11-01 20:58:13
IS,Validation Accuracy does not grow and stays in constant running alexnet on ImageNet1k,I set exact same initial settings as alexnet paper suggested and ran alexnet on 1 machine in CPU only mode kvstore 'none' The dataset is ImageNet1k size 24G But the val accuracy stays constant during initial 20 Epochs Does anyone have any suggestion why the val accuracy does not grow for Alexnet image,,szha,2017-08-02 16:37:55,2017-11-02 00:26:28
IS,Performance of Adam different in Mxnet 0 9 3 and 0 10 0,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Operating System ubuntu 14 04 Package used Python 2 7 6 MXNet version 0 9 3 and 0 10 0 I get different results when I run the same piece of code in mxnet 0 9 3 and 0 10 versions The network converges in the former case where as in the latter it does not I assume this is due to a issue with the Adam optimizer Sample output on mxnet 0 10 0 machine Epoch 0 iteration 0 Training 'mse' 0 0035316271241754293 Epoch 0 iteration 200 Training 'mse' 0 0020998953378282082 Epoch 0 iteration 400 Training 'mse' 0 0020730258888818159 Epoch 0 iteration 600 Training 'mse' 0 0020803247674131228 Epoch 0 iteration 800 Training 'mse' 0 0020828090243904238 Epoch 0 iteration 1000 Training 'mse' 0 0020948627337508208 Epoch 0 iteration 1200 Training 'mse' 0 0020971355235109494 Epoch 0 iteration 1400 Training 'mse' 0 0021050738676455798 Epoch 0 iteration 1600 Training 'mse' 0 002108772054051679 Epoch 0 iteration 1800 Training 'mse' 0 0021108072839298806 Epoch 0 iteration 2000 Training 'mse' 0 0021140645785808288 Epoch 0 iteration 2200 Training 'mse' 0 0021199630068802107 Epoch 0 iteration 2400 Training 'mse' 0 0021227942717597912 Sample output on mxnet 0 9 3 machine Epoch 0 iteration 0 Training 'mse' 0 0035316271241754293 Epoch 0 iteration 200 Training 'mse' 0 0020357351889138792 Epoch 0 iteration 400 Training 'mse' 0 0019829668106971089 Epoch 0 iteration 600 Training 'mse' 0 0019657226051196159 Epoch 0 iteration 800 Training 'mse' 0 0019437202295176823 Epoch 0 iteration 1000 Training 'mse' 0 0019294929921628906 Epoch 0 iteration 1200 Training 'mse' 0 0019097013292767473 Epoch 0 iteration 1400 Training 'mse' 0 0018983960284955858 Epoch 0 iteration 1600 Training 'mse' 0 0018860451705657239 Epoch 0 iteration 1800 Training 'mse' 0 0018750134976254957 Epoch 0 iteration 2000 Training 'mse' 0 0018656182529947914 Epoch 0 iteration 2200 Training 'mse' 0 0018591857366913381 Epoch 0 iteration 2400 Training 'mse' 0 0018510462387049122 I'm assuming the inconsistency is with the optimizer I use Adam as both networks report identical losses at the initial iteration When I use SGD I get almost identical outputs Output obtained with SGD is as follows mxnet 0 9 3 machine Epoch 0 iteration 0 Training 'mse' 0 0035316271241754293 Epoch 0 iteration 200 Training 'mse' 0 0025281838178449305 Epoch 0 iteration 400 Training 'mse' 0 0024361704746748963 Epoch 0 iteration 600 Training 'mse' 0 0024052463359929338 Epoch 0 iteration 800 Training 'mse' 0 0023750611564921174 Epoch 0 iteration 1000 Training 'mse' 0 0023559516919851231 Epoch 0 iteration 1200 Training 'mse' 0 0023335584792751295 Epoch 0 iteration 1400 Training 'mse' 0 0023223736288017997 Epoch 0 iteration 1600 Training 'mse' 0 0023103310963776495 mxnet 0 10 0 machine Epoch 0 iteration 0 Training 'mse' 0 0035316271241754293 Epoch 0 iteration 200 Training 'mse' 0 0025281838647585675 Epoch 0 iteration 400 Training 'mse' 0 0024361706192505329 Epoch 0 iteration 600 Training 'mse' 0 0024052464653863364 Epoch 0 iteration 800 Training 'mse' 0 0023750612553216041 Epoch 0 iteration 1000 Training 'mse' 0 0023559517588570612 Epoch 0 iteration 1200 Training 'mse' 0 0023335585225067977 Epoch 0 iteration 1400 Training 'mse' 0 002322373659214368 Epoch 0 iteration 1600 Training 'mse' 0 002310331112374758 The two lines of codes used for SGD and Adam are as follows opt mx optimizer SGD momentum 0 9 learning rate lr wd 0 0001 lr scheduler lr sch rescale grad 1 0 batchsize opt mx optimizer Adam learning rate lr wd 0 0001 lr scheduler lr sch rescale grad 1 0 batchsize Some of the symbols I hope to use in my code is not available in mxnet 0 93 Therefore I'm looking at getting my code running on mxnet 0 10 At the same time I need to use Adam for optimization What changes should I make in my code to get the same performance in 0 10 as in 0 93,,"piiswrong,szha",2017-08-01 22:50:14,2017-11-02 00:26:29
IS,difficulty replicating layers with non visible outputs e g Dropout with python CustomOp,I'm trying to replicate existing layers in python the goal is to modify layers initially in python and am having particular difficulty with layers that hold onto information from training such as the mask in Dropout Is this possible with CustomOp For example in mxnet src operator dropout inl h two outputs are created with the second output being the mask used for dropping out connections This second output is then used in backprop so the right weights are used Naively a python layer for Dropout would assign a mask of indices to the second output like so self assign out data 1 req 0 mx nd array indices The trouble with CustomOp is it does not take into account that we only want one output to go on to the next layer so the bind will fail if I add a second output naively in my CustomOp In C C mxnet this is handled by the Dropout class implementing a member function for NumVisibleOutputs to return 1 and NumOutputs to return 2 What is the right way to do this in the python CustomOp or is the only way at present to use an auxiliary output to hold it class Dropout mx operator CustomOp def init self keep self keep float keep self drop 1 self keep self scale 1 0 self keep self is train True super Dropout self init def forward self is train req in data out data aux x in data 0 asnumpy if we are in training stage drop out randomly otherwise do not drop anything if is train self keep idx np random rand x shape self keep self is train True x keep idx 0 self assign out data 0 req 0 mx nd array x self scale self assign out data 1 req 0 mx nd array keep idx else self is train False self assign out data 0 req 0 mx nd array x def backward self req out grad in data out data in grad aux y out grad 0 asnumpy keep idx out data 1 asnumpy if self is train True y keep idx 0 self assign in grad 0 req 0 mx nd array y self scale else self assign in grad 0 req 0 mx nd array y mx operator register replicated dropout class DropoutProp mx operator CustomOpProp def init self keep self keep keep super DropoutProp self init need top grad True def list arguments self return wouldata' def list outputs self return 'output' return wouldata' 'mask' def infer shape self in shape data shape in shape 0 output shape in shape 0 aux shape in shape 0 return data shape output shape output shape def infer type self in type dtype in type 0 return dtype dtype dtype def create operator self ctx shapes dtypes print 'creating Dropout operator' return Dropout self keep net stock mx sym Dropout data net name istock drop' p 0 5 net custom mx symbol Custom data net name 'custom drop' op type areplicated dropout' keep 0 5,,"piiswrong,szha",2017-07-17 18:50:45,2017-11-02 00:26:29
IS,RAM consumption MobileNets,I'm trying to re train mobilenet network this one I have a strange and huge RAM consumption that increase over time and when there is no ram available the process is killed I trained this model with about 300k images 299 x 299 and do not got issues on a standard inception bn It seem the deepwise convolution layer made with argument num group in the convolution layer have memory leak,,"qingzhouzhen,qingzhouzhen,szha",2017-07-24 07:33:43,2017-11-02 00:26:30
IS,ld library not found for llibopencv stitching 3 3 1 dylib,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description Build from source on macOS failed Environment info Required macOS 10 13 opencv 3 3 1 The full error information is I have checked the opencv library in the link path it is there,,"szha,szha,szha,szha,szha",2017-11-01 05:11:34,2017-11-02 03:53:41
IS,upload result in segnet,label2 res2,,solin319,2017-11-02 06:28:19,2017-11-02 06:28:23
IS,feature req question slurm scheduler support for launch py distrubted processing multi machines support,from reading the docs it seems like it would be natural to add have support for the SLURM cluster job scheduler in launch py and i guess then also in dmlc core dmlc tracker but before i start working on it does that make sense is it already done and or somehow not needed is there interest in a PR to add such support if i write it should i post a similar issue in dmlc core either in addition to or instead of here any other advice notes thanks mwm,,"piiswrong,szha",2017-06-02 21:52:27,2017-11-02 12:26:27
PR,bump up version to 0 12 1,Description Bump up version to 0 12 1 Checklist Essentials x Changes are complete i e I finished coding on this PR Changes x bump up version in include base h x bump up version in python libinfo py x bump up version in R description x bump up version in scala pom files,,szha,2017-10-31 18:17:36,2017-11-02 17:24:44
PR,fix makenonlossgrad bug,zhreshold,,"piiswrong,zhreshold",2017-11-01 19:10:49,2017-11-02 17:29:13
PR,fix expand dims if axis 0,Description fix expand dims if axis 0 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Comments 2 3 expand dims axis 1 should be 2 3 1 to be consistent with numpy Currently it propagate 2 1 3,,"zhreshold,piiswrong,piiswrong",2017-10-31 18:56:36,2017-11-02 17:29:58
PR,Correct Initialization Description in Finetune Tutorial,Description The current description of this step references calls to init params and set params but the code does not reflect this Checklist Essentials x Changes are complete i e I finished coding on this PR x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Update Finetune Tutorial,,,2017-11-02 16:07:40,2017-11-02 23:44:13
IS,there are no setupenv cmd in Windows Pre built Packages,I follow the steps to install Pre built Packages on window in installing pre built packages on windows and find there are no setupenv cmd in Windows Pre built Packages in How to install it,,"yajiedesign,szha",2016-10-28 15:42:48,2017-11-03 00:26:27
IS,Implement Dense Sparse Dense DSD Training or freezing lots of weights,Consider the DSD paper Basically the steps are 1 Train 2 Find weights close to 0 Set them to 0 and set their learning rate to 0 3 Train Unfreeze all weights Train again In theory this is possible by using set lr mult in optimizer api yet it might be really cumbersome and slow there will be a very long list of freezed weights and I suppose the optimizer will go through the whole list for each parameter Or does mxnet store the learning rate of each weight so there will be no need to search In that case can we go through all weights and change the learning rate of each weight directly instead of feeding a very long list to the optimizer,,"piiswrong,Guneet-Dhillon,szha",2017-02-02 06:40:00,2017-11-03 00:26:28
PR,wrong definition of cross entropy in make loss,Cross entropy is a cost function and lower cost is better cost so the expression should add a minus and mean w r t batch size,,"chinakook,szha,chinakook,piiswrong,szha",2017-07-20 06:54:09,2017-11-03 00:26:28
PR,WIP Proof of concept multi processing,This is a proof of concept multi processing IO refactor only works for linux mac It can be tested with This also fixes the problem that mxnet does not work with fork,,"piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,ArtanisCV,piiswrong,ArtanisCV,piiswrong",2017-10-24 19:37:23,2017-11-03 04:28:17
PR,gluon improvement,changes include 1 use generator expr for arguments when calling cached op 2 merge two separate for loops for assembling parameters into one so that the check for parameter membership in cargs is cached 3 fill shapes after parameter data is set to be reflected in block is repr,,"szha,piiswrong,szha,szha,piiswrong,piiswrong,piiswrong,mli,szha,piiswrong,szha,szha,cjolivier01,szha",2017-10-05 03:45:08,2017-11-03 04:33:54
IS,some questions about sparse lr examples,I just study sparse lr examples and have some quesitons why weight need to be define shape features 2 what is the meaning of shape 0 shape 1 in weight if i just like to use LogisticRegressionOutput as loss x mx symbol Variable data stype 'csr' norm init mx initializer Normal sigma 0 01 weight with row sparse storage type to enable sparse gradient updates weight mx symbol Variable weight shape num features 1 init norm init stype 'row sparse' bias mx symbol Variable bias shape 1 dot mx symbol sparse dot x weight pred mx symbol broadcast add dot bias y mx symbol Variable softmax label model mx sym Custom pred y op type 'weighted softmax ce loss' positive cls weight positive cls weight name 'out' return mx sym MakeLoss model net mx symbol LogisticRegressionOutput data pred name softmax return net it will report self update label pred File search data mxnet new example sparse python mxnet metric py line 987 in update prob pred numpy arange num examples dtype numpy int64 numpy int64 label IndexError index 1 is out of bounds for axis 1 with size 1 in metri how can i fix it haibin lin,,"anirudh2290,eric-haibin-lin",2017-10-27 03:05:03,2017-11-03 07:08:49
IS,Does anyone know PVAnet How to configure its CRelu structure using mxnet,The pvanet is proposed in It use caffe to train and infer Like title I want to know how to implement the CRelu structure The structure is that Conv bn concat bn bn scale Relu How to configure it Should I use fix gamma parameter,,"qingzhouzhen,szha",2017-07-17 14:14:23,2017-11-03 12:26:26
PR,gluon fill parameter shape,Description Fill parameter shape when parameter data is changed Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Update parameter is shape attribute when set data is called x In block is repr use parameter is shape instead of local attribute,,szha,2017-11-03 04:52:29,2017-11-03 17:34:57
PR,Fix typo in gluon l1loss docstring,Description Fix docstring typo Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Fix docstring typo Comments NA,,"astonzhang,zhreshold,astonzhang",2017-10-17 22:07:41,2017-11-03 17:35:33
PR,Fix a Docstring,Description I fixed a Docstring about the Core ML converter Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Docstring,,kkk669,2017-11-01 09:07:00,2017-11-03 17:35:56
PR,Make gluon Block cooperative in multiple inheritance setting,Pass further init kwargs to super i e make Block cooperative in a multiple inheritance setting This PR enables a setting where a class inherits both from Gluon and another class such as traitlets This was not possible before as gluon Block would not pass on relevant kwargs to the init function higher up in MRO Method Resolution Order,,"leezu,piiswrong,leezu,piiswrong,leezu,piiswrong",2017-10-09 01:27:39,2017-11-03 17:36:45
PR,Tweak a markdown list in note engine md,This is a minor change to the markdown to make the two step list into a numbered list,,bowman,2017-11-03 15:39:24,2017-11-03 21:57:25
IS,Imperative Programs Tend to be More Flexible but do not make much sense,In there is a python code that does not run range d and adding zeros to zeros does not make much sense either I do not really get what is the intended message ts,,szha,2017-08-04 12:46:29,2017-11-04 00:26:26
IS,Error in infer shape for deconvolution when the input is 5D batch num filter z y x,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Mac Compiler Package used Python R Scala Julia python MXNet version 0 10 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python 2 7 10 Error Message 1L 1L 64L 64L 64L infer shape error Arguments data 1 1 64 64 64 MXNetError Traceback most recent call last ipython input 16 bb4f7eff4e4d in module 7 net mx sym Deconvolution source kernel 3 3 3 pad 0 0 0 stride 2 2 2 num filter 32 ctx mx gpu 0 8 9 net infer shape data 1 1 64 64 64 Users mas PycharmProjects NCF mxnet python mxnet symbol pyc in infer shape self args kwargs 457 458 try 459 return self infer shape impl False args kwargs 460 except MXNetError 461 print infer shape error Arguments Users mas PycharmProjects NCF mxnet python mxnet symbol pyc in infer shape impl self partial args kwargs 524 ctypes byref aux shape ndim 525 ctypes byref aux shape data 526 ctypes byref complete 527 if complete value 0 528 arg shapes Users mas PycharmProjects NCF mxnet python mxnet base pyc in check call ret 75 76 if ret 0 77 raise MXNetError py str LIB MXGetLastError 78 79 if sys version info 0 3 MXNetError InferShape Error in deconvolution2 11 11 00 src operator deconvolution inl h 357 Check failed dshape ndim 4 Input data should be 4D in batch num filter y x Minimum reproducible example import mxnet as mx source mx sym Variable data a b c source infer shape data 1 1 64 64 64 print b net mx sym Deconvolution source kernel 3 3 3 pad 0 0 0 stride 2 2 2 num filter 32 ctx mx gpu 0 net infer shape data 1 1 64 64 64 What have you tried to solve it 1 Upgraded mxnet to latest release 2 3,,szha,2017-05-27 07:17:38,2017-11-04 00:26:27
IS,Gradients are wrong for is train False,I am trying to synthesise adversarial examples for which I need to compute gradients during test time However for the testing phase the gradients seem to be wrong Here is a short example Setting is train False yields gradients that are either NaN or very large with entries on the order of 1e18 Setting is train True yields the correct but stochastic gradients with entries in the order of 1e 13 I am not yet very experienced with MXNet Am I making a mistake,,"piiswrong,szha",2017-07-25 16:59:45,2017-11-04 00:26:27
IS,resnet,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 16 04 Compiler gcc 5 4 Package used Python R Scala Julia MXNet version 0 10 1 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Is there are resnet base pretrained model such as vgg reduced,,"thirdwing,szha",2017-08-02 13:37:50,2017-11-04 00:26:28
PR,Move OpenMP calculations into its own class,Description Splitting out some things from the 'tuner' branch OMP tuning in order to make the tuner PR more palatable when it posts smaller simpler Since OMP behavior has little to do with the engine classes themselves and the OMP logic will get more complex move that code into its own class Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,szha,szha,cjolivier01,srochel,hyandell",2017-10-30 19:08:51,2017-11-04 03:28:42
PR,Fix for issue 8491 elemwise mul nan behavior,Description Fix for Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,cjolivier01",2017-11-01 00:08:49,2017-11-04 03:29:07
PR,fixed practioners to practitioners,Description Brief description on what this PR is about Fixed practioners to practitioners in the main landing page,,"thinksanky,sandeep-krishnamurthy",2017-11-03 17:36:58,2017-11-04 05:17:39
IS,forward time fluctuate and physical memory gradually increases,,,,2017-11-04 10:04:08,2017-11-04 10:49:00
IS,Gluon F pad does not accept symbol input even in HybridBlock,Hi I am trying to dump a json string from this very nice style transfer model written in gluon API I modified the net definition so that all blocks inherent from HybridBlock and use HybridSequential The modified one is here I think this is enough to dump a json from gluon model But when I try this The error is occurring at here file net modified py L60 inside HybridBlock Am I missing something or is F pad indeed supposed to only accept NDArray inputs Thanks,,,2017-11-04 01:10:43,2017-11-04 15:02:30
PR,rename output layer of resnet,Description This is a missed change from 8446 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes reorganize and rename output layer of resnetv2,,szha,2017-11-03 21:47:24,2017-11-05 00:08:45
PR,Add asscipy feature and coo constructor,Description This PR contains changes to asscipy to convert from CSRNDArray to scipy csr matrix This also contains support for COO format for the csr matrix construction Performance wise it is 20X slower on construction when compared to CSR format which is expected because of multiple scipy calls to create a scipy coo matrix convert it to a scipy csr matrix and then convert it back to CSRNDArray haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x asscipy tests x coo format tests Comments Completes two TODOs in 8168,,"anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin,anirudh2290",2017-10-20 20:33:38,2017-11-05 00:10:11
PR,fix custom op error when using auxiliary states,Fixes,,piiswrong,2017-10-17 19:57:05,2017-11-05 00:12:36
IS,How to visualize the mxnet network written in json format,How to visualize the mxnet network written in json format Environment info Operating System Ubuntu 16 0 4 1 Compiler Package used Python R Scala Julia Python MXNet version 0 9 x Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution 2 7 13 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,"chinakook,szha",2017-08-01 10:58:17,2017-11-05 00:26:27
IS,I think mxnet is lack of a execution order iterator of a symbol,I think mxnet is lack of a execution order iterator of a symbol Usually If a symbol is binded the execution graph is determined I want to iterate traverse the execution graph to get all node attributes by myself to calculate something useful such as receptive field RF size and RF offset of each layer but I can not do it now In addtional given a symbol how can I get the 'op' type of that symbol,,"chinakook,chinakook,szha",2017-07-28 03:08:50,2017-11-05 00:26:27
PR,cpp package fix cpp example,Description cpp package fix for issue 7725 sync mnist data path avoiding duplication Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc,,Adesun,2017-11-05 12:43:00,2017-11-05 15:22:07
PR,Update NEWS md for 0 12 1,Description Updated NEWS md Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Updated NEWS md as requested Comments This PR must be included in the patch release 0 12 1,,mbaijal,2017-11-05 01:57:00,2017-11-05 15:35:35
PR,Fix pip test libgfortran3 installation needs flag ' y',Description This is a fix to a previous PR by me where I added a step to pip test to instal libgfortran3 Though this ran fine for a while it now started failing because it needed a runtime 'yes' This PR adds the flag y Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Added flag y to libgfortran3 installation,,mbaijal,2017-11-03 20:48:34,2017-11-05 17:57:22
IS,The intellisence support for mxnet with Python,Operating System Linux Compiler Vscode Pycharm Package used Python R Scala Julia Python 2 7 13 MXNet version 0 10 1 After installing mxnet I found I could get intellisense when typing mx io but no intellisense when typing mx sym say looking for flatten I wonder whether there is some setting I missed,,szha,2017-08-06 02:55:43,2017-11-06 00:26:26
IS,Where is the test nn py file,Hi guys The test nn is needed in the following file L14 L14 but I can not find it could you please give a hand Thanks,,szha,2017-08-06 17:05:42,2017-11-06 00:26:27
IS,2 networks which shared same weight on some layers,hello I have two datasets different with differents kind of label different So I have two different networks but I want to shared some layers the first convolutional between them during the training Exemple How can I shared the weight with two networks and not only one network because I have two different datasets,,"Ldpe2G,Ldpe2G,zihaolucky,zihaolucky,zihaolucky,zihaolucky,eric-haibin-lin,szha",2017-07-11 13:07:16,2017-11-06 00:26:28
IS,How to switch a layer to cpu mode while the other layers are in gpu mode,Hi I found the layer I implemented in cpu mode is working fine but got segmentation fault in gpu mode I guess it is just because there is some difference between gpu memory and cpu memory in terms of operations Is there any way that I only use cpu mode for this single layer and keep the other layers in gpu mode Thank you very much,,"eric-haibin-lin,szha",2017-07-14 02:40:45,2017-11-06 00:26:29
IS,where PS lite implementation of server manager hash ring,while reading the ps lite model I can just find how server work scheduler created but cannot find 1 server manager which handles the hash ring management I just found the likely code in void Van Receiving fun int id node role Node SERVER Postoffice ServerRankToID num servers Postoffice WorkerRankToID num workers are these the so called direct mapped DHT design in the Mu Li paper OSDI 2014 so where is hash ring and server manager so not implemented yet or in some models I just not found 2 since no hash ring found virtual servers which can improve LB and recovery are also not found so where consistent Hashing in paper delcared in mxnet since it is so important in distributed Machine Learning anyone help is appreciated in andance,,"formath,formath,szha",2017-08-03 01:30:12,2017-11-06 00:26:29
IS,Typographic Error on Landing Page,Description The word practioner is used in the MXNet landing page Practitioner is probably the word required Think tank analysts suggest that this type of error is left on cover pages to imply a deep compromise in the software Environment info Required Unknown Steps to reproduce curl What have you tried to solve it Created Issue in Git Repository,,"sandeep-krishnamurthy,sandeep-krishnamurthy",2017-11-02 19:00:01,2017-11-06 03:05:36
IS,Convolution dilate run not as expected in cpu context,Description I run the same Convolution with dilate on cpu and gpu but the result is different Environment info Required Ubuntu 14 04 Python 2 7 mxnet version 0 9 Test Code output code in cpu 110 112 114 116 58 60 2 4 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66 69 72 75 78 81 84 87 90 93 96 99 102 105 108 111 114 117 120 123 126 129 132 135 138 141 144 147 150 153 156 159 code in gpu 4 6 8 10 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66 69 72 75 78 81 84 87 90 93 96 99 102 105 108 111 114 117 120 123 126 129 132 135 138 141 144 147 150 153 156 159 162 165 108 110 112 114,,solin319,2017-11-06 02:54:12,2017-11-06 03:13:45
IS,How to use Prelu in gluon,I see a layer called Prelu in caffe and pytorch is it include in mxnet especially in gluon interface,,bradcar,2017-11-06 07:39:41,2017-11-06 07:53:07
IS,Autograd bug in mxnet cu80 0 12,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description Autograd behaves not properly Environment info Required OS Ubuntu 16 04 Package used Python R Scala Julia Python I'm using Error Message Paste the complete error message including stack trace Minimum reproducible example If you are using your own code please provide a short script that reproduces the error Otherwise please provide link to the existing example Steps to reproduce Paste the commands you ran that produced the error 1 Run the above snippet 2 What have you tried to solve it 1 Assign b a 1 2,,"szha,szha,szha,zhanghang1989,zhanghang1989",2017-11-01 04:57:46,2017-11-06 10:02:09
IS,Multi Training Task on the same GPU card,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos 7 2 Compiler gcc 4 8 5 Package used Python R Scala Julia Python MXNet version 0 9 3 Or if installed from source installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide python 2 7 Python version and distribution python 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace When I run a single process on a single gpu card I got about 56 samples sec When I run two single process on a single gpu card I got about 27 samples sec I think it is mainly the band width in IO for GPU which cause the low trainning speed How can I run multi trainning task on the sample single GPU card with a high training performence And My GPU is version is k40 12G Thank you very much Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-08-06 01:44:57,2017-11-06 12:26:27
PR,cpp package fix cpp issue,Description cpp package fix for issue 7725 remove some unused include op h is already included in MxNetCpp h use same mnist data path avoiding duplication Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Feature1 tests and when applicable API doc x Feature2 tests and when applicable API doc,,Adesun,2017-11-05 16:10:58,2017-11-06 18:13:08
IS,About distribute trainning How can I set different gpu device id on different worker,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Compiler Package used Python R Scala Julia MXNet version Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-08-07 16:27:45,2017-11-07 00:26:27
IS,Error when trying to build docker image with GPU and S3 support,Hi all I tried to customize the official MXNet docker image to get the Amazon S3 and GPU support The dockerfile I use to create such image is as follows FYI It seems that it is because I set the S3 flag on If I did not set the S3 flag and using the exactly the same dockerfile to build the image everything goes well I have also tried cloning the master branch of MXNet but got the same error message The dockerfile file is based on Thanks in advance Bo,,"piiswrong,szha",2017-08-07 19:06:02,2017-11-07 00:26:28
PR,Prep for 0 12 1 Version Updates,Description MERGE INTO RELEASE BRANCH v0 12 0 ONLY 1 Version Updates 2 Added 0 12 1 to the Whats New in README md This tag does not exist yet 3 NEWS md is not updated yet Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"mbaijal,mbaijal,cjolivier01",2017-11-07 01:01:30,2017-11-07 03:04:05
IS,How to adding an axes in ndarray,I want to add an axes in a ndarray for example 3 4 5 5 to 1 3 4 5 5 I do not know the ndarray shape in advance I refer to the reshape api but can not understand the 4 means,,solin319,2017-11-07 03:16:48,2017-11-07 04:00:33
PR,Fix unused var warning in release build only used in assert on debug,builds Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-11-06 19:23:51,2017-11-07 05:21:20
PR,Engine reserves cores from OMP Set some defaults for dynamic and recursion,piiswrong Description Engine reserves cores from OMP Set some defaults for dynamic and recursion unless environment variables are set Should be last major PR before tuning PR No performance degradation was seen when enabling OMP with one or two cores reserved depending on the number of available cores Generally OMP operations are not occurring in parallel to GPU operations Values can be adjusted if use cases can be found Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,piiswrong,piiswrong,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,cjolivier01",2017-11-05 21:41:30,2017-11-07 05:43:39
IS,How to build a library without the BLAS dependency,As I saw from mxnet document Deep Learning in a Single File for Smart Devices there is a word saying MXNet provides an amalgamation script that compiles all code needed for prediction based on trained DL models into a single cc file containing approximately 30K lines of code This code only depends on the BLAS library Moreover we ve also created an even more minimal version with the BLAS dependency removed But I did not find any description about how to build the minimal version that without BLAS dependency Could anyone tell me anything about this Any help is appreciated thanks,,szha,2017-08-08 07:55:52,2017-11-07 12:26:27
IS,include dmlc logging h 235 00 59 21 src io local filesys cc 154 Check failed allow null LocalFileSystem fail to open model det1 symbol json,hello Now I use mxnet and face detection model 'mtcnn' to inference an image but it shows error include dmlc logging h 235 00 38 08 src io local filesys cc 154 Check failed allow null LocalFileSystem fail to open model det1 symbol json I doubt that whether it is caused by mxnet or not Can you give me some advices how to solve this problem,,"ysh329,ysh329,szha",2017-03-03 09:06:45,2017-11-07 12:26:27
IS,c predict api doesnot support multi predictor,It seems that c predict api doesnot support create multi predictor on different devices gpus in one program and it will raise invalid device function error,,szha,2017-08-08 12:15:18,2017-11-07 12:26:28
IS,Ubuntu Compilation issues with gcc 5 and CUDA Ubuntu 17 04,Description SSE AVX instructions causing build issues in ubuntu 17 04 with CUDA build CC gcc 5 CXX g 5 CCBIN g 5 cmake DUSE CUDA ON DUSE LAPACK OFF GNinja usr lib gcc x86 64 linux gnu 5 include avx512vlintrin h 10713 error argument of type void is incompatible with parameter of type long long usr lib gcc x86 64 linux gnu 5 include avx512vlintrin h 10724 error argument of type void is incompatible with parameter of type long long 92 errors detected in the compilation of tmp tmpxft 00000ab4 00000000 17 ndarray function compute 61 cpp1 ii Looks as the same problem as in,,"larroy,larroy",2017-11-07 14:36:41,2017-11-07 15:09:03
IS,CoreML Converter Installer for python 3,The current mxnet to coreml installer only supports python 2 7 We need to provide an installer for python 3 too,,"pracheer,kkk669,pracheer",2017-10-28 02:40:54,2017-11-07 19:05:55
PR,csr ndarray iter with shuffle,Description support NDArrayIter for CSRNDArray with shuffle True As a feature requested in cc haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x unittest Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,ZiyueHuang",2017-11-05 15:11:27,2017-11-07 19:30:02
PR,div scalar for sparse,Description div scalar for sparse As a feature requested in cc haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x add unittest Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,ZiyueHuang",2017-11-04 09:16:29,2017-11-07 19:35:30
PR,cpu sparse embedding op,Description The SparseEmbedding op takes indices and rowsparse weight as input and produces dense result in the forward pass In backward pass it outputs rowsparse gradient for the weight which is useful for sparse gradient update 8x faster on a c4 8xlarge machine 36 cores compared to dense embedding With sparse embedding OMP NUM THREADS 32 python matrix factorization py print every 1000 Note SparseEmbedding checks if any input is out of bound and throws an exception if found one Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-10-28 06:39:21,2017-11-07 21:52:16
PR,Automatic OMP operator tuning based upon kernel operation workload,piiswrong Description Automatic OMP operator tuning based upon kernel operation workload Determines weight of a unary or binary kernel op and then uses this to determine if OMP should be used given of iterations required and threads to perform the job Correct decision accuracy is tested in gtest OMP TUNING test suite by comparing with OMP without OMP and Auto times For example AWS c4 8xlarge Success rate for type float 0 90278 Success rate for type double 0 88889 Success rate for type mshadow half half t 0 83333 Success rate for type unsigned char 0 86111 Success rate for type int 0 95833 Success rate for type long 0 88889 desktop 12 core 6 real CPU cores hyperthreading Success rate for type float 0 79167 Success rate for type double 0 75000 Success rate for type unsigned char 0 72222 Success rate for type int 0 94444 Success rate for type long 1 00000 A sample output from OMP TUNING tests including staticstical data tune all txt Currently autotuned kernel operators tuning at startup takes a total of 10ms mxnet op PopulateFullIdxRspKernel mxnet op mxnet op set to int 0 mxnet op mshadow op smooth l1 gradient mxnet op mshadow op smooth l1 loss mxnet op mshadow op eq mxnet op mshadow op ne mxnet op mshadow op le mxnet op mshadow op lt mxnet op mshadow op hypot grad right mxnet op mshadow op hypot grad left mxnet op mshadow op hypot mxnet op mshadow op arctanh grad mxnet op mshadow op arctan grad mxnet op mshadow op cosh mxnet op mshadow op rpower mxnet op mshadow op minimum mxnet op mshadow op arctan mxnet op mshadow op reciprocal square root mxnet op mshadow op rminus mxnet op mshadow op arccosh grad mxnet op mshadow op square root grad mxnet op mshadow op arctanh mxnet op mshadow op floor mxnet op mshadow op cosh grad mxnet op mshadow op ceil mxnet op mshadow op cos grad mxnet op mshadow op reciprocal cube root grad mxnet op mshadow op arcsinh grad mxnet op mshadow op sin mxnet op mshadow op arcsin mxnet op mshadow op log10 grad mxnet op mshadow op log1p grad mxnet op mshadow op mod grad mxnet op mshadow op arccos grad mxnet op mshadow op exp mxnet op mshadow op tanh grad mxnet op mshadow op log1p mxnet op mshadow op rint mshadow op minus mxnet op mshadow op relu grad mxnet op mshadow op identity mxnet op mshadow op maximum mxnet op mshadow op reciprocal grad mshadow op div mxnet op mshadow op rmod grad mxnet op mshadow op arcsin grad mxnet op mshadow op ge mxnet op mshadow op gammaln grad mxnet op mshadow op sigmoid mxnet op mshadow op power rgrad mxnet op mshadow op identity grad mxnet op mshadow op tan mxnet op mshadow op gamma mxnet op mshadow op arcsinh mshadow op identity mxnet op mshadow op square root mxnet op mshadow op reciprocal square root grad mxnet op mshadow op cos mxnet op mshadow op log2 mxnet op mshadow op tanh mxnet op mshadow op arccosh mxnet op mshadow op negation mxnet op mshadow op log10 mxnet op mshadow op cube root grad mxnet op mshadow op expm1 mxnet op mshadow op arccos mxnet op mshadow op rmod mxnet op mshadow op softrelu grad mxnet op mshadow op sinh mxnet op mshadow op log grad mxnet op mshadow op sin grad mxnet op mshadow op rdiv grad mxnet op mshadow op log mxnet op mshadow op softrelu mxnet op mshadow op square grad mxnet op mshadow op log2 grad mxnet op mshadow op cube root mxnet op mshadow op reciprocal cube root mxnet op mshadow op sign mxnet op mshadow op square mxnet op mshadow op sign grad mxnet op mshadow op round mxnet op mshadow op trunc mxnet op mshadow op mod rgrad mxnet op mshadow op reciprocal mxnet op mshadow op fix mxnet op mshadow op gamma grad mxnet op mshadow op gammaln mxnet op mshadow op degrees mshadow op right mxnet op mshadow op sinh grad mxnet op mshadow op degrees grad mshadow op plus mxnet op mshadow op radians mxnet op mshadow op sigmoid grad mxnet op mshadow op radians grad mxnet op mshadow op gt mxnet op mshadow op mod mshadow op mul mxnet op mshadow op rdiv mxnet op mshadow op tan grad mxnet op mshadow op div grad mxnet op mshadow op div rgrad mxnet op mshadow op left mxnet op mshadow op right mxnet op mshadow op power mxnet op mshadow op power grad mxnet op mshadow op relu mxnet op mshadow op abs mxnet op mshadow op rpower grad Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,cjolivier01,cjolivier01,cjolivier01",2017-11-07 16:57:17,2017-11-07 23:15:28
PR,Operators for mean csr axis 0 and mean csr axis 1,Description Adds operators for mean csr axis 0 and mean csr axis 1 This builds on top of the sum csr axis 0 and sum csr axis 1 This requires the PR 8174 to be merged Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x mx nd mean axis 0 tests and when applicable API doc x mx nd mean axis 1 tests and when applicable API doc Comments PR 8174 being merged is a prerequisite for merge Completes one TOD in 8168 haibin lin,,"anirudh2290,eric-haibin-lin,anirudh2290,cjolivier01,cjolivier01,cjolivier01,cjolivier01,anirudh2290,anirudh2290,anirudh2290,piiswrong,anirudh2290",2017-10-13 21:06:59,2017-11-08 00:08:28
PR,Update mshadow to 2d7780c3f2eefe4453fa419862d1b2089bedb8d5,Description update mshadow Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,larroy,2017-11-02 14:31:10,2017-11-08 00:47:09
PR,sparse slice for csr on two dimensions cpu implementation,Description slice axis for csr cpu implementation This is used in cases like Wide Deep model e g slice the linear features to feed into wide model As a feature request in cc haibin lin for review Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x slice axis for csr cpu implementation add unittest Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,ZiyueHuang,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,anirudh2290,anirudh2290,ZiyueHuang,ZiyueHuang,eric-haibin-lin,ZiyueHuang,ZiyueHuang,anirudh2290,ZiyueHuang,piiswrong,ZiyueHuang",2017-10-18 04:00:50,2017-11-08 01:56:48
IS,API document needs to be updated,Here module mxnet ndarray random the API document says that the mxnet ndarray random is the random distribution generator module but in the current version everything about random generator is in mxnet random,,,2017-11-08 01:54:32,2017-11-08 02:38:28
IS,Request finish python gpu enabled guide for install,Please complete the python GPU enabled install for MacOS This guide is complete following it to complete results in errors Also please update mx gpu to return 1 if no gpu detected Right now I run mx gpu and it returns 0 which means it should be connected to my first gpu a functioning external NVIDIA GeForce TITAN X but trying to run anything with this command e g in the demo mx nd ones 100 100 mx gpu results in errors Environment info Operating System MacOS 10 12 6 Package used Python R Scala Julia Python MXNet version 0 11 MXNet commit hash git rev parse HEAD a5edbf94094581ee27157eae4f2113115a3994e7 Python version and distribution 3 6 Error Message Please paste the full error message including stack trace b mx nd ones 100 100 gpu device 14 16 34 src c api c api ndarray cc 147 GPU support is disabled Compile MXNet with USE CUDA 1 to enable GPU support 14 16 34 Users travis build dmlc mxnet distro mxnet build dmlc core include dmlc logging h 308 14 16 34 src c api c api ndarray cc 416 Operator ones is not implemented for GPU Stack trace returned 5 entries bt 0 0 libmxnet so 0x0000000104be1ad8 ZN4dmlc15LogMessageFatalD2Ev 40 bt 1 1 libmxnet so 0x00000001052e0bbf Z20ImperativeInvokeImplRKN5mxnet7ContextERKN4nnvm9NodeAttrsEPNSt3 16vectorINS 7NDArrayENS7 9allocatorIS9 EEEESD 2079 bt 2 2 libmxnet so 0x00000001052e17f1 MXImperativeInvoke 433 bt 3 3 ctypes cpython 36m darwin so 0x0000000103d3742f ffi call unix64 79 bt 4 4 0x00007fff5c4ba950 0x0 140734741850448 Traceback most recent call last File stdin line 1 in module File usr local lib python3 6 site packages mxnet ndarray py line 1216 in ones return internal ones shape shape ctx ctx dtype dtype kwargs File string line 15 in ones File usr local lib python3 6 site packages mxnet ctypes ndarray py line 89 in imperative invoke c array ctypes c char p c str str val for val in vals File usr local lib python3 6 site packages mxnet base py line 129 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 14 16 34 src c api c api ndarray cc 416 Operator ones is not implemented for GPU Stack trace returned 5 entries bt 0 0 libmxnet so 0x0000000104be1ad8 ZN4dmlc15LogMessageFatalD2Ev 40 bt 1 1 libmxnet so 0x00000001052e0bbf Z20ImperativeInvokeImplRKN5mxnet7ContextERKN4nnvm9NodeAttrsEPNSt3 16vectorINS 7NDArrayENS7 9allocatorIS9 EEEESD 2079 bt 2 2 libmxnet so 0x00000001052e17f1 MXImperativeInvoke 433 bt 3 3 ctypes cpython 36m darwin so 0x0000000103d3742f ffi call unix64 79 bt 4 4 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error gpu device mx gpu b mx nd ones 100 100 gpu device Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 uninstalled and reinstalled using the above linked guide 2 ensured other CUDA powered code still worked with my external GPU 3,,"szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha,szha",2017-09-14 12:16:43,2017-11-08 08:26:38
IS,Error caused by two different dependencies in protobuf version when using both distributed mode and tensorboard,When trying to use distributed mode in MXNet and tensorboard for MXNet because distributed MXNet especially from ps lite needs protobuf 2 5 0 version see but tensorboard requires protobuf 3 2 0 version and it causes below error see Error Message To use both distributed machine and tensorboard for MXNet I think the 1 protobuf version should be the same 2 5 0 to 3 2 0 upgrade protobuf version for ps lite or 2 another work around code should be introduced Tensorboard features are very helpful for developers so I look forward to be able to use it under distributed mode Thanks in advance Environment info Operating System Ubuntu 16 04 Compiler Package used Python R Scala Julia Python MXNet version 0 9 5 Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution Python2 7 If you are using R package please provide R sessionInfo Error Message libprotobuf FATAL google protobuf stubs common cc 61 This program requires ver sion 3 2 0 of the Protocol Buffer runtime library but the installed version is 2 5 0 Please update your library If you compiled the program yourself make sure that your headers are from the same version of Protocol Buffers as your lin k time library Version verification failed in google protobuf descriptor pb cc terminate called after throwing an instance of 'google protobuf FatalException ' what This program requires version 3 2 0 of the Protocol Buffer runtime li brary but the installed version is 2 5 0 Please update your library If you compiled the program yourself make sure that your headers are from the same ver sion of Protocol Buffers as your link time library Version verification faile d in google protobuf descriptor pb cc Aborted core dumped Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 Set USE DIST KVSTORE 1 in config mk to use distributed machine 2 Install tensorboard for mxnet from 3 test some tensorboard related code with mxnet What have you tried to solve it 1 2 3,,"Soonhwan-Kwon,zihaolucky,szha",2017-04-14 06:40:18,2017-11-08 12:26:26
IS,bgr or rgb input to vgg16,Hi Everyone Should the input image to vgg16 model in mxnet be bgr or rgb The original caffe model supports bgr I am not sure what is the case for mxnet Thanks a lot,,szha,2017-07-11 15:05:17,2017-11-08 12:26:27
PR,add center loss,Implement Center Loss in Gluon tested individually,,"piiswrong,piiswrong,piiswrong,piiswrong,szha,szha",2017-10-09 19:14:42,2017-11-08 17:05:15
PR,Passing user specified environment variables when using launch py,Description Passes additional user specific environment variables when using launch py This is very helpful in passing variables like PYTHONPATH MXNET ENGINE TYPE I think this makes launching jobs for distributed training easier If you have suggestions for a different name please let me know The current usage is Requires this PR in dmlc core to be merged Provides a way to solve this issue apache incubator mxnet 7857 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Add additional arg in launch py x Make dmlc tracker submit function to accept this variable Comments I have only tested this with local and ssh Not tested this with yarn mpi sge since I do not have those environments set up But I think this should work even in those environments Or should I remove them and only support local and ssh,,rahul003,2017-11-01 18:54:42,2017-11-08 18:53:22
IS,the error mshadow cuda AddTakeGrad' ambiguous call to overloaded function indexing op h,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description Brief description of the problem in no more than 2 sentences Environment info Required Package used Python R Scala Julia I'm using For Scala user please provide 1 Java version java version 2 Maven version mvn version 3 Scala runtime if applicable scala version For R user please provide R sessionInfo Build info Required if built from source Compiler gcc clang mingw visual studio MXNet commit hash Paste the output of git rev parse HEAD here Build config Paste the content of config mk or the build command Error Message Paste the complete error message including stack trace Minimum reproducible example If you are using your own code please provide a short script that reproduces the error Otherwise please provide link to the existing example Steps to reproduce Paste the commands you ran that produced the error 1 2 What have you tried to solve it 1 2,,eric-haibin-lin,2017-11-08 06:14:03,2017-11-08 19:39:01
PR,slice operator supporting arbitrary values of step,Description Re implemented slice operator using Kernel Launch Added support for arbitrary values of step along with begin and end i e slice data begin end step where step is an optional parameter Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Comments The kernel implementation is based upon the parallelization approach of slice operator in mshadow with additional support for arbitrary value of step This operator will be used in to support slicing NDArray with step 1 Currently it uses gather nd op to realize that functionality which has heavy overhead of making index NDArray from slices Mini benchmark slice v1 mshadow version and slice Kernel Launch version Hardware p2 xlarge 4 omp threads Commit ba9de66 GPU build mx nd slice v1 10000 repeats costs 0 561281 seconds mx nd slice 10000 repeats costs 0 562561 seconds CPU only build mx nd slice v1 10000 repeats costs 1 049587 seconds mx nd slice 10000 repeats costs 1 130866 seconds Benchmark script haibin lin,,"reminisce,cjolivier01,cjolivier01,piiswrong,reminisce,reminisce,eric-haibin-lin,reminisce,cjolivier01,reminisce,reminisce",2017-11-06 03:57:39,2017-11-08 22:41:05
PR,gluon with multiple data type,zhreshold,,piiswrong,2017-11-02 22:50:26,2017-11-08 22:59:06
PR,reimplement cross entropy operator in recommender example with NDArray,8583 In the recommender example reimplement cross entropy operator with NDArray,,solin319,2017-11-08 02:18:42,2017-11-08 22:59:38
IS,train cifar10 py not running,I am able to run train mnist py both using single CPU node with no GPUs and using multi node CPU cluster distributed training However I am seeing that training is very slow with the MNIST example After looking into the suggestion of using bigger workload from following thread I am now trying to run train cifar10 py instead However I am unable to run the example even with a single node Following is the issue I am facing Whenever I am trying to run the example the process is getting killed as below 0 python train cifar10 py kv store local INFO root start with arguments Namespace batch size 128 benchmark 0 data nthreads 4 data train wouldata cifar10 train rec' data val wouldata cifar10 val rec' disp batches 20 gpus None image shape '3 28 28' kv store 'local' load epoch None lr 0 05 lr factor 0 1 lr step epochs '200 250' max random aspect ratio 0 max random h 36 max random l 50 max random rotate angle 0 max random s 50 max random scale 1 max random shear ratio 0 min random scale 1 model prefix None mom 0 9 monitor 0 network aresnet' num classes 10 num epochs 300 num examples 50000 num layers 110 optimizer isgd' pad size 4 random crop 1 random mirror 1 rgb mean '123 68 116 779 103 939' test io 0 top k 0 wd 0 0001 05 30 23 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 train rec use 1 threads for decoding 05 30 23 src io iter image recordio cc 221 ImageRecordIOParser data cifar10 val rec use 1 threads for decoding Killed Steps 1 mkdir train cifar10 cd train cifar10 2 Have following files directories in the current directory from mxnet repository a common b data cifar10 train rec cifar10 val rec c hosts d init py e mxnet f symbols g train cifar10 py 3 python train cifar10 py kv store local Can anyone please help me out with fixing this issue,,szha,2017-03-07 05:37:30,2017-11-09 00:26:27
IS,Inconsistent behavior of sparse elemwise mul and dense elemwise mul with nan,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description mx nd sparse elemwise mul row sparse dense does not deal with nan the same way as the dense operator does Environment info Package used Python R Scala Julia python I'm using Build info Required if built from source Compiler gcc clang mingw visual studio MXNet commit hash 100eb88add1c5a18185226eebde0664cc313f557 Paste the output of git rev parse HEAD here Build config Paste the content of config mk or the build command Error Message Paste the complete error message including stack trace Minimum reproducible example If you are using your own code please provide a short script that reproduces the error Otherwise please provide link to the existing example Steps to reproduce Paste the commands you ran that produced the error 1 run the above script 2,,eric-haibin-lin,2017-10-31 23:44:59,2017-11-09 06:12:46
IS,How to implement NG nonlinearity generator activate function in mxnet,According to this paper,,szha,2017-08-10 02:59:35,2017-11-09 12:26:27
IS,concat operator optimization,It seems the current implementation of concat operator is based on mshadow And if the input of concat has multiple NDArray on gpu it will launch kernel for many times Tensorflow has customized kernel for concat operator it will do kernel launch only once Any plan to optimize this,,szha,2017-08-10 01:30:42,2017-11-09 12:26:27
IS,Error with dist sync on two machines Thank you,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos6 4 Compiler gcc 4 8 5 Package used Python R Scala Julia python MXNet version 0 9 5 Or if installed from source Yes MXNet commit hash git rev parse HEAD If you are using python package please provide anaconda python 2 7 Python version and distribution anaconda python 2 7 If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-08-09 08:33:37,2017-11-09 12:26:29
IS,About van when using distribute training,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System centos6 4 centos7 2 Compiler gcc 4 8 5 Package used Python R Scala Julia python MXNet version 0 9 5 Or if installed from source Yes MXNet commit hash git rev parse HEAD If you are using python package please provide python 2 7 Python version and distribution python 2 7 If you are using R package please provide R sessionInfo Error Message thr program is looping in van cc Start function when mx kvstore create while ready LOG INFO wait ready why Below are some debug info when run launch py with train mnist py some log are added by me in thr source before MXKVStoreCreate 13 36 47 src kvstore kvstore dist h 36 KVStoreDist 13 36 47 src kvstore kvstore dist h 51 is worker node7 13 36 47 src kvstore kvstore dist h 53 is worker node8 after MXKVStoreCreate 13 36 47 src postoffice cc 26 Postoffice Constructor 13 36 47 src postoffice cc 83 Add customer 13 36 47 src postoffice cc 61 Van Start 13 36 47 src van cc 40 is scheduler 0 scheduler hostname 10 15 240 189 scheduler port 9191 scheduler role 2 13 36 47 src van cc 53 DMLC INTERFACE eth0 13 36 47 src van cc 58 DMLC INTERFACE 10 15 240 189 13 36 47 src van cc 66 Available port 57097 13 36 47 src van cc 70 ip empty 10 15 240 189 13 36 47 src van cc 71 port empty 57097 13 36 47 src van cc 89 connect to scheduler 13 36 47 src van cc 172 1 Meta request 0 timestamp 0 control cmd ADD NODE node role server ip 10 15 240 189 port 57097 is recovery 0 13 36 47 src postoffice cc 168 get dead nodes before MXKVStoreCreate 13 36 47 src kvstore kvstore dist h 36 KVStoreDist 13 36 47 src kvstore kvstore dist h 38 is worker node1 13 36 47 src postoffice cc 26 Postoffice Constructor 13 36 47 src postoffice cc 83 Add customer 13 36 47 src kvstore kvstore dist h 40 is worker node2 13 36 47 src postoffice cc 61 Van Start 13 36 47 src van cc 40 is scheduler 0 scheduler hostname 10 15 240 189 scheduler port 9191 scheduler role 2 13 36 47 src van cc 53 DMLC INTERFACE eth0 13 36 47 src van cc 58 DMLC INTERFACE 10 15 240 189 13 36 47 src van cc 66 Available port 34210 13 36 47 src van cc 70 ip empty 10 15 240 189 13 36 47 src van cc 71 port empty 34210 13 36 47 src van cc 89 connect to scheduler 13 36 47 src van cc 172 1 Meta request 0 timestamp 0 control cmd ADD NODE node role worker ip 10 15 240 189 port 34210 is recovery 0 13 36 47 src postoffice cc 168 get dead nodes before MXKVStoreCreate after MXKVStoreCreate 05 47 37 src kvstore kvstore dist h 36 KVStoreDist 05 47 37 src kvstore kvstore dist h 51 is worker node7 05 47 37 src kvstore kvstore dist h 53 is worker node8 05 47 37 src postoffice cc 26 Postoffice Constructor 05 47 37 src postoffice cc 83 Add customer 05 47 37 src postoffice cc 61 Van Start 05 47 37 src van cc 40 is scheduler 0 scheduler hostname 10 15 240 189 scheduler port 9191 scheduler role 2 05 47 37 src van cc 53 DMLC INTERFACE eth0 05 47 37 src van cc 58 DMLC INTERFACE 10 15 133 82 05 47 37 src van cc 66 Available port 39259 05 47 37 src van cc 70 ip empty 10 15 133 82 05 47 37 src van cc 71 port empty 39259 05 47 37 src van cc 89 connect to scheduler 13 36 47 src van cc 172 1 Meta request 0 timestamp 0 control cmd ADD NODE node role server ip 10 15 133 82 port 39259 is recovery 0 13 36 47 src postoffice cc 168 get dead nodes before MXKVStoreCreate 05 47 38 src kvstore kvstore dist h 36 KVStoreDist 05 47 38 src kvstore kvstore dist h 38 is worker node1 05 47 38 src postoffice cc 26 Postoffice Constructor 05 47 38 src postoffice cc 83 Add customer 05 47 38 src kvstore kvstore dist h 40 is worker node2 05 47 38 src postoffice cc 61 Van Start 05 47 38 src van cc 40 is scheduler 0 scheduler hostname 10 15 240 189 scheduler port 9191 scheduler role 2 05 47 38 src van cc 53 DMLC INTERFACE eth0 05 47 38 src van cc 58 DMLC INTERFACE 10 15 133 82 05 47 38 src van cc 66 Available port 52447 05 47 38 src van cc 70 ip empty 10 15 133 82 05 47 38 src van cc 71 port empty 52447 05 47 38 src van cc 89 connect to scheduler 13 36 47 src van cc 172 1 Meta request 0 timestamp 0 control cmd ADD NODE node role worker ip 10 15 133 82 port 52447 is recovery 0 13 36 47 src postoffice cc 168 get dead nodes 13 36 47 src van cc 246 assign rank 9 to node role worker ip 10 15 240 189 port 34210 is recovery 0 13 36 47 src van cc 246 assign rank 8 to node role server ip 10 15 240 189 port 57097 is recovery 0 13 36 47 src van cc 246 assign rank 10 to node role server ip 10 15 133 82 port 39259 is recovery 0 13 36 47 src van cc 246 assign rank 11 to node role worker ip 10 15 133 82 port 52447 is recovery 0 13 36 47 src van cc 147 9 Meta request 0 timestamp 0 control cmd ADD NODE node role worker id 9 ip 10 15 240 189 port 34210 is recovery 0 role server id 8 ip 10 15 240 189 port 57097 is recovery 0 role server id 10 ip 10 15 133 82 port 39259 is recovery 0 role worker id 11 ip 10 15 133 82 port 52447 is recovery 0 role scheduler id 1 ip 10 15 240 189 port 9191 is recovery 0 13 36 47 src van cc 147 11 Meta request 0 timestamp 1 control cmd ADD NODE node role worker id 9 ip 10 15 240 189 port 34210 is recovery 0 role server id 8 ip 10 15 240 189 port 57097 is recovery 0 role server id 10 ip 10 15 133 82 port 39259 is recovery 0 role worker id 11 ip 10 15 133 82 port 52447 is recovery 0 role scheduler id 1 ip 10 15 240 189 port 9191 is recovery 0 13 36 47 src van cc 147 8 Meta request 0 timestamp 2 control cmd ADD NODE node role worker id 9 ip 10 15 240 189 port 34210 is recovery 0 role server id 8 ip 10 15 240 189 port 57097 is recovery 0 role server id 10 ip 10 15 133 82 port 39259 is recovery 0 role worker id 11 ip 10 15 133 82 port 52447 is recovery 0 role scheduler id 1 ip 10 15 240 189 port 9191 is recovery 0 13 36 47 src van cc 147 13 36 47 src postoffice cc 168 get dead nodes 10 Meta request 0 timestamp 3 control cmd ADD NODE node role worker id 9 ip 10 15 240 189 port 34210 is recovery 0 role server id 8 ip 10 15 240 189 port 57097 is recovery 0 role server id 10 ip 10 15 133 82 port 39259 is recovery 0 role worker id 11 ip 10 15 133 82 port 52447 is recovery 0 role scheduler id 1 ip 10 15 240 189 port 9191 is recovery 0 13 36 47 src postoffice cc 168 get dead nodes 13 36 47 src van cc 262 the scheduler is connected to 2 workers and 2 servers 13 36 47 src postoffice cc 72 Van Start Do Barrier 13 36 47 src kvstore kvstore dist h 42 is worker node3 13 36 47 src kvstore kvstore dist h 44 is worker node4 13 36 47 src postoffice cc 117 Barrier role 1 13 36 47 src postoffice cc 136 Barrier van send 13 36 47 src postoffice cc 72 Van Start Do Barrier 13 36 47 src postoffice cc 117 Barrier role 0 13 36 47 src postoffice cc 136 Barrier van send 13 36 47 src postoffice cc 72 Van Start Do Barrier 13 36 47 src postoffice cc 117 Barrier role 2 13 36 47 src van cc 147 1 Meta request 1 timestamp 4 control cmd BARRIER barrier group 7 13 36 47 src postoffice cc 136 Barrier van send 13 36 47 src van cc 172 1 1 Meta request 1 timestamp 4 control cmd BARRIER barrier group 7 13 36 47 src van cc 302 Barrier count for 7 1 Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 2 3 What have you tried to solve it 1 2 3,,szha,2017-08-10 05:39:12,2017-11-09 12:26:30
IS,When executing train mnist py with two machines no response returns,I want to run the train mnist py on multi machines I run the command as follow however no responses return user machineA image classification tools launch py n 2 H hosts sync dst dir tmp mxnet python train mnist py network mlp kv store dist sync 2017 03 18 03 16 32 929 INFO rsync home user mxnet example image classification user machineB tmp mxnet 03 16 33 src van cc 75 Bind to role scheduler id 1 ip machineA is IP address port 9094 is recovery 0 When input machineA is IP address into hosts instead of machineB is IP address and execute it it executed correctly And I already set up ssh environment so I can not guess the cause of this situation Please help me how I can run it on multi machines thanks Environment info Operating System CentOS7 3 1611 Package used Python R Scala Julia Python MXNet version 0 9 4 Python version and distribution Python 2 7 5,,szha,2017-03-17 09:38:22,2017-11-09 12:26:30
IS,Caffe converter convert model py fail,Hi tried converting a caffe model The convert symbol ran successfully but the convert model crashed I know the model works fine on caffe I do not know any details about the model Environment info Operating System Ubuntu 16 04 Package used Python R Scala Julia Python MXNet commit hash git rev parse HEAD 89e3ee3ea7c223db8c65ddd8c94c6e787d7c52df If you are using python package please provide Python version and distribution python 2 7 Error Message Please paste the full error message including stack trace converting layer fc6 wmat shape 22440 512 fc6 bias not found in arg shape dic skipping layer softmaxloss1 of type SoftmaxWithLoss skipping layer center loss 1 of type CenterLoss Traceback most recent call last File convert model py line 220 in module main File convert model py line 216 in main convert model args prototxt args caffemodel args save model name File convert model py line 198 in convert model assert len layer blobs 0 AssertionError Steps to reproduce 1 run tools caffe converter python convert model py proto caffemodel filename,,szha,2017-08-10 09:25:03,2017-11-09 12:26:31
PR,Tuner PR,piiswrong Description Automatic OMP operator tuning based upon kernel operation workload Determines weight of a unary or binary kernel op and then uses this to determine if OMP should be used given of iterations required and threads to perform the job Correct decision accuracy is tested in gtest OMP TUNING test suite by comparing with OMP without OMP and Auto times For example AWS c4 8xlarge Success rate for type float 0 90278 Success rate for type double 0 88889 Success rate for type mshadow half half t 0 83333 Success rate for type unsigned char 0 86111 Success rate for type int 0 95833 Success rate for type long 0 88889 desktop 12 core 6 real CPU cores hyperthreading Success rate for type float 0 79167 Success rate for type double 0 75000 Success rate for type unsigned char 0 72222 Success rate for type int 0 94444 Success rate for type long 1 00000 A sample output from OMP TUNING tests including staticstical data tune all txt Currently autotuned kernel operators tuning at startup takes a total of 10ms mxnet op PopulateFullIdxRspKernel mxnet op mxnet op set to int 0 mxnet op mshadow op smooth l1 gradient mxnet op mshadow op smooth l1 loss mxnet op mshadow op eq mxnet op mshadow op ne mxnet op mshadow op le mxnet op mshadow op lt mxnet op mshadow op hypot grad right mxnet op mshadow op hypot grad left mxnet op mshadow op hypot mxnet op mshadow op arctanh grad mxnet op mshadow op arctan grad mxnet op mshadow op cosh mxnet op mshadow op rpower mxnet op mshadow op minimum mxnet op mshadow op arctan mxnet op mshadow op reciprocal square root mxnet op mshadow op rminus mxnet op mshadow op arccosh grad mxnet op mshadow op square root grad mxnet op mshadow op arctanh mxnet op mshadow op floor mxnet op mshadow op cosh grad mxnet op mshadow op ceil mxnet op mshadow op cos grad mxnet op mshadow op reciprocal cube root grad mxnet op mshadow op arcsinh grad mxnet op mshadow op sin mxnet op mshadow op arcsin mxnet op mshadow op log10 grad mxnet op mshadow op log1p grad mxnet op mshadow op mod grad mxnet op mshadow op arccos grad mxnet op mshadow op exp mxnet op mshadow op tanh grad mxnet op mshadow op log1p mxnet op mshadow op rint mshadow op minus mxnet op mshadow op relu grad mxnet op mshadow op identity mxnet op mshadow op maximum mxnet op mshadow op reciprocal grad mshadow op div mxnet op mshadow op rmod grad mxnet op mshadow op arcsin grad mxnet op mshadow op ge mxnet op mshadow op gammaln grad mxnet op mshadow op sigmoid mxnet op mshadow op power rgrad mxnet op mshadow op identity grad mxnet op mshadow op tan mxnet op mshadow op gamma mxnet op mshadow op arcsinh mshadow op identity mxnet op mshadow op square root mxnet op mshadow op reciprocal square root grad mxnet op mshadow op cos mxnet op mshadow op log2 mxnet op mshadow op tanh mxnet op mshadow op arccosh mxnet op mshadow op negation mxnet op mshadow op log10 mxnet op mshadow op cube root grad mxnet op mshadow op expm1 mxnet op mshadow op arccos mxnet op mshadow op rmod mxnet op mshadow op softrelu grad mxnet op mshadow op sinh mxnet op mshadow op log grad mxnet op mshadow op sin grad mxnet op mshadow op rdiv grad mxnet op mshadow op log mxnet op mshadow op softrelu mxnet op mshadow op square grad mxnet op mshadow op log2 grad mxnet op mshadow op cube root mxnet op mshadow op reciprocal cube root mxnet op mshadow op sign mxnet op mshadow op square mxnet op mshadow op sign grad mxnet op mshadow op round mxnet op mshadow op trunc mxnet op mshadow op mod rgrad mxnet op mshadow op reciprocal mxnet op mshadow op fix mxnet op mshadow op gamma grad mxnet op mshadow op gammaln mxnet op mshadow op degrees mshadow op right mxnet op mshadow op sinh grad mxnet op mshadow op degrees grad mshadow op plus mxnet op mshadow op radians mxnet op mshadow op sigmoid grad mxnet op mshadow op radians grad mxnet op mshadow op gt mxnet op mshadow op mod mshadow op mul mxnet op mshadow op rdiv mxnet op mshadow op tan grad mxnet op mshadow op div grad mxnet op mshadow op div rgrad mxnet op mshadow op left mxnet op mshadow op right mxnet op mshadow op power mxnet op mshadow op power grad mxnet op mshadow op relu mxnet op mshadow op abs mxnet op mshadow op rpower grad Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"cjolivier01,cjolivier01",2017-11-09 19:09:50,2017-11-09 19:09:55
PR,set number of real cores 1,,,,2017-11-09 23:58:49,2017-11-10 00:10:20
IS,cannot use ImageIter in Perl package,Environment info Operating System Mac OS Package used Python R Scala Julia Perl MXNet version 0 10 0 Error Message loading image list Use of uninitialized value fname in concatenation or string at Users user perl5 lib perl5 AI MXNet Image pm line 866 Minimum reproducible example use AI MXNet qw 'mx' my ite mx img ImageIter batch size 1 data shape 3 224 224 label width 1 path imglist data custom lst for data ite print data 0 custom lst is separated by tab 1 1 data test jpg,,"sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,sergeykolychev,szha",2017-08-04 13:45:11,2017-11-10 00:26:27
PR,sparse bug fix file not exist in python3,open both ok in python2 3 haibin lin,,formath,2017-11-09 09:55:49,2017-11-10 01:25:33
IS,Equivalent parameters for lr mult and decay mult of convolution in caffe in mxnet,I want to convert PSPNET written in caffe to mxnet In caffe under convolution and batch normalization lr mult and decay mult parameters are there sample prototxt below layer name conv5 3 pool2 conv type Convolution bottom conv5 3 pool2 top conv5 3 pool2 conv param lr mult 10 decay mult 1 convolution param num output 512 kernel size 1 stride 1 weight filler type msra bias term false layer name conv5 3 pool2 conv bn type BN bottom conv5 3 pool2 conv top conv5 3 pool2 conv param lr mult 10 decay mult 0 param lr mult 10 decay mult 0 param lr mult 0 decay mult 0 param lr mult 0 decay mult 0 bn param slope filler type constant value 1 bias filler type constant value 0 frozen true momentum 0 95 I would like to have lr mult and decay mult for convolution and batch normalization of caffe equivalent in mxnet Kindly suggest,,"solin319,chinakook",2017-11-08 01:25:56,2017-11-10 02:05:55
IS,how to print debug log in gpu mode,If I would like to print out the emdedding forward out data in gpu how can I do it It will crash if print it when run on gpus,,,2017-11-07 11:59:20,2017-11-10 08:46:20
IS,simple bind failed error,Hello I am trying to train my data with mxnet the data are 20x20 images I put my data in a csv files in the form label pixel1 pixel2 pixel400 0 1 and my labels are 0 or 1 this is the code I used batch size 2 train data pd read csv training set flatten rows mxnet csv keys 'pixel ' str i for i in range 1 402 X train train data keys 1 get values X train X train reshape 2779 1 20 20 Y train train data 'pixel 1' get values reshape 2779 1 validate data pd read csv validation set flatten rows mxnet csv keys 'pixel ' str i for i in range 1 402 X validate validate data keys 1 get values X validate X validate reshape 692 1 20 20 Y validate validate data 'pixel 1' get values reshape 692 1 train iterator mx io NDArrayIter X train Y train batch size batch size validate iterator mx io NDArrayIter X validate Y validate batch size batch size first convelutional layer conv1 mx sym Convolution data data kernel 3 3 num filter 6 relu1 mx sym Activation data conv1 act type relu pool1 mx sym Pooling data relu1 pool type max kernel 2 2 stride 2 2 second convelutional layer conv2 mx sym Convolution data pool1 kernel 6 6 num filter 12 relu2 mx sym Activation data conv2 act type relu pool2 mx sym Pooling data relu2 pool type max kernel 2 2 stride 2 2 first fully connected layer flatten mx sym flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 12 softmax loss lenet mx sym SoftmaxOutput data fc1 name isoftmax' create a trainable module on CPU 0 lenet model mx mod Module symbol lenet context mx cpu device mx cpu train using parameters ''' model mx model FeedForward create lenet model X X train y Y train ctx device num epoch 10 ''' lenet model fit train iterator eval data validate iterator optimizer isgd' optimizer params 'learning rate' 0 1 eval metric 'acc' batch end callback mx callback Speedometer batch size 100 num epoch 10 the error is Traceback most recent call last File ipython input 173 bf3d68caa271 line 7 in module num epoch 10 File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module base module py line 459 in fit for training True force rebind force rebind File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module module py line 388 in bind state names self state names File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module executor group py line 214 in init self bind exec data shapes label shapes shared group File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module executor group py line 310 in bind exec shared group File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module executor group py line 582 in bind ith exec shared buffer shared data arrays input shapes File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet symbol py line 1375 in simple bind raise RuntimeError isimple bind failed' RuntimeError simple bind failed If I comment this line in the model fit batch end callback mx callback Speedometer batch size 100 I get error WARNING root Already bound ignoring bind Traceback most recent call last File ipython input 176 a2b29eed591d line 7 in module num epoch 10 File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module base module py line 463 in fit allow missing allow missing force init force init File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module module py line 272 in init params for name arr in self arg params items AttributeError 'NoneType' object has no attribute 'items' I do not know what does this mean and what shall I do help me please I am using win10 python2 mxnet 0 10 anaconda2 Thanks in advance,,szha,2017-08-11 01:15:06,2017-11-10 12:26:26
IS,why rnn train speed is not stable sometims very slow,hi everyone recently I train a cnn rnn model for ocr but during the training process i found that the training speed is not stable sometimes 30 samples sec after a while it may become 1000 samples sec so i wonder why this happen I have tried updating mxnet from 0 9 5 to 0 10 not solved,,"eric-haibin-lin,szha",2017-08-10 06:57:22,2017-11-10 12:26:27
IS,Data provided by data shapes do not match names specified by data names,I am trying to train my data with mxnet the data are 20x20 images I put my data in a csv files in the form image1 px1 px2 px400 and labels 1 0 and my labels are 0 or 1 this is the code I used batch size 2 train iterator mx io NDArrayIter X train Y train batch size batch size validate iterator mx io NDArrayIter X validate Y validate batch size batch size first convelutional layer conv1 mx sym Convolution data data kernel 3 3 num filter 6 relu1 mx sym Activation data conv1 act type relu pool1 mx sym Pooling data relu1 pool type max kernel 2 2 stride 2 2 second convelutional layer conv2 mx sym Convolution data pool1 kernel 6 6 num filter 12 relu2 mx sym Activation data conv2 act type relu pool2 mx sym Pooling data relu2 pool type max kernel 2 2 stride 2 2 first fully connected layer flatten mx sym flatten data pool2 fc1 mx symbol FullyConnected data flatten num hidden 12 softmax loss lenet mx sym SoftmaxOutput data fc1 name isoftmax' create a trainable module on CPU 0 lenet model mx mod Module symbol lenet context mx cpu device mx cpu train using parameters ''' model mx model FeedForward create lenet model X X train y Y train ctx device num epoch 10 ''' lenet model fit train iterator eval data validate iterator optimizer isgd' optimizer params 'learning rate' 0 1 eval metric 'acc' batch end callback mx callback Speedometer batch size 100 num epoch 10 the error is Traceback most recent call last File ipython input 245 a2b29eed591d line 7 in module num epoch 10 File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module base module py line 459 in fit for training True force rebind force rebind File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module module py line 372 in bind self data names self label names data shapes label shapes File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module base module py line 70 in parse data desc check names match data names data shapes wouldata' True File C Users Anaconda2 lib site packages mxnet 0 10 1 py2 7 egg mxnet module base module py line 62 in check names match raise ValueError msg ValueError Data provided by data shapes do not match names specified by data names DataDesc 0 data 2 type 'numpy float32' NCHW DataDesc 1 data 2 type 'numpy float32' NCHW DataDesc 2 data 2 type 'numpy float32' NCHW DataDesc 3 data 2 type 'numpy float32' NCHW DataDesc 4 data 2 type 'numpy float32' NCHW DataDesc 5 data 2 type 'numpy float32' NCHW DataDesc 6 data 2 type 'numpy float32' NCHW DataDesc 7 data 2 type 'numpy float32' NCHW DataDesc 8 data 2 type 'numpy float32' NCHW DataDesc 9 data 2 type 'numpy float32' NCHW DataDesc 10 data 2 type 'numpy float32' NCHW DataDesc 11 data 2 type 'numpy float32' NCHW DataDesc 12 data 2 type 'numpy float32' NCHW DataDesc 13 data 2 type 'numpy float32' NCHW DataDesc 14 data 2 type 'numpy float32' NCHW DataDesc 15 data 2 type 'numpy float32' NCHW DataDesc 16 data 2 type 'numpy float32' NCHW DataDesc 17 data 2 type 'numpy float32' NCHW DataDesc 2773 data 2 type 'numpy float32' NCHW DataDesc 2774 data 2 type 'numpy float32' NCHW DataDesc 2775 data 2 type 'numpy float32' NCHW DataDesc 2776 data 2 type 'numpy float32' NCHW DataDesc 2777 data 2 type 'numpy float32' NCHW vs wouldata' What should the data look like what is wrong in the code please I am using win10 python2 mxnet 0 10 anaconda2 Thanks in advance,,"szha,szha,szha",2017-08-11 02:01:04,2017-11-10 12:26:28
PR,Update PR Issue Template,Description Encourage users to discuss Q A questions on the forum Add h documentation requirement to the check list for cpp,,"eric-haibin-lin,KellenSunderland,eric-haibin-lin",2017-11-06 00:09:47,2017-11-10 18:32:02
PR,expose group2ctx to module,Description Expose group2ctx to module Will be helpful for model parallel e g some sparse operators on cpu and other dense operators on gpu As a feature requested in cc haibin lin for review Also thanks for your valuable suggestions Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x add unittest Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,ZiyueHuang,ZiyueHuang,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,ZiyueHuang,eric-haibin-lin,ZiyueHuang",2017-11-04 08:44:12,2017-11-10 20:40:51
PR,support for lapack functions with mkl,Changes for using the lapack functionalities in MKL This is a backward compatible change that does not alter any existing behaviour except that linalg operators will automatically use MKL blas whenever USE BLAS mkl is set todays behaviour is that the operators do not work at all in this case It also allows a user that has a complete installation of MKL not only MKL2017 MKL ML to use MKL is lapack routines This would still require some editing of configs and mshadows makefiles as the configuration is currently not setup for using the whole MKL but at least it is now doable In a second step the make system should be cleaned w r t MKL build but I would like to separate it from the C changes,,"asmushetzel,mseeger,mseeger,mseeger,mseeger,asmushetzel,asmushetzel,asmushetzel,asmushetzel,piiswrong,asmushetzel,piiswrong,asmushetzel",2017-11-07 15:56:54,2017-11-10 23:37:18
PR,Fix for name epochs is not defined,Description Fix for name epochs is not defined when testing image classification py with symbolic mode Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Comments,,"ptrendx,piiswrong",2017-10-23 21:23:21,2017-11-11 00:02:11
IS,AttributeError 'module' object has no attribute istack',For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System OSX and Ubuntu Compiler Not sure Installed according to Package used Python R Scala Julia Python MXNet version Not sure Installed according to Or if installed from source MXNet commit hash git rev parse HEAD If you are using python package please provide Python version and distribution If you are using R package please provide R sessionInfo Error Message Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 import mxnet as mx 2 mx nd stack Comment I installed both on my Apple Mbp pip and Ubuntu 14 04 docker following the steps in Not sure,,szha,2017-08-11 15:55:39,2017-11-11 00:26:27
IS,Parameter wouldtype' in the function 'ImageRecordIter' does not work,In mxnet 0 10 version I add wouldtype float16' wouldtype float64' wouldtype uint8' in the function 'ImageRecordIter' in example image classifacation common data py But the data read from this function is always 'float32' I find this parameter only used in the 266 line of the file iter image recordio 2 cc It pass a new type to a variable wouldtype' But this variable was no longer be used Is it a bug How can I read data in float16 format,,"solin319,ptrendx,szha",2017-08-08 10:52:14,2017-11-11 00:26:28
IS,run errors,For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System ubuntu 14 Compiler AuthenticAMD Package used Python R Scala Julia Python MXNet version 0 9 Or if installed from source yes MXNet commit hash git rev parse HEAD git If you are using python package please provide Python version and distribution 2 7 If you are using R package please provide R sessionInfo Error Message mxnet base MXNetError src c api c api ndarray cc 270 Operrator zeros cannot be run requires at least one of FCompute xpu NDArrayFunction FCreateOperator be registered image Please paste the full error message including stack trace Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 run the demo py of 2 3 What have you tried to solve it 1 have no idea to solve it 2 3,,"piiswrong,piiswrong,loofahcus,szha",2017-01-17 08:47:03,2017-11-11 00:26:29
PR,R Initializer fix and adjustments to RNN API,Fix for 8120 Uses mx nd random uniform and mx nd random normal for the initialisation Note that GPU context seems to only work with mx nd random uniform Might be better to force the initialisation on cpu,,"jeremiedb,thirdwing,jeremiedb,piiswrong,thirdwing,jeremiedb,jeremiedb,thirdwing",2017-10-01 18:34:48,2017-11-11 02:24:26
PR,use first class cuda with cmake 3 9 and cuda9 0 support,Description use first class cuda with cmake 3 9 and cuda9 0 support fix lapack auto open with openbals Checklist Essentials Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Comments,,"yajiedesign,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,cjolivier01,yajiedesign,yajiedesign,yajiedesign,yajiedesign,yajiedesign,cjolivier01,cjolivier01,yajiedesign,yajiedesign,cjolivier01,cjolivier01,cjolivier01,cjolivier01,yajiedesign,cjolivier01,yajiedesign,cjolivier01,cjolivier01,yajiedesign,yajiedesign",2017-11-07 08:46:20,2017-11-11 02:55:45
IS,how can i print all weights in monitor,I just use monitor to print the weights Because lots of weights it can not print all weights like that 10 5 0 0 0 0 0 how can i modify monitor code My code is that def print weight d result for i in range 0 d size result str d i return result and it will report error,,,2017-11-10 08:50:21,2017-11-11 10:04:57
IS,closed,,,,2017-11-11 09:55:37,2017-11-11 11:29:46
PR,generalize array dataset,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,piiswrong,2017-11-10 21:14:14,2017-11-11 19:56:34
PR,optimize broadcast,reminisce This can also be used to optimize slice stack etc,,"piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,cjolivier01,cjolivier01,cjolivier01,piiswrong,piiswrong,reminisce,piiswrong,reminisce,piiswrong,reminisce",2017-11-06 23:44:38,2017-11-11 19:58:19
PR,Pull more optimize and simplification changes from tuner branch,piiswrong Description Various minor tweaks for optimization simplification and leading up to tuner Remove redundant ops and function KernelCompute Launch calls OpenMP class directly instead of engine engine calls into OpenMP class to reserve cores Unit testing perf testing tweaks Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,cjolivier01,2017-11-09 19:39:08,2017-11-11 20:05:57
IS,mx contrib sym ctc loss slow down extremely on gpu with large alphabet size but faster on cpu,When I train large alphabet with size say 3000 on gpu the mx contrib sym ctc loss training speed is very slow 0 2 samples per second but on cpu the speed is faster 16 samples per second How to get work with mx contrib sym ctc loss when the alphabet size is large,,szha,2017-07-21 22:17:04,2017-11-12 00:26:27
PR,Fix python 3 docstring,Description Fixes a docstring for Python 3 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes See description Comments,,,2017-11-11 12:15:14,2017-11-12 02:46:31
PR,Fix changed OMP call in LaunchEx add broadcast perf test,piiswrong Description Build problem in master fixed Performance test for broadcast op No critical functionality changed Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,cjolivier01,2017-11-12 04:33:42,2017-11-12 07:34:43
IS,How to resize the image to same shape in ImageRecordIter,Hi I have a set of images with various shapes and I write it to rec file and using mx io ImageRecordIter to load it But I met an error I tried different ways to solve it but not works such as set resize mode inter method any helps will be appreciated,,piiswrong,2017-11-10 15:11:37,2017-11-12 09:01:41
IS,Large Rec File Memory and Speed Issues,Hi all I just had a question about the usage of recordio files in mxnet I have a large rec file 35 GB on a system with 32GB of RAM Whenever I try training a model I see my memory consumption overflowing into swap memory eating around as much RAM as the rec file itself Changing prefetch buffer and preprocess threads constants do not seem to change this Is mxnet loading the entire rec file into memory I saw this issue which is similar to my issue but working off of mxnet is latest commit did not solve Also curiously I am still able to train albeit slowly but running the validation set between epochs takes 3 hrs when each epoch is only about 1 hr Is this normal Thanks so much for the help,,"tornadomeet,szha",2016-08-31 01:21:16,2017-11-12 12:26:26
IS,ImageRecordIOParser std bad alloc Error when or after decoding,Environment info Jetson TX1 4G RAM Ubuntu16 04 64bit MXNet 0 94 1 RAM is enough I excluded the cause of the memory firstly I made rec format data less 1MB using im2rec py However it still occurs this problem message I use caltech 256 data set from mxnet example image classification data caltech256 sh and train a DeepID model based on mxnet example image classification train cifar10 py Error Message,,"ysh329,piiswrong,ysh329,ptrendx,szha",2017-03-22 02:02:43,2017-11-12 12:26:28
PR,add group2ctxs to movie factorization machine,Description Add a g2c example cc haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,ZiyueHuang,2017-11-12 18:59:07,2017-11-12 19:12:04
PR,linear classification bug fix kv may be None,eric haibin lin,,"formath,formath,ZiyueHuang",2017-11-13 08:19:29,2017-11-13 08:41:39
IS,c predict api BUG in new mxnet version,One of my project using MXPredForward of c predict api on CPU context consumes 940MB memory when running on the version on about Oct 30 2017 but on the 0 12 0 Release version or versions after that It consumes up to maximum 9GB memory I have tested it in two computers and both occur So I think there is a bug after Oct 30 2017,,"chinakook,chinakook",2017-11-13 07:27:15,2017-11-13 08:57:38
IS,out of memory when training imagenet with rec file,Environment info Operating System ubuntu 16 04 Compiler gcc Ubuntu 5 4 0 6ubuntu1 16 04 4 5 4 0 20160609 Package used Python R Scala Julia Python MXNet commit hash 8ad3c8a7a98dfa6bd6f5065cf9c3688f2414c3d4 Python version and distribution Python2 7 12 Error Message The usage of memory at the beginning of training is 5 7 7 9 after a while 3 4h it takes 27 and finally it run out of my memory waked up by the alarm message Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 cd examples image classification python train imagenet py network my net gpus 0 1 2 3 num epochs 100 lr 0 01 lr step epochs 30 60 80 110 batch size 256 top k 5 data train data shared datasets ILSVRC2015 rec train 480 q100 rec data val data shared datasets ILSVRC2015 rec val 480 q100 rec rgb mean 123 68 116 779 103 939 data nthreads 4 model prefix my net What have you tried to solve it 1 set the prefetch buffer 1 but the accuracy of my model drop to 20 and set prefetch buffer back to 2 4 8 the accuracy is right 2 cpu memory increase continually when set the prefetch buffer 1 3 also find some similar issues,,szha,2017-08-14 03:36:31,2017-11-13 12:26:27
IS,Not appending to end of linked list correctly,Hi I am reading money is source code and am confused in following block line 64 67 in threaded engine cc head next new var block head trigger opr block head new var block Is this trying to append a new var block to head why the last line is not head head next,,szha,2017-08-14 08:44:14,2017-11-13 12:26:28
IS,Pb in import of mxnet,I have a Pb in import of mxnet in python Environment info Operating System windows 10 Compiler visual 2015 64 Package used Python R Scala Julia Python MXNet version 0 10 or 0 11 installed from source If you are using python package please provide Python version and distribution 2 7 13 Error Message Python 2 7 13 v2 7 13 a06454b1afa1 Dec 17 2016 20 53 40 MSC v 1500 64 bit AMD64 on win32 Type help copyright credits or license for more information import mxnet Traceback most recent call last File line 1 in File mxnet init py line 25 in from base import MXNetError File mxnet base py line 86 in LIB load lib File mxnet base py line 78 in load lib lib ctypes CDLL lib path 0 ctypes RTLD LOCAL File C Python27 lib ctypes init py line 362 in init self handle dlopen self name mode WindowsError Error 126 Le module sp cifi est introuvable Minimum reproducible example if you are using your own code please provide a short script that reproduces the error Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 AFTER COMPILATION Import mxnet 2 3 What have you tried to solve it 1 different version 2 3,,szha,2017-08-14 08:54:12,2017-11-13 12:26:29
IS,training speed of batch norm is less than batch norm v1,if set use global stats True then the training speed of batch norm is less than batch norm v1 so what is the change the newer batch norm,,"tornadomeet,chinakook,szha",2017-08-07 05:35:46,2017-11-13 12:26:29
IS,inconsistent accuracy ImageRecordIter vs ImageIter,ImageRecordIter gets much better accuracy Any reasons for that,,"melody-rain,melody-rain,melody-rain,szha",2017-08-10 01:16:56,2017-11-13 12:26:30
PR,Continued Work on Advanced Indexing,Description This is a continued work of addressing Implementation based upon Numpy is indexing documentation advanced indexing Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Support NDArray setitem and getitem using advanced indices excluding Numpy Ellipsis newaxis and boolean array indexing boolean array indexing When index is a single list only a list of integers is supported Nested lists as indices is supported when index is a tuple of basic and advanced indices That is a 1 2 is supported a 1 2 is NOT supported but a 1 1 2 1 2 2 5 is supported x Support NDArray indexing using slices with arbitrary value of step x Re implemented operators slice slice assign and slice assign scalar using Kernel Launch and support step 1 x Fixed this issue haibin lin,,"reminisce,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,reminisce,reminisce,reminisce,reminisce,piiswrong,piiswrong,piiswrong,reminisce,reminisce,reminisce,reminisce,piiswrong,piiswrong,szha,reminisce,cjolivier01,reminisce,cjolivier01,reminisce,cjolivier01,JeanKossaifi,reminisce,reminisce,reminisce,reminisce,reminisce,JeanKossaifi,cjolivier01,reminisce,cjolivier01",2017-10-12 22:08:47,2017-11-13 19:30:36
PR,EXPERIMENT increasing build timeout to 24hrs,Description All builds are timing out in the build queue since the number of ubuntu slaves is less Ideally this timeout is meant for the build time on the executor but somehow it even times out while a task waits in the queue Checklist Essentials Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Changed Timeout to 1440 minutes 24hrs Comments This is temporary until we move to a new CI mbaijal to monitor if this change helps,,mbaijal,2017-11-11 00:32:41,2017-11-13 22:52:50
PR,fix custom op error when using auxiliary states,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"piiswrong,yajiedesign",2017-11-05 00:14:21,2017-11-13 23:01:25
IS,Question about networks for cifar100,What networks are suitable for cifar100 with relatively high testing accuracy,,szha,2017-08-14 13:50:45,2017-11-14 00:26:26
PR,Return to the upstream CUB,Upstream CUB was cleaned to reduce the size of the history and uses 20 MB now,,"ptrendx,szha",2017-08-14 19:31:32,2017-11-14 00:26:27
IS,gluon nomenclature for hybrid computation,I find that names like HybridBlock HybridSequential and hybridize in gluon are not effectively conveying what their utilities and usages are To me 'HybridBlock' sounds like a mixture of 'Block' and something else although it is actually an extension of a Block Also calling hybridize sounds like imperative and symbolic program will be mixed in executions although in reality calling it makes an one way transition of the block from an imperative program to a symbolic program Lastly 'hybrid' is an ambiguous word which meaning is unclear until a user takes a careful look at the hybrid computation tutorial Proposal Why do not we call these CompilableBlock CompilableSequential and compile Then it should be clear CompilableBlocks are extensions of Blocks and executions of CompilableBlocks will be faster after calling compile than they were before because 'compile' is a concept most programmers are familiar with Furthermore from the API design point of view it might be clearer if compile creates a new Block rather than changing the state of an exiting HybridBlock Then it should be clear that CompilableBlock is still an imperative program whereas CompilableBlock compile is a declarative program I am curious what others' thoughts on this are Tagging since I talked with him on this separately,,"bikestra,szha,szha",2017-08-14 21:24:32,2017-11-14 00:26:28
PR,WIP Image Augmenter,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes RandomBrightness Comments,,yajiedesign,2017-11-13 10:04:51,2017-11-14 00:32:48
IS,how to set dataiter with multi data,hey guys I tried to rewrite Dataiter to provide multi data for func provide data I set it be wouldata1' shape1 wouldata2' shape2 and bind this iter with module But there is problem in parse data desc function The data name must be wouldata' cannot change to any other name so how can I solve this problem,,szha,2017-08-11 08:12:43,2017-11-14 12:26:26
IS,Minimal C example fails to register operators,This is the most minimal example of what should be used to load a symbol with the c api and it does not work Unless I'm missing something completely like a compiler flag otherwise Environment info Operating System centos Compiler gcc Package used Python R Scala Julia C MXNet version 0 9 3 Error Message Please paste the full error message including stack trace 16 17 17 share tools mxnet dmlc core include dmlc logging h 300 16 17 17 src core op cc 55 Check failed op nullptr Operator FullyConnected is not registered Stack trace returned 10 entries bt 0 share tools mxnet lib libmxnet so ZN4nnvm2Op3GetERKSs 0x329 0x7fcb0323f179 bt 1 share tools mxnet lib libmxnet so 0xef8268 0x7fcb03227268 bt 2 share tools mxnet lib libmxnet so ZN4dmlc20JSONObjectReadHelper13ReadAllFieldsEPNS 10JSONReaderE 0x100 0x7fcb0322d680 bt 3 share tools mxnet lib libmxnet so 0xef70ef 0x7fcb032260ef bt 4 share tools mxnet lib libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7fcb02e8c3ef bt 5 share tools mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7fcb03232b51 bt 6 share tools mxnet lib libmxnet so ZN5mxnet18LoadLegacyJSONPassEN4nnvm5GraphE 0x180 0x7fcb02e851c0 bt 7 share tools mxnet lib libmxnet so ZNSt17 Function handlerIFN4nnvm5GraphES1 EPS2 E9 M invokeERKSt9 Any dataS1 0x11f 0x7fcb02e8c3ef bt 8 share tools mxnet lib libmxnet so ZN4nnvm11ApplyPassesENS 5GraphERKSt6vectorISsSaISsEE 0x501 0x7fcb03232b51 bt 9 share tools mxnet lib libmxnet so ZN4nnvm9ApplyPassENS 5GraphERKSs 0x8e 0x7fcb0318006e Minimum reproducible example test c include stdio h include mxnet c api h int main void const char symfn net symbol json SymbolHandle sym MXSymbolCreateFromFile symfn sym return 0 Steps to reproduce or if you are running standard examples please provide the commands you have run that lead to the error 1 compiled with gcc I include L Wl whole archive lmxnet Wl no whole archive test c o testrun 2 run What have you tried to solve it 1 including every header in the include mxnet directory 2 copying all compiler flags from the make file,,"dabraude,dabraude,szha",2017-08-14 15:27:21,2017-11-14 12:26:27
IS,Training error when using cifar100,I train the network given in with cifar100 But here comes a training error like this qq 20170815175608 What is the problem aileli,,szha,2017-08-15 10:10:53,2017-11-14 12:26:28
PR,WIP Vision,remove oepncv use,,yajiedesign,2017-11-14 13:10:48,2017-11-14 13:11:54
PR,add stub,add stub,,yajiedesign,2017-11-14 13:28:08,2017-11-14 13:28:35
IS,v0 11 0 Amalgamation for Javascript JS has unresolved symbol cxa thread atexit,The amalgamation for Javascript in mxnet v0 9 2 worked fine however for the latest 0 11 0 version I got this error while running make clean libmxnet predict js MIN 1 warning unresolved symbol cxa thread atexit and the compiled js code crashed upon loading with the same unresolved symbol error It seems this may have something to do with C 11 features support in emcc,,szha,2017-08-15 18:02:22,2017-11-15 00:26:27
PR,WIP vision fix,add brightness contrast saturation,,yajiedesign,2017-11-15 03:00:29,2017-11-15 04:16:23
PR,check format of sparse ndrray,Description check format of ndrray mainly for csr Many constraints are adapted from check format in scipy As a feature requested in cc haibin lin for review Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,piiswrong,piiswrong,ZiyueHuang,ZiyueHuang,piiswrong,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,piiswrong,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,ZiyueHuang,ZiyueHuang,ZiyueHuang,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,sxjscience,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,piiswrong,ZiyueHuang,piiswrong,piiswrong,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,ZiyueHuang,ZiyueHuang,ZiyueHuang,piiswrong,ZiyueHuang,ZiyueHuang",2017-10-13 17:40:47,2017-11-15 05:20:48
IS,Block Gradient document issue,link mxnet symbol BlockGrad The example uses stop gradient other than BlockGrad which is not consistent with the symbol name,,Godricly,2017-10-12 03:53:10,2017-11-15 09:34:38
PR,Fix HybridLambda repr,Description Could not print a model containing a HybridLambda block this fixes the offending typo Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR n a All changes have test coverage n a For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,JulianSlzr,2017-11-06 22:55:51,2017-11-15 16:50:18
PR,fix custom op error when using auxiliary states,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,piiswrong,2017-11-13 23:02:56,2017-11-15 18:27:19
PR,Imperative bulk execution,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,piiswrong,2017-11-02 22:03:04,2017-11-15 18:28:48
PR,fix image random compile,yajiedesign,,yzhliu,2017-11-15 15:42:03,2017-11-15 18:40:34
PR,WIP Operator tuning,piiswrong Description Operator tuning Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,"cjolivier01,cjolivier01,cjolivier01",2017-11-15 18:43:23,2017-11-15 18:43:29
PR,Add nightly test for dist device sync,Description this test requires multiple GPUs Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,rahul003",2017-11-09 18:48:04,2017-11-15 23:03:27
PR,Add Gluon data transform,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,piiswrong,2017-11-15 23:12:18,2017-11-15 23:16:52
IS,Why the data size can not be bigger than batch size,L626 Why can not it have consistent behavior as treating the whole data as the last batch and padding zeros This can reduce the work to pad zeros to the data when it is smaller than batch size,,szha,2017-08-16 20:30:17,2017-11-16 00:26:27
IS,Sparse tensors causing a deadlock,We identified some issues with a deadlock We suspect that they are caused by running multiple tests on the same machine Trace,,"marcoabreu,marcoabreu,marcoabreu",2017-11-16 04:33:43,2017-11-16 05:14:13
PR,multiprocessing Gluon DataLoader,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"piiswrong,zhreshold",2017-11-03 00:24:51,2017-11-16 06:31:09
IS,mxnet blocked when binding gpu device,mxnet 0 12 0 cuda 7 5 mxnet blocked python codes train exec bind exec sym ctxi data shapes self param names need grad True base exec shared exec shared data arrays self shared data arrays i input types data types Stack info 1 0x00007f363621dea7 in from usr lib64 libnvidia ptxjitcompiler so 375 20 2 0x00007f363623e9ea in from usr lib64 libnvidia ptxjitcompiler so 375 20 3 0x00007f3636246fef in from usr lib64 libnvidia ptxjitcompiler so 375 20 4 0x00007f36367ef3dd in from usr lib64 libnvidia ptxjitcompiler so 375 20 5 0x00007f36362319df in from usr lib64 libnvidia ptxjitcompiler so 375 20 6 0x00007f36362360a5 in from usr lib64 libnvidia ptxjitcompiler so 375 20 7 0x00007f36367ef3dd in from usr lib64 libnvidia ptxjitcompiler so 375 20 8 0x00007f3636247bc0 in from usr lib64 libnvidia ptxjitcompiler so 375 20 9 0x00007f36361f4415 in from usr lib64 libnvidia ptxjitcompiler so 375 20 10 0x00007f36367ef3dd in from usr lib64 libnvidia ptxjitcompiler so 375 20 11 0x00007f36361f7b94 in from usr lib64 libnvidia ptxjitcompiler so 375 20 12 0x00007f36361f92e9 in from usr lib64 libnvidia ptxjitcompiler so 375 20 13 0x00007f36361efabc in cuda CallJitEntryPoint from usr lib64 libnvidia ptxjitcompiler so 375 20 14 0x00007f392fb63582 in fatBinaryCtl Compile from usr lib64 libnvidia fatbinaryloader so 375 20 15 0x00007f3937f52e42 in from usr lib64 libcuda so 1 16 0x00007f3937f539c3 in from usr lib64 libcuda so 1 17 0x00007f3937eac35e in from usr lib64 libcuda so 1 18 0x00007f3937eac640 in from usr lib64 libcuda so 1 19 0x00007f39448cd52d in from usr local cuda targets x86 64 linux lib libcudart so 7 5 20 0x00007f39448c1ba0 in from usr local cuda targets x86 64 linux lib libcudart so 7 5 21 0x00007f39448cc796 in from usr local cuda targets x86 64 linux lib libcudart so 7 5 22 0x00007f39448d0ed1 in from usr local cuda targets x86 64 linux lib libcudart so 7 5 23 0x00007f39448c445e in from usr local cuda targets x86 64 linux lib libcudart so 7 5 24 0x00007f39448b22ee in from usr local cuda targets x86 64 linux lib libcudart so 7 5 25 0x00007f39448e6194 in cudaStreamCreate from usr local cuda targets x86 64 linux lib libcudart so 7 5 26 0x00007f3947383e7c in mshadow Stream mshadow gpu mshadow NewStream mshadow gpu bool bool int from search odin mxnet wzl train python mxnet lib libmxnet so 27 0x00007f394739fe4f in void mxnet engine ThreadedEnginePerDevice GPUWorker dmlc ConcurrentQueueType 0 mxnet Context bool mxnet engine ThreadedEnginePerDevice ThreadWorkerBlock dmlc ConcurrentQueueType 0 std shared ptr mxnet engine ThreadPool SimpleEvent from search odin mxnet wzl train python mxnet lib libmxnet so,,wzl12356,2017-11-15 00:51:38,2017-11-16 08:30:09
IS,UnsatisfiedLinkError with Amalgamation for Android,Description I have compiled libmxpredict so for Android following the guide found at 7146 but loading it inside an app causes a java lang UnsatisfiedLinkError at runtime Environment info Operating System macOS 10 13 1 NDK version android ndk r14b Compiler arm linux androideabi clang MXNet version v0 12 0 MXNet commit hash 0 12 0 4f2af2d Error message Minimum reproducible example I am compiling the WhatsThis sample app from my repo with Android Studio 3 0 and running it on a Nexus 5 with Android 6 0 1 The repository includes the libmx predict so I recompiled together with libopenblas so and libc shared so whose absence caused linker errors now solved Steps to reproduce 1 Compile and run the Android application,,,2017-11-15 15:51:22,2017-11-16 11:02:13
PR,log epoch number for tensorboard,,,"zihaolucky,zihaolucky,szha",2017-08-17 00:22:17,2017-11-16 12:26:26
IS,Importing caffe in python causes mxnet leaky relu to output wrong results,I have very strange issue on my workstation After importing caffe python module any slope less than 1 0 causes results to be zero Please check example below I have also another installation of mxnet on another machine almost same and there everything works fine For bugs or installation issues please provide the following information The more information you provide the more likely people will be able to help you Environment info Operating System Arch linux Compiler Package used Python R Scala Julia Python MXNet version 0 10 0 post2 from pip Caffe version 1 0 compiled from source with gcc 5 4 0 Python version and distribution Python 2 7 13 default Jul 21 2017 03 24 34 GCC 7 1 1 20170630 on linux2,,szha,2017-08-17 07:42:14,2017-11-16 12:26:27
IS,can not find sf1 train lst for the cpp example,I'm trying to run resnet cpp example can not find sf1 train lst sf1 train rec data from I'm new to machine learning I want to know is sf1 train a standard data set or any data processed by im2rec py is fine for the example,,szha,2017-08-17 02:54:30,2017-11-16 12:26:27
IS,Add build instructions for Fedora to the Installation page,It would be great if the Installation page contains a step by step list of things that need to be done to compile MXNet on Fedora,,"kottmann,kottmann,szha",2017-07-19 15:28:18,2017-11-16 12:26:28
IS,mxnet random seed should not be fixed by default,mxnet seems to use a fixed seed for its random number generator This is not good To make sure research results are valid experiments need to be repeated with different random initializations To the best of my knowledge it is common practice that the seed for the random number generator is chosen by default randomly at startup At least that is the behaviour of numpy As mxnet does not follow the expectated behaviour researchers may wrongly assume that their results are statistically significant given that they would expect a random seed Compare what happens when running the following two files repeatedly I e mxnet random number generator uses the same seed,,"leezu,leezu,szha,larroy",2017-08-10 03:14:53,2017-11-16 12:26:29
IS,Restrict running some operators on only one GPU,Is it possible to restrict some operators to run on only one GPU For example pytorch calculate the Loss on only one GPU This benefits some losses like Triplet Loss where we need to search hard positives and hard negatives The search space is N GPU times larger,,,2017-10-24 16:10:08,2017-11-16 13:01:13
IS,Error Adding file set for 'ml dmlc mxnet libmxnet scala osx x86 64 cpu jnilib v0 11 1a',Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues and bug reports For non technical issues and feature requests feel free to present the information in what you believe is the best form For Q A and discussion please start a discussion thread at Description I am currently running into an error when trying to build a single jar on OSX I get the following error Steps to reproduce Paste the commands you ran that produced the error 1 Use dependencies and plug from pom above What have you tried to solve it 1 I added the assembly xml page but did not have any luck with jni,,"dmmiller612,dmmiller612",2017-11-16 17:54:07,2017-11-16 19:32:26
PR,fix bug in TryCrop,The conditions for the TryCrop is incorrect,,"zhreshold,zhreshold,piiswrong,zhreshold",2017-11-14 04:41:28,2017-11-16 21:36:31
PR,Support sparse for custom python operators,Description This PR is to support sparse for custom python operators Specifically one should be able to use sparse storage types with custom python operators right now only default storage types are supported Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Support for infer storage type in forward pass x Support for infer storage type in backward pass Comments Requires 8543 to be merged Completes one TODO in 8168 haibin lin,,"anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,anirudh2290,anirudh2290,anirudh2290,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-11-12 02:51:28,2017-11-16 23:11:56
PR,sparse embedding operator gpu implementation,Description running on the machine of 20 cores CPU and 8G 1080 GPU As a feature requested in cc haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin",2017-11-14 11:33:36,2017-11-16 23:13:01
PR,multi processing and fork fix,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,piiswrong,2017-11-16 06:32:53,2017-11-16 23:13:32
PR,ctypes speed improvement,Description Improvements on ctypes calls for creating arrays The changes are based on the following micro benchmarks Results time in seconds the lower the better python version int before int after strings before strings after handles before handles after 2 7 13 2 50 0 54 1 52 0 30 0 46 0 23 3 6 2 2 36 0 13 1 13 0 58 0 32 0 12 Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x update ctypes c array calls,,"szha,piiswrong,szha,piiswrong,szha",2017-11-14 03:25:14,2017-11-16 23:19:10
IS,SSD example error,Simply run SSD example got this error,,"sandeep-krishnamurthy,szha",2017-08-17 08:10:44,2017-11-17 00:26:27
IS,Perl MNIST using record file format Test not passed,Environment info Operating System Mac OS Package used Python R Scala Julia Perl MXNet version 0 10 0 Information I am trying to use MNIST program using mxnet record file format It seemed to data loading is going good but accuracy test is not passed Source file is uploaded GitHub below,,"sergeykolychev,szha",2017-08-17 13:31:58,2017-11-17 00:26:28
IS,How to view results from recommender examples,I am new to MXNet and recommender engines and wanted to test out the matrix factorization demos provided with MXNet I can run the python notebooks such as demo1 MF ipynb just fine but I am confused as to how to view the final output of recommendations After training the model it plots a graph of RMSE over time but how do I view the actual recommendations that the model should predict,,szha,2017-08-18 00:05:07,2017-11-17 00:26:29
IS,a 1 indexing does not work on NDArray,This works for numpy a mx nd array 1 2 3 2 11 a 1 03 54 14 home ubuntu incubator mxnet dmlc core include dmlc logging h 308 03 54 14 src ndarray ndarray cc 100 Check failed begin end 4294967295 vs 0 Invalid slicing range 4294967295 0 Stack trace returned 10 entries bt 0 home ubuntu incubator mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3f 0x7fc9cb2a9c6f bt 1 home ubuntu incubator mxnet python mxnet lib libmxnet so ZNK5mxnet7NDArray5SliceEjj 0x19f 0x7fc9cc6d0c0f bt 2 home ubuntu incubator mxnet python mxnet lib libmxnet so ZN5mxnet7NDArray15SliceWithRecordEjj 0x57 0x7fc9cc6d118f bt 3 home ubuntu incubator mxnet python mxnet lib libmxnet so ZN5mxnet7NDArray12AtWithRecordEj 0xfe 0x7fc9cc6d193c bt 4 home ubuntu incubator mxnet python mxnet lib libmxnet so MXNDArrayAt 0x6c 0x7fc9ccc34721 bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7fc9fba13e40 bt 6 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7fc9fba138ab bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7fc9fbc233df bt 8 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7fc9fbc27d82 bt 9 python PyObject Call 0x43 0x4b0cb3 Traceback most recent call last File stdin line 1 in module File home ubuntu incubator mxnet python mxnet ndarray ndarray py line 508 in getitem return self at key File home ubuntu incubator mxnet python mxnet ndarray ndarray py line 685 in at self handle idx ctypes byref handle File home ubuntu incubator mxnet python mxnet base py line 145 in check call raise MXNetError py str LIB MXGetLastError mxnet base MXNetError 03 54 14 src ndarray ndarray cc 100 Check failed begin end 4294967295 vs 0 Invalid slicing range 4294967295 0 Stack trace returned 10 entries bt 0 home ubuntu incubator mxnet python mxnet lib libmxnet so ZN4dmlc15LogMessageFatalD1Ev 0x3f 0x7fc9cb2a9c6f bt 1 home ubuntu incubator mxnet python mxnet lib libmxnet so ZNK5mxnet7NDArray5SliceEjj 0x19f 0x7fc9cc6d0c0f bt 2 home ubuntu incubator mxnet python mxnet lib libmxnet so ZN5mxnet7NDArray15SliceWithRecordEjj 0x57 0x7fc9cc6d118f bt 3 home ubuntu incubator mxnet python mxnet lib libmxnet so ZN5mxnet7NDArray12AtWithRecordEj 0xfe 0x7fc9cc6d193c bt 4 home ubuntu incubator mxnet python mxnet lib libmxnet so MXNDArrayAt 0x6c 0x7fc9ccc34721 bt 5 usr lib x86 64 linux gnu libffi so 6 ffi call unix64 0x4c 0x7fc9fba13e40 bt 6 usr lib x86 64 linux gnu libffi so 6 ffi call 0x2eb 0x7fc9fba138ab bt 7 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so ctypes callproc 0x48f 0x7fc9fbc233df bt 8 usr lib python2 7 lib dynload ctypes x86 64 linux gnu so 0x11d82 0x7fc9fbc27d82 bt 9 python PyObject Call 0x43 0x4b0cb3,,"eric-haibin-lin,piiswrong,eric-haibin-lin,reminisce",2017-10-06 04:04:08,2017-11-17 00:39:06
IS,MXNet Build Failure with DEV 1,Note Providing complete information in the most concise form is the best way to get help This issue template serves as the checklist for essential information to most of the technical issues If the issue is non technical feel free to present the information in what you believe is the best form Description Brief description of the problem in no more than 2 sentences Environment info Required DL AMI Ubuntu Minimum reproducible example If you are using your own code please provide a short script that reproduces the error Otherwise please provide link to the existing example Steps to reproduce Paste the commands you ran that produced the error 1 build with DEV 1 in config mk 2 What have you tried to solve it 1 build with DEV 1 passes for commit ffa6e45aad4eeca8e6d27764789cd615d132fcb9 seems this is introduced by 7152 2,,"eric-haibin-lin,ptrendx,ptrendx",2017-10-17 00:11:03,2017-11-17 00:41:12
PR,Disable long running tests,Description As discussed here in a few threads segmenting tests will go a long way in stabilizing our CI This PR is a WIP that aims to remove some of the most problematic tests All tests ignored here either crashed on a P3 instance with the release version r 0 12 of mxnet from the deep learning AMI or they took longer to run than a minute With these tests removed all tests were running in less than 2 minutes on a P3 Tests are removed with test annotations aka decorators Initially I have only used test annotations for example Once we have decorated tests with annotations we can selectively choose to run them by passing the a arg to nose as shown in the new Jenkinsfile Checklist Essentials Passed code style checking make lint not run x Changes are complete i e I finished coding on this PR WIP x All changes have test coverage removes tests x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Remove problem tests slow running or crashing,,"KellenSunderland,marcoabreu,KellenSunderland,piiswrong,marcoabreu,KellenSunderland,larroy,mseeger,larroy,KellenSunderland",2017-11-05 22:13:11,2017-11-17 03:25:17
PR,Image Operator to tensor,to tensor fix lint typo,,yzhliu,2017-11-17 06:37:16,2017-11-17 07:01:37
PR,test,,,"qingzhouzhen,szha,qingzhouzhen,szha,chinakook,szha,piiswrong,qingzhouzhen,piiswrong,qingzhouzhen,piiswrong,piiswrong,qingzhouzhen,piiswrong,qingzhouzhen,szha,qingzhouzhen,szha,szha",2017-09-20 03:02:58,2017-11-17 07:43:43
IS,Question Use LSTM for Sine Wave prediction,I was planning to learn mxnet and also try to contribute to some simple examples I posted below question on stackoverflow but unfortunately no response got So I would try here Below is the code I wrote trying to reproduce the Tensorflow example location at This code runs but the train accuracy is always Nan and validation accuracy always 1 The question is how to make it correct And since unrolled out shape has sequence length how can it match to label shape Does my FC1 net make sense Or if any dev looked here please give me some hint on how should I debug,,szha,2017-08-18 05:38:43,2017-11-17 12:26:26
IS,Support for fancy indexing,Fancy indexing at least being able to index one dimension of an NDArray with a list of indices is a vital functionality needed for a lot of applications Without it one has to loop through dimensions and elements which is prohibitively slow PR 7910 addresses this partly However syntaxes such as Another possibility would be to extend mxnet ndarray take to support several axis or at least allow to support axis other than 0,,"JeanKossaifi,JeanKossaifi,JeanKossaifi",2017-09-28 22:04:07,2017-11-17 15:04:47
PR,independent test related changes pulled from broadcast tuning branch,Description Also fixed a couple of warnings Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,cjolivier01,2017-11-14 04:10:57,2017-11-17 19:14:27
PR,Fix linker undefined reference errors,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,"cjolivier01,cjolivier01",2017-11-17 19:53:54,2017-11-17 19:57:20
IS,R Bug introduced in initializer between 0 10 1 and 0 11 1,Issue with MXNet R package first observed in 0 11 1 Compiled on Ubuntu 16 04 and 14 04 The initializers uniform normal Xavier now return all near 0s The bugs appear to come from the mx runif and mx rnorm in Random R which themselves rely on mx nd internal sample uniform and mx nd internal sample normal The usage of the random functions such as mx nd random uniform instead of the sample functions seems more appropriate for the initialization task Right now as all weights are initialized to zeros no model training is possible I will open a pull with a quick fix for the initializers,,jeremiedb,2017-10-01 18:29:04,2017-11-18 01:06:22
IS,WIP RNN with bucketing and mask R package,Since there was not an API yet for bucketing and masking in the R package I started building some functionnalities to address this Inference from iterator is also provided Further work remains to render the interface more flexible to different structures seq to seq vs seq to single used to demo I have tried to stay as close as possible to the current model R for standard feedforward training I have detailed the approach and put the code here I was wondering if there were already plans to develop bucketing utilities in the R package If not would you see this approach as a decent one it and worth buildind further on it There are some specificities with how I dealt with the iterarators it assumes that a pre processing is performed to put the data and label arrays in lists for each bucket Also noticed that shared module options seems not supported in the symbol bind operator not sure if this may result in memory management issues In any case I can turn it into an example to add to the docs CNN examples reaches 89 accuracy on IMDB sentiment in few minutes on laptop CPU,,"jeremiedb,piiswrong,thirdwing,thirdwing,thirdwing",2017-03-20 00:49:05,2017-11-18 01:08:26
IS,bug dropout backward with openmp,The backward implement of dropout op using openmp is wrong should not multiply pk 1,,sxjscience,2017-11-17 06:24:06,2017-11-18 02:02:46
PR,Fix sparse dot test failure due to launching kernel when nnr 0 and bug of square sum,Description 1 Reported by When performing mx nd sparse dot csr dns transpose a True if the output a zero rsp the CUDA kernel should not be launched Fixed the issue and added unit test case 2 Fixed a bug of square sum Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,"reminisce,eric-haibin-lin,DickJC123",2017-10-30 05:59:43,2017-11-18 06:38:44
IS,how to calculate an evaluation metric using intermediate layer output,Hi Dear mxnet users and developers I understand that there is example of how to use intermediate layer output in prediction However in training and validation is it possible to use intermediate layer output to calculate an evaluation metric Thanks a lot,,szha,2017-08-18 15:19:57,2017-11-18 12:26:27
PR,Doc updates for sparse operators,Description This PR adds many missing operators to the sparse documentation page Doc previews ndarray and symbol Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,eric-haibin-lin,2017-11-14 03:04:45,2017-11-18 21:35:27
PR,mac build fix,Description Fix build for mac Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x fix link error,,"szha,KellenSunderland",2017-11-17 06:59:07,2017-11-18 21:35:40
IS,Do we need to subtract mean pixel values when extracting cnn feautre,Hi I followed this notebook to extract image feature using pre trained CNN model I wonder do we need to subtract mean pixel values of imagenet before feature extraction Other deep learning frameworks usually have this process but I did not find it in this notebook,,"chinakook,szha",2017-08-15 04:40:36,2017-11-19 00:26:26
PR,Remove experimental warning on Gluon and add Gluon tutorials,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,"piiswrong,szha",2017-11-16 06:19:40,2017-11-19 05:16:00
PR,2bit gradient compression,Description Implements 2bit gradient compression by quantizing each value in gradient array to 2bits using user specified threshold Shows about 2x speedup on large models with components like fully connected layers and LSTM layers haibin lin Important files to review GC gc inl h gc cc KVStore local comm h KVStore dist kvstore dist h kvstore dist server h Documentation about gradient compression kvstore py Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Gradient compression class x Reduce operation in kvstore local comm h x Distributed kvstore changes at worker and server x Tests for local kvstore distributed kvstore with predefined and random data The results have been compared with expected values by implementing this logic in python x API changes for Kvstore Module and Trainer in python x Addressed comments from last PR Comments Problem When training large scale deep learning models especially with distributed training communication becomes a bottleneck for networks whose computation is not high compared to the communication Approach We can compress the gradients by considering only those elements that exceed a threshold Only these elements are encoded and sent The elements of the gradient that are near zero can safely be delayed by aggregating them in a residual array When the updated residual with gradient of next iterations exceed the threshold these values are sent Effectively these values are updated at a lower frequency On the receiver is end we decompress the data and use the decompressed weights Specifically in this PR 2bit quantization has been implemented Two bit quantization Any positive value greater than or equal to the threshold is set to one value say 11 any negative value whose absolute value is greater or equal to the threshold is set to second value say 10 and others are set to third value say 0 We need three values to represent data in this fashion and hence two bits We understand this leads to one bit going waste but that is an optimization to be done later The error in quantization is accumulated as residual and carried over to the next iterations This is added in the next iteration to the gradient before quantizing An example below with thresholds of 2 0 and 2 0 This format leads to the reduction of gradient size by 1 16th Quantization at work Format of compressed gradient Eac element represents upto 16 elements in the original array For the example above we get an element whose binary representation is Results Summary Shows about 2x speedup when models are large have fully connected components for distributed training On local training speedup is about 1 2x when there is no P2P communication 1 For MLP with 4 fully connected layers of 1500 size and one fully connected layer of 3000 size input dim model size speedup batch size on each gpu 300 50MB 1 7x 256 300 50MB 1 4x 1024 150000 2x 64 270000 900MB 2x 128 For smaller models the overhead of launching OMP threads is costing a bit to get around it if training using GPUs setting OMP NUM THREADS 1 results in gradient compression is needed 2 Shows speedup when communication is expensive The above speedup was seen on g2 8x machines which have lower network bandwidth than p2 16x machines p2 16x did not see as much speedup 3 Network types On models for imagenet input input dim 3 299 299 on g2 8x large 15 node cluster used all 4 gpus on each node network type speedup LSTM BiLSTM about 1 25 1 5x VGG11 1 8x MLP Alexnet 2x 4 Accuracy LSTM on PennTreeBank with 200dim 2 layers LSTM on Penntree bank MNIST on MLP MNIST on MLP CIFAR with resnet Cifar with resnet Accuracy starts off slow but the network converges to similar accuracy Accuracies at a few epochs epoch 101 2bit 0 80645 none 0 83572 difference 0 029 epoch153 2bit 0 841 none 0 851 difference 0 0108 CIFAR resnet with pretraining pretrained resnet Pre training without gradient compression for some time 2 epochs leads to better convergence We see that in this case we start off much closer and reaches similar accuracies earlier In general the graphs are much closer Let is look at epoch 33 Earlier without pretraining 2bit compression had an accuracy degradation of 0 154 when compared to the case without gradient compression Now when both models start with a pretrained network which did not use gradient compression it has a degradation of only 0 04 Reference although compressed representation is different,,"rahul003,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,rahul003,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,rahul003,piiswrong,rahul003,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,eric-haibin-lin,rahul003,rahul003,rahul003,rahul003,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,piiswrong,eric-haibin-lin,rahul003,rahul003,rahul003,rahul003",2017-11-15 08:22:48,2017-11-19 05:16:21
PR,Restored some copyright attribution that were accidentally removed,Description At some point we accidentally removed some copyright information when migrating to Apache This commit restores those Copyrights This commit also updates the script to ensure that Copyrights are not accidentally removed again when run Checklist Essentials x Passed code style checking make lint some linting issues also present in master x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Restored some copyright attribution that were accidentally removed Pinging for a review Worked together with on the PR To reviewer If possible please avoid squashing when merging,,KellenSunderland,2017-11-17 03:02:27,2017-11-19 05:16:48
PR,Invert environment check,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,cjolivier01,2017-11-18 16:13:56,2017-11-19 09:11:45
PR,Disable 'test operator test depthwise convolution' which fails in Python2 MKLML CPU and Python3 MKLML CPU,Description Disable 'test operator test depthwise convolution' which fails in Python2 MKLML CPU and Python3 MKLML CPU I have only found this one test to cause failure in several builds 1 2 3 Issue is being tracked here Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change,,mbaijal,2017-11-19 08:45:08,2017-11-19 20:06:52
PR,COREML Update the json getter,Description Brief description on what this PR is about Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc,,"tqchen,tqchen,piiswrong,piiswrong,tqchen,pracheer,tqchen,tqchen,pracheer",2017-11-17 21:10:47,2017-11-19 20:53:19
PR,Fixing the monitor callback of the bucketing module,Description install monitor was broken for the bucketing module as it would only register the monitor with the buckets present at the time of the call which most likely is only the module for the default bucket With this PR we keep a reference to the monitor in order to be able to install it once we create the other modules executors,,"tdomhan,mbaijal",2017-11-17 18:27:40,2017-11-19 21:02:36
PR,optimization for dot csr T dense rsp,Description Use prefix sum to compute nnr in order to allocate the row sparse output Currently dot csr T dense rsp will allocate the dense output and then cast it to row sparse but not free the unused memory I use run benchmark context lhs csr rhs default lhs trans True in mxnet benchmark python sparse dot py Please correct me if I'm wrong But is dot csr T dense rsp in master slow like this Might due to others are using my machine at the same time Performance of origin dot csr T dense rsp As a feature requested in cc haibin lin Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x unittests already exist Comments If this change is a backward incompatible change why must this change be made Intersting edge cases to note here,,"ZiyueHuang,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,eric-haibin-lin,eric-haibin-lin,ZiyueHuang,eric-haibin-lin",2017-11-10 17:34:27,2017-11-19 21:07:11
PR,replace has key by in,Description This issue is the same as 2037 2038 2668 but there still exists codes using has key function which has been removed in Python3 x Please have a check Checklist Essentials x Passed code style checking make lint All passed x Changes are complete i e I finished coding on this PR All changes have test coverage x For user facing API changes API doc string has been updated x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x replace has key function by in method Comments None,,"chihming,eric-haibin-lin",2017-10-17 16:08:23,2017-11-19 21:07:49
IS,WIP NewFeature ONNX support for MXNet,Description Use case is to import pretrained ONNX model and run inference in MXNet and similarly export MXNet model to ONNX protobuf format So the idea is MXNet will have new module serde temporary module name for serialization deserialization purpose which will support only ONNX format for now can add other formats later if needed And user should be just able to do the following to get MXNet model and parameters Any concerns or suggestions on this approach cc,,"Roshrini,tqchen,lupesko,tqchen,Roshrini,nswamy,tqchen,mli",2017-10-17 17:54:54,2017-11-19 21:44:42
PR,Revert 2bit gradient compression,Reverts apache incubator mxnet 8662 which was merged prematurely please resubmit your work in another PR and go through review after this Thanks,,szha,2017-11-19 07:01:45,2017-11-19 23:51:43
IS,Request on supporting Pooling symbol with NHWC,A new feature requested as title Now if I want to do a NHWC type pool I should transfer it to NCHW type data first It is not convenient,,,2017-08-30 07:38:36,2017-11-20 01:00:30
IS,nn block save params not working properly,When saving model using block save params filename I was not able to load checkpoint back using block collect params load filename However when I save using block collect params save filename it does work Is this expected behavior,,"Jerryzcn,szha,Jerryzcn,szha,Jerryzcn",2017-11-20 03:43:39,2017-11-20 05:43:26
PR,fix group2ctx with null reqs,Description It looks like the index into arg grad ctxes was not correct because g outputs do not include the arg grad whose grad req is null Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,eric-haibin-lin,2017-11-19 19:01:39,2017-11-20 05:55:31
PR,Remove experimental warning on Gluon and add Gluon tutorials,Description Brief description on what this PR is about Checklist Essentials Passed code style checking make lint Changes are complete i e I finished coding on this PR All changes have test coverage For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes Feature1 tests and when applicable API doc Feature2 tests and when applicable API doc Comments If this change is a backward incompatible change why must this change be made Interesting edge cases to note here,,"piiswrong,bhavinthaker",2017-11-19 06:02:41,2017-11-20 05:56:00
PR,Doc src and fix,Description Add a link for viewing source code in api doc Updated doc can be found at Checklist Essentials x Passed code style checking make lint x Changes are complete i e I finished coding on this PR x All changes have test coverage x For user facing API changes API doc string has been updated For new C functions in header files their functionalities and arguments are well documented x To my best knowledge examples are either not affected by this change or have been fixed to be compatible with this change Changes x add sphinx ext viewcode x fix links Comments Code that is generated at run time is not available for viewing e g frontend functions for operators,,szha,2017-11-19 21:19:44,2017-11-20 05:57:15
IS,Can i fix bias to 0 and gbias also 0 in BatchNorm,How can i fix bias to 0 and gbias also 0 in BatchNorm,,,2017-11-04 06:23:30,2017-11-20 06:07:35
IS,Periodic Loss Value when training with step learning rate policy,When training deep CNN a common way is to use SGD with momentum with a step learning rate policy e g learning rate set to be 0 1 0 01 0 001 at different stages of training But I encounter an unexpected phenomenon when training with this strategy under MXNet That is the periodic training loss value image The above is the training loss at a fixed learning rate 0 01 where the loss is decreasing normally image However at the second stage of training with lr 0 001 the loss goes up and down periodically and the period is exactly an epoch So I thought it might be the problem of data shuffling but it cannot explain why it does not happen in the first stage Actually I used ImageRecordIter as the DataIter and reset it after every epoch is there anything I missed or set mistakenly Actually the loss can converge but it takes too long I have suffered from this problem for a long period of time on different network and different datasets I did not have this problem when using Caffe Is this due to the implementation difference Anyone can give some suggestions,,,2017-10-09 07:39:03,2017-11-20 07:20:44
